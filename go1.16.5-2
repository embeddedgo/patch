diff --git a/api/go1embedded.txt b/api/go1embedded.txt
new file mode 100644
index 0000000000..780735995f
--- /dev/null
+++ b/api/go1embedded.txt
@@ -0,0 +1,128 @@
+pkg embedded/mmio, func MB()
+pkg embedded/mmio, method (*U16) Addr() uintptr
+pkg embedded/mmio, method (*U16) LoadBit(int) int
+pkg embedded/mmio, method (*U16) LoadBits(uint16) uint16
+pkg embedded/mmio, method (*U16) ClearBit(int)
+pkg embedded/mmio, method (*U16) ClearBits(uint16)
+pkg embedded/mmio, method (*U16) Load() uint16
+pkg embedded/mmio, method (*U16) SetBit(int)
+pkg embedded/mmio, method (*U16) SetBits(uint16)
+pkg embedded/mmio, method (*U16) Store(uint16)
+pkg embedded/mmio, method (*U16) StoreBit(int, int)
+pkg embedded/mmio, method (*U16) StoreBits(uint16, uint16)
+pkg embedded/mmio, method (*U32) Addr() uintptr
+pkg embedded/mmio, method (*U32) LoadBit(int) int
+pkg embedded/mmio, method (*U32) LoadBits(uint32) uint32
+pkg embedded/mmio, method (*U32) ClearBit(int)
+pkg embedded/mmio, method (*U32) ClearBits(uint32)
+pkg embedded/mmio, method (*U32) Load() uint32
+pkg embedded/mmio, method (*U32) SetBit(int)
+pkg embedded/mmio, method (*U32) SetBits(uint32)
+pkg embedded/mmio, method (*U32) Store(uint32)
+pkg embedded/mmio, method (*U32) StoreBit(int, int)
+pkg embedded/mmio, method (*U32) StoreBits(uint32, uint32)
+pkg embedded/mmio, method (*U64) Addr() uintptr
+pkg embedded/mmio, method (*U64) ClearBit(int)
+pkg embedded/mmio, method (*U64) ClearBits(uint64)
+pkg embedded/mmio, method (*U64) Load() uint64
+pkg embedded/mmio, method (*U64) LoadBit(int) int
+pkg embedded/mmio, method (*U64) LoadBits(uint64) uint64
+pkg embedded/mmio, method (*U64) SetBit(int)
+pkg embedded/mmio, method (*U64) SetBits(uint64)
+pkg embedded/mmio, method (*U64) Store(uint64)
+pkg embedded/mmio, method (*U64) StoreBit(int, int)
+pkg embedded/mmio, method (*U64) StoreBits(uint64, uint64)
+pkg embedded/mmio, method (*U8) Addr() uintptr
+pkg embedded/mmio, method (*U8) LoadBit(int) int
+pkg embedded/mmio, method (*U8) LoadBits(uint8) uint8
+pkg embedded/mmio, method (*U8) ClearBit(int)
+pkg embedded/mmio, method (*U8) ClearBits(uint8)
+pkg embedded/mmio, method (*U8) Load() uint8
+pkg embedded/mmio, method (*U8) SetBit(int)
+pkg embedded/mmio, method (*U8) SetBits(uint8)
+pkg embedded/mmio, method (*U8) Store(uint8)
+pkg embedded/mmio, method (*U8) StoreBit(int, int)
+pkg embedded/mmio, method (*U8) StoreBits(uint8, uint8)
+pkg embedded/mmio, method (UM16) Clear()
+pkg embedded/mmio, method (UM16) Load() uint16
+pkg embedded/mmio, method (UM16) Set()
+pkg embedded/mmio, method (UM16) Store(uint16)
+pkg embedded/mmio, method (UM32) Clear()
+pkg embedded/mmio, method (UM32) Load() uint32
+pkg embedded/mmio, method (UM32) Set()
+pkg embedded/mmio, method (UM32) Store(uint32)
+pkg embedded/mmio, method (UM64) Clear()
+pkg embedded/mmio, method (UM64) Load() uint64
+pkg embedded/mmio, method (UM64) Set()
+pkg embedded/mmio, method (UM64) Store(uint64)
+pkg embedded/mmio, method (UM8) Clear()
+pkg embedded/mmio, method (UM8) Load() uint8
+pkg embedded/mmio, method (UM8) Set()
+pkg embedded/mmio, method (UM8) Store(uint8)
+pkg embedded/mmio, type U16 struct
+pkg embedded/mmio, type U32 struct
+pkg embedded/mmio, type U64 struct
+pkg embedded/mmio, type U8 struct
+pkg embedded/mmio, type UM16 struct
+pkg embedded/mmio, type UM16 struct, Mask uint16
+pkg embedded/mmio, type UM16 struct, R *U16
+pkg embedded/mmio, type UM32 struct
+pkg embedded/mmio, type UM32 struct, Mask uint32
+pkg embedded/mmio, type UM32 struct, R *U32
+pkg embedded/mmio, type UM64 struct
+pkg embedded/mmio, type UM64 struct, Mask uint64
+pkg embedded/mmio, type UM64 struct, R *U64
+pkg embedded/mmio, type UM8 struct
+pkg embedded/mmio, type UM8 struct, Mask uint8
+pkg embedded/mmio, type UM8 struct, R *U8
+pkg embedded/rtos, const IntPrioCurrent = -1
+pkg embedded/rtos, const IntPrioCurrent ideal-int
+pkg embedded/rtos, const IntPrioHigh = 0
+pkg embedded/rtos, const IntPrioHigh ideal-int
+pkg embedded/rtos, const IntPrioHighest = 0
+pkg embedded/rtos, const IntPrioHighest ideal-int
+pkg embedded/rtos, const IntPrioLow = 0
+pkg embedded/rtos, const IntPrioLow ideal-int
+pkg embedded/rtos, const IntPrioLowest = 0
+pkg embedded/rtos, const IntPrioLowest ideal-int
+pkg embedded/rtos, const IntPrioMid = 0
+pkg embedded/rtos, const IntPrioMid ideal-int
+pkg embedded/rtos, const IntPrioSysCall = 0
+pkg embedded/rtos, const IntPrioSysCall ideal-int
+pkg embedded/rtos, const IntPrioSysTimer = 0
+pkg embedded/rtos, const IntPrioSysTimer ideal-int
+pkg embedded/rtos, func HandlerMode() bool
+pkg embedded/rtos, func Nanotime() time.Duration
+pkg embedded/rtos, func SetPrivLevel(int) (int, error)
+pkg embedded/rtos, func SetSystemTimer(func() int64, func(int64)) error
+pkg embedded/rtos, func SetSystemWriter(func(int, []uint8) int) error
+pkg embedded/rtos, method (*Error) Error() string
+pkg embedded/rtos, method (*Note) Clear()
+pkg embedded/rtos, method (*Note) Sleep(time.Duration) bool
+pkg embedded/rtos, method (*Note) Wakeup()
+pkg embedded/rtos, method (IRQ) Disable(IntCtx) error
+pkg embedded/rtos, method (IRQ) Enable(int, IntCtx) error
+pkg embedded/rtos, method (IRQ) Status(IntCtx) (bool, int, error)
+pkg embedded/rtos, type Error struct
+pkg embedded/rtos, type IRQ int
+pkg embedded/rtos, type IntCtx int
+pkg embedded/rtos, type Note struct
+pkg embedded/rtos, var ErrBadIntNumber *Error
+pkg embedded/rtos, var ErrBadIntPrio *Error
+pkg embedded/rtos, var ErrBadPrivLevel *Error
+pkg embedded/rtos, var ErrInsufPrivLevel *Error
+pkg embedded/rtos, var ErrNotSuppoted *Error
+pkg embedded/rtos, var ErrBadIntCtx *Error
+pkg embedded/rtos, var ErrUknown *Error
+pkg embedded/rtos, func Mount(FS, string) error
+pkg embedded/rtos, func Mounts() []*MountPoint
+pkg embedded/rtos, func Unmount(FS, string) error
+pkg embedded/rtos, type FS interface { Name, OpenWithFinalizer, Type, Usage }
+pkg embedded/rtos, type FS interface, Name() string
+pkg embedded/rtos, type FS interface, OpenWithFinalizer(string, int, fs.FileMode, func()) (fs.File, error)
+pkg embedded/rtos, type FS interface, Type() string
+pkg embedded/rtos, type FS interface, Usage() (int, int, int64, int64)
+pkg embedded/rtos, type MountPoint struct
+pkg embedded/rtos, type MountPoint struct, FS FS
+pkg embedded/rtos, type MountPoint struct, OpenCount int32
+pkg embedded/rtos, type MountPoint struct, Prefix string
diff --git a/src/cmd/asm/internal/arch/arch.go b/src/cmd/asm/internal/arch/arch.go
index a62e55191e..f98e0f22de 100644
--- a/src/cmd/asm/internal/arch/arch.go
+++ b/src/cmd/asm/internal/arch/arch.go
@@ -13,6 +13,7 @@ import (
 	"cmd/internal/obj/ppc64"
 	"cmd/internal/obj/riscv"
 	"cmd/internal/obj/s390x"
+	"cmd/internal/obj/thumb"
 	"cmd/internal/obj/wasm"
 	"cmd/internal/obj/x86"
 	"fmt"
@@ -76,6 +77,8 @@ func Set(GOARCH string) *Arch {
 		return archRISCV64()
 	case "s390x":
 		return archS390x()
+	case "thumb":
+		return archThumb()
 	case "wasm":
 		return archWasm()
 	}
@@ -697,6 +700,82 @@ func archS390x() *Arch {
 	}
 }
 
+func archThumb() *Arch {
+	register := make(map[string]int16)
+	// Create maps for easy lookup of instruction names etc.
+	// Note that there is no list of names as there is for x86.
+	for i := thumb.REG_R0; i <= thumb.REG_FPSCR; i++ {
+		register[obj.Rconv(i)] = int16(i)
+	}
+	// Avoid unintentionally clobbering g using R10.
+	delete(register, "R10")
+	register["g"] = thumb.REG_R10
+	for i := 0; i < 16; i++ {
+		register[fmt.Sprintf("C%d", i)] = int16(i)
+	}
+
+	// Pseudo-registers.
+	register["SB"] = RSB
+	register["FP"] = RFP
+	register["PC"] = RPC
+	register["SP"] = RSP
+	registerPrefix := map[string]bool{
+		"F": true,
+		"R": true,
+	}
+
+	// special operands for DMB/DSB/ISB instructions
+	register["MB_SY"] = thumb.REG_MB_SY
+	register["MB_ST"] = thumb.REG_MB_ST
+	register["MB_ISH"] = thumb.REG_MB_ISH
+	register["MB_ISHST"] = thumb.REG_MB_ISHST
+	register["MB_NSH"] = thumb.REG_MB_NSH
+	register["MB_NSHST"] = thumb.REG_MB_NSHST
+	register["MB_OSH"] = thumb.REG_MB_OSH
+	register["MB_OSHST"] = thumb.REG_MB_OSHST
+
+	// secial operands for IT instuction
+	register["EQ"] = thumb.REG_EQ
+	register["NE"] = thumb.REG_NE
+	register["HS"] = thumb.REG_HS
+	register["CS"] = thumb.REG_HS
+	register["LO"] = thumb.REG_LO
+	register["CC"] = thumb.REG_LO
+	register["MI"] = thumb.REG_MI
+	register["PL"] = thumb.REG_PL
+	register["VS"] = thumb.REG_VS
+	register["VC"] = thumb.REG_VC
+	register["HI"] = thumb.REG_HI
+	register["LS"] = thumb.REG_LS
+	register["GE"] = thumb.REG_GE
+	register["LT"] = thumb.REG_LT
+	register["GT"] = thumb.REG_GT
+	register["LE"] = thumb.REG_LE
+	register["AL"] = thumb.REG_AL
+
+	instructions := make(map[string]obj.As)
+	for i, s := range obj.Anames {
+		instructions[s] = obj.As(i)
+	}
+	for i, s := range thumb.Anames {
+		if obj.As(i) >= obj.A_ARCHSPECIFIC {
+			instructions[s] = obj.As(i) + obj.ABaseThumb
+		}
+	}
+	// Annoying aliases.
+	instructions["B"] = obj.AJMP
+	instructions["BL"] = obj.ACALL
+
+	return &Arch{
+		LinkArch:       &thumb.Link,
+		Instructions:   instructions,
+		Register:       register,
+		RegisterPrefix: registerPrefix,
+		RegisterNumber: thumbRegisterNumber,
+		IsJump:         jumpThumb,
+	}
+}
+
 func archWasm() *Arch {
 	instructions := make(map[string]obj.As)
 	for i, s := range obj.Anames {
diff --git a/src/cmd/asm/internal/arch/thumb.go b/src/cmd/asm/internal/arch/thumb.go
new file mode 100644
index 0000000000..22a2024c0d
--- /dev/null
+++ b/src/cmd/asm/internal/arch/thumb.go
@@ -0,0 +1,207 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// This file encapsulates some of the odd characteristics of the Thumb
+// instruction set, to minimize its interaction with the core of the
+// assembler.
+
+package arch
+
+import (
+	"strings"
+
+	"cmd/internal/obj"
+	"cmd/internal/obj/thumb"
+)
+
+var thumbLS = map[string]uint8{
+	"U":  thumb.C_UBIT,
+	"S":  thumb.C_SBIT,
+	"W":  thumb.C_WBIT,
+	"P":  thumb.C_PBIT,
+	"PW": thumb.C_WBIT | thumb.C_PBIT,
+	"WP": thumb.C_WBIT | thumb.C_PBIT,
+}
+
+var thumbSCOND = map[string]uint8{
+	"EQ":  thumb.C_SCOND_EQ,
+	"NE":  thumb.C_SCOND_NE,
+	"CS":  thumb.C_SCOND_HS,
+	"HS":  thumb.C_SCOND_HS,
+	"CC":  thumb.C_SCOND_LO,
+	"LO":  thumb.C_SCOND_LO,
+	"MI":  thumb.C_SCOND_MI,
+	"PL":  thumb.C_SCOND_PL,
+	"VS":  thumb.C_SCOND_VS,
+	"VC":  thumb.C_SCOND_VC,
+	"HI":  thumb.C_SCOND_HI,
+	"LS":  thumb.C_SCOND_LS,
+	"GE":  thumb.C_SCOND_GE,
+	"LT":  thumb.C_SCOND_LT,
+	"GT":  thumb.C_SCOND_GT,
+	"LE":  thumb.C_SCOND_LE,
+	"AL":  thumb.C_SCOND_NONE,
+	"U":   thumb.C_UBIT,
+	"S":   thumb.C_SBIT,
+	"W":   thumb.C_WBIT,
+	"P":   thumb.C_PBIT,
+	"PW":  thumb.C_WBIT | thumb.C_PBIT,
+	"WP":  thumb.C_WBIT | thumb.C_PBIT,
+	"F":   thumb.C_FBIT,
+	"IAW": thumb.C_WBIT | thumb.C_UBIT,
+	"DBW": thumb.C_WBIT | thumb.C_PBIT,
+	"IA":  thumb.C_UBIT,
+	"DB":  thumb.C_PBIT,
+}
+
+var thumbJump = map[string]bool{
+	"B":    true,
+	"BL":   true,
+	"BEQ":  true,
+	"BNE":  true,
+	"BCS":  true,
+	"BHS":  true,
+	"BCC":  true,
+	"BLO":  true,
+	"BMI":  true,
+	"BPL":  true,
+	"BVS":  true,
+	"BVC":  true,
+	"BHI":  true,
+	"BLS":  true,
+	"BGE":  true,
+	"BLT":  true,
+	"BGT":  true,
+	"BLE":  true,
+	"CBZ":  true,
+	"CBNZ": true,
+	"CALL": true,
+	"JMP":  true,
+}
+
+func jumpThumb(word string) bool {
+	return thumbJump[word]
+}
+
+// IsThumbCMP reports whether the op (as defined by an thumb.A* constant) is
+// one of the comparison instructions that require special handling.
+func IsThumbCMP(op obj.As) bool {
+	switch op {
+	case thumb.ACMN, thumb.ACMP, thumb.ATEQ, thumb.ATST, thumb.ATBB, thumb.ATBH:
+		return true
+	}
+	return false
+}
+
+// IsThumbSTREX reports whether the op (as defined by an thumb.A* constant) is
+// one of the STREX-like instructions that require special handling.
+func IsThumbSTREX(op obj.As) bool {
+	switch op {
+	case thumb.ASTREX, thumb.ASTREXB, thumb.ASTREXH:
+		return true
+	}
+	return false
+}
+
+// IsThumbBFX reports whether the op (as defined by an thumb.A* constant) is one the
+// BFX-like instructions which are in the form of "op $width, $LSB, (Reg,) Reg".
+func IsThumbBFX(op obj.As) bool {
+	switch op {
+	case thumb.ABFX, thumb.ABFXU, thumb.ABFC, thumb.ABFI:
+		return true
+	}
+	return false
+}
+
+// IsThumbFloatCmp reports whether the op is a floating comparison instruction.
+func IsThumbFloatCmp(op obj.As) bool {
+	switch op {
+	case thumb.ACMPF, thumb.ACMPD:
+		return true
+	}
+	return false
+}
+
+// IsThumbMULA reports whether the op (as defined by an thumb.A* constant) is
+// MULA, MULS, MMULA, MMULS, MULABB, MULAWB or MULAWT, the 4-operand instructions.
+func IsThumbMULA(op obj.As) bool {
+	switch op {
+	case thumb.AMULA, thumb.AMULS, thumb.AMMULA, thumb.AMMULS, thumb.AMULABB, thumb.AMULAWB, thumb.AMULAWT:
+		return true
+	}
+	return false
+}
+
+var thumbBcode = []obj.As{
+	thumb.ABEQ,
+	thumb.ABNE,
+	thumb.ABCS,
+	thumb.ABCC,
+	thumb.ABMI,
+	thumb.ABPL,
+	thumb.ABVS,
+	thumb.ABVC,
+	thumb.ABHI,
+	thumb.ABLS,
+	thumb.ABGE,
+	thumb.ABLT,
+	thumb.ABGT,
+	thumb.ABLE,
+	thumb.AB,
+	obj.ANOP,
+}
+
+// ThumbConditionCodes handles the special condition code situation for the Thumb.
+// It returns a boolean to indicate success; failure means cond was unrecognized.
+func ThumbConditionCodes(prog *obj.Prog, cond string) bool {
+	if cond == "" {
+		return true
+	}
+	bits, ok := ParseThumbCondition(cond)
+	if ok {
+		prog.Scond = bits
+	}
+	return ok
+}
+
+// ParseThumbCondition parses the conditions attached to an Thumb instruction.
+// The input is a single string consisting of period-separated condition
+// codes, such as ".P.W". An initial period is ignored.
+func ParseThumbCondition(cond string) (uint8, bool) {
+	return parseThumbCondition(cond, thumbLS, thumbSCOND)
+}
+
+func parseThumbCondition(cond string, ls, scond map[string]uint8) (uint8, bool) {
+	cond = strings.TrimPrefix(cond, ".")
+	if cond == "" {
+		return thumb.C_SCOND_NONE, true
+	}
+	names := strings.Split(cond, ".")
+	bits := uint8(0)
+	for _, name := range names {
+		if b, present := ls[name]; present {
+			bits |= b
+			continue
+		}
+		if b, present := scond[name]; present {
+			bits = (bits &^ thumb.C_SCOND) | b
+			continue
+		}
+		return 0, false
+	}
+	return bits, true
+}
+
+func thumbRegisterNumber(name string, n int16) (int16, bool) {
+	if n < 0 || 15 < n {
+		return 0, false
+	}
+	switch name {
+	case "R":
+		return thumb.REG_R0 + n, true
+	case "F":
+		return thumb.REG_F0 + n, true
+	}
+	return 0, false
+}
diff --git a/src/cmd/asm/internal/asm/asm.go b/src/cmd/asm/internal/asm/asm.go
index c4032759bb..51d68ede48 100644
--- a/src/cmd/asm/internal/asm/asm.go
+++ b/src/cmd/asm/internal/asm/asm.go
@@ -34,6 +34,12 @@ func (p *Parser) append(prog *obj.Prog, cond string, doLabel bool) {
 				return
 			}
 
+		case sys.Thumb:
+			if !arch.ThumbConditionCodes(prog, cond) {
+				p.errorf("unrecognized condition code .%q", cond)
+				return
+			}
+
 		case sys.ARM64:
 			if !arch.ARM64Suffix(prog, cond) {
 				p.errorf("unrecognized suffix .%q", cond)
@@ -575,6 +581,18 @@ func (p *Parser) asmInstruction(op obj.As, cond string, a []obj.Addr) {
 				prog.Reg = p.getRegister(prog, op, &a[1])
 				break
 			}
+		} else if p.arch.Family == sys.Thumb {
+			if arch.IsThumbCMP(op) {
+				prog.From = a[0]
+				prog.Reg = p.getRegister(prog, op, &a[1])
+				break
+			}
+			// Strange special cases.
+			if arch.IsThumbFloatCmp(op) {
+				prog.From = a[0]
+				prog.Reg = p.getRegister(prog, op, &a[1])
+				break
+			}
 		} else if p.arch.Family == sys.ARM64 && arch.IsARM64CMP(op) {
 			prog.From = a[0]
 			prog.Reg = p.getRegister(prog, op, &a[1])
@@ -617,6 +635,29 @@ func (p *Parser) asmInstruction(op obj.As, cond string, a []obj.Addr) {
 			prog.From = a[0]
 			prog.Reg = p.getRegister(prog, op, &a[1])
 			prog.To = a[2]
+		case sys.Thumb:
+			// Special cases.
+			if arch.IsThumbSTREX(op) {
+				/*
+					STREX x, (y), z
+						from=(y) reg=x to=z
+				*/
+				prog.From = a[1]
+				prog.Reg = p.getRegister(prog, op, &a[0])
+				prog.To = a[2]
+				break
+			}
+			if arch.IsThumbBFX(op) {
+				// a[0] and a[1] must be constants, a[2] must be a register
+				prog.From = a[0]
+				prog.SetFrom3(a[1])
+				prog.To = a[2]
+				break
+			}
+			// Otherwise the 2nd operand (a[1]) must be a register.
+			prog.From = a[0]
+			prog.Reg = p.getRegister(prog, op, &a[1])
+			prog.To = a[2]
 		case sys.AMD64:
 			prog.From = a[0]
 			prog.SetFrom3(a[1])
@@ -735,6 +776,29 @@ func (p *Parser) asmInstruction(op obj.As, cond string, a []obj.Addr) {
 				break
 			}
 		}
+		if p.arch.Family == sys.Thumb {
+			if arch.IsThumbBFX(op) {
+				// a[0] and a[1] must be constants, a[2] and a[3] must be registers
+				prog.From = a[0]
+				prog.SetFrom3(a[1])
+				prog.Reg = p.getRegister(prog, op, &a[2])
+				prog.To = a[3]
+				break
+			}
+			if arch.IsThumbMULA(op) {
+				// All must be registers.
+				p.getRegister(prog, op, &a[0])
+				r1 := p.getRegister(prog, op, &a[1])
+				r2 := p.getRegister(prog, op, &a[2])
+				p.getRegister(prog, op, &a[3])
+				prog.From = a[0]
+				prog.To = a[3]
+				prog.To.Type = obj.TYPE_REGREG2
+				prog.To.Offset = int64(r2)
+				prog.Reg = r1
+				break
+			}
+		}
 		if p.arch.Family == sys.AMD64 {
 			prog.From = a[0]
 			prog.SetRestArgs([]obj.Addr{a[1], a[2]})
diff --git a/src/cmd/asm/internal/asm/parse.go b/src/cmd/asm/internal/asm/parse.go
index 154cf9c7a7..d30879d53d 100644
--- a/src/cmd/asm/internal/asm/parse.go
+++ b/src/cmd/asm/internal/asm/parse.go
@@ -208,7 +208,7 @@ next:
 		for {
 			tok = p.nextToken()
 			if len(operands) == 0 && len(items) == 0 {
-				if p.arch.InFamily(sys.ARM, sys.ARM64, sys.AMD64, sys.I386) && tok == '.' {
+				if p.arch.InFamily(sys.ARM, sys.Thumb, sys.ARM64, sys.AMD64, sys.I386) && tok == '.' {
 					// Suffixes: ARM conditionals or x86 modifiers.
 					tok = p.nextToken()
 					str := p.lex.Text()
@@ -532,7 +532,7 @@ func (p *Parser) atStartOfRegister(name string) bool {
 // We have consumed the register or R prefix.
 func (p *Parser) atRegisterShift() bool {
 	// ARM only.
-	if !p.arch.InFamily(sys.ARM, sys.ARM64) {
+	if !p.arch.InFamily(sys.ARM, sys.Thumb, sys.ARM64) {
 		return false
 	}
 	// R1<<...
@@ -604,7 +604,7 @@ func (p *Parser) register(name string, prefix rune) (r1, r2 int16, scale int8, o
 		// Check the architectures match the syntax.
 		switch p.next().ScanToken {
 		case ',':
-			if !p.arch.InFamily(sys.ARM, sys.ARM64) {
+			if !p.arch.InFamily(sys.ARM, sys.Thumb, sys.ARM64) {
 				p.errorf("(register,register) not supported on this architecture")
 				return
 			}
@@ -946,7 +946,7 @@ func (p *Parser) registerIndirect(a *obj.Addr, prefix rune) {
 	a.Reg = r1
 	if r2 != 0 {
 		// TODO: Consistency in the encoding would be nice here.
-		if p.arch.InFamily(sys.ARM, sys.ARM64) {
+		if p.arch.InFamily(sys.ARM, sys.Thumb, sys.ARM64) {
 			// Special form
 			// ARM: destination register pair (R1, R2).
 			// ARM64: register pair (R1, R2) for LDP/STP.
@@ -1044,7 +1044,7 @@ func (p *Parser) registerListARM(a *obj.Addr) {
 	var bits uint16
 	var arrangement int64
 	switch p.arch.Family {
-	case sys.ARM:
+	case sys.ARM, sys.Thumb:
 		maxReg = 16
 	case sys.ARM64:
 		maxReg = 32
@@ -1092,7 +1092,7 @@ ListLoop:
 			}
 			regCnt++
 			nextReg = (nextReg + 1) % 32
-		case sys.ARM:
+		case sys.ARM, sys.Thumb:
 			// Parse the upper and lower bounds.
 			lo := p.registerNumber(tok.String())
 			hi := lo
@@ -1120,7 +1120,7 @@ ListLoop:
 	}
 	a.Type = obj.TYPE_REGLIST
 	switch p.arch.Family {
-	case sys.ARM:
+	case sys.ARM, sys.Thumb:
 		a.Offset = int64(bits)
 	case sys.ARM64:
 		offset, err := arch.ARM64RegisterListOffset(firstReg, regCnt, arrangement)
@@ -1168,7 +1168,7 @@ func (p *Parser) registerListX86(a *obj.Addr) {
 
 // register number is ARM-specific. It returns the number of the specified register.
 func (p *Parser) registerNumber(name string) uint16 {
-	if p.arch.Family == sys.ARM && name == "g" {
+	if (p.arch.Family == sys.ARM || p.arch.Family == sys.Thumb) && name == "g" {
 		return 10
 	}
 	if name[0] != 'R' {
diff --git a/src/cmd/asm/internal/asm/testdata/thumb.s b/src/cmd/asm/internal/asm/testdata/thumb.s
new file mode 100644
index 0000000000..2947e04a57
--- /dev/null
+++ b/src/cmd/asm/internal/asm/testdata/thumb.s
@@ -0,0 +1,49 @@
+#include "../../../../../runtime/textflag.h"
+
+TEXT foo(SB), DUPOK|NOSPLIT, $0
+
+// Load/store
+	MOVW  0x28(R15), R7  // 4f0a
+	MOVW  R1, (R2)(R7)   // 51d1
+	MOVW  R8, (R9)(R7)   // f849 8007
+	MOVW  R1, (R7)       // 6039
+	MOVW  g, (R7)        // f8c7 a000
+	MOVW  (R2)(R7), R1   // 59d1
+	MOVW  (R9)(R7), R8   // f859 8007
+	MOVW  (R7), R1       // 6839
+	MOVW  (R7), R10      // f8d7 a000
+
+	MOVB   (R7), R3  // f997 3000
+	MOVH   (R7), R3  // f9b7 3000
+	MOVBU  (R7), R3  // 783b
+	MOVHU  (R7), R3  // 883b
+	MOVW   (R7), R3  // 683b
+	MOVB   R3, (R7)  // 703b
+	MOVH   R3, (R7)  // 803b
+	MOVW   R3, (R7)  // 603b
+
+// FP load/store
+	MOVF  0x10(R4), F0   // ed94 0a10
+	MOVF  0x20(R4), F1   // ed94 1a20
+	MOVF  0x30(R4), F2   // ed94 2a30
+	MOVD  -0x10(R4), F0  // ed14 0b10
+	MOVD  -0x20(R4), F1  // ed14 1b20
+	MOVD  -0x30(R4), F2  // ed14 2b30
+	MOVF  F0, -0x10(R4)  // ed04 0a10
+	MOVF  F1, -0x20(R4)  // ed04 1a20
+	MOVF  F2, -0x30(R4)  // ed04 2a30
+	MOVD  F0, 0x10(R4)   // ed84 0b10
+	MOVD  F1, 0x20(R4)   // ed84 1b20
+	MOVD  F2, 0x30(R4)   // ed84 2b30
+
+// FP data processing
+	SQRTF  F3, F4  // eeb1 4ac3
+	SQRTD  F5, F6  // eeb1 6bc5
+
+// System registers
+	MOVW  R1, IAPSR  // f381 8801
+	MOVW  APSR, R3   // f3ef 8300
+
+// Branch
+	B   (R14)  // 4770
+
diff --git a/src/cmd/cgo/gcc.go b/src/cmd/cgo/gcc.go
index b5e28e3254..72927e2e2b 100644
--- a/src/cmd/cgo/gcc.go
+++ b/src/cmd/cgo/gcc.go
@@ -1569,6 +1569,8 @@ func (p *Package) gccMachine() []string {
 		return []string{"-mabi=64"}
 	case "mips", "mipsle":
 		return []string{"-mabi=32"}
+	case "thumb":
+		return []string{"-mthumb"}
 	}
 	return nil
 }
diff --git a/src/cmd/cgo/main.go b/src/cmd/cgo/main.go
index c1116e28ec..0a8f0897e7 100644
--- a/src/cmd/cgo/main.go
+++ b/src/cmd/cgo/main.go
@@ -191,6 +191,7 @@ var ptrSizeMap = map[string]int64{
 	"shbe":     4,
 	"sparc":    4,
 	"sparc64":  8,
+	"thumb":    4,
 }
 
 var intSizeMap = map[string]int64{
@@ -216,6 +217,7 @@ var intSizeMap = map[string]int64{
 	"shbe":     4,
 	"sparc":    4,
 	"sparc64":  8,
+	"thumb":    4,
 }
 
 var cPrefix string
diff --git a/src/cmd/compile/internal/gc/gsubr.go b/src/cmd/compile/internal/gc/gsubr.go
index d599a383e7..86cc0148c4 100644
--- a/src/cmd/compile/internal/gc/gsubr.go
+++ b/src/cmd/compile/internal/gc/gsubr.go
@@ -274,6 +274,9 @@ func (f *Func) initLSym(hasBody bool) {
 	if f.ReflectMethod() {
 		flag |= obj.REFLECTMETHOD
 	}
+	if f.Pragma&Interrupthandler != 0 {
+		flag |= obj.ISR
+	}
 
 	// Clumsy but important.
 	// See test/recover.go for test cases and src/reflect/value.go
diff --git a/src/cmd/compile/internal/gc/lex.go b/src/cmd/compile/internal/gc/lex.go
index 7cce371408..777bb4b3e8 100644
--- a/src/cmd/compile/internal/gc/lex.go
+++ b/src/cmd/compile/internal/gc/lex.go
@@ -47,6 +47,7 @@ const (
 	Nowritebarrier     // emit compiler error instead of write barrier
 	Nowritebarrierrec  // error on write barrier in this or recursive callees
 	Yeswritebarrierrec // cancels Nowritebarrierrec in this function and callees
+	Interrupthandler   // generate interrupt handler prologue / epilogue
 
 	// Runtime and cgo type pragmas
 	NotInHeap // values of this type must not be heap allocated
@@ -67,7 +68,8 @@ const (
 		Systemstack |
 		Nowritebarrier |
 		Nowritebarrierrec |
-		Yeswritebarrierrec
+		Yeswritebarrierrec |
+		Interrupthandler
 
 	TypePragmas = NotInHeap
 )
@@ -115,6 +117,8 @@ func pragmaFlag(verb string) PragmaFlag {
 		return UintptrEscapes
 	case "go:notinheap":
 		return NotInHeap
+	case "go:interrupthandler":
+		return Interrupthandler
 	}
 	return 0
 }
diff --git a/src/cmd/compile/internal/gc/main.go b/src/cmd/compile/internal/gc/main.go
index a6963a3d66..5636e2d326 100644
--- a/src/cmd/compile/internal/gc/main.go
+++ b/src/cmd/compile/internal/gc/main.go
@@ -726,7 +726,7 @@ func Main(archInit func(*Arch)) {
 	// checking. This must happen before transformclosure.
 	// We'll do the final check after write barriers are
 	// inserted.
-	if compiling_runtime {
+	if compiling_runtime || Ctxt.Headtype == objabi.Hnoos {
 		nowritebarrierrecCheck = newNowritebarrierrecChecker()
 	}
 
diff --git a/src/cmd/compile/internal/gc/noder.go b/src/cmd/compile/internal/gc/noder.go
index 7494c3ef6b..304aacd127 100644
--- a/src/cmd/compile/internal/gc/noder.go
+++ b/src/cmd/compile/internal/gc/noder.go
@@ -1654,6 +1654,9 @@ func (p *noder) pragma(pos syntax.Pos, blankLine bool, text string, old syntax.P
 		if !compiling_runtime && flag&runtimePragmas != 0 {
 			p.error(syntax.Error{Pos: pos, Msg: fmt.Sprintf("//%s only allowed in runtime", verb)})
 		}
+		if flag == Interrupthandler {
+			flag |= Nosplit | Nowritebarrierrec | Nowritebarrier
+		}
 		if flag == 0 && !allowedStdPragmas[verb] && compiling_std {
 			p.error(syntax.Error{Pos: pos, Msg: fmt.Sprintf("//%s is not allowed in the standard library", verb)})
 		}
diff --git a/src/cmd/compile/internal/gc/pgen.go b/src/cmd/compile/internal/gc/pgen.go
index 353f4b08c9..3ae7cd5de5 100644
--- a/src/cmd/compile/internal/gc/pgen.go
+++ b/src/cmd/compile/internal/gc/pgen.go
@@ -182,7 +182,7 @@ func (s *ssafn) AllocFrame(f *ssa.Func) {
 		} else {
 			lastHasPtr = false
 		}
-		if thearch.LinkArch.InFamily(sys.MIPS, sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.S390X) {
+		if thearch.LinkArch.InFamily(sys.MIPS, sys.MIPS64, sys.ARM, sys.ARM64, sys.PPC64, sys.S390X, sys.Thumb) {
 			s.stksize = Rnd(s.stksize, int64(Widthptr))
 		}
 		n.Xoffset = -s.stksize
diff --git a/src/cmd/compile/internal/gc/range.go b/src/cmd/compile/internal/gc/range.go
index 1b4d765d42..1ffa0f243e 100644
--- a/src/cmd/compile/internal/gc/range.go
+++ b/src/cmd/compile/internal/gc/range.go
@@ -142,7 +142,7 @@ func cheapComputableIndex(width int64) bool {
 	// but the architecture supports it.
 	case sys.PPC64, sys.S390X:
 		return width == 1
-	case sys.AMD64, sys.I386, sys.ARM64, sys.ARM:
+	case sys.AMD64, sys.I386, sys.ARM64, sys.ARM, sys.Thumb:
 		switch width {
 		case 1, 2, 4, 8:
 			return true
diff --git a/src/cmd/compile/internal/gc/scope_test.go b/src/cmd/compile/internal/gc/scope_test.go
index b0e038d27f..6c6703c7a3 100644
--- a/src/cmd/compile/internal/gc/scope_test.go
+++ b/src/cmd/compile/internal/gc/scope_test.go
@@ -441,7 +441,11 @@ func entryToVar(e *dwarf.Entry, kind string, typ dwarf.Type) variable {
 func (scope *lexblock) markLines(pcln objfile.Liner, lines map[line][]*lexblock) {
 	for _, r := range scope.ranges {
 		for pc := r[0]; pc < r[1]; pc++ {
-			file, lineno, _ := pcln.PCToLine(pc)
+			pc1 := pc
+			if runtime.GOARCH == "thumb" {
+				pc1++
+			}
+			file, lineno, _ := pcln.PCToLine(pc1)
 			l := line{file, lineno}
 			if len(lines[l]) == 0 || lines[l][len(lines[l])-1] != scope {
 				lines[l] = append(lines[l], scope)
diff --git a/src/cmd/compile/internal/gc/ssa.go b/src/cmd/compile/internal/gc/ssa.go
index 5b74754b53..3327b73436 100644
--- a/src/cmd/compile/internal/gc/ssa.go
+++ b/src/cmd/compile/internal/gc/ssa.go
@@ -3413,22 +3413,22 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz32, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb)
 	addF("runtime/internal/sys", "Ctz64",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz64, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb)
 	addF("runtime/internal/sys", "Bswap32",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBswap32, types.Types[TUINT32], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.Thumb)
 	addF("runtime/internal/sys", "Bswap64",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBswap64, types.Types[TUINT64], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.Thumb)
 
 	/******** runtime/internal/atomic ********/
 	addF("runtime/internal/atomic", "Load",
@@ -3710,7 +3710,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpSqrt, types.Types[TFLOAT64], args[0])
 		},
-		sys.I386, sys.AMD64, sys.ARM, sys.ARM64, sys.MIPS, sys.MIPS64, sys.PPC64, sys.RISCV64, sys.S390X, sys.Wasm)
+		sys.I386, sys.AMD64, sys.ARM, sys.ARM64, sys.MIPS, sys.MIPS64, sys.PPC64, sys.RISCV64, sys.S390X, sys.Thumb, sys.Wasm)
 	addF("math", "Trunc",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpTrunc, types.Types[TFLOAT64], args[0])
@@ -3740,7 +3740,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpAbs, types.Types[TFLOAT64], args[0])
 		},
-		sys.ARM64, sys.ARM, sys.PPC64, sys.Wasm)
+		sys.ARM64, sys.ARM, sys.PPC64, sys.Thumb, sys.Wasm)
 	addF("math", "Copysign",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue2(ssa.OpCopysign, types.Types[TFLOAT64], args[0], args[1])
@@ -3815,7 +3815,7 @@ func init() {
 			s.startBlock(bEnd)
 			return s.variable(n, types.Types[TFLOAT64])
 		},
-		sys.ARM)
+		sys.ARM, sys.Thumb)
 
 	makeRoundAMD64 := func(op ssa.Op) func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 		return func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
@@ -3863,12 +3863,12 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz64, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "TrailingZeros32",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz32, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "TrailingZeros16",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			x := s.newValue1(ssa.OpZeroExt16to32, types.Types[TUINT32], args[0])
@@ -3881,7 +3881,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz16, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.I386, sys.ARM, sys.ARM64, sys.Wasm)
+		sys.AMD64, sys.I386, sys.ARM, sys.ARM64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "TrailingZeros16",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			x := s.newValue1(ssa.OpZeroExt16to64, types.Types[TUINT64], args[0])
@@ -3902,7 +3902,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz8, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM, sys.ARM64, sys.Wasm)
+		sys.AMD64, sys.ARM, sys.ARM64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "TrailingZeros8",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			x := s.newValue1(ssa.OpZeroExt8to64, types.Types[TUINT64], args[0])
@@ -3919,7 +3919,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "Len32",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBitLen32, types.Types[TINT], args[0])
@@ -3933,7 +3933,7 @@ func init() {
 			x := s.newValue1(ssa.OpZeroExt32to64, types.Types[TUINT64], args[0])
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], x)
 		},
-		sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "Len16",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			if s.config.PtrSize == 4 {
@@ -3943,7 +3943,7 @@ func init() {
 			x := s.newValue1(ssa.OpZeroExt16to64, types.Types[TUINT64], args[0])
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], x)
 		},
-		sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "Len16",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBitLen16, types.Types[TINT], args[0])
@@ -3958,7 +3958,7 @@ func init() {
 			x := s.newValue1(ssa.OpZeroExt8to64, types.Types[TUINT64], args[0])
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], x)
 		},
-		sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "Len8",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBitLen8, types.Types[TINT], args[0])
@@ -3971,7 +3971,7 @@ func init() {
 			}
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Thumb, sys.Wasm)
 	// LeadingZeros is handled because it trivially calls Len.
 	addF("math/bits", "Reverse64",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
@@ -4015,7 +4015,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue2(ssa.OpRotateLeft32, types.Types[TUINT32], args[0], args[1])
 		},
-		sys.AMD64, sys.ARM, sys.ARM64, sys.S390X, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM, sys.ARM64, sys.S390X, sys.PPC64, sys.Thumb, sys.Wasm)
 	addF("math/bits", "RotateLeft64",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue2(ssa.OpRotateLeft64, types.Types[TUINT64], args[0], args[1])
@@ -4170,6 +4170,61 @@ func init() {
 			return s.newValue2(ssa.OpMul64uhilo, types.NewTuple(types.Types[TUINT64], types.Types[TUINT64]), args[0], args[1])
 		},
 		sys.ArchAMD64, sys.ArchARM64, sys.ArchPPC64LE, sys.ArchPPC64, sys.ArchS390X)
+
+	/******** embedded/mmio ********/
+	add("embedded/mmio", "load32",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			v := s.newValue2(ssa.OpMMIOLoad32, types.NewTuple(types.Types[TUINT32], types.TypeMem), args[0], s.mem())
+			s.vars[&memVar] = s.newValue1(ssa.OpSelect1, types.TypeMem, v)
+			return s.newValue1(ssa.OpSelect0, types.Types[TUINT32], v)
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "load16",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			v := s.newValue2(ssa.OpMMIOLoad16, types.NewTuple(types.Types[TUINT16], types.TypeMem), args[0], s.mem())
+			s.vars[&memVar] = s.newValue1(ssa.OpSelect1, types.TypeMem, v)
+			return s.newValue1(ssa.OpSelect0, types.Types[TUINT16], v)
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "load8",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			v := s.newValue2(ssa.OpMMIOLoad8, types.NewTuple(types.Types[TUINT8], types.TypeMem), args[0], s.mem())
+			s.vars[&memVar] = s.newValue1(ssa.OpSelect1, types.TypeMem, v)
+			return s.newValue1(ssa.OpSelect0, types.Types[TUINT8], v)
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "store32",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue3(ssa.OpMMIOStore32, types.TypeMem, args[0], args[1], s.mem())
+			return nil
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "store16",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue3(ssa.OpMMIOStore16, types.TypeMem, args[0], args[1], s.mem())
+			return nil
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "store8",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue3(ssa.OpMMIOStore8, types.TypeMem, args[0], args[1], s.mem())
+			return nil
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "MB",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue1(ssa.OpMMIOMB, types.TypeMem, s.mem())
+			return nil
+		},
+		sys.ArchThumb)
+
+	/******** embedded/rtos ********/
+	add("embedded/rtos", "publicationBarrier",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue1(ssa.OpPublicationBarrier, types.TypeMem, s.mem())
+			return nil
+		},
+		sys.ArchThumb)
 }
 
 // findIntrinsic returns a function which builds the SSA equivalent of the
@@ -6916,7 +6971,7 @@ func (s *SSAGenState) Call(v *ssa.Value) *obj.Prog {
 		switch thearch.LinkArch.Family {
 		case sys.AMD64, sys.I386, sys.PPC64, sys.RISCV64, sys.S390X, sys.Wasm:
 			p.To.Type = obj.TYPE_REG
-		case sys.ARM, sys.ARM64, sys.MIPS, sys.MIPS64:
+		case sys.ARM, sys.ARM64, sys.MIPS, sys.MIPS64, sys.Thumb:
 			p.To.Type = obj.TYPE_MEM
 		default:
 			Fatalf("unknown indirect call family")
diff --git a/src/cmd/compile/internal/gc/walk.go b/src/cmd/compile/internal/gc/walk.go
index 02a7269ff8..0d43448c48 100644
--- a/src/cmd/compile/internal/gc/walk.go
+++ b/src/cmd/compile/internal/gc/walk.go
@@ -1642,7 +1642,7 @@ func rtconvfn(src, dst *types.Type) (param, result types.EType) {
 	}
 
 	switch thearch.LinkArch.Family {
-	case sys.ARM, sys.MIPS:
+	case sys.ARM, sys.MIPS, sys.Thumb:
 		if src.IsFloat() {
 			switch dst.Etype {
 			case TINT64, TUINT64:
diff --git a/src/cmd/compile/internal/riscv64/ssa.go b/src/cmd/compile/internal/riscv64/ssa.go
index 0beb5b4bd1..1b8a9de80c 100644
--- a/src/cmd/compile/internal/riscv64/ssa.go
+++ b/src/cmd/compile/internal/riscv64/ssa.go
@@ -10,6 +10,7 @@ import (
 	"cmd/compile/internal/types"
 	"cmd/internal/obj"
 	"cmd/internal/obj/riscv"
+	"cmd/internal/objabi"
 )
 
 // ssaRegToReg maps ssa register numbers to obj register numbers.
@@ -578,6 +579,10 @@ func ssaGenValue(s *gc.SSAGenState, v *ssa.Value) {
 		gc.Patch(p5, p)
 
 	case ssa.OpRISCV64LoweredNilCheck:
+		if objabi.GOOS == "noos" {
+			// BUG: avoid nil check because of MMIO
+			break
+		}
 		// Issue a load which will fault if arg is nil.
 		// TODO: optimizations. See arm and amd64 LoweredNilCheck.
 		p := s.Prog(riscv.AMOVB)
diff --git a/src/cmd/compile/internal/ssa/block.go b/src/cmd/compile/internal/ssa/block.go
index 519ac214ca..077afa42f6 100644
--- a/src/cmd/compile/internal/ssa/block.go
+++ b/src/cmd/compile/internal/ssa/block.go
@@ -111,7 +111,7 @@ func (e Edge) String() string {
 //    Plain                []            [next]
 //       If   [boolean Value]      [then, else]
 //    Defer             [mem]  [nopanic, panic]  (control opcode should be OpStaticCall to runtime.deferproc)
-type BlockKind int8
+type BlockKind int16
 
 // short form print
 func (b *Block) String() string {
diff --git a/src/cmd/compile/internal/ssa/config.go b/src/cmd/compile/internal/ssa/config.go
index 0fe0337ddf..352ac839bd 100644
--- a/src/cmd/compile/internal/ssa/config.go
+++ b/src/cmd/compile/internal/ssa/config.go
@@ -245,6 +245,17 @@ func NewConfig(arch string, types Types, ctxt *obj.Link, optimize bool) *Config
 		c.FPReg = framepointerRegARM
 		c.LinkReg = linkRegARM
 		c.hasGReg = true
+	case "thumb":
+		c.PtrSize = 4
+		c.RegSize = 4
+		c.lowerBlock = rewriteBlockThumb
+		c.lowerValue = rewriteValueThumb
+		c.registers = registersThumb[:]
+		c.gpRegMask = gpRegMaskThumb
+		c.fpRegMask = fpRegMaskThumb
+		c.FPReg = framepointerRegThumb
+		c.LinkReg = linkRegThumb
+		c.hasGReg = true
 	case "arm64":
 		c.PtrSize = 8
 		c.RegSize = 8
diff --git a/src/cmd/compile/internal/ssa/gen/Thumb.rules b/src/cmd/compile/internal/ssa/gen/Thumb.rules
new file mode 100644
index 0000000000..44a8f2d747
--- /dev/null
+++ b/src/cmd/compile/internal/ssa/gen/Thumb.rules
@@ -0,0 +1,1333 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+(Add(Ptr|32|16|8) ...) => (ADD ...)
+(Add(32|64)F ...) => (ADD(F|D) ...)
+(Add32carry ...) => (ADDS ...)
+(Add32withcarry ...) => (ADC ...)
+
+(Sub(Ptr|32|16|8) ...) => (SUB ...)
+(Sub(32|64)F ...) => (SUB(F|D) ...)
+(Sub32carry ...) => (SUBS ...)
+(Sub32withcarry ...) => (SBC ...)
+
+(Mul(32|16|8) ...) => (MUL ...)
+(Mul(32|64)F ...) => (MUL(F|D) ...)
+(Hmul(32|32u) ...) => (HMU(L|LU) ...)
+(Mul32uhilo ...) => (MULLU ...)
+
+(Div32 [c] x y) => (DIV x y)
+(Div32u ...) => (DIVU ...)
+(Div16 [c] x y) => (DIV (SignExt16to32 x) (SignExt16to32 y))
+(Div16u x y) => (DIVU (ZeroExt16to32 x) (ZeroExt16to32 y))
+(Div8 x y) => (DIV (SignExt8to32 x) (SignExt8to32 y))
+(Div8u x y) => (DIVU (ZeroExt8to32 x) (ZeroExt8to32 y))
+(Div(32|64)F ...) => (DIV(F|D) ...)
+
+(Mod32 x y) => (SUB x (MUL <y.Type> y (DIV <x.Type> x y)))
+(Mod32u x y) => (SUB x (MUL <y.Type> y (DIVU <x.Type> x y)))
+(Mod16 x y) => (Mod32 (SignExt16to32 x) (SignExt16to32 y))
+(Mod16u x y) => (Mod32u (ZeroExt16to32 x) (ZeroExt16to32 y))
+(Mod8 x y) => (Mod32 (SignExt8to32 x) (SignExt8to32 y))
+(Mod8u x y) => (Mod32u (ZeroExt8to32 x) (ZeroExt8to32 y))
+
+// (x + y) / 2 with x>=y => (x - y) / 2 + y
+(Avg32u <t> x y) => (ADD (SRLconst <t> (SUB <t> x y) [1]) y)
+
+(And(32|16|8) ...) => (AND ...)
+(Or(32|16|8) ...) => (OR ...)
+(Xor(32|16|8) ...) => (XOR ...)
+
+// unary ops
+(Neg(32|16|8) x) => (RSBconst [0] x)
+(Neg(32|64)F ...) => (NEG(F|D) ...)
+
+(Com(32|16|8) ...) => (MVN ...)
+
+(Sqrt ...) => (SQRTD ...)
+(Abs ...) => (ABSD ...)
+
+(Ctz32NonZero ...) => (Ctz32 ...)
+(Ctz16NonZero ...) => (Ctz32 ...)
+(Ctz8NonZero ...) => (Ctz32 ...)
+
+(Ctz32 <t> x) => (CLZ <t> (RBIT <t> x))
+(Ctz16 <t> x) => (CLZ <t> (RBIT <typ.UInt32> (ORconst <typ.UInt32> [0x10000] x)))
+(Ctz8 <t> x) => (CLZ <t> (RBIT <typ.UInt32> (ORconst <typ.UInt32> [0x100] x)))
+
+// bit length
+(BitLen32 <t> x) => (RSBconst [32] (CLZ <t> x))
+
+(Bswap32 ...) => (REV ...)
+
+// boolean ops -- booleans are represented with 0=false, 1=true
+(AndB ...) => (AND ...)
+(OrB ...) => (OR ...)
+(EqB x y) => (XORconst [1] (XOR <typ.Bool> x y))
+(NeqB ...) => (XOR ...)
+(Not x) => (XORconst [1] x)
+
+// shifts
+// hardware instruction uses only the low byte of the shift
+// we compare to 256 to ensure Go semantics for large shifts
+(Lsh32x32 x y) => (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+(Lsh32x16 x y) => (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Lsh32x8  x y) => (SLL x (ZeroExt8to32 y))
+
+(Lsh16x32 x y) => (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+(Lsh16x16 x y) => (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Lsh16x8  x y) => (SLL x (ZeroExt8to32 y))
+
+(Lsh8x32 x y) => (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+(Lsh8x16 x y) => (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Lsh8x8  x y) => (SLL x (ZeroExt8to32 y))
+
+(Rsh32Ux32 x y) => (CMOVWHSconst (SRL <x.Type> x y) (CMPconst [256] y) [0])
+(Rsh32Ux16 x y) => (CMOVWHSconst (SRL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Rsh32Ux8  x y) => (SRL x (ZeroExt8to32 y))
+
+(Rsh16Ux32 x y) => (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) y) (CMPconst [256] y) [0])
+(Rsh16Ux16 x y) => (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Rsh16Ux8  x y) => (SRL (ZeroExt16to32 x) (ZeroExt8to32 y))
+
+(Rsh8Ux32 x y) => (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) y) (CMPconst [256] y) [0])
+(Rsh8Ux16 x y) => (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Rsh8Ux8  x y) => (SRL (ZeroExt8to32 x) (ZeroExt8to32 y))
+
+(Rsh32x32 x y) => (SRAcond x y (CMPconst [256] y))
+(Rsh32x16 x y) => (SRAcond x (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+(Rsh32x8  x y) => (SRA x (ZeroExt8to32 y))
+
+(Rsh16x32 x y) => (SRAcond (SignExt16to32 x) y (CMPconst [256] y))
+(Rsh16x16 x y) => (SRAcond (SignExt16to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+(Rsh16x8  x y) => (SRA (SignExt16to32 x) (ZeroExt8to32 y))
+
+(Rsh8x32 x y) => (SRAcond (SignExt8to32 x) y (CMPconst [256] y))
+(Rsh8x16 x y) => (SRAcond (SignExt8to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+(Rsh8x8  x y) => (SRA (SignExt8to32 x) (ZeroExt8to32 y))
+
+// constant shifts
+// generic opt rewrites all constant shifts to shift by Const64
+(Lsh32x64 x (Const64 [c])) && uint64(c) < 32 => (SLLconst x [int32(c)])
+(Rsh32x64 x (Const64 [c])) && uint64(c) < 32 => (SRAconst x [int32(c)])
+(Rsh32Ux64 x (Const64 [c])) && uint64(c) < 32 => (SRLconst x [int32(c)])
+(Lsh16x64 x (Const64 [c])) && uint64(c) < 16 => (SLLconst x [int32(c)])
+(Rsh16x64 x (Const64 [c])) && uint64(c) < 16 => (SRAconst (SLLconst <typ.UInt32> x [16]) [int32(c+16)])
+(Rsh16Ux64 x (Const64 [c])) && uint64(c) < 16 => (SRLconst (SLLconst <typ.UInt32> x [16]) [int32(c+16)])
+(Lsh8x64 x (Const64 [c])) && uint64(c) < 8 => (SLLconst x [int32(c)])
+(Rsh8x64 x (Const64 [c])) && uint64(c) < 8 => (SRAconst (SLLconst <typ.UInt32> x [24]) [int32(c+24)])
+(Rsh8Ux64 x (Const64 [c])) && uint64(c) < 8 => (SRLconst (SLLconst <typ.UInt32> x [24]) [int32(c+24)])
+
+// large constant shifts
+(Lsh32x64 _ (Const64 [c])) && uint64(c) >= 32 => (Const32 [0])
+(Rsh32Ux64 _ (Const64 [c])) && uint64(c) >= 32 => (Const32 [0])
+(Lsh16x64 _ (Const64 [c])) && uint64(c) >= 16 => (Const16 [0])
+(Rsh16Ux64 _ (Const64 [c])) && uint64(c) >= 16 => (Const16 [0])
+(Lsh8x64 _ (Const64 [c])) && uint64(c) >= 8 => (Const8 [0])
+(Rsh8Ux64 _ (Const64 [c])) && uint64(c) >= 8 => (Const8 [0])
+
+// large constant signed right shift, we leave the sign bit
+(Rsh32x64 x (Const64 [c])) && uint64(c) >= 32 => (SRAconst x [31])
+(Rsh16x64 x (Const64 [c])) && uint64(c) >= 16 => (SRAconst (SLLconst <typ.UInt32> x [16]) [31])
+(Rsh8x64 x (Const64 [c])) && uint64(c) >= 8 => (SRAconst (SLLconst <typ.UInt32> x [24]) [31])
+
+// constants
+(Const(8|16|32) [val]) => (MOVWconst [int32(val)])
+(Const(32|64)F [val]) => (MOV(F|D)const [float64(val)])
+(ConstNil) => (MOVWconst [0])
+(ConstBool [b]) => (MOVWconst [b2i32(b)])
+
+// truncations
+// Because we ignore high parts of registers, truncates are just copies.
+(Trunc16to8 ...) => (Copy ...)
+(Trunc32to8 ...) => (Copy ...)
+(Trunc32to16 ...) => (Copy ...)
+
+// Zero-/Sign-extensions
+(ZeroExt8to16 ...) => (MOVBUreg ...)
+(ZeroExt8to32 ...) => (MOVBUreg ...)
+(ZeroExt16to32 ...) => (MOVHUreg ...)
+
+(SignExt8to16 ...) => (MOVBreg ...)
+(SignExt8to32 ...) => (MOVBreg ...)
+(SignExt16to32 ...) => (MOVHreg ...)
+
+(Signmask x) => (SRAconst x [31])
+(Zeromask x) => (SRAconst (RSBshiftRL <typ.Int32> x x [1]) [31]) // sign bit of uint32(x)>>1 - x
+(Slicemask <t> x) => (SRAconst (RSBconst <t> [0] x) [31])
+
+// float <=> int conversion
+(Cvt32to32F ...) => (MOVWF ...)
+(Cvt32to64F ...) => (MOVWD ...)
+(Cvt32Uto32F ...) => (MOVWUF ...)
+(Cvt32Uto64F ...) => (MOVWUD ...)
+(Cvt32Fto32 ...) => (MOVFW ...)
+(Cvt64Fto32 ...) => (MOVDW ...)
+(Cvt32Fto32U ...) => (MOVFWU ...)
+(Cvt64Fto32U ...) => (MOVDWU ...)
+(Cvt32Fto64F ...) => (MOVFD ...)
+(Cvt64Fto32F ...) => (MOVDF ...)
+
+(Round(32|64)F ...) => (Copy ...)
+
+(CvtBoolToUint8 ...) => (Copy ...)
+
+// fused-multiply-add
+(FMA x y z) => (FMULAD z x y)
+
+// comparisons
+(Eq8 x y)  => (Equal (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Eq16 x y) => (Equal (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Eq32 x y) => (Equal (CMP x y))
+(EqPtr x y) => (Equal (CMP x y))
+(Eq(32|64)F x y) => (Equal (CMP(F|D) x y))
+
+(Neq8 x y)  => (NotEqual (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Neq16 x y) => (NotEqual (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Neq32 x y) => (NotEqual (CMP x y))
+(NeqPtr x y) => (NotEqual (CMP x y))
+(Neq(32|64)F x y) => (NotEqual (CMP(F|D) x y))
+
+(Less8 x y)  => (LessThan (CMP (SignExt8to32 x) (SignExt8to32 y)))
+(Less16 x y) => (LessThan (CMP (SignExt16to32 x) (SignExt16to32 y)))
+(Less32 x y) => (LessThan (CMP x y))
+(Less(32|64)F x y) => (GreaterThan (CMP(F|D) y x)) // reverse operands to work around NaN
+
+(Less8U x y)  => (LessThanU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Less16U x y) => (LessThanU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Less32U x y) => (LessThanU (CMP x y))
+
+(Leq8 x y)  => (LessEqual (CMP (SignExt8to32 x) (SignExt8to32 y)))
+(Leq16 x y) => (LessEqual (CMP (SignExt16to32 x) (SignExt16to32 y)))
+(Leq32 x y) => (LessEqual (CMP x y))
+(Leq(32|64)F x y) => (GreaterEqual (CMP(F|D) y x)) // reverse operands to work around NaN
+
+(Leq8U x y)  => (LessEqualU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Leq16U x y) => (LessEqualU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Leq32U x y) => (LessEqualU (CMP x y))
+
+(OffPtr [off] ptr:(SP)) => (MOVWaddr [int32(off)] ptr)
+(OffPtr [off] ptr) => (ADDconst [int32(off)] ptr)
+
+(Addr {sym} base) => (MOVWaddr {sym} base)
+(LocalAddr {sym} base _) => (MOVWaddr {sym} base)
+
+// loads
+(Load <t> ptr mem) && t.IsBoolean() => (MOVBUload ptr mem)
+(Load <t> ptr mem) && (is8BitInt(t) && isSigned(t)) => (MOVBload ptr mem)
+(Load <t> ptr mem) && (is8BitInt(t) && !isSigned(t)) => (MOVBUload ptr mem)
+(Load <t> ptr mem) && (is16BitInt(t) && isSigned(t)) => (MOVHload ptr mem)
+(Load <t> ptr mem) && (is16BitInt(t) && !isSigned(t)) => (MOVHUload ptr mem)
+(Load <t> ptr mem) && (is32BitInt(t) || isPtr(t)) => (MOVWload ptr mem)
+(Load <t> ptr mem) && is32BitFloat(t) => (MOVFload ptr mem)
+(Load <t> ptr mem) && is64BitFloat(t) => (MOVDload ptr mem)
+
+// stores
+(Store {t} ptr val mem) && t.Size() == 1 => (MOVBstore ptr val mem)
+(Store {t} ptr val mem) && t.Size() == 2 => (MOVHstore ptr val mem)
+(Store {t} ptr val mem) && t.Size() == 4 && !is32BitFloat(val.Type) => (MOVWstore ptr val mem)
+(Store {t} ptr val mem) && t.Size() == 4 && is32BitFloat(val.Type) => (MOVFstore ptr val mem)
+(Store {t} ptr val mem) && t.Size() == 8 && is64BitFloat(val.Type) => (MOVDstore ptr val mem)
+
+// mmio intrinsics
+(MMIOLoad8 ptr mem) => (LoadOnce8 [0] ptr mem)
+(MMIOLoad16 ptr mem) => (LoadOnce16 [0] ptr mem)
+(MMIOLoad32 ptr mem) => (LoadOnce32 [0] ptr mem)
+(MMIOStore8 ptr val mem) => (StoreOnce8 [0] ptr val mem)
+(MMIOStore16 ptr val mem) => (StoreOnce16 [0] ptr val mem)
+(MMIOStore32 ptr val mem) => (StoreOnce32 [0] ptr val mem)
+(MMIOMB ...) => (DSB ...) // use DSB instead of DMB because of memory mapped CPU controll registers and to ensure memory update before the sleep (before WFI, WFE)
+
+// publication barrier
+(PublicationBarrier ...) => (DMB_ST ...)
+
+// zero instructions
+(Zero [0] _ mem) => mem
+(Zero [1] ptr mem) => (MOVBstore ptr (MOVWconst [0]) mem)
+(Zero [2] {t} ptr mem) && t.Alignment()%2 == 0 =>
+	(MOVHstore ptr (MOVWconst [0]) mem)
+(Zero [2] ptr mem) =>
+	(MOVBstore [1] ptr (MOVWconst [0])
+		(MOVBstore [0] ptr (MOVWconst [0]) mem))
+(Zero [4] {t} ptr mem) && t.Alignment()%4 == 0 =>
+	(MOVWstore ptr (MOVWconst [0]) mem)
+(Zero [4] {t} ptr mem) && t.Alignment()%2 == 0 =>
+	(MOVHstore [2] ptr (MOVWconst [0])
+		(MOVHstore [0] ptr (MOVWconst [0]) mem))
+(Zero [4] ptr mem) =>
+	(MOVBstore [3] ptr (MOVWconst [0])
+		(MOVBstore [2] ptr (MOVWconst [0])
+			(MOVBstore [1] ptr (MOVWconst [0])
+				(MOVBstore [0] ptr (MOVWconst [0]) mem))))
+
+(Zero [3] ptr mem) =>
+	(MOVBstore [2] ptr (MOVWconst [0])
+		(MOVBstore [1] ptr (MOVWconst [0])
+			(MOVBstore [0] ptr (MOVWconst [0]) mem)))
+
+// Medium zeroing uses a duff device
+// 4 and 128 are magic constants, see runtime/mkduff.go
+(Zero [s] {t} ptr mem)
+	&& s%4 == 0 && s > 4 && s <= 512
+	&& t.Alignment()%4 == 0 && !config.noDuffDevice =>
+	(DUFFZERO [4 * (128 - s/4)] ptr (MOVWconst [0]) mem)
+
+// Large zeroing uses a loop
+(Zero [s] {t} ptr mem)
+	&& (s > 512 || config.noDuffDevice) || t.Alignment()%4 != 0 =>
+	(LoweredZero [t.Alignment()]
+		ptr
+		(ADDconst <ptr.Type> ptr [int32(s-moveSize(t.Alignment(), config))])
+		(MOVWconst [0])
+		mem)
+
+// moves
+(Move [0] _ _ mem) => mem
+(Move [1] dst src mem) => (MOVBstore dst (MOVBUload src mem) mem)
+(Move [2] {t} dst src mem) && t.Alignment()%2 == 0 =>
+	(MOVHstore dst (MOVHUload src mem) mem)
+(Move [2] dst src mem) =>
+	(MOVBstore [1] dst (MOVBUload [1] src mem)
+		(MOVBstore dst (MOVBUload src mem) mem))
+(Move [4] {t} dst src mem) && t.Alignment()%4 == 0 =>
+	(MOVWstore dst (MOVWload src mem) mem)
+(Move [4] {t} dst src mem) && t.Alignment()%2 == 0 =>
+	(MOVHstore [2] dst (MOVHUload [2] src mem)
+		(MOVHstore dst (MOVHUload src mem) mem))
+(Move [4] dst src mem) =>
+	(MOVBstore [3] dst (MOVBUload [3] src mem)
+		(MOVBstore [2] dst (MOVBUload [2] src mem)
+			(MOVBstore [1] dst (MOVBUload [1] src mem)
+				(MOVBstore dst (MOVBUload src mem) mem))))
+
+(Move [3] dst src mem) =>
+	(MOVBstore [2] dst (MOVBUload [2] src mem)
+		(MOVBstore [1] dst (MOVBUload [1] src mem)
+			(MOVBstore dst (MOVBUload src mem) mem)))
+
+// Medium move uses a duff device
+// 8 and 128 are magic constants, see runtime/mkduff.go
+(Move [s] {t} dst src mem)
+	&& s%4 == 0 && s > 4 && s <= 512
+	&& t.Alignment()%4 == 0 && !config.noDuffDevice && logLargeCopy(v, s) =>
+	(DUFFCOPY [8 * (128 - s/4)] dst src mem)
+
+// Large move uses a loop
+(Move [s] {t} dst src mem)
+	&& ((s > 512 || config.noDuffDevice) || t.Alignment()%4 != 0) && logLargeCopy(v, s) =>
+	(LoweredMove [t.Alignment()]
+		dst
+		src
+		(ADDconst <src.Type> src [int32(s-moveSize(t.Alignment(), config))])
+		mem)
+
+// calls
+(StaticCall ...) => (CALLstatic ...)
+(ClosureCall ...) => (CALLclosure ...)
+(InterCall ...) => (CALLinter ...)
+
+// checks
+(NilCheck ...) => (LoweredNilCheck ...)
+(IsNonNil ptr) => (NotEqual (CMPconst [0] ptr))
+(IsInBounds idx len) => (LessThanU (CMP idx len))
+(IsSliceInBounds idx len) => (LessEqualU (CMP idx len))
+
+// pseudo-ops
+(GetClosurePtr ...) => (LoweredGetClosurePtr ...)
+(GetCallerSP ...) => (LoweredGetCallerSP ...)
+(GetCallerPC ...) => (LoweredGetCallerPC ...)
+
+// Absorb pseudo-ops into blocks.
+(If (Equal cc) yes no) => (EQ cc yes no)
+(If (NotEqual cc) yes no) => (NE cc yes no)
+(If (LessThan cc) yes no) => (LT cc yes no)
+(If (LessThanU cc) yes no) => (ULT cc yes no)
+(If (LessEqual cc) yes no) => (LE cc yes no)
+(If (LessEqualU cc) yes no) => (ULE cc yes no)
+(If (GreaterThan cc) yes no) => (GT cc yes no)
+(If (GreaterThanU cc) yes no) => (UGT cc yes no)
+(If (GreaterEqual cc) yes no) => (GE cc yes no)
+(If (GreaterEqualU cc) yes no) => (UGE cc yes no)
+
+(If cond yes no) => (NE (CMPconst [0] cond) yes no)
+
+// Absorb boolean tests into block
+(NE (CMPconst [0] (Equal cc)) yes no) => (EQ cc yes no)
+(NE (CMPconst [0] (NotEqual cc)) yes no) => (NE cc yes no)
+(NE (CMPconst [0] (LessThan cc)) yes no) => (LT cc yes no)
+(NE (CMPconst [0] (LessThanU cc)) yes no) => (ULT cc yes no)
+(NE (CMPconst [0] (LessEqual cc)) yes no) => (LE cc yes no)
+(NE (CMPconst [0] (LessEqualU cc)) yes no) => (ULE cc yes no)
+(NE (CMPconst [0] (GreaterThan cc)) yes no) => (GT cc yes no)
+(NE (CMPconst [0] (GreaterThanU cc)) yes no) => (UGT cc yes no)
+(NE (CMPconst [0] (GreaterEqual cc)) yes no) => (GE cc yes no)
+(NE (CMPconst [0] (GreaterEqualU cc)) yes no) => (UGE cc yes no)
+
+// Write barrier.
+(WB ...) => (LoweredWB ...)
+
+(PanicBounds [kind] x y mem) && boundsABI(kind) == 0 => (LoweredPanicBoundsA [kind] x y mem)
+(PanicBounds [kind] x y mem) && boundsABI(kind) == 1 => (LoweredPanicBoundsB [kind] x y mem)
+(PanicBounds [kind] x y mem) && boundsABI(kind) == 2 => (LoweredPanicBoundsC [kind] x y mem)
+
+(PanicExtend [kind] hi lo y mem) && boundsABI(kind) == 0 => (LoweredPanicExtendA [kind] hi lo y mem)
+(PanicExtend [kind] hi lo y mem) && boundsABI(kind) == 1 => (LoweredPanicExtendB [kind] hi lo y mem)
+(PanicExtend [kind] hi lo y mem) && boundsABI(kind) == 2 => (LoweredPanicExtendC [kind] hi lo y mem)
+
+// Optimizations
+
+// zero-extend of small and => small and
+(MOVBUreg y:(ANDconst [c] _)) && uint64(c) <= 0xFF => y
+(MOVHUreg y:(ANDconst [c] _)) && uint64(c) <= 0xFFFF => y
+
+// small and of zero-extend => either zero-extend or small and
+(ANDconst [c] y:(MOVBUreg _)) && c&0xFF == 0xFF => y
+(ANDconst [c] y:(MOVHUreg _))  && c&0xFFFF == 0xFFFF => y
+
+// normal case
+(ANDconst [c] (MOVBUreg x)) => (ANDconst [c&0xFF] x)
+(ANDconst [c] (MOVHUreg x)) => (ANDconst [c&0xFFFF] x)
+
+// double sign/zero extend
+(MOVBreg (MOVBUreg x)) => (MOVBreg x)
+(MOV(B|H)reg (MOVBreg x)) => (MOVBreg x)
+(MOVHreg (MOVHreg x)) => (MOVHreg x)
+(MOVHreg (MOVHUreg x)) => (MOVHreg x)
+
+(MOVBUreg (MOVBreg x)) => (MOVBUreg x)
+(MOV(B|H)Ureg (MOVBUreg x)) => (MOVBUreg x)
+(MOVHUreg (MOVHUreg x)) => (MOVHUreg x)
+(MOVHUreg (MOVHreg x)) => (MOVHUreg x)
+
+// Eliminate unnecessary sign/zero extend following right shift or bit field extract
+(MOV(B|H)reg <t> (SRAconst [c] (MOVBreg x))) => (SRAconst [c] (MOVBreg <t> x))
+(MOVHreg <t> (SRAconst [c] (MOVHreg x))) => (SRAconst [c] (MOVHreg <t> x))
+(MOV(B|H)reg (BFX [c] x)) && (c>>8 <= 8) => (BFX [c] x)
+(MOVHreg (BFX [c] x)) && (c>>8 <= 16) => (BFX [c] x)
+//(MOVHreg (SRAconst [c] x)) && sizeof(x.Type) <= 16 => (SRAconst [c] x) can we rely on sizeof(x.Type)?
+//(MOVBreg (SRAconst [c] x)) && sizeof(x.Type) == 8 => (SRAconst [c] x)
+
+(MOV(B|H)Ureg <t> (SRLconst [c] (MOVBUreg x))) => (SRLconst [c] (MOVBUreg <t> x))
+(MOVHUreg <t> (SRLconst [c] (MOVHUreg x))) => (SRLconst [c] (MOVHUreg <t> x))
+(MOV(B|H)Ureg (BFXU [c] x)) && (c>>8 <= 8) => (BFXU [c] x)
+(MOVHUreg (BFXU [c] x)) && (c>>8 <= 16) => (BFXU [c] x)
+//(MOVHUreg (SRLconst [c] x)) && sizeof(x.Type) <= 16 => (SRLconst [c] x) can we rely on sizeof(x.Type)?
+//(MOVBUreg (SRLconst [c] x)) && sizeof(x.Type) == 8 => (SRLconst [c] x)
+
+// initial right shift will handle sign/zero extend
+(MOVBUreg (SRLconst [c] x)) && c>=24 => (SRLconst [c] x)
+(MOVBreg (SRLconst [c] x)) && c>24 => (SRLconst [c] x)
+(MOVBreg (SRLconst [c] x)) && c==24 => (SRAconst [c] x)
+(MOVBreg (SRAconst [c] x)) && c>=24 => (SRAconst [c] x)
+
+(MOVHUreg (SRLconst [c] x)) && c>=16 => (SRLconst [c] x)
+(MOVHreg (SRLconst [c] x)) && c>16 => (SRLconst [c] x)
+(MOVHreg (SRAconst [c] x)) && c>=16 => (SRAconst [c] x)
+(MOVHreg (SRLconst [c] x)) && c==16 => (SRAconst [c] x)
+
+// fold offset into address
+(ADDconst [off1] (MOVWaddr [off2] {sym} ptr)) => (MOVWaddr [off1+off2] {sym} ptr)
+(SUBconst [off1] (MOVWaddr [off2] {sym} ptr)) => (MOVWaddr [off2-off1] {sym} ptr)
+
+// fold address into load/store
+(MOVBload [off1] {sym} (ADDconst [off2] ptr) mem) => (MOVBload [off1+off2] {sym} ptr mem)
+(MOVBload [off1] {sym} (SUBconst [off2] ptr) mem) => (MOVBload [off1-off2] {sym} ptr mem)
+(MOVBUload [off1] {sym} (ADDconst [off2] ptr) mem) => (MOVBUload [off1+off2] {sym} ptr mem)
+(MOVBUload [off1] {sym} (SUBconst [off2] ptr) mem) => (MOVBUload [off1-off2] {sym} ptr mem)
+(MOVHload [off1] {sym} (ADDconst [off2] ptr) mem) => (MOVHload [off1+off2] {sym} ptr mem)
+(MOVHload [off1] {sym} (SUBconst [off2] ptr) mem) => (MOVHload [off1-off2] {sym} ptr mem)
+(MOVHUload [off1] {sym} (ADDconst [off2] ptr) mem) => (MOVHUload [off1+off2] {sym} ptr mem)
+(MOVHUload [off1] {sym} (SUBconst [off2] ptr) mem) => (MOVHUload [off1-off2] {sym} ptr mem)
+(MOVWload [off1] {sym} (ADDconst [off2] ptr) mem) => (MOVWload [off1+off2] {sym} ptr mem)
+(MOVWload [off1] {sym} (SUBconst [off2] ptr) mem) => (MOVWload [off1-off2] {sym} ptr mem)
+(MOVFload [off1] {sym} (ADDconst [off2] ptr) mem) => (MOVFload [off1+off2] {sym} ptr mem)
+(MOVFload [off1] {sym} (SUBconst [off2] ptr) mem) => (MOVFload [off1-off2] {sym} ptr mem)
+(MOVDload [off1] {sym} (ADDconst [off2] ptr) mem) => (MOVDload [off1+off2] {sym} ptr mem)
+(MOVDload [off1] {sym} (SUBconst [off2] ptr) mem) => (MOVDload [off1-off2] {sym} ptr mem)
+
+(LoadOnce8 [off1] (ADDconst [off2] ptr) mem) => (LoadOnce8 [off1+off2] ptr mem)
+(LoadOnce8 [off1] (SUBconst [off2] ptr) mem) => (LoadOnce8 [off1-off2] ptr mem)
+(LoadOnce16 [off1] (ADDconst [off2] ptr) mem) => (LoadOnce16 [off1+off2] ptr mem)
+(LoadOnce16 [off1] (SUBconst [off2] ptr) mem) => (LoadOnce16 [off1-off2] ptr mem)
+(LoadOnce32 [off1] (ADDconst [off2] ptr) mem) => (LoadOnce32 [off1+off2] ptr mem)
+(LoadOnce32 [off1] (SUBconst [off2] ptr) mem) => (LoadOnce32 [off1-off2] ptr mem)
+
+(MOVBstore [off1] {sym} (ADDconst [off2] ptr) val mem) => (MOVBstore [off1+off2] {sym} ptr val mem)
+(MOVBstore [off1] {sym} (SUBconst [off2] ptr) val mem) => (MOVBstore [off1-off2] {sym} ptr val mem)
+(MOVHstore [off1] {sym} (ADDconst [off2] ptr) val mem) => (MOVHstore [off1+off2] {sym} ptr val mem)
+(MOVHstore [off1] {sym} (SUBconst [off2] ptr) val mem) => (MOVHstore [off1-off2] {sym} ptr val mem)
+(MOVWstore [off1] {sym} (ADDconst [off2] ptr) val mem) => (MOVWstore [off1+off2] {sym} ptr val mem)
+(MOVWstore [off1] {sym} (SUBconst [off2] ptr) val mem) => (MOVWstore [off1-off2] {sym} ptr val mem)
+(MOVFstore [off1] {sym} (ADDconst [off2] ptr) val mem) => (MOVFstore [off1+off2] {sym} ptr val mem)
+(MOVFstore [off1] {sym} (SUBconst [off2] ptr) val mem) => (MOVFstore [off1-off2] {sym} ptr val mem)
+(MOVDstore [off1] {sym} (ADDconst [off2] ptr) val mem) => (MOVDstore [off1+off2] {sym} ptr val mem)
+(MOVDstore [off1] {sym} (SUBconst [off2] ptr) val mem) => (MOVDstore [off1-off2] {sym} ptr val mem)
+
+(StoreOnce8 [off1] (ADDconst [off2] ptr) val mem) => (StoreOnce8 [off1+off2] ptr val mem)
+(StoreOnce8 [off1] (SUBconst [off2] ptr) val mem) => (StoreOnce8 [off1-off2] ptr val mem)
+(StoreOnce16 [off1] (ADDconst [off2] ptr) val mem) => (StoreOnce16 [off1+off2] ptr val mem)
+(StoreOnce16 [off1] (SUBconst [off2] ptr) val mem) => (StoreOnce16 [off1-off2] ptr val mem)
+(StoreOnce32 [off1] (ADDconst [off2] ptr) val mem) => (StoreOnce32 [off1+off2] ptr val mem)
+(StoreOnce32 [off1] (SUBconst [off2] ptr) val mem) => (StoreOnce32 [off1-off2] ptr val mem)
+
+(MOVBload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+	(MOVBload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVBUload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+	(MOVBUload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVHload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+	(MOVHload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVHUload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+	(MOVHUload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVWload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+	(MOVWload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVFload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+	(MOVFload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVDload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+	(MOVDload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+
+//(LoadOnce8 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+//	(LoadOnce8 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+//(LoadOnce16 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+//	(LoadOnce16 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+//(LoadOnce32 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) =>
+//	(LoadOnce32 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+
+(MOVBstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) =>
+	(MOVBstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(MOVHstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) =>
+	(MOVHstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(MOVWstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) =>
+	(MOVWstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(MOVFstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) =>
+	(MOVFstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(MOVDstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) =>
+	(MOVDstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+
+//(StoreOnce8 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) =>
+//	(StoreOnce8 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+//(StoreOnce16 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) =>
+//	(StoreOnce16 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+//(StoreOnce32 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) =>
+//	(StoreOnce32 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+
+// replace load from same location as preceding store with zero/sign extension (or copy in case of full width)
+(MOVBload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) => (MOVBreg x)
+(MOVBUload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) => (MOVBUreg x)
+(MOVHload [off] {sym} ptr (MOVHstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) => (MOVHreg x)
+(MOVHUload [off] {sym} ptr (MOVHstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) => (MOVHUreg x)
+(MOVWload [off] {sym} ptr (MOVWstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) => x
+
+(MOVFload [off] {sym} ptr (MOVFstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) => x
+(MOVDload [off] {sym} ptr (MOVDstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) => x
+
+// no MOVWloadshiftRL, MOVWloadshiftRA in Thumb2
+(MOVWloadidx ptr idx (MOVWstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) => x
+(MOVWloadshiftLL ptr idx [c] (MOVWstoreshiftLL ptr2 idx [d] x _)) && c==d && isSamePtr(ptr, ptr2) => x
+(MOVBUloadidx ptr idx (MOVBstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) => (MOVBUreg x)
+(MOVBUloadshiftLL ptr idx [c] (MOVBstoreshiftLL ptr2 idx [d] x _)) && c==d && isSamePtr(ptr, ptr2) => (MOVBUreg x)
+(MOVBloadidx ptr idx (MOVBstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) => (MOVBreg x)
+(MOVBloadshiftLL ptr idx [c] (MOVBstoreshiftLL ptr2 idx [d] x _)) && c==d && isSamePtr(ptr, ptr2) => (MOVBreg x)
+(MOVHUloadidx ptr idx (MOVHstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) => (MOVHUreg x)
+(MOVHUloadshiftLL ptr idx [c] (MOVHstoreshiftLL ptr2 idx [d] x _)) && c==d && isSamePtr(ptr, ptr2) => (MOVHUreg x)
+(MOVHloadidx ptr idx (MOVHstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) => (MOVHreg x)
+(MOVHloadshiftLL ptr idx [c] (MOVHstoreshiftLL ptr2 idx [d] x _)) && c==d && isSamePtr(ptr, ptr2) => (MOVHreg x)
+
+// fold constant into arithmatic ops
+(ADD x (MOVWconst [c])) => (ADDconst [c] x)
+(SUB (MOVWconst [c]) x) => (RSBconst [c] x)
+(SUB x (MOVWconst [c])) => (SUBconst [c] x)
+(RSB (MOVWconst [c]) x) => (SUBconst [c] x)
+(RSB x (MOVWconst [c])) => (RSBconst [c] x)
+
+(ADDS x (MOVWconst [c])) => (ADDSconst [c] x)
+(SUBS x (MOVWconst [c])) => (SUBSconst [c] x)
+
+(ADC (MOVWconst [c]) x flags) => (ADCconst [c] x flags)
+(SBC x (MOVWconst [c]) flags) => (SBCconst [c] x flags)
+// no RSCconst in Thumb2
+
+(AND x (MOVWconst [c])) => (ANDconst [c] x)
+(OR  x (MOVWconst [c])) => (ORconst [c] x)
+(ORN x (MOVWconst [c])) => (ORNconst [c] x)
+(XOR x (MOVWconst [c])) => (XORconst [c] x)
+(BIC x (MOVWconst [c])) => (BICconst [c] x)
+
+(SLL x (MOVWconst [c])) => (SLLconst x [c&31]) // Note: I don't think we ever generate bad constant shifts (i.e. c>=32)
+(SRL x (MOVWconst [c])) => (SRLconst x [c&31])
+(SRA x (MOVWconst [c])) => (SRAconst x [c&31])
+
+(CMP x (MOVWconst [c])) => (CMPconst [c] x)
+(CMP (MOVWconst [c]) x) => (InvertFlags (CMPconst [c] x))
+(CMN x (MOVWconst [c])) => (CMNconst [c] x)
+(TST x (MOVWconst [c])) => (TSTconst [c] x)
+(TEQ x (MOVWconst [c])) => (TEQconst [c] x)
+
+// Canonicalize the order of arguments to comparisons - helps with CSE.
+(CMP x y) && x.ID > y.ID => (InvertFlags (CMP y x))
+
+// don't extend after proper load
+// MOVWreg instruction is not emitted if src and dst registers are same, but it ensures the type.
+(MOVBreg x:(MOVBload _ _)) => (MOVWreg x)
+(MOVBUreg x:(MOVBUload _ _)) => (MOVWreg x)
+(MOVHreg x:(MOVBload _ _)) => (MOVWreg x)
+(MOVHreg x:(MOVBUload _ _)) => (MOVWreg x)
+(MOVHreg x:(MOVHload _ _)) => (MOVWreg x)
+(MOVHUreg x:(MOVBUload _ _)) => (MOVWreg x)
+(MOVHUreg x:(MOVHUload _ _)) => (MOVWreg x)
+
+(MOVBreg x:(MOVBloadidx _ _ _)) => (MOVWreg x)
+(MOVBUreg x:(MOVBUloadidx _ _ _)) => (MOVWreg x)
+(MOVHreg x:(MOVBloadidx _ _ _)) => (MOVWreg x)
+(MOVHreg x:(MOVBUloadidx _ _ _)) => (MOVWreg x)
+(MOVHreg x:(MOVHloadidx _ _ _)) => (MOVWreg x)
+(MOVHUreg x:(MOVBUloadidx _ _ _)) => (MOVWreg x)
+(MOVHUreg x:(MOVHUloadidx _ _ _)) => (MOVWreg x)
+
+// fold extensions and ANDs together
+(MOVBUreg (ANDconst [c] x)) => (ANDconst [c&0xff] x)
+(MOVHUreg (ANDconst [c] x)) => (ANDconst [c&0xffff] x)
+(MOVBreg (ANDconst [c] x)) && c & 0x80 == 0 => (ANDconst [c&0x7f] x)
+(MOVHreg (ANDconst [c] x)) && c & 0x8000 == 0 => (ANDconst [c&0x7fff] x)
+
+// fold double extensions
+(MOVBreg x:(MOVBreg _)) => (MOVWreg x)
+(MOVBUreg x:(MOVBUreg _)) => (MOVWreg x)
+(MOVHreg x:(MOVBreg _)) => (MOVWreg x)
+(MOVHreg x:(MOVBUreg _)) => (MOVWreg x)
+(MOVHreg x:(MOVHreg _)) => (MOVWreg x)
+(MOVHUreg x:(MOVBUreg _)) => (MOVWreg x)
+(MOVHUreg x:(MOVHUreg _)) => (MOVWreg x)
+
+// don't extend before store
+(MOVBstore [off] {sym} ptr (MOVBreg x) mem) => (MOVBstore [off] {sym} ptr x mem)
+(MOVBstore [off] {sym} ptr (MOVBUreg x) mem) => (MOVBstore [off] {sym} ptr x mem)
+(MOVBstore [off] {sym} ptr (MOVHreg x) mem) => (MOVBstore [off] {sym} ptr x mem)
+(MOVBstore [off] {sym} ptr (MOVHUreg x) mem) => (MOVBstore [off] {sym} ptr x mem)
+(MOVHstore [off] {sym} ptr (MOVHreg x) mem) => (MOVHstore [off] {sym} ptr x mem)
+(MOVHstore [off] {sym} ptr (MOVHUreg x) mem) => (MOVHstore [off] {sym} ptr x mem)
+
+// if a register move has only 1 use, just use the same register without emitting instruction
+// MOVWnop doesn't emit instruction, only for ensuring the type.
+(MOVWreg x) && x.Uses == 1 => (MOVWnop x)
+
+// mul by constant
+(MUL x (MOVWconst [c])) && int32(c) == -1 => (RSBconst [0] x)
+(MUL _ (MOVWconst [0])) => (MOVWconst [0])
+(MUL x (MOVWconst [1])) => x
+(MUL x (MOVWconst [c])) && isPowerOfTwo32(c) => (SLLconst [int32(log32(c))] x)
+(MUL x (MOVWconst [c])) && isPowerOfTwo32(c-1) && c >= 3 => (ADDshiftLL x x [int32(log32(c-1))])
+(MUL x (MOVWconst [c])) && isPowerOfTwo32(c+1) && c >= 7 => (RSBshiftLL x x [int32(log32(c+1))])
+(MUL x (MOVWconst [c])) && c%3 == 0 && isPowerOfTwo32(c/3) => (SLLconst [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1]))
+(MUL x (MOVWconst [c])) && c%5 == 0 && isPowerOfTwo32(c/5) => (SLLconst [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2]))
+(MUL x (MOVWconst [c])) && c%7 == 0 && isPowerOfTwo32(c/7) => (SLLconst [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3]))
+(MUL x (MOVWconst [c])) && c%9 == 0 && isPowerOfTwo32(c/9) => (SLLconst [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3]))
+
+(MULA x (MOVWconst [c]) a) && c == -1 => (SUB a x)
+(MULA _ (MOVWconst [0]) a) => a
+(MULA x (MOVWconst [1]) a) => (ADD x a)
+(MULA x (MOVWconst [c]) a) && isPowerOfTwo32(c) => (ADD (SLLconst <x.Type> [int32(log32(c))] x) a)
+(MULA x (MOVWconst [c]) a) && isPowerOfTwo32(c-1) && c >= 3 => (ADD (ADDshiftLL <x.Type> x x [int32(log32(c-1))]) a)
+(MULA x (MOVWconst [c]) a) && isPowerOfTwo32(c+1) && c >= 7 => (ADD (RSBshiftLL <x.Type> x x [int32(log32(c+1))]) a)
+(MULA x (MOVWconst [c]) a) && c%3 == 0 && isPowerOfTwo32(c/3) => (ADD (SLLconst <x.Type> [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1])) a)
+(MULA x (MOVWconst [c]) a) && c%5 == 0 && isPowerOfTwo32(c/5) => (ADD (SLLconst <x.Type> [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2])) a)
+(MULA x (MOVWconst [c]) a) && c%7 == 0 && isPowerOfTwo32(c/7) => (ADD (SLLconst <x.Type> [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3])) a)
+(MULA x (MOVWconst [c]) a) && c%9 == 0 && isPowerOfTwo32(c/9) => (ADD (SLLconst <x.Type> [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3])) a)
+
+(MULA (MOVWconst [c]) x a) && c == -1 => (SUB a x)
+(MULA (MOVWconst [0]) _ a) => a
+(MULA (MOVWconst [1]) x a) => (ADD x a)
+(MULA (MOVWconst [c]) x a) && isPowerOfTwo32(c) => (ADD (SLLconst <x.Type> [int32(log32(c))] x) a)
+(MULA (MOVWconst [c]) x a) && isPowerOfTwo32(c-1) && c >= 3 => (ADD (ADDshiftLL <x.Type> x x [int32(log32(c-1))]) a)
+(MULA (MOVWconst [c]) x a) && isPowerOfTwo32(c+1) && c >= 7 => (ADD (RSBshiftLL <x.Type> x x [int32(log32(c+1))]) a)
+(MULA (MOVWconst [c]) x a) && c%3 == 0 && isPowerOfTwo32(c/3) => (ADD (SLLconst <x.Type> [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1])) a)
+(MULA (MOVWconst [c]) x a) && c%5 == 0 && isPowerOfTwo32(c/5) => (ADD (SLLconst <x.Type> [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2])) a)
+(MULA (MOVWconst [c]) x a) && c%7 == 0 && isPowerOfTwo32(c/7) => (ADD (SLLconst <x.Type> [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3])) a)
+(MULA (MOVWconst [c]) x a) && c%9 == 0 && isPowerOfTwo32(c/9) => (ADD (SLLconst <x.Type> [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3])) a)
+
+(MULS x (MOVWconst [c]) a) && c == -1 => (ADD a x)
+(MULS _ (MOVWconst [0]) a) => a
+(MULS x (MOVWconst [1]) a) => (RSB x a)
+(MULS x (MOVWconst [c]) a) && isPowerOfTwo32(c) => (RSB (SLLconst <x.Type> [int32(log32(c))] x) a)
+(MULS x (MOVWconst [c]) a) && isPowerOfTwo32(c-1) && c >= 3 => (RSB (ADDshiftLL <x.Type> x x [int32(log32(c-1))]) a)
+(MULS x (MOVWconst [c]) a) && isPowerOfTwo32(c+1) && c >= 7 => (RSB (RSBshiftLL <x.Type> x x [int32(log32(c+1))]) a)
+(MULS x (MOVWconst [c]) a) && c%3 == 0 && isPowerOfTwo32(c/3) => (RSB (SLLconst <x.Type> [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1])) a)
+(MULS x (MOVWconst [c]) a) && c%5 == 0 && isPowerOfTwo32(c/5) => (RSB (SLLconst <x.Type> [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2])) a)
+(MULS x (MOVWconst [c]) a) && c%7 == 0 && isPowerOfTwo32(c/7) => (RSB (SLLconst <x.Type> [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3])) a)
+(MULS x (MOVWconst [c]) a) && c%9 == 0 && isPowerOfTwo32(c/9) => (RSB (SLLconst <x.Type> [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3])) a)
+
+(MULS (MOVWconst [c]) x a) && c == -1 => (ADD a x)
+(MULS (MOVWconst [0]) _ a) => a
+(MULS (MOVWconst [1]) x a) => (RSB x a)
+(MULS (MOVWconst [c]) x a) && isPowerOfTwo32(c) => (RSB (SLLconst <x.Type> [int32(log32(c))] x) a)
+(MULS (MOVWconst [c]) x a) && isPowerOfTwo32(c-1) && c >= 3 => (RSB (ADDshiftLL <x.Type> x x [int32(log32(c-1))]) a)
+(MULS (MOVWconst [c]) x a) && isPowerOfTwo32(c+1) && c >= 7 => (RSB (RSBshiftLL <x.Type> x x [int32(log32(c+1))]) a)
+(MULS (MOVWconst [c]) x a) && c%3 == 0 && isPowerOfTwo32(c/3) => (RSB (SLLconst <x.Type> [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1])) a)
+(MULS (MOVWconst [c]) x a) && c%5 == 0 && isPowerOfTwo32(c/5) => (RSB (SLLconst <x.Type> [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2])) a)
+(MULS (MOVWconst [c]) x a) && c%7 == 0 && isPowerOfTwo32(c/7) => (RSB (SLLconst <x.Type> [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3])) a)
+(MULS (MOVWconst [c]) x a) && c%9 == 0 && isPowerOfTwo32(c/9) => (RSB (SLLconst <x.Type> [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3])) a)
+
+// div by constant
+(DIV x (MOVWconst [1])) => x
+(DIVU x (MOVWconst [1])) => x
+(DIV x (MOVWconst [c])) && isPowerOfTwo32(c) => (SRAconst [int32(log32(c))] x)
+(DIVU x (MOVWconst [c])) && isPowerOfTwo32(c) => (SRLconst [int32(log32(c))] x)
+
+// constant comparisons
+(CMPconst (MOVWconst [x]) [y]) => (FlagConstant [subFlags32(x,y)])
+(CMNconst (MOVWconst [x]) [y]) => (FlagConstant [addFlags32(x,y)])
+(TSTconst (MOVWconst [x]) [y]) => (FlagConstant [logicFlags32(x&y)])
+(TEQconst (MOVWconst [x]) [y]) => (FlagConstant [logicFlags32(x^y)])
+
+// other known comparisons
+(CMPconst (MOVBUreg _) [c]) && 0xff < c => (FlagConstant [subFlags32(0, 1)])
+(CMPconst (MOVHUreg _) [c]) && 0xffff < c => (FlagConstant [subFlags32(0, 1)])
+(CMPconst (ANDconst _ [m]) [n]) && 0 <= m && m < n => (FlagConstant [subFlags32(0, 1)])
+(CMPconst (SRLconst _ [c]) [n]) && 0 <= n && 0 < c && c <= 32 && (1<<uint32(32-c)) <= uint32(n) => (FlagConstant [subFlags32(0, 1)])
+
+// absorb flag constants into branches
+(EQ (FlagConstant [fc]) yes no) &&  fc.eq() => (First yes no)
+(EQ (FlagConstant [fc]) yes no) && !fc.eq() => (First no yes)
+
+(NE (FlagConstant [fc]) yes no) &&  fc.ne() => (First yes no)
+(NE (FlagConstant [fc]) yes no) && !fc.ne() => (First no yes)
+
+(LT (FlagConstant [fc]) yes no) &&  fc.lt() => (First yes no)
+(LT (FlagConstant [fc]) yes no) && !fc.lt() => (First no yes)
+
+(LE (FlagConstant [fc]) yes no) &&  fc.le() => (First yes no)
+(LE (FlagConstant [fc]) yes no) && !fc.le() => (First no yes)
+
+(GT (FlagConstant [fc]) yes no) &&  fc.gt() => (First yes no)
+(GT (FlagConstant [fc]) yes no) && !fc.gt() => (First no yes)
+
+(GE (FlagConstant [fc]) yes no) &&  fc.ge() => (First yes no)
+(GE (FlagConstant [fc]) yes no) && !fc.ge() => (First no yes)
+
+(ULT (FlagConstant [fc]) yes no) &&  fc.ult() => (First yes no)
+(ULT (FlagConstant [fc]) yes no) && !fc.ult() => (First no yes)
+
+(ULE (FlagConstant [fc]) yes no) &&  fc.ule() => (First yes no)
+(ULE (FlagConstant [fc]) yes no) && !fc.ule() => (First no yes)
+
+(UGT (FlagConstant [fc]) yes no) &&  fc.ugt() => (First yes no)
+(UGT (FlagConstant [fc]) yes no) && !fc.ugt() => (First no yes)
+
+(UGE (FlagConstant [fc]) yes no) &&  fc.uge() => (First yes no)
+(UGE (FlagConstant [fc]) yes no) && !fc.uge() => (First no yes)
+
+(LTnoov (FlagConstant [fc]) yes no) &&  fc.ltNoov() => (First yes no)
+(LTnoov (FlagConstant [fc]) yes no) && !fc.ltNoov() => (First no yes)
+
+(LEnoov (FlagConstant [fc]) yes no) &&  fc.leNoov() => (First yes no)
+(LEnoov (FlagConstant [fc]) yes no) && !fc.leNoov() => (First no yes)
+
+(GTnoov (FlagConstant [fc]) yes no) &&  fc.gtNoov() => (First yes no)
+(GTnoov (FlagConstant [fc]) yes no) && !fc.gtNoov() => (First no yes)
+
+(GEnoov (FlagConstant [fc]) yes no) &&  fc.geNoov() => (First yes no)
+(GEnoov (FlagConstant [fc]) yes no) && !fc.geNoov() => (First no yes)
+
+// absorb InvertFlags into branches
+(LT (InvertFlags cmp) yes no) => (GT cmp yes no)
+(GT (InvertFlags cmp) yes no) => (LT cmp yes no)
+(LE (InvertFlags cmp) yes no) => (GE cmp yes no)
+(GE (InvertFlags cmp) yes no) => (LE cmp yes no)
+(ULT (InvertFlags cmp) yes no) => (UGT cmp yes no)
+(UGT (InvertFlags cmp) yes no) => (ULT cmp yes no)
+(ULE (InvertFlags cmp) yes no) => (UGE cmp yes no)
+(UGE (InvertFlags cmp) yes no) => (ULE cmp yes no)
+(EQ (InvertFlags cmp) yes no) => (EQ cmp yes no)
+(NE (InvertFlags cmp) yes no) => (NE cmp yes no)
+(LTnoov (InvertFlags cmp) yes no) => (GTnoov cmp yes no)
+(GEnoov (InvertFlags cmp) yes no) => (LEnoov cmp yes no)
+(LEnoov (InvertFlags cmp) yes no) => (GEnoov cmp yes no)
+(GTnoov (InvertFlags cmp) yes no) => (LTnoov cmp yes no)
+
+// absorb flag constants into boolean values
+(Equal (FlagConstant [fc])) => (MOVWconst [b2i32(fc.eq())])
+(NotEqual (FlagConstant [fc])) => (MOVWconst [b2i32(fc.ne())])
+(LessThan (FlagConstant [fc])) => (MOVWconst [b2i32(fc.lt())])
+(LessThanU (FlagConstant [fc])) => (MOVWconst [b2i32(fc.ult())])
+(LessEqual (FlagConstant [fc])) => (MOVWconst [b2i32(fc.le())])
+(LessEqualU (FlagConstant [fc])) => (MOVWconst [b2i32(fc.ule())])
+(GreaterThan (FlagConstant [fc])) => (MOVWconst [b2i32(fc.gt())])
+(GreaterThanU (FlagConstant [fc])) => (MOVWconst [b2i32(fc.ugt())])
+(GreaterEqual (FlagConstant [fc])) => (MOVWconst [b2i32(fc.ge())])
+(GreaterEqualU (FlagConstant [fc])) => (MOVWconst [b2i32(fc.uge())])
+
+// absorb InvertFlags into boolean values
+(Equal (InvertFlags x)) => (Equal x)
+(NotEqual (InvertFlags x)) => (NotEqual x)
+(LessThan (InvertFlags x)) => (GreaterThan x)
+(LessThanU (InvertFlags x)) => (GreaterThanU x)
+(GreaterThan (InvertFlags x)) => (LessThan x)
+(GreaterThanU (InvertFlags x)) => (LessThanU x)
+(LessEqual (InvertFlags x)) => (GreaterEqual x)
+(LessEqualU (InvertFlags x)) => (GreaterEqualU x)
+(GreaterEqual (InvertFlags x)) => (LessEqual x)
+(GreaterEqualU (InvertFlags x)) => (LessEqualU x)
+
+// absorb flag constants into conditional instructions
+(CMOVWLSconst _ (FlagConstant [fc]) [c]) && fc.ule() => (MOVWconst [c])
+(CMOVWLSconst x (FlagConstant [fc]) [c]) && fc.ugt() => x
+
+(CMOVWHSconst _ (FlagConstant [fc]) [c]) && fc.uge() => (MOVWconst [c])
+(CMOVWHSconst x (FlagConstant [fc]) [c]) && fc.ult() => x
+
+(CMOVWLSconst x (InvertFlags flags) [c]) => (CMOVWHSconst x flags [c])
+(CMOVWHSconst x (InvertFlags flags) [c]) => (CMOVWLSconst x flags [c])
+
+(SRAcond x _ (FlagConstant [fc])) && fc.uge() => (SRAconst x [31])
+(SRAcond x y (FlagConstant [fc])) && fc.ult() => (SRA x y)
+
+// remove redundant *const ops
+(ADDconst [0] x) => x
+(SUBconst [0] x) => x
+(ANDconst [0] _) => (MOVWconst [0])
+(ANDconst [c] x) && int32(c)==-1 => x
+(ORconst [0] x) => x
+(ORconst [c] _) && int32(c)==-1 => (MOVWconst [-1])
+(ORNconst [0] _) => (MOVWconst [-1])
+(ORNconst [c] x) && int32(c)==-1 => x
+(XORconst [0] x) => x
+(BICconst [0] x) => x
+(BICconst [c] _) && int32(c)==-1 => (MOVWconst [0])
+
+// generic constant folding (no RSCconst in Thumb2)
+(ADDconst [c] (MOVWconst [d])) => (MOVWconst [c+d])
+(ADDconst [c] (ADDconst [d] x)) => (ADDconst [c+d] x)
+(ADDconst [c] (SUBconst [d] x)) => (ADDconst [c-d] x)
+(ADDconst [c] (RSBconst [d] x)) => (RSBconst [c+d] x)
+(ADCconst [c] (ADDconst [d] x) flags) => (ADCconst [c+d] x flags)
+(ADCconst [c] (SUBconst [d] x) flags) => (ADCconst [c-d] x flags)
+(SUBconst [c] (MOVWconst [d])) => (MOVWconst [d-c])
+(SUBconst [c] (SUBconst [d] x)) => (ADDconst [-c-d] x)
+(SUBconst [c] (ADDconst [d] x)) => (ADDconst [-c+d] x)
+(SUBconst [c] (RSBconst [d] x)) => (RSBconst [-c+d] x)
+(SBCconst [c] (ADDconst [d] x) flags) => (SBCconst [c-d] x flags)
+(SBCconst [c] (SUBconst [d] x) flags) => (SBCconst [c+d] x flags)
+(RSBconst [c] (MOVWconst [d])) => (MOVWconst [c-d])
+(RSBconst [c] (RSBconst [d] x)) => (ADDconst [c-d] x)
+(RSBconst [c] (ADDconst [d] x)) => (RSBconst [c-d] x)
+(RSBconst [c] (SUBconst [d] x)) => (RSBconst [c+d] x)
+(SLLconst [c] (MOVWconst [d])) => (MOVWconst [d<<uint64(c)])
+(SRLconst [c] (MOVWconst [d])) => (MOVWconst [int32(uint32(d)>>uint64(c))])
+(SRAconst [c] (MOVWconst [d])) => (MOVWconst [d>>uint64(c)])
+(MUL (MOVWconst [c]) (MOVWconst [d])) => (MOVWconst [c*d])
+(MULA (MOVWconst [c]) (MOVWconst [d]) a) => (ADDconst [c*d] a)
+(MULS (MOVWconst [c]) (MOVWconst [d]) a) => (SUBconst [c*d] a)
+(DIV (MOVWconst [c]) (MOVWconst [d])) && d != 0 => (MOVWconst [c/d])
+(DIVU (MOVWconst [c]) (MOVWconst [d])) && d != 0 => (MOVWconst [int32(uint32(c)/uint32(d))])
+(ANDconst [c] (MOVWconst [d])) => (MOVWconst [c&d])
+(ANDconst [c] (ANDconst [d] x)) => (ANDconst [c&d] x)
+(ORconst [c] (MOVWconst [d])) => (MOVWconst [c|d])
+(ORconst [c] (ORconst [d] x)) => (ORconst [c|d] x)
+(ORNconst [c] (MOVWconst [d])) => (MOVWconst [^c|d])
+(ORNconst [c] (ORNconst [d] x)) => (ORconst [^c|^d] x)
+(XORconst [c] (MOVWconst [d])) => (MOVWconst [c^d])
+(XORconst [c] (XORconst [d] x)) => (XORconst [c^d] x)
+(BICconst [c] (MOVWconst [d])) => (MOVWconst [d&^c])
+(BICconst [c] (BICconst [d] x)) => (BICconst [c|d] x)
+(MVN (MOVWconst [c])) => (MOVWconst [^c])
+(MOVBreg (MOVWconst [c])) => (MOVWconst [int32(int8(c))])
+(MOVBUreg (MOVWconst [c])) => (MOVWconst [int32(uint8(c))])
+(MOVHreg (MOVWconst [c])) => (MOVWconst [int32(int16(c))])
+(MOVHUreg (MOVWconst [c])) => (MOVWconst [int32(uint16(c))])
+(MOVWreg (MOVWconst [c])) => (MOVWconst [c])
+// BFX: Width = c >> 8, LSB = c & 0xff, result = d << (32 - Width - LSB) >> (32 - Width)
+(BFX [c] (MOVWconst [d])) => (MOVWconst [d<<(32-uint32(c&0xff)-uint32(c>>8))>>(32-uint32(c>>8))])
+(BFXU [c] (MOVWconst [d])) => (MOVWconst [int32(uint32(d)<<(32-uint32(c&0xff)-uint32(c>>8))>>(32-uint32(c>>8)))])
+
+// absorb shifts into ops (no ADDshiftRLreg, ADDshiftRAreg, RSCshiftRL, RSCshiftRA in Thumb2)
+(ADD x (SLLconst [c] y)) => (ADDshiftLL x y [c])
+(ADD x (SRLconst [c] y)) => (ADDshiftRL x y [c])
+(ADD x (SRAconst [c] y)) => (ADDshiftRA x y [c])
+(ADC x (SLLconst [c] y) flags) => (ADCshiftLL x y [c] flags)
+(ADC x (SRLconst [c] y) flags) => (ADCshiftRL x y [c] flags)
+(ADC x (SRAconst [c] y) flags) => (ADCshiftRA x y [c] flags)
+(ADDS x (SLLconst [c] y)) => (ADDSshiftLL x y [c])
+(ADDS x (SRLconst [c] y)) => (ADDSshiftRL x y [c])
+(ADDS x (SRAconst [c] y)) => (ADDSshiftRA x y [c])
+(SUB x (SLLconst [c] y)) => (SUBshiftLL x y [c])
+(SUB (SLLconst [c] y) x) => (RSBshiftLL x y [c])
+(SUB x (SRLconst [c] y)) => (SUBshiftRL x y [c])
+(SUB (SRLconst [c] y) x) => (RSBshiftRL x y [c])
+(SUB x (SRAconst [c] y)) => (SUBshiftRA x y [c])
+(SUB (SRAconst [c] y) x) => (RSBshiftRA x y [c])
+(SBC x (SLLconst [c] y) flags) => (SBCshiftLL x y [c] flags)
+(SBC x (SRLconst [c] y) flags) => (SBCshiftRL x y [c] flags)
+(SBC x (SRAconst [c] y) flags) => (SBCshiftRA x y [c] flags)
+(SUBS x (SLLconst [c] y)) => (SUBSshiftLL x y [c])
+(SUBS (SLLconst [c] y) x) => (RSBSshiftLL x y [c])
+(SUBS x (SRLconst [c] y)) => (SUBSshiftRL x y [c])
+(SUBS (SRLconst [c] y) x) => (RSBSshiftRL x y [c])
+(SUBS x (SRAconst [c] y)) => (SUBSshiftRA x y [c])
+(SUBS (SRAconst [c] y) x) => (RSBSshiftRA x y [c])
+(RSB x (SLLconst [c] y)) => (RSBshiftLL x y [c])
+(RSB (SLLconst [c] y) x) => (SUBshiftLL x y [c])
+(RSB x (SRLconst [c] y)) => (RSBshiftRL x y [c])
+(RSB (SRLconst [c] y) x) => (SUBshiftRL x y [c])
+(RSB x (SRAconst [c] y)) => (RSBshiftRA x y [c])
+(RSB (SRAconst [c] y) x) => (SUBshiftRA x y [c])
+(AND x (SLLconst [c] y)) => (ANDshiftLL x y [c])
+(AND x (SRLconst [c] y)) => (ANDshiftRL x y [c])
+(AND x (SRAconst [c] y)) => (ANDshiftRA x y [c])
+(OR x (SLLconst [c] y)) => (ORshiftLL x y [c])
+(OR x (SRLconst [c] y)) => (ORshiftRL x y [c])
+(OR x (SRAconst [c] y)) => (ORshiftRA x y [c])
+(ORN x (SLLconst [c] y)) => (ORNshiftLL x y [c])
+(ORN x (SRLconst [c] y)) => (ORNshiftRL x y [c])
+(ORN x (SRAconst [c] y)) => (ORNshiftRA x y [c])
+(XOR x (SLLconst [c] y)) => (XORshiftLL x y [c])
+(XOR x (SRLconst [c] y)) => (XORshiftRL x y [c])
+(XOR x (SRAconst [c] y)) => (XORshiftRA x y [c])
+(XOR x (SRRconst [c] y)) => (XORshiftRR x y [c])
+(BIC x (SLLconst [c] y)) => (BICshiftLL x y [c])
+(BIC x (SRLconst [c] y)) => (BICshiftRL x y [c])
+(BIC x (SRAconst [c] y)) => (BICshiftRA x y [c])
+(MVN (SLLconst [c] x)) => (MVNshiftLL x [c])
+(MVN (SRLconst [c] x)) => (MVNshiftRL x [c])
+(MVN (SRAconst [c] x)) => (MVNshiftRA x [c])
+
+(CMP x (SLLconst [c] y)) => (CMPshiftLL x y [c])
+(CMP (SLLconst [c] y) x) => (InvertFlags (CMPshiftLL x y [c]))
+(CMP x (SRLconst [c] y)) => (CMPshiftRL x y [c])
+(CMP (SRLconst [c] y) x) => (InvertFlags (CMPshiftRL x y [c]))
+(CMP x (SRAconst [c] y)) => (CMPshiftRA x y [c])
+(CMP (SRAconst [c] y) x) => (InvertFlags (CMPshiftRA x y [c]))
+(TST x (SLLconst [c] y)) => (TSTshiftLL x y [c])
+(TST x (SRLconst [c] y)) => (TSTshiftRL x y [c])
+(TST x (SRAconst [c] y)) => (TSTshiftRA x y [c])
+(TEQ x (SLLconst [c] y)) => (TEQshiftLL x y [c])
+(TEQ x (SRLconst [c] y)) => (TEQshiftRL x y [c])
+(TEQ x (SRAconst [c] y)) => (TEQshiftRA x y [c])
+(CMN x (SLLconst [c] y)) => (CMNshiftLL x y [c])
+(CMN x (SRLconst [c] y)) => (CMNshiftRL x y [c])
+(CMN x (SRAconst [c] y)) => (CMNshiftRA x y [c])
+
+// prefer *const ops to *shift ops
+(ADDshiftLL (MOVWconst [c]) x [d]) => (ADDconst [c] (SLLconst <x.Type> x [d]))
+(ADDshiftRL (MOVWconst [c]) x [d]) => (ADDconst [c] (SRLconst <x.Type> x [d]))
+(ADDshiftRA (MOVWconst [c]) x [d]) => (ADDconst [c] (SRAconst <x.Type> x [d]))
+(ADCshiftLL (MOVWconst [c]) x [d] flags) => (ADCconst [c] (SLLconst <x.Type> x [d]) flags)
+(ADCshiftRL (MOVWconst [c]) x [d] flags) => (ADCconst [c] (SRLconst <x.Type> x [d]) flags)
+(ADCshiftRA (MOVWconst [c]) x [d] flags) => (ADCconst [c] (SRAconst <x.Type> x [d]) flags)
+(ADDSshiftLL (MOVWconst [c]) x [d]) => (ADDSconst [c] (SLLconst <x.Type> x [d]))
+(ADDSshiftRL (MOVWconst [c]) x [d]) => (ADDSconst [c] (SRLconst <x.Type> x [d]))
+(ADDSshiftRA (MOVWconst [c]) x [d]) => (ADDSconst [c] (SRAconst <x.Type> x [d]))
+(SUBshiftLL (MOVWconst [c]) x [d]) => (RSBconst [c] (SLLconst <x.Type> x [d]))
+(SUBshiftRL (MOVWconst [c]) x [d]) => (RSBconst [c] (SRLconst <x.Type> x [d]))
+(SUBshiftRA (MOVWconst [c]) x [d]) => (RSBconst [c] (SRAconst <x.Type> x [d]))
+(SUBSshiftLL (MOVWconst [c]) x [d]) => (RSBSconst [c] (SLLconst <x.Type> x [d]))
+(SUBSshiftRL (MOVWconst [c]) x [d]) => (RSBSconst [c] (SRLconst <x.Type> x [d]))
+(SUBSshiftRA (MOVWconst [c]) x [d]) => (RSBSconst [c] (SRAconst <x.Type> x [d]))
+(RSBshiftLL (MOVWconst [c]) x [d]) => (SUBconst [c] (SLLconst <x.Type> x [d]))
+(RSBshiftRL (MOVWconst [c]) x [d]) => (SUBconst [c] (SRLconst <x.Type> x [d]))
+(RSBshiftRA (MOVWconst [c]) x [d]) => (SUBconst [c] (SRAconst <x.Type> x [d]))
+(RSBSshiftLL (MOVWconst [c]) x [d]) => (SUBSconst [c] (SLLconst <x.Type> x [d]))
+(RSBSshiftRL (MOVWconst [c]) x [d]) => (SUBSconst [c] (SRLconst <x.Type> x [d]))
+(RSBSshiftRA (MOVWconst [c]) x [d]) => (SUBSconst [c] (SRAconst <x.Type> x [d]))
+(ANDshiftLL (MOVWconst [c]) x [d]) => (ANDconst [c] (SLLconst <x.Type> x [d]))
+(ANDshiftRL (MOVWconst [c]) x [d]) => (ANDconst [c] (SRLconst <x.Type> x [d]))
+(ANDshiftRA (MOVWconst [c]) x [d]) => (ANDconst [c] (SRAconst <x.Type> x [d]))
+(ORshiftLL (MOVWconst [c]) x [d]) => (ORconst [c] (SLLconst <x.Type> x [d]))
+(ORshiftRL (MOVWconst [c]) x [d]) => (ORconst [c] (SRLconst <x.Type> x [d]))
+(ORshiftRA (MOVWconst [c]) x [d]) => (ORconst [c] (SRAconst <x.Type> x [d]))
+(XORshiftLL (MOVWconst [c]) x [d]) => (XORconst [c] (SLLconst <x.Type> x [d]))
+(XORshiftRL (MOVWconst [c]) x [d]) => (XORconst [c] (SRLconst <x.Type> x [d]))
+(XORshiftRA (MOVWconst [c]) x [d]) => (XORconst [c] (SRAconst <x.Type> x [d]))
+(XORshiftRR (MOVWconst [c]) x [d]) => (XORconst [c] (SRRconst <x.Type> x [d]))
+(CMPshiftLL (MOVWconst [c]) x [d]) => (InvertFlags (CMPconst [c] (SLLconst <x.Type> x [d])))
+(CMPshiftRL (MOVWconst [c]) x [d]) => (InvertFlags (CMPconst [c] (SRLconst <x.Type> x [d])))
+(CMPshiftRA (MOVWconst [c]) x [d]) => (InvertFlags (CMPconst [c] (SRAconst <x.Type> x [d])))
+(TSTshiftLL (MOVWconst [c]) x [d]) => (TSTconst [c] (SLLconst <x.Type> x [d]))
+(TSTshiftRL (MOVWconst [c]) x [d]) => (TSTconst [c] (SRLconst <x.Type> x [d]))
+(TSTshiftRA (MOVWconst [c]) x [d]) => (TSTconst [c] (SRAconst <x.Type> x [d]))
+(TEQshiftLL (MOVWconst [c]) x [d]) => (TEQconst [c] (SLLconst <x.Type> x [d]))
+(TEQshiftRL (MOVWconst [c]) x [d]) => (TEQconst [c] (SRLconst <x.Type> x [d]))
+(TEQshiftRA (MOVWconst [c]) x [d]) => (TEQconst [c] (SRAconst <x.Type> x [d]))
+(CMNshiftLL (MOVWconst [c]) x [d]) => (CMNconst [c] (SLLconst <x.Type> x [d]))
+(CMNshiftRL (MOVWconst [c]) x [d]) => (CMNconst [c] (SRLconst <x.Type> x [d]))
+(CMNshiftRA (MOVWconst [c]) x [d]) => (CMNconst [c] (SRAconst <x.Type> x [d]))
+
+// constant folding in *shift ops
+(ADDshiftLL x (MOVWconst [c]) [d]) => (ADDconst x [c<<uint64(d)])
+(ADDshiftRL x (MOVWconst [c]) [d]) => (ADDconst x [int32(uint32(c)>>uint64(d))])
+(ADDshiftRA x (MOVWconst [c]) [d]) => (ADDconst x [c>>uint64(d)])
+(ADCshiftLL x (MOVWconst [c]) [d] flags) => (ADCconst x [c<<uint64(d)] flags)
+(ADCshiftRL x (MOVWconst [c]) [d] flags) => (ADCconst x [int32(uint32(c)>>uint64(d))] flags)
+(ADCshiftRA x (MOVWconst [c]) [d] flags) => (ADCconst x [c>>uint64(d)] flags)
+(ADDSshiftLL x (MOVWconst [c]) [d]) => (ADDSconst x [c<<uint64(d)])
+(ADDSshiftRL x (MOVWconst [c]) [d]) => (ADDSconst x [int32(uint32(c)>>uint64(d))])
+(ADDSshiftRA x (MOVWconst [c]) [d]) => (ADDSconst x [c>>uint64(d)])
+(SUBshiftLL x (MOVWconst [c]) [d]) => (SUBconst x [c<<uint64(d)])
+(SUBshiftRL x (MOVWconst [c]) [d]) => (SUBconst x [int32(uint32(c)>>uint64(d))])
+(SUBshiftRA x (MOVWconst [c]) [d]) => (SUBconst x [c>>uint64(d)])
+(SBCshiftLL x (MOVWconst [c]) [d] flags) => (SBCconst x [c<<uint64(d)] flags)
+(SBCshiftRL x (MOVWconst [c]) [d] flags) => (SBCconst x [int32(uint32(c)>>uint64(d))] flags)
+(SBCshiftRA x (MOVWconst [c]) [d] flags) => (SBCconst x [c>>uint64(d)] flags)
+(SUBSshiftLL x (MOVWconst [c]) [d]) => (SUBSconst x [c<<uint64(d)])
+(SUBSshiftRL x (MOVWconst [c]) [d]) => (SUBSconst x [int32(uint32(c)>>uint64(d))])
+(SUBSshiftRA x (MOVWconst [c]) [d]) => (SUBSconst x [c>>uint64(d)])
+(RSBshiftLL x (MOVWconst [c]) [d]) => (RSBconst x [c<<uint64(d)])
+(RSBshiftRL x (MOVWconst [c]) [d]) => (RSBconst x [int32(uint32(c)>>uint64(d))])
+(RSBshiftRA x (MOVWconst [c]) [d]) => (RSBconst x [c>>uint64(d)])
+(RSBSshiftLL x (MOVWconst [c]) [d]) => (RSBSconst x [c<<uint64(d)])
+(RSBSshiftRL x (MOVWconst [c]) [d]) => (RSBSconst x [int32(uint32(c)>>uint64(d))])
+(RSBSshiftRA x (MOVWconst [c]) [d]) => (RSBSconst x [c>>uint64(d)])
+(ANDshiftLL x (MOVWconst [c]) [d]) => (ANDconst x [c<<uint64(d)])
+(ANDshiftRL x (MOVWconst [c]) [d]) => (ANDconst x [int32(uint32(c)>>uint64(d))])
+(ANDshiftRA x (MOVWconst [c]) [d]) => (ANDconst x [c>>uint64(d)])
+(ORshiftLL x (MOVWconst [c]) [d]) => (ORconst x [c<<uint64(d)])
+(ORshiftRL x (MOVWconst [c]) [d]) => (ORconst x [int32(uint32(c)>>uint64(d))])
+(ORshiftRA x (MOVWconst [c]) [d]) => (ORconst x [c>>uint64(d)])
+(ORNshiftLL x (MOVWconst [c]) [d]) => (ORNconst x [c<<uint64(d)])
+(ORNshiftRL x (MOVWconst [c]) [d]) => (ORNconst x [int32(uint32(c)>>uint64(d))])
+(ORNshiftRA x (MOVWconst [c]) [d]) => (ORNconst x [c>>uint64(d)])
+(XORshiftLL x (MOVWconst [c]) [d]) => (XORconst x [c<<uint64(d)])
+(XORshiftRL x (MOVWconst [c]) [d]) => (XORconst x [int32(uint32(c)>>uint64(d))])
+(XORshiftRA x (MOVWconst [c]) [d]) => (XORconst x [c>>uint64(d)])
+(XORshiftRR x (MOVWconst [c]) [d]) => (XORconst x [int32(uint32(c)>>uint64(d)|uint32(c)<<uint64(32-d))])
+(BICshiftLL x (MOVWconst [c]) [d]) => (BICconst x [c<<uint64(d)])
+(BICshiftRL x (MOVWconst [c]) [d]) => (BICconst x [int32(uint32(c)>>uint64(d))])
+(BICshiftRA x (MOVWconst [c]) [d]) => (BICconst x [c>>uint64(d)])
+(MVNshiftLL (MOVWconst [c]) [d]) => (MOVWconst [^(c<<uint64(d))])
+(MVNshiftRL (MOVWconst [c]) [d]) => (MOVWconst [^int32(uint32(c)>>uint64(d))])
+(MVNshiftRA (MOVWconst [c]) [d]) => (MOVWconst [^(int32(c)>>uint64(d))])
+(CMPshiftLL x (MOVWconst [c]) [d]) => (CMPconst x [c<<uint64(d)])
+(CMPshiftRL x (MOVWconst [c]) [d]) => (CMPconst x [int32(uint32(c)>>uint64(d))])
+(CMPshiftRA x (MOVWconst [c]) [d]) => (CMPconst x [c>>uint64(d)])
+(TSTshiftLL x (MOVWconst [c]) [d]) => (TSTconst x [c<<uint64(d)])
+(TSTshiftRL x (MOVWconst [c]) [d]) => (TSTconst x [int32(uint32(c)>>uint64(d))])
+(TSTshiftRA x (MOVWconst [c]) [d]) => (TSTconst x [c>>uint64(d)])
+(TEQshiftLL x (MOVWconst [c]) [d]) => (TEQconst x [c<<uint64(d)])
+(TEQshiftRL x (MOVWconst [c]) [d]) => (TEQconst x [int32(uint32(c)>>uint64(d))])
+(TEQshiftRA x (MOVWconst [c]) [d]) => (TEQconst x [c>>uint64(d)])
+(CMNshiftLL x (MOVWconst [c]) [d]) => (CMNconst x [c<<uint64(d)])
+(CMNshiftRL x (MOVWconst [c]) [d]) => (CMNconst x [int32(uint32(c)>>uint64(d))])
+(CMNshiftRA x (MOVWconst [c]) [d]) => (CMNconst x [c>>uint64(d)])
+
+// Generate rotates
+(ADDshiftLL [c] (SRLconst x [32-c]) x) => (SRRconst [32-c] x)
+( ORshiftLL [c] (SRLconst x [32-c]) x) => (SRRconst [32-c] x)
+(XORshiftLL [c] (SRLconst x [32-c]) x) => (SRRconst [32-c] x)
+(ADDshiftRL [c] (SLLconst x [32-c]) x) => (SRRconst [   c] x)
+( ORshiftRL [c] (SLLconst x [32-c]) x) => (SRRconst [   c] x)
+(XORshiftRL [c] (SLLconst x [32-c]) x) => (SRRconst [   c] x)
+
+(RotateLeft32 x (MOVWconst [c])) => (SRRconst [-c&31] x)
+(RotateLeft16 <t> x (MOVWconst [c])) => (Or16 (Lsh16x32 <t> x (MOVWconst [c&15])) (Rsh16Ux32 <t> x (MOVWconst [-c&15])))
+(RotateLeft8 <t> x (MOVWconst [c])) => (Or8 (Lsh8x32 <t> x (MOVWconst [c&7])) (Rsh8Ux32 <t> x (MOVWconst [-c&7])))
+(RotateLeft32 x y) => (SRR x (RSBconst [0] <y.Type> y))
+
+// ((x>>8) | (x<<8)) => (REV16 x), the type of x is uint16, "|" can also be "^" or "+".
+((ADDshiftLL|ORshiftLL|XORshiftLL) <typ.UInt16> [8] (BFXU <typ.UInt16> [int32(armBFAuxInt(8, 8))] x) x) => (REV16 x)
+((ADDshiftLL|ORshiftLL|XORshiftLL) <typ.UInt16> [8] (SRLconst <typ.UInt16> [24] (SLLconst [16] x)) x) => (REV16 x)
+
+// use indexed loads and stores
+(MOVWload [0] {sym} (ADD ptr idx) mem) && sym == nil => (MOVWloadidx ptr idx mem)
+(MOVHload [0] {sym} (ADD ptr idx) mem) && sym == nil => (MOVHloadidx ptr idx mem)
+(MOVHUload [0] {sym} (ADD ptr idx) mem) && sym == nil => (MOVHUloadidx ptr idx mem)
+(MOVBload [0] {sym} (ADD ptr idx) mem) && sym == nil => (MOVBloadidx ptr idx mem)
+(MOVBUload [0] {sym} (ADD ptr idx) mem) && sym == nil => (MOVBUloadidx ptr idx mem)
+(LoadOnce8 [0] (ADD ptr idx) mem) => (LoadOnce8idx ptr idx mem)
+(LoadOnce16 [0] (ADD ptr idx) mem) => (LoadOnce16idx ptr idx mem)
+(LoadOnce32 [0] (ADD ptr idx) mem) => (LoadOnce32idx ptr idx  mem)
+
+(MOVWstore [0] {sym} (ADD ptr idx) val mem) && sym == nil => (MOVWstoreidx ptr idx val mem)
+(MOVBstore [0] {sym} (ADD ptr idx) val mem) && sym == nil => (MOVBstoreidx ptr idx val mem)
+(MOVHstore [0] {sym} (ADD ptr idx) val mem) && sym == nil => (MOVHstoreidx ptr idx val mem)
+(StoreOnce8 [0] (ADD ptr idx) val mem)  => (StoreOnce8idx ptr idx val mem)
+(StoreOnce16 [0] (ADD ptr idx) val mem) => (StoreOnce16idx ptr idx val mem)
+(StoreOnce32 [0] (ADD ptr idx) val mem) => (StoreOnce32idx ptr idx val mem)
+
+(MOVWload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 => (MOVWloadshiftLL ptr idx [c] mem)
+(MOVHload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 => (MOVHloadshiftLL ptr idx [c] mem)
+(MOVHUload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 => (MOVHUloadshiftLL ptr idx [c] mem)
+(MOVBload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 => (MOVBloadshiftLL ptr idx [c] mem)
+(MOVBUload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 => (MOVBUloadshiftLL ptr idx [c] mem)
+(LoadOnce8 [0] (ADDshiftLL ptr idx [c]) mem) && c <= 3 => (LoadOnce8shiftLL ptr idx [c] mem)
+(LoadOnce16 [0] (ADDshiftLL ptr idx [c]) mem) && c <= 3 => (LoadOnce16shiftLL ptr idx [c] mem)
+(LoadOnce32 [0] (ADDshiftLL ptr idx [c]) mem) && c <= 3 => (LoadOnce32shiftLL ptr idx [c] mem)
+
+(MOVWstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 => (MOVWstoreshiftLL ptr idx [c] val mem)
+(MOVHstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 => (MOVHstoreshiftLL ptr idx [c] val mem)
+(MOVBstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 => (MOVBstoreshiftLL ptr idx [c] val mem)
+(StoreOnce8 [0] (ADDshiftLL ptr idx [c]) val mem) && c <= 3 => (StoreOnce8shiftLL ptr idx [c] val mem)
+(StoreOnce16 [0] (ADDshiftLL ptr idx [c]) val mem) && c <= 3 => (StoreOnce16shiftLL ptr idx [c] val mem)
+(StoreOnce32 [0] (ADDshiftLL ptr idx [c]) val mem) && c <= 3 => (StoreOnce32shiftLL ptr idx [c] val mem)
+
+// constant folding in indexed loads and stores
+(MOVWloadidx ptr (MOVWconst [c]) mem) => (MOVWload [c] ptr mem)
+(MOVWloadidx (MOVWconst [c]) ptr mem) => (MOVWload [c] ptr mem)
+(MOVHloadidx ptr (MOVWconst [c]) mem) => (MOVHload [c] ptr mem)
+(MOVHloadidx (MOVWconst [c]) ptr mem) => (MOVHload [c] ptr mem)
+(MOVHUloadidx ptr (MOVWconst [c]) mem) => (MOVHUload [c] ptr mem)
+(MOVHUloadidx (MOVWconst [c]) ptr mem) => (MOVHUload [c] ptr mem)
+(MOVBloadidx ptr (MOVWconst [c]) mem) => (MOVBload [c] ptr mem)
+(MOVBloadidx (MOVWconst [c]) ptr mem) => (MOVBload [c] ptr mem)
+(MOVBUloadidx ptr (MOVWconst [c]) mem) => (MOVBUload [c] ptr mem)
+(MOVBUloadidx (MOVWconst [c]) ptr mem) => (MOVBUload [c] ptr mem)
+(LoadOnce32idx ptr (MOVWconst [c]) mem) => (LoadOnce32 [c] ptr mem)
+(LoadOnce32idx (MOVWconst [c]) ptr mem) => (LoadOnce32 [c] ptr mem)
+(LoadOnce16idx ptr (MOVWconst [c]) mem) => (LoadOnce16 [c] ptr mem)
+(LoadOnce16idx (MOVWconst [c]) ptr mem) => (LoadOnce16 [c] ptr mem)
+(LoadOnce8idx ptr (MOVWconst [c]) mem) => (LoadOnce8 [c] ptr mem)
+(LoadOnce8idx (MOVWconst [c]) ptr mem) => (LoadOnce8 [c] ptr mem)
+
+(MOVWstoreidx ptr (MOVWconst [c]) val mem) => (MOVWstore [c] ptr val mem)
+(MOVWstoreidx (MOVWconst [c]) ptr val mem) => (MOVWstore [c] ptr val mem)
+(MOVHstoreidx ptr (MOVWconst [c]) val mem) => (MOVHstore [c] ptr val mem)
+(MOVHstoreidx (MOVWconst [c]) ptr val mem) => (MOVHstore [c] ptr val mem)
+(MOVBstoreidx ptr (MOVWconst [c]) val mem) => (MOVBstore [c] ptr val mem)
+(MOVBstoreidx (MOVWconst [c]) ptr val mem) => (MOVBstore [c] ptr val mem)
+(StoreOnce32idx ptr (MOVWconst [c]) val mem) => (StoreOnce32 [c] ptr val mem)
+(StoreOnce32idx (MOVWconst [c]) ptr val mem) => (StoreOnce32 [c] ptr val mem)
+(StoreOnce16idx ptr (MOVWconst [c]) val mem) => (StoreOnce16 [c] ptr val mem)
+(StoreOnce16idx (MOVWconst [c]) ptr val mem) => (StoreOnce16 [c] ptr val mem)
+(StoreOnce8idx ptr (MOVWconst [c]) val mem) => (StoreOnce8 [c] ptr val mem)
+(StoreOnce8idx (MOVWconst [c]) ptr val mem) => (StoreOnce8 [c] ptr val mem)
+
+(MOVWloadidx ptr (SLLconst idx [c]) mem) && c <= 3 => (MOVWloadshiftLL ptr idx [c] mem)
+(MOVWloadidx (SLLconst idx [c]) ptr mem) && c <= 3 => (MOVWloadshiftLL ptr idx [c] mem)
+(MOVHUloadidx ptr (SLLconst idx [c]) mem) && c <= 3 => (MOVHUloadshiftLL ptr idx [c] mem)
+(MOVHUloadidx (SLLconst idx [c]) ptr mem) && c <= 3 => (MOVHUloadshiftLL ptr idx [c] mem)
+(MOVHloadidx ptr (SLLconst idx [c]) mem) && c <= 3 => (MOVHloadshiftLL ptr idx [c] mem)
+(MOVHloadidx (SLLconst idx [c]) ptr mem) && c <= 3 => (MOVHloadshiftLL ptr idx [c] mem)
+(MOVBUloadidx ptr (SLLconst idx [c]) mem) && c <= 3 => (MOVBUloadshiftLL ptr idx [c] mem)
+(MOVBUloadidx (SLLconst idx [c]) ptr mem) && c <= 3 => (MOVBUloadshiftLL ptr idx [c] mem)
+(MOVBloadidx ptr (SLLconst idx [c]) mem) && c <= 3 => (MOVBloadshiftLL ptr idx [c] mem)
+(MOVBloadidx (SLLconst idx [c]) ptr mem) && c <= 3 => (MOVBloadshiftLL ptr idx [c] mem)
+(LoadOnce32idx ptr (SLLconst idx [c]) mem) && c <= 3 => (LoadOnce32shiftLL ptr idx [c] mem)
+(LoadOnce16idx ptr (SLLconst idx [c]) mem) && c <= 3 => (LoadOnce16shiftLL ptr idx [c] mem)
+(LoadOnce8idx ptr (SLLconst idx [c]) mem) && c <= 3 => (LoadOnce8shiftLL ptr idx [c] mem)
+
+(MOVWstoreidx ptr (SLLconst idx [c]) val mem) && c <= 3 => (MOVWstoreshiftLL ptr idx [c] val mem)
+(MOVWstoreidx (SLLconst idx [c]) ptr val mem) && c <= 3 => (MOVWstoreshiftLL ptr idx [c] val mem)
+(MOVHstoreidx ptr (SLLconst idx [c]) val mem) && c <= 3 => (MOVHstoreshiftLL ptr idx [c] val mem)
+(MOVHstoreidx (SLLconst idx [c]) ptr val mem) && c <= 3 => (MOVHstoreshiftLL ptr idx [c] val mem)
+(MOVBstoreidx ptr (SLLconst idx [c]) val mem) && c <= 3 => (MOVBstoreshiftLL ptr idx [c] val mem)
+(MOVBstoreidx (SLLconst idx [c]) ptr val mem) && c <= 3 => (MOVBstoreshiftLL ptr idx [c] val mem)
+(StoreOnce32idx ptr (SLLconst idx [c]) val mem) && c <= 3 => (StoreOnce32shiftLL ptr idx [c] val mem)
+(StoreOnce32idx (SLLconst idx [c]) ptr val mem) && c <= 3 => (StoreOnce32shiftLL ptr idx [c] val mem)
+(StoreOnce16idx ptr (SLLconst idx [c]) val mem) && c <= 3 => (StoreOnce16shiftLL ptr idx [c] val mem)
+(StoreOnce16idx (SLLconst idx [c]) ptr val mem) && c <= 3 => (StoreOnce16shiftLL ptr idx [c] val mem)
+(StoreOnce8idx ptr (SLLconst idx [c]) val mem) && c <= 3 => (StoreOnce8shiftLL ptr idx [c] val mem)
+(StoreOnce8idx (SLLconst idx [c]) ptr val mem) && c <= 3 => (StoreOnce8shiftLL ptr idx [c] val mem)
+
+(MOVWloadshiftLL ptr (MOVWconst [c]) [d] mem) => (MOVWload [int32(uint32(c)<<uint64(d))] ptr mem)
+(MOVHUloadshiftLL ptr (MOVWconst [c]) [d] mem) => (MOVHUload [int32(uint32(c)<<uint64(d))] ptr mem)
+(MOVHloadshiftLL ptr (MOVWconst [c]) [d] mem) => (MOVHload [int32(uint32(c)<<uint64(d))] ptr mem)
+(MOVBUloadshiftLL ptr (MOVWconst [c]) [d] mem) => (MOVBUload [int32(uint32(c)<<uint64(d))] ptr mem)
+(MOVBloadshiftLL ptr (MOVWconst [c]) [d] mem) => (MOVBload [int32(uint32(c)<<uint64(d))] ptr mem)
+
+(MOVWstoreshiftLL ptr (MOVWconst [c]) [d] val mem) => (MOVWstore [int32(uint32(c)<<uint64(d))] ptr val mem)
+(MOVHstoreshiftLL ptr (MOVWconst [c]) [d] val mem) => (MOVHstore [int32(uint32(c)<<uint64(d))] ptr val mem)
+(MOVBstoreshiftLL ptr (MOVWconst [c]) [d] val mem) => (MOVBstore [int32(uint32(c)<<uint64(d))] ptr val mem)
+
+// generic simplifications
+(ADD x (RSBconst [0] y)) => (SUB x y)
+(ADD <t> (RSBconst [c] x) (RSBconst [d] y)) => (RSBconst [c+d] (ADD <t> x y))
+(SUB x x) => (MOVWconst [0])
+(RSB x x) => (MOVWconst [0])
+(AND x x) => x
+(OR x x) => x
+(ORN x x) => (MOVWconst [-1])
+(XOR x x) => (MOVWconst [0])
+(BIC x x) => (MOVWconst [0])
+
+(ADD (MUL x y) a) => (MULA x y a)
+(SUB a (MUL x y)) => (MULS x y a)
+(RSB (MUL x y) a) => (MULS x y a)
+
+(NEGF (MULF x y)) => (NMULF x y)
+(NEGD (MULD x y)) => (NMULD x y)
+(MULF (NEGF x) y) => (NMULF x y)
+(MULD (NEGD x) y) => (NMULD x y)
+(NMULF (NEGF x) y) => (MULF x y)
+(NMULD (NEGD x) y) => (MULD x y)
+
+// the result will overwrite the addend, since they are in the same register
+(ADDF a (MULF x y)) && a.Uses == 1 => (MULAF a x y)
+(ADDF a (NMULF x y)) && a.Uses == 1 => (MULSF a x y)
+(ADDD a (MULD x y)) && a.Uses == 1 => (MULAD a x y)
+(ADDD a (NMULD x y)) && a.Uses == 1 => (MULSD a x y)
+(SUBF a (MULF x y)) && a.Uses == 1 => (MULSF a x y)
+(SUBF a (NMULF x y)) && a.Uses == 1 => (MULAF a x y)
+(SUBD a (MULD x y)) && a.Uses == 1 => (MULSD a x y)
+(SUBD a (NMULD x y)) && a.Uses == 1 => (MULAD a x y)
+
+(AND x (MVN y)) => (BIC x y)
+(OR  x (MVN y)) => (ORN x y)
+
+// simplification with *shift ops
+(SUBshiftLL x (SLLconst x [c]) [d]) && c==d => (MOVWconst [0])
+(SUBshiftRL x (SRLconst x [c]) [d]) && c==d => (MOVWconst [0])
+(SUBshiftRA x (SRAconst x [c]) [d]) && c==d => (MOVWconst [0])
+(RSBshiftLL x (SLLconst x [c]) [d]) && c==d => (MOVWconst [0])
+(RSBshiftRL x (SRLconst x [c]) [d]) && c==d => (MOVWconst [0])
+(RSBshiftRA x (SRAconst x [c]) [d]) && c==d => (MOVWconst [0])
+(ANDshiftLL x y:(SLLconst x [c]) [d]) && c==d => y
+(ANDshiftRL x y:(SRLconst x [c]) [d]) && c==d => y
+(ANDshiftRA x y:(SRAconst x [c]) [d]) && c==d => y
+(ORshiftLL x y:(SLLconst x [c]) [d]) && c==d => y
+(ORshiftRL x y:(SRLconst x [c]) [d]) && c==d => y
+(ORshiftRA x y:(SRAconst x [c]) [d]) && c==d => y
+(XORshiftLL x (SLLconst x [c]) [d]) && c==d => (MOVWconst [0])
+(XORshiftRL x (SRLconst x [c]) [d]) && c==d => (MOVWconst [0])
+(XORshiftRA x (SRAconst x [c]) [d]) && c==d => (MOVWconst [0])
+(BICshiftLL x (SLLconst x [c]) [d]) && c==d => (MOVWconst [0])
+(BICshiftRL x (SRLconst x [c]) [d]) && c==d => (MOVWconst [0])
+(BICshiftRA x (SRAconst x [c]) [d]) && c==d => (MOVWconst [0])
+(AND x (MVNshiftLL y [c])) => (BICshiftLL x y [c])
+(AND x (MVNshiftRL y [c])) => (BICshiftRL x y [c])
+(AND x (MVNshiftRA y [c])) => (BICshiftRA x y [c])
+
+// floating point optimizations
+(CMPF x (MOVFconst [0])) => (CMPF0 x)
+(CMPD x (MOVDconst [0])) => (CMPD0 x)
+
+// bit extraction
+(SRAconst (SLLconst x [c]) [d]) && uint64(d)>=uint64(c) && uint64(d)<=31 => (BFX [(d-c)|(32-d)<<8] x)
+(SRLconst (SLLconst x [c]) [d]) && uint64(d)>=uint64(c) && uint64(d)<=31 => (BFXU [(d-c)|(32-d)<<8] x)
+
+// comparison simplification
+((LT|LE|EQ|NE|GE|GT) (CMP x (RSBconst [0] y))) => ((LT|LE|EQ|NE|GE|GT) (CMN x y)) // sense of carry bit not preserved
+((LT|LE|EQ|NE|GE|GT) (CMN x (RSBconst [0] y))) => ((LT|LE|EQ|NE|GE|GT) (CMP x y)) // sense of carry bit not preserved
+(EQ (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 => (EQ (CMP x y) yes no)
+(EQ (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 => (EQ (CMP a (MUL <x.Type> x y)) yes no)
+(EQ (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 => (EQ (CMPconst [c] x) yes no)
+(EQ (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 => (EQ (CMPshiftLL x y [c]) yes no)
+(EQ (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 => (EQ (CMPshiftRL x y [c]) yes no)
+(EQ (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 => (EQ (CMPshiftRA x y [c]) yes no)
+(NE (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 => (NE (CMP x y) yes no)
+(NE (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 => (NE (CMP a (MUL <x.Type> x y)) yes no)
+(NE (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 => (NE (CMPconst [c] x) yes no)
+(NE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 => (NE (CMPshiftLL x y [c]) yes no)
+(NE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 => (NE (CMPshiftRL x y [c]) yes no)
+(NE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 => (NE (CMPshiftRA x y [c]) yes no)
+(EQ (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 => (EQ (CMN x y) yes no)
+(EQ (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 => (EQ (CMN a (MUL <x.Type> x y)) yes no)
+(EQ (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 => (EQ (CMNconst [c] x) yes no)
+(EQ (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 => (EQ (CMNshiftLL x y [c]) yes no)
+(EQ (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 => (EQ (CMNshiftRL x y [c]) yes no)
+(EQ (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 => (EQ (CMNshiftRA x y [c]) yes no)
+(NE (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 => (NE (CMN x y) yes no)
+(NE (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 => (NE (CMN a (MUL <x.Type> x y)) yes no)
+(NE (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 => (NE (CMNconst [c] x) yes no)
+(NE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 => (NE (CMNshiftLL x y [c]) yes no)
+(NE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 => (NE (CMNshiftRL x y [c]) yes no)
+(NE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 => (NE (CMNshiftRA x y [c]) yes no)
+(EQ (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 => (EQ (TST x y) yes no)
+(EQ (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 => (EQ (TSTconst [c] x) yes no)
+(EQ (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 => (EQ (TSTshiftLL x y [c]) yes no)
+(EQ (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 => (EQ (TSTshiftRL x y [c]) yes no)
+(EQ (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 => (EQ (TSTshiftRA x y [c]) yes no)
+(NE (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 => (NE (TST x y) yes no)
+(NE (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 => (NE (TSTconst [c] x) yes no)
+(NE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 => (NE (TSTshiftLL x y [c]) yes no)
+(NE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 => (NE (TSTshiftRL x y [c]) yes no)
+(NE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 => (NE (TSTshiftRA x y [c]) yes no)
+(EQ (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 => (EQ (TEQ x y) yes no)
+(EQ (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 => (EQ (TEQconst [c] x) yes no)
+(EQ (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 => (EQ (TEQshiftLL x y [c]) yes no)
+(EQ (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 => (EQ (TEQshiftRL x y [c]) yes no)
+(EQ (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 => (EQ (TEQshiftRA x y [c]) yes no)
+(NE (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 => (NE (TEQ x y) yes no)
+(NE (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 => (NE (TEQconst [c] x) yes no)
+(NE (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 => (NE (TEQshiftLL x y [c]) yes no)
+(NE (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 => (NE (TEQshiftRL x y [c]) yes no)
+(NE (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 => (NE (TEQshiftRA x y [c]) yes no)
+(LT (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 => (LTnoov (CMP x y) yes no)
+(LT (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 => (LTnoov (CMP a (MUL <x.Type> x y)) yes no)
+(LT (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 => (LTnoov (CMPconst [c] x) yes no)
+(LT (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 => (LTnoov (CMPshiftLL x y [c]) yes no)
+(LT (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 => (LTnoov (CMPshiftRL x y [c]) yes no)
+(LT (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 => (LTnoov (CMPshiftRA x y [c]) yes no)
+(LE (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 => (LEnoov (CMP x y) yes no)
+(LE (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 => (LEnoov (CMP a (MUL <x.Type> x y)) yes no)
+(LE (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 => (LEnoov (CMPconst [c] x) yes no)
+(LE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 => (LEnoov (CMPshiftLL x y [c]) yes no)
+(LE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 => (LEnoov (CMPshiftRL x y [c]) yes no)
+(LE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 => (LEnoov (CMPshiftRA x y [c]) yes no)
+(LT (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 => (LTnoov (CMN x y) yes no)
+(LT (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 => (LTnoov (CMN a (MUL <x.Type> x y)) yes no)
+(LT (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 => (LTnoov (CMNconst [c] x) yes no)
+(LT (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 => (LTnoov (CMNshiftLL x y [c]) yes no)
+(LT (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 => (LTnoov (CMNshiftRL x y [c]) yes no)
+(LT (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 => (LTnoov (CMNshiftRA x y [c]) yes no)
+(LE (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 => (LEnoov (CMN x y) yes no)
+(LE (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 => (LEnoov (CMN a (MUL <x.Type> x y)) yes no)
+(LE (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1  => (LEnoov (CMNconst [c] x) yes no)
+(LE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 => (LEnoov (CMNshiftLL x y [c]) yes no)
+(LE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 => (LEnoov (CMNshiftRL x y [c]) yes no)
+(LE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 => (LEnoov (CMNshiftRA x y [c]) yes no)
+(LT (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 => (LTnoov (TST x y) yes no)
+(LT (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 => (LTnoov (TSTconst [c] x) yes no)
+(LT (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 => (LTnoov (TSTshiftLL x y [c]) yes no)
+(LT (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 => (LTnoov (TSTshiftRL x y [c]) yes no)
+(LT (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 => (LTnoov (TSTshiftRA x y [c]) yes no)
+(LE (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 => (LEnoov (TST x y) yes no)
+(LE (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 => (LEnoov (TSTconst [c] x) yes no)
+(LE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 => (LEnoov (TSTshiftLL x y [c]) yes no)
+(LE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 => (LEnoov (TSTshiftRL x y [c]) yes no)
+(LE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 => (LEnoov (TSTshiftRA x y [c]) yes no)
+(LT (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 => (LTnoov (TEQ x y) yes no)
+(LT (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 => (LTnoov (TEQconst [c] x) yes no)
+(LT (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 => (LTnoov (TEQshiftLL x y [c]) yes no)
+(LT (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 => (LTnoov (TEQshiftRL x y [c]) yes no)
+(LT (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 => (LTnoov (TEQshiftRA x y [c]) yes no)
+(LE (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 => (LEnoov (TEQ x y) yes no)
+(LE (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1  => (LEnoov (TEQconst [c] x) yes no)
+(LE (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 => (LEnoov (TEQshiftLL x y [c]) yes no)
+(LE (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 => (LEnoov (TEQshiftRL x y [c]) yes no)
+(LE (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 => (LEnoov (TEQshiftRA x y [c]) yes no)
+(GT (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 => (GTnoov (CMP x y) yes no)
+(GT (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 => (GTnoov (CMP a (MUL <x.Type> x y)) yes no)
+(GT (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 => (GTnoov (CMPconst [c] x) yes no)
+(GT (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 => (GTnoov (CMPshiftLL x y [c]) yes no)
+(GT (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 => (GTnoov (CMPshiftRL x y [c]) yes no)
+(GT (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 => (GTnoov (CMPshiftRA x y [c]) yes no)
+(GE (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 => (GEnoov (CMP x y) yes no)
+(GE (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 => (GEnoov (CMP a (MUL <x.Type> x y)) yes no)
+(GE (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 => (GEnoov (CMPconst [c] x) yes no)
+(GE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 => (GEnoov (CMPshiftLL x y [c]) yes no)
+(GE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 => (GEnoov (CMPshiftRL x y [c]) yes no)
+(GE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 => (GEnoov (CMPshiftRA x y [c]) yes no)
+(GT (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 => (GTnoov (CMN x y) yes no)
+(GT (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 => (GTnoov (CMNconst [c] x) yes no)
+(GT (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 => (GTnoov (CMNshiftLL x y [c]) yes no)
+(GT (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 => (GTnoov (CMNshiftRL x y [c]) yes no)
+(GT (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 => (GTnoov (CMNshiftRA x y [c]) yes no)
+(GE (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 => (GEnoov (CMN x y) yes no)
+(GE (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 => (GEnoov (CMN a (MUL <x.Type> x y)) yes no)
+(GE (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 => (GEnoov (CMNconst [c] x) yes no)
+(GE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 => (GEnoov (CMNshiftLL x y [c]) yes no)
+(GE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 => (GEnoov (CMNshiftRL x y [c]) yes no)
+(GE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 => (GEnoov (CMNshiftRA x y [c]) yes no)
+(GT (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 => (GTnoov (CMN a (MUL <x.Type> x y)) yes no)
+(GT (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 => (GTnoov (TST x y) yes no)
+(GT (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 => (GTnoov (TSTconst [c] x) yes no)
+(GT (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 => (GTnoov (TSTshiftLL x y [c]) yes no)
+(GT (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 => (GTnoov (TSTshiftRL x y [c]) yes no)
+(GT (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 => (GTnoov (TSTshiftRA x y [c]) yes no)
+(GE (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 => (GEnoov (TST x y) yes no)
+(GE (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 => (GEnoov (TSTconst [c] x) yes no)
+(GE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 => (GEnoov (TSTshiftLL x y [c]) yes no)
+(GE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 => (GEnoov (TSTshiftRL x y [c]) yes no)
+(GE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 => (GEnoov (TSTshiftRA x y [c]) yes no)
+(GT (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 => (GTnoov (TEQ x y) yes no)
+(GT (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 => (GTnoov (TEQconst [c] x) yes no)
+(GT (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 => (GTnoov (TEQshiftLL x y [c]) yes no)
+(GT (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 => (GTnoov (TEQshiftRL x y [c]) yes no)
+(GT (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 => (GTnoov (TEQshiftRA x y [c]) yes no)
+(GE (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 => (GEnoov (TEQ x y) yes no)
+(GE (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 => (GEnoov (TEQconst [c] x) yes no)
+(GE (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 => (GEnoov (TEQshiftLL x y [c]) yes no)
+(GE (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 => (GEnoov (TEQshiftRL x y [c]) yes no)
+(GE (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 => (GEnoov (TEQshiftRA x y [c]) yes no)
+
+(MOVBUload [off] {sym} (SB) _) && symIsRO(sym) => (MOVWconst [int32(read8(sym, int64(off)))])
+(MOVHUload [off] {sym} (SB) _) && symIsRO(sym) => (MOVWconst [int32(read16(sym, int64(off), config.ctxt.Arch.ByteOrder))])
+(MOVWload [off] {sym} (SB) _) && symIsRO(sym) => (MOVWconst [int32(read32(sym, int64(off), config.ctxt.Arch.ByteOrder))])
diff --git a/src/cmd/compile/internal/ssa/gen/ThumbOps.go b/src/cmd/compile/internal/ssa/gen/ThumbOps.go
new file mode 100644
index 0000000000..110a029ab7
--- /dev/null
+++ b/src/cmd/compile/internal/ssa/gen/ThumbOps.go
@@ -0,0 +1,562 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build ignore
+
+package main
+
+import "strings"
+
+// Notes:
+//  - Integer types live in the low portion of registers. Upper portions are junk.
+//  - Boolean types use the low-order byte of a register. 0=false, 1=true.
+//    Upper bytes are junk.
+//  - *const instructions may use a constant larger than the instruction can encode.
+//    In this case the assembler expands to multiple instructions and uses tmp
+//    register (R7).
+
+// Suffixes encode the bit width of various instructions.
+// W (word)      = 32 bit
+// H (half word) = 16 bit
+// HU            = 16 bit unsigned
+// B (byte)      = 8 bit
+// BU            = 8 bit unsigned
+// F (float)     = 32 bit float
+// D (double)    = 64 bit float
+
+var regNamesThumb = []string{
+	"R0",
+	"R1",
+	"R2",
+	"R3",
+	"R4",
+	"R5",
+	"R6",
+	"R7", // tmp
+	"R8",
+	"R9",
+	"g", // aka R10
+	"R11",
+	"R12",
+	"SP",  // aka R13
+	"R14", // link
+	"R15", // pc
+
+	"F0",
+	"F1",
+	"F2",
+	"F3",
+	"F4",
+	"F5",
+	"F6",
+	"F7",
+	"F8",
+	"F9",
+	"F10",
+	"F11",
+	"F12",
+	"F13",
+	"F14",
+	"F15", // tmp
+
+	// If you add registers, update asyncPreempt in runtime.
+
+	// pseudo-registers
+	"SB",
+}
+
+func init() {
+	// Make map from reg names to reg integers.
+	if len(regNamesThumb) > 64 {
+		panic("too many registers")
+	}
+	num := map[string]int{}
+	for i, name := range regNamesThumb {
+		num[name] = i
+	}
+	buildReg := func(s string) regMask {
+		m := regMask(0)
+		for _, r := range strings.Split(s, " ") {
+			if n, ok := num[r]; ok {
+				m |= regMask(1) << uint(n)
+				continue
+			}
+			panic("register " + r + " not found")
+		}
+		return m
+	}
+
+	// Common individual register masks
+	var (
+		gp         = buildReg("R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14")
+		gpg        = gp | buildReg("g")
+		gpsp       = gp | buildReg("SP")
+		gpspg      = gpg | buildReg("SP")
+		gpspsbg    = gpspg | buildReg("SB")
+		fp         = buildReg("F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15")
+		callerSave = gp | fp | buildReg("g") // runtime.setg (and anything calling it) may clobber g
+		r0         = buildReg("R0")
+		r1         = buildReg("R1")
+		r2         = buildReg("R2")
+		r3         = buildReg("R3")
+		r4         = buildReg("R4")
+	)
+	// Common regInfo
+	var (
+		gp01      = regInfo{inputs: nil, outputs: []regMask{gp}}
+		gp11      = regInfo{inputs: []regMask{gpg}, outputs: []regMask{gp}}
+		gp11carry = regInfo{inputs: []regMask{gpg}, outputs: []regMask{gp, 0}}
+		gp11sp    = regInfo{inputs: []regMask{gpspg}, outputs: []regMask{gp}}
+		gp1flags  = regInfo{inputs: []regMask{gpg}}
+		gp1flags1 = regInfo{inputs: []regMask{gp}, outputs: []regMask{gp}}
+		gp21      = regInfo{inputs: []regMask{gpg, gpg}, outputs: []regMask{gp}}
+		gp21carry = regInfo{inputs: []regMask{gpg, gpg}, outputs: []regMask{gp, 0}}
+		gp2flags  = regInfo{inputs: []regMask{gpg, gpg}}
+		gp2flags1 = regInfo{inputs: []regMask{gp, gp}, outputs: []regMask{gp}}
+		gp22      = regInfo{inputs: []regMask{gpg, gpg}, outputs: []regMask{gp, gp}}
+		gp31      = regInfo{inputs: []regMask{gp, gp, gp}, outputs: []regMask{gp}}
+		gpload    = regInfo{inputs: []regMask{gpspsbg}, outputs: []regMask{gp}}
+		gpstore   = regInfo{inputs: []regMask{gpspsbg, gpg}}
+		gp2load   = regInfo{inputs: []regMask{gpspsbg, gpg}, outputs: []regMask{gp}}
+		gp2store  = regInfo{inputs: []regMask{gpspsbg, gpg, gpg}}
+		fp01      = regInfo{inputs: nil, outputs: []regMask{fp}}
+		fp11      = regInfo{inputs: []regMask{fp}, outputs: []regMask{fp}}
+		fp1flags  = regInfo{inputs: []regMask{fp}}
+		fpgp      = regInfo{inputs: []regMask{fp}, outputs: []regMask{gp}, clobbers: buildReg("F15")} // int-float conversion uses F15 as tmp
+		gpfp      = regInfo{inputs: []regMask{gp}, outputs: []regMask{fp}, clobbers: buildReg("F15")}
+		fp21      = regInfo{inputs: []regMask{fp, fp}, outputs: []regMask{fp}}
+		fp31      = regInfo{inputs: []regMask{fp, fp, fp}, outputs: []regMask{fp}}
+		fp2flags  = regInfo{inputs: []regMask{fp, fp}}
+		fpload    = regInfo{inputs: []regMask{gpspsbg}, outputs: []regMask{fp}}
+		fpstore   = regInfo{inputs: []regMask{gpspsbg, fp}}
+		readflags = regInfo{inputs: nil, outputs: []regMask{gp}}
+	)
+	// Thumb instruction can be encoded as 32 or 16 bit - assembler decides. Unlike arm, many thumb
+	// instruction encodigs don't support .S bit. Especially all 16-bit data processing instructions
+	// don't support it and can clobber flags if outside of IT block.
+	//
+	// For example, for ADD instruction assembler chooses shortest encoding that may but may not set
+	// flags. For ADD.S it chooses shortest encoding that sets flags according to the result. For
+	// ADD.P assembler generates shortest encoding that don't touch flags.
+	//
+	// Current compiler have no idea about 16/32 bit instructions, IT blocks, .P modifier, so any data
+	// processing instructions tha can be 16-bit encoded should be marked clobberFlags. Even *const
+	// instructions that have no direct 16-bit encoding can be encoded by assebler as pair of
+	// `load litoffset(PC), REGTMP; op REGTMP, Rdn` and can clober flags.
+	//
+	// TODO: Use .P modifier if appropriate.
+	ops := []opData{
+		// binary ops
+		{name: "ADD", argLength: 2, reg: gp21, asm: "ADD", commutative: true, clobberFlags: true},      // arg0 + arg1
+		{name: "ADDconst", argLength: 1, reg: gp11sp, asm: "ADD", aux: "Int32", clobberFlags: true},    // arg0 + auxInt
+		{name: "SUB", argLength: 2, reg: gp21, asm: "SUB", clobberFlags: true},                         // arg0 - arg1
+		{name: "SUBconst", argLength: 1, reg: gp11, asm: "SUB", aux: "Int32", clobberFlags: true},      // arg0 - auxInt
+		{name: "RSB", argLength: 2, reg: gp21, asm: "RSB", clobberFlags: true},                         // arg1 - arg0
+		{name: "RSBconst", argLength: 1, reg: gp11, asm: "RSB", aux: "Int32", clobberFlags: true},      // auxInt - arg0
+		{name: "MUL", argLength: 2, reg: gp21, asm: "MUL", commutative: true, clobberFlags: true},      // arg0 * arg1
+		{name: "HMUL", argLength: 2, reg: gp21, asm: "MULL", commutative: true},                        // (arg0 * arg1) >> 32, signed
+		{name: "HMULU", argLength: 2, reg: gp21, asm: "MULLU", commutative: true},                      // (arg0 * arg1) >> 32, unsigned
+		{name: "DIV", argLength: 2, reg: gp21, asm: "DIV"},                                             // arg0 / arg1, signed
+		{name: "DIVU", argLength: 2, reg: gp21, asm: "DIVU"},                                           // arg0 / arg1, unsigned
+		{name: "ADDS", argLength: 2, reg: gp21carry, asm: "ADD", commutative: true},                    // arg0 + arg1, set carry flag
+		{name: "ADDSconst", argLength: 1, reg: gp11carry, asm: "ADD", aux: "Int32"},                    // arg0 + auxInt, set carry flag
+		{name: "ADC", argLength: 3, reg: gp2flags1, asm: "ADC", commutative: true, clobberFlags: true}, // arg0 + arg1 + carry, arg2=flags
+		{name: "ADCconst", argLength: 2, reg: gp1flags1, asm: "ADC", aux: "Int32", clobberFlags: true}, // arg0 + auxInt + carry, arg1=flags
+		{name: "SUBS", argLength: 2, reg: gp21carry, asm: "SUB"},                                       // arg0 - arg1, set carry flag
+		{name: "SUBSconst", argLength: 1, reg: gp11carry, asm: "SUB", aux: "Int32"},                    // arg0 - auxInt, set carry flag
+		{name: "RSBSconst", argLength: 1, reg: gp11carry, asm: "RSB", aux: "Int32"},                    // auxInt - arg0, set carry flag
+		{name: "SBC", argLength: 3, reg: gp2flags1, asm: "SBC", clobberFlags: true},                    // arg0 - arg1 - carry, arg2=flags
+		{name: "SBCconst", argLength: 2, reg: gp1flags1, asm: "SBC", aux: "Int32", clobberFlags: true}, // arg0 - auxInt - carry, arg1=flags
+
+		{name: "MULLU", argLength: 2, reg: gp22, asm: "MULLU", commutative: true}, // arg0 * arg1, high 32 bits in out0, low 32 bits in out1
+		{name: "MULA", argLength: 3, reg: gp31, asm: "MULA"},                      // arg0 * arg1 + arg2
+		{name: "MULS", argLength: 3, reg: gp31, asm: "MULS"},                      // arg2 - arg0 * arg1
+
+		{name: "ADDF", argLength: 2, reg: fp21, asm: "ADDF", commutative: true},   // arg0 + arg1
+		{name: "ADDD", argLength: 2, reg: fp21, asm: "ADDD", commutative: true},   // arg0 + arg1
+		{name: "SUBF", argLength: 2, reg: fp21, asm: "SUBF"},                      // arg0 - arg1
+		{name: "SUBD", argLength: 2, reg: fp21, asm: "SUBD"},                      // arg0 - arg1
+		{name: "MULF", argLength: 2, reg: fp21, asm: "MULF", commutative: true},   // arg0 * arg1
+		{name: "MULD", argLength: 2, reg: fp21, asm: "MULD", commutative: true},   // arg0 * arg1
+		{name: "NMULF", argLength: 2, reg: fp21, asm: "NMULF", commutative: true}, // -(arg0 * arg1)
+		{name: "NMULD", argLength: 2, reg: fp21, asm: "NMULD", commutative: true}, // -(arg0 * arg1)
+		{name: "DIVF", argLength: 2, reg: fp21, asm: "DIVF"},                      // arg0 / arg1
+		{name: "DIVD", argLength: 2, reg: fp21, asm: "DIVD"},                      // arg0 / arg1
+
+		{name: "MULAF", argLength: 3, reg: fp31, asm: "MULAF", resultInArg0: true}, // arg0 + (arg1 * arg2)
+		{name: "MULAD", argLength: 3, reg: fp31, asm: "MULAD", resultInArg0: true}, // arg0 + (arg1 * arg2)
+		{name: "MULSF", argLength: 3, reg: fp31, asm: "MULSF", resultInArg0: true}, // arg0 - (arg1 * arg2)
+		{name: "MULSD", argLength: 3, reg: fp31, asm: "MULSD", resultInArg0: true}, // arg0 - (arg1 * arg2)
+
+		// FMULAD only exists on platforms with the VFPv4 instruction set.
+		// Any use must be preceded by a successful check of runtime.arm_support_vfpv4.
+		{name: "FMULAD", argLength: 3, reg: fp31, asm: "FMULAD", resultInArg0: true}, // arg0 + (arg1 * arg2)
+
+		{name: "AND", argLength: 2, reg: gp21, asm: "AND", commutative: true, clobberFlags: true}, // arg0 & arg1
+		{name: "ANDconst", argLength: 1, reg: gp11, asm: "AND", aux: "Int32", clobberFlags: true}, // arg0 & auxInt
+		{name: "OR", argLength: 2, reg: gp21, asm: "ORR", commutative: true, clobberFlags: true},  // arg0 | arg1
+		{name: "ORconst", argLength: 1, reg: gp11, asm: "ORR", aux: "Int32", clobberFlags: true},  // arg0 | auxInt
+		{name: "ORN", argLength: 2, reg: gp21, asm: "ORN", clobberFlags: true},                    // arg0 | ^arg1
+		{name: "ORNconst", argLength: 1, reg: gp11, asm: "ORN", aux: "Int32", clobberFlags: true}, // arg0 | ^auxInt
+		{name: "XOR", argLength: 2, reg: gp21, asm: "EOR", commutative: true, clobberFlags: true}, // arg0 ^ arg1
+		{name: "XORconst", argLength: 1, reg: gp11, asm: "EOR", aux: "Int32", clobberFlags: true}, // arg0 ^ auxInt
+		{name: "BIC", argLength: 2, reg: gp21, asm: "BIC", clobberFlags: true},                    // arg0 &^ arg1
+		{name: "BICconst", argLength: 1, reg: gp11, asm: "BIC", aux: "Int32", clobberFlags: true}, // arg0 &^ auxInt
+
+		// bit extractio1n, AuxInt = Width<<8 | LSB
+		{name: "BFX", argLength: 1, reg: gp11, asm: "BFX", aux: "Int32"},   // extract W bits from bit L in arg0, then signed extend
+		{name: "BFXU", argLength: 1, reg: gp11, asm: "BFXU", aux: "Int32"}, // extract W bits from bit L in arg0, then unsigned extend
+
+		// unary ops
+		{name: "MVN", argLength: 1, reg: gp11, asm: "MVN", clobberFlags: true}, // ^arg0
+
+		{name: "NEGF", argLength: 1, reg: fp11, asm: "NEGF"},   // -arg0, float32
+		{name: "NEGD", argLength: 1, reg: fp11, asm: "NEGD"},   // -arg0, float64
+		{name: "SQRTD", argLength: 1, reg: fp11, asm: "SQRTD"}, // sqrt(arg0), float64
+		{name: "ABSD", argLength: 1, reg: fp11, asm: "ABSD"},   // abs(arg0), float64
+
+		{name: "CLZ", argLength: 1, reg: gp11, asm: "CLZ"},     // count leading zero
+		{name: "REV", argLength: 1, reg: gp11, asm: "REV"},     // reverse byte order
+		{name: "REV16", argLength: 1, reg: gp11, asm: "REV16"}, // reverse byte order in 16-bit halfwords
+		{name: "RBIT", argLength: 1, reg: gp11, asm: "RBIT"},   // reverse bit order
+
+		// shifts
+		{name: "SLL", argLength: 2, reg: gp21, asm: "SLL", clobberFlags: true},                    // arg0 << arg1, shift amount is mod 256
+		{name: "SLLconst", argLength: 1, reg: gp11, asm: "SLL", aux: "Int32", clobberFlags: true}, // arg0 << auxInt
+		{name: "SRL", argLength: 2, reg: gp21, asm: "SRL", clobberFlags: true},                    // arg0 >> arg1, unsigned, shift amount is mod 256
+		{name: "SRLconst", argLength: 1, reg: gp11, asm: "SRL", aux: "Int32", clobberFlags: true}, // arg0 >> auxInt, unsigned
+		{name: "SRA", argLength: 2, reg: gp21, asm: "SRA", clobberFlags: true},                    // arg0 >> arg1, signed, shift amount is mod 256
+		{name: "SRAconst", argLength: 1, reg: gp11, asm: "SRA", aux: "Int32", clobberFlags: true}, // arg0 >> auxInt, signed
+		{name: "SRR", argLength: 2, reg: gp21, asm: "SRR", clobberFlags: true},                    // arg0 right rotate by arg1 bits
+		{name: "SRRconst", argLength: 1, reg: gp11, asm: "SRR", aux: "Int32", clobberFlags: true}, // arg0 right rotate by auxInt bits
+
+		{name: "ADDshiftLL", argLength: 2, reg: gp21, asm: "ADD", aux: "Int32"}, // arg0 + arg1<<auxInt
+		{name: "ADDshiftRL", argLength: 2, reg: gp21, asm: "ADD", aux: "Int32"}, // arg0 + arg1>>auxInt, unsigned shift
+		{name: "ADDshiftRA", argLength: 2, reg: gp21, asm: "ADD", aux: "Int32"}, // arg0 + arg1>>auxInt, signed shift
+		{name: "SUBshiftLL", argLength: 2, reg: gp21, asm: "SUB", aux: "Int32"}, // arg0 - arg1<<auxInt
+		{name: "SUBshiftRL", argLength: 2, reg: gp21, asm: "SUB", aux: "Int32"}, // arg0 - arg1>>auxInt, unsigned shift
+		{name: "SUBshiftRA", argLength: 2, reg: gp21, asm: "SUB", aux: "Int32"}, // arg0 - arg1>>auxInt, signed shift
+		{name: "RSBshiftLL", argLength: 2, reg: gp21, asm: "RSB", aux: "Int32"}, // arg1<<auxInt - arg0
+		{name: "RSBshiftRL", argLength: 2, reg: gp21, asm: "RSB", aux: "Int32"}, // arg1>>auxInt - arg0, unsigned shift
+		{name: "RSBshiftRA", argLength: 2, reg: gp21, asm: "RSB", aux: "Int32"}, // arg1>>auxInt - arg0, signed shift
+		{name: "ANDshiftLL", argLength: 2, reg: gp21, asm: "AND", aux: "Int32"}, // arg0 & (arg1<<auxInt)
+		{name: "ANDshiftRL", argLength: 2, reg: gp21, asm: "AND", aux: "Int32"}, // arg0 & (arg1>>auxInt), unsigned shift
+		{name: "ANDshiftRA", argLength: 2, reg: gp21, asm: "AND", aux: "Int32"}, // arg0 & (arg1>>auxInt), signed shift
+		{name: "ORshiftLL", argLength: 2, reg: gp21, asm: "ORR", aux: "Int32"},  // arg0 | arg1<<auxInt
+		{name: "ORshiftRL", argLength: 2, reg: gp21, asm: "ORR", aux: "Int32"},  // arg0 | arg1>>auxInt, unsigned shift
+		{name: "ORshiftRA", argLength: 2, reg: gp21, asm: "ORR", aux: "Int32"},  // arg0 | arg1>>auxInt, signed shift
+		{name: "ORNshiftLL", argLength: 2, reg: gp21, asm: "ORN", aux: "Int32"}, // arg0 | ^(arg1<<auxInt)
+		{name: "ORNshiftRL", argLength: 2, reg: gp21, asm: "ORN", aux: "Int32"}, // arg0 | ^(arg1>>auxInt), unsigned shift
+		{name: "ORNshiftRA", argLength: 2, reg: gp21, asm: "ORN", aux: "Int32"}, // arg0 | ^(arg1>>auxInt), signed shift
+		{name: "XORshiftLL", argLength: 2, reg: gp21, asm: "EOR", aux: "Int32"}, // arg0 ^ arg1<<auxInt
+		{name: "XORshiftRL", argLength: 2, reg: gp21, asm: "EOR", aux: "Int32"}, // arg0 ^ arg1>>auxInt, unsigned shift
+		{name: "XORshiftRA", argLength: 2, reg: gp21, asm: "EOR", aux: "Int32"}, // arg0 ^ arg1>>auxInt, signed shift
+		{name: "XORshiftRR", argLength: 2, reg: gp21, asm: "EOR", aux: "Int32"}, // arg0 ^ (arg1 right rotate by auxInt)
+		{name: "BICshiftLL", argLength: 2, reg: gp21, asm: "BIC", aux: "Int32"}, // arg0 &^ (arg1<<auxInt)
+		{name: "BICshiftRL", argLength: 2, reg: gp21, asm: "BIC", aux: "Int32"}, // arg0 &^ (arg1>>auxInt), unsigned shift
+		{name: "BICshiftRA", argLength: 2, reg: gp21, asm: "BIC", aux: "Int32"}, // arg0 &^ (arg1>>auxInt), signed shift
+		{name: "MVNshiftLL", argLength: 1, reg: gp11, asm: "MVN", aux: "Int32"}, // ^(arg0<<auxInt)
+		{name: "MVNshiftRL", argLength: 1, reg: gp11, asm: "MVN", aux: "Int32"}, // ^(arg0>>auxInt), unsigned shift
+		{name: "MVNshiftRA", argLength: 1, reg: gp11, asm: "MVN", aux: "Int32"}, // ^(arg0>>auxInt), signed shift
+
+		{name: "ADCshiftLL", argLength: 3, reg: gp2flags1, asm: "ADC", aux: "Int32"}, // arg0 + arg1<<auxInt + carry, arg2=flags
+		{name: "ADCshiftRL", argLength: 3, reg: gp2flags1, asm: "ADC", aux: "Int32"}, // arg0 + arg1>>auxInt + carry, unsigned shift, arg2=flags
+		{name: "ADCshiftRA", argLength: 3, reg: gp2flags1, asm: "ADC", aux: "Int32"}, // arg0 + arg1>>auxInt + carry, signed shift, arg2=flags
+		{name: "SBCshiftLL", argLength: 3, reg: gp2flags1, asm: "SBC", aux: "Int32"}, // arg0 - arg1<<auxInt - carry, arg2=flags
+		{name: "SBCshiftRL", argLength: 3, reg: gp2flags1, asm: "SBC", aux: "Int32"}, // arg0 - arg1>>auxInt - carry, unsigned shift, arg2=flags
+		{name: "SBCshiftRA", argLength: 3, reg: gp2flags1, asm: "SBC", aux: "Int32"}, // arg0 - arg1>>auxInt - carry, signed shift, arg2=flags
+
+		{name: "ADDSshiftLL", argLength: 2, reg: gp21carry, asm: "ADD", aux: "Int32"}, // arg0 + arg1<<auxInt, set carry flag
+		{name: "ADDSshiftRL", argLength: 2, reg: gp21carry, asm: "ADD", aux: "Int32"}, // arg0 + arg1>>auxInt, unsigned shift, set carry flag
+		{name: "ADDSshiftRA", argLength: 2, reg: gp21carry, asm: "ADD", aux: "Int32"}, // arg0 + arg1>>auxInt, signed shift, set carry flag
+		{name: "SUBSshiftLL", argLength: 2, reg: gp21carry, asm: "SUB", aux: "Int32"}, // arg0 - arg1<<auxInt, set carry flag
+		{name: "SUBSshiftRL", argLength: 2, reg: gp21carry, asm: "SUB", aux: "Int32"}, // arg0 - arg1>>auxInt, unsigned shift, set carry flag
+		{name: "SUBSshiftRA", argLength: 2, reg: gp21carry, asm: "SUB", aux: "Int32"}, // arg0 - arg1>>auxInt, signed shift, set carry flag
+		{name: "RSBSshiftLL", argLength: 2, reg: gp21carry, asm: "RSB", aux: "Int32"}, // arg1<<auxInt - arg0, set carry flag
+		{name: "RSBSshiftRL", argLength: 2, reg: gp21carry, asm: "RSB", aux: "Int32"}, // arg1>>auxInt - arg0, unsigned shift, set carry flag
+		{name: "RSBSshiftRA", argLength: 2, reg: gp21carry, asm: "RSB", aux: "Int32"}, // arg1>>auxInt - arg0, signed shift, set carry flag
+
+		// comparisons
+		{name: "CMP", argLength: 2, reg: gp2flags, asm: "CMP", typ: "Flags"},                    // arg0 compare to arg1
+		{name: "CMPconst", argLength: 1, reg: gp1flags, asm: "CMP", aux: "Int32", typ: "Flags"}, // arg0 compare to auxInt
+		{name: "CMN", argLength: 2, reg: gp2flags, asm: "CMN", typ: "Flags", commutative: true}, // arg0 compare to -arg1
+		{name: "CMNconst", argLength: 1, reg: gp1flags, asm: "CMN", aux: "Int32", typ: "Flags"}, // arg0 compare to -auxInt
+		{name: "TST", argLength: 2, reg: gp2flags, asm: "TST", typ: "Flags", commutative: true}, // arg0 & arg1 compare to 0
+		{name: "TSTconst", argLength: 1, reg: gp1flags, asm: "TST", aux: "Int32", typ: "Flags"}, // arg0 & auxInt compare to 0
+		{name: "TEQ", argLength: 2, reg: gp2flags, asm: "TEQ", typ: "Flags", commutative: true}, // arg0 ^ arg1 compare to 0
+		{name: "TEQconst", argLength: 1, reg: gp1flags, asm: "TEQ", aux: "Int32", typ: "Flags"}, // arg0 ^ auxInt compare to 0
+		{name: "CMPF", argLength: 2, reg: fp2flags, asm: "CMPF", typ: "Flags"},                  // arg0 compare to arg1, float32
+		{name: "CMPD", argLength: 2, reg: fp2flags, asm: "CMPD", typ: "Flags"},                  // arg0 compare to arg1, float64
+
+		{name: "CMPshiftLL", argLength: 2, reg: gp2flags, asm: "CMP", aux: "Int32", typ: "Flags"}, // arg0 compare to arg1<<auxInt
+		{name: "CMPshiftRL", argLength: 2, reg: gp2flags, asm: "CMP", aux: "Int32", typ: "Flags"}, // arg0 compare to arg1>>auxInt, unsigned shift
+		{name: "CMPshiftRA", argLength: 2, reg: gp2flags, asm: "CMP", aux: "Int32", typ: "Flags"}, // arg0 compare to arg1>>auxInt, signed shift
+		{name: "CMNshiftLL", argLength: 2, reg: gp2flags, asm: "CMN", aux: "Int32", typ: "Flags"}, // arg0 compare to -(arg1<<auxInt)
+		{name: "CMNshiftRL", argLength: 2, reg: gp2flags, asm: "CMN", aux: "Int32", typ: "Flags"}, // arg0 compare to -(arg1>>auxInt), unsigned shift
+		{name: "CMNshiftRA", argLength: 2, reg: gp2flags, asm: "CMN", aux: "Int32", typ: "Flags"}, // arg0 compare to -(arg1>>auxInt), signed shift
+		{name: "TSTshiftLL", argLength: 2, reg: gp2flags, asm: "TST", aux: "Int32", typ: "Flags"}, // arg0 & (arg1<<auxInt) compare to 0
+		{name: "TSTshiftRL", argLength: 2, reg: gp2flags, asm: "TST", aux: "Int32", typ: "Flags"}, // arg0 & (arg1>>auxInt) compare to 0, unsigned shift
+		{name: "TSTshiftRA", argLength: 2, reg: gp2flags, asm: "TST", aux: "Int32", typ: "Flags"}, // arg0 & (arg1>>auxInt) compare to 0, signed shift
+		{name: "TEQshiftLL", argLength: 2, reg: gp2flags, asm: "TEQ", aux: "Int32", typ: "Flags"}, // arg0 ^ (arg1<<auxInt) compare to 0
+		{name: "TEQshiftRL", argLength: 2, reg: gp2flags, asm: "TEQ", aux: "Int32", typ: "Flags"}, // arg0 ^ (arg1>>auxInt) compare to 0, unsigned shift
+		{name: "TEQshiftRA", argLength: 2, reg: gp2flags, asm: "TEQ", aux: "Int32", typ: "Flags"}, // arg0 ^ (arg1>>auxInt) compare to 0, signed shift
+
+		{name: "CMPF0", argLength: 1, reg: fp1flags, asm: "CMPF", typ: "Flags"}, // arg0 compare to 0, float32
+		{name: "CMPD0", argLength: 1, reg: fp1flags, asm: "CMPD", typ: "Flags"}, // arg0 compare to 0, float64
+
+		// moves
+		//{name: "MOVWconst", argLength: 0, reg: gp01, aux: "Int32", asm: "MOVW", typ: "UInt32", rematerializeable: true}, // 32 low bits of auxint
+		{name: "MOVWconst", argLength: 0, reg: gp01, aux: "Int32", asm: "MOVW", typ: "UInt32", clobberFlags: true},         // 32 low bits of auxint
+		{name: "MOVFconst", argLength: 0, reg: fp01, aux: "Float64", asm: "MOVF", typ: "Float32", rematerializeable: true}, // auxint as 64-bit float, convert to 32-bit float
+		{name: "MOVDconst", argLength: 0, reg: fp01, aux: "Float64", asm: "MOVD", typ: "Float64", rematerializeable: true}, // auxint as 64-bit float
+
+		{name: "MOVWaddr", argLength: 1, reg: regInfo{inputs: []regMask{buildReg("SP") | buildReg("SB")}, outputs: []regMask{gp}}, aux: "SymOff", asm: "MOVW", rematerializeable: true, symEffect: "Addr"}, // arg0 + auxInt + aux.(*gc.Sym), arg0=SP/SB
+
+		{name: "MOVBload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVB", typ: "Int8", faultOnNilArg0: true, symEffect: "Read"},     // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVBUload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVBU", typ: "UInt8", faultOnNilArg0: true, symEffect: "Read"},  // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVHload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVH", typ: "Int16", faultOnNilArg0: true, symEffect: "Read"},    // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVHUload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVHU", typ: "UInt16", faultOnNilArg0: true, symEffect: "Read"}, // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVWload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVW", typ: "UInt32", faultOnNilArg0: true, symEffect: "Read"},   // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVFload", argLength: 2, reg: fpload, aux: "SymOff", asm: "MOVF", typ: "Float32", faultOnNilArg0: true, symEffect: "Read"},  // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVDload", argLength: 2, reg: fpload, aux: "SymOff", asm: "MOVD", typ: "Float64", faultOnNilArg0: true, symEffect: "Read"},  // load from arg0 + auxInt + aux.  arg1=mem.
+
+		{name: "MOVBstore", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVB", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 1 byte of arg1 to arg0 + auxInt + aux.  arg2=mem.
+		{name: "MOVHstore", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVH", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 2 bytes of arg1 to arg0 + auxInt + aux.  arg2=mem.
+		{name: "MOVWstore", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVW", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 4 bytes of arg1 to arg0 + auxInt + aux.  arg2=mem.
+		{name: "MOVFstore", argLength: 3, reg: fpstore, aux: "SymOff", asm: "MOVF", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 4 bytes of arg1 to arg0 + auxInt + aux.  arg2=mem.
+		{name: "MOVDstore", argLength: 3, reg: fpstore, aux: "SymOff", asm: "MOVD", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 8 bytes of arg1 to arg0 + auxInt + aux.  arg2=mem.
+
+		{name: "MOVWloadidx", argLength: 3, reg: gp2load, asm: "MOVW", typ: "UInt32"},                     // load from arg0 + arg1. arg2=mem
+		{name: "MOVHloadidx", argLength: 3, reg: gp2load, asm: "MOVH", typ: "Int16"},                      // load from arg0 + arg1. arg2=mem
+		{name: "MOVHUloadidx", argLength: 3, reg: gp2load, asm: "MOVHU", typ: "UInt16"},                   // load from arg0 + arg1. arg2=mem
+		{name: "MOVBloadidx", argLength: 3, reg: gp2load, asm: "MOVB", typ: "Int8"},                       // load from arg0 + arg1. arg2=mem
+		{name: "MOVBUloadidx", argLength: 3, reg: gp2load, asm: "MOVBU", typ: "UInt8"},                    // load from arg0 + arg1. arg2=mem
+		{name: "MOVWloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVW", aux: "Int32", typ: "UInt32"},   // load from arg0 + arg1<<auxInt. arg2=mem
+		{name: "MOVHloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVH", aux: "Int32", typ: "Int16"},    // load from arg0 + arg1<<auxInt. arg2=mem
+		{name: "MOVHUloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVHU", aux: "Int32", typ: "UInt16"}, // load from arg0 + arg1<<auxInt. arg2=mem
+		{name: "MOVBloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVB", aux: "Int32", typ: "Int8"},     // load from arg0 + arg1<<auxInt. arg2=mem
+		{name: "MOVBUloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVBU", aux: "Int32", typ: "UInt8"},  // load from arg0 + arg1<<auxInt. arg2=mem
+
+		{name: "MOVWstoreidx", argLength: 4, reg: gp2store, asm: "MOVW", typ: "Mem"},                   // store arg2 to arg0 + arg1. arg3=mem
+		{name: "MOVBstoreidx", argLength: 4, reg: gp2store, asm: "MOVB", typ: "Mem"},                   // store arg2 to arg0 + arg1. arg3=mem
+		{name: "MOVHstoreidx", argLength: 4, reg: gp2store, asm: "MOVH", typ: "Mem"},                   // store arg2 to arg0 + arg1. arg3=mem
+		{name: "MOVWstoreshiftLL", argLength: 4, reg: gp2store, asm: "MOVW", aux: "Int32", typ: "Mem"}, // store arg2 to arg0 + arg1<<auxInt. arg3=mem
+		{name: "MOVHstoreshiftLL", argLength: 4, reg: gp2store, asm: "MOVH", aux: "Int32", typ: "Mem"}, // store arg2 to arg0 + arg1<<auxInt. arg3=mem
+		{name: "MOVBstoreshiftLL", argLength: 4, reg: gp2store, asm: "MOVB", aux: "Int32", typ: "Mem"}, // store arg2 to arg0 + arg1<<auxInt. arg3=mem
+
+		{name: "MOVBreg", argLength: 1, reg: gp11, asm: "MOVBS"},  // move from arg0, sign-extended from byte
+		{name: "MOVBUreg", argLength: 1, reg: gp11, asm: "MOVBU"}, // move from arg0, unsign-extended from byte
+		{name: "MOVHreg", argLength: 1, reg: gp11, asm: "MOVHS"},  // move from arg0, sign-extended from half
+		{name: "MOVHUreg", argLength: 1, reg: gp11, asm: "MOVHU"}, // move from arg0, unsign-extended from half
+		{name: "MOVWreg", argLength: 1, reg: gp11, asm: "MOVW"},   // move from arg0
+
+		{name: "MOVWnop", argLength: 1, reg: regInfo{inputs: []regMask{gp}, outputs: []regMask{gp}}, resultInArg0: true}, // nop, return arg0 in same register
+
+		{name: "MOVWF", argLength: 1, reg: gpfp, asm: "MOVWF"},  // int32 -> float32
+		{name: "MOVWD", argLength: 1, reg: gpfp, asm: "MOVWD"},  // int32 -> float64
+		{name: "MOVWUF", argLength: 1, reg: gpfp, asm: "MOVWF"}, // uint32 -> float32, set U bit in the instruction
+		{name: "MOVWUD", argLength: 1, reg: gpfp, asm: "MOVWD"}, // uint32 -> float64, set U bit in the instruction
+		{name: "MOVFW", argLength: 1, reg: fpgp, asm: "MOVFW"},  // float32 -> int32
+		{name: "MOVDW", argLength: 1, reg: fpgp, asm: "MOVDW"},  // float64 -> int32
+		{name: "MOVFWU", argLength: 1, reg: fpgp, asm: "MOVFW"}, // float32 -> uint32, set U bit in the instruction
+		{name: "MOVDWU", argLength: 1, reg: fpgp, asm: "MOVDW"}, // float64 -> uint32, set U bit in the instruction
+		{name: "MOVFD", argLength: 1, reg: fp11, asm: "MOVFD"},  // float32 -> float64
+		{name: "MOVDF", argLength: 1, reg: fp11, asm: "MOVDF"},  // float64 -> float32
+
+		// conditional instructions, for lowering shifts
+		{name: "CMOVWHSconst", argLength: 2, reg: gp1flags1, asm: "MOVW", aux: "Int32", resultInArg0: true}, // replace arg0 w/ const if flags indicates HS, arg1=flags
+		{name: "CMOVWLSconst", argLength: 2, reg: gp1flags1, asm: "MOVW", aux: "Int32", resultInArg0: true}, // replace arg0 w/ const if flags indicates LS, arg1=flags
+		{name: "SRAcond", argLength: 3, reg: gp2flags1, asm: "SRA"},                                         // arg0 >> 31 if flags indicates HS, arg0 >> arg1 otherwise, signed shift, arg2=flags
+
+		// function calls
+		{name: "CALLstatic", argLength: 1, reg: regInfo{clobbers: callerSave}, aux: "CallOff", clobberFlags: true, call: true},                                               // call static function aux.(*obj.LSym).  arg0=mem, auxint=argsize, returns mem
+		{name: "CALLclosure", argLength: 3, reg: regInfo{inputs: []regMask{gpsp, buildReg("R11"), 0}, clobbers: callerSave}, aux: "CallOff", clobberFlags: true, call: true}, // call function via closure.  arg0=codeptr, arg1=closure, arg2=mem, auxint=argsize, returns mem
+		{name: "CALLinter", argLength: 2, reg: regInfo{inputs: []regMask{gp}, clobbers: callerSave}, aux: "CallOff", clobberFlags: true, call: true},                         // call fn by pointer.  arg0=codeptr, arg1=mem, auxint=argsize, returns mem
+
+		{name: "LoadOnce8", argLength: 2, reg: gpload, aux: "Int32", asm: "MOVBU", typ: "(UInt8,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce16", argLength: 2, reg: gpload, aux: "Int32", asm: "MOVHU", typ: "(UInt16,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce32", argLength: 2, reg: gpload, aux: "Int32", asm: "MOVW", typ: "(UInt32,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce8", argLength: 3, reg: gpstore, aux: "Int32", asm: "MOVB", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce16", argLength: 3, reg: gpstore, aux: "Int32", asm: "MOVH", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce32", argLength: 3, reg: gpstore, aux: "Int32", asm: "MOVW", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce8idx", argLength: 3, reg: gp2load, asm: "MOVBU", typ: "(UInt8,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce16idx", argLength: 3, reg: gp2load, asm: "MOVHU", typ: "(UInt16,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce32idx", argLength: 3, reg: gp2load, asm: "MOVW", typ: "(UInt32,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce8idx", argLength: 4, reg: gp2store, asm: "MOVB", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce16idx", argLength: 4, reg: gp2store, asm: "MOVH", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce32idx", argLength: 4, reg: gp2store, asm: "MOVW", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce8shiftLL", argLength: 3, reg: gp2load, aux: "Int32", asm: "MOVBU", typ: "(UInt8,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce16shiftLL", argLength: 3, reg: gp2load, aux: "Int32", asm: "MOVHU", typ: "(UInt16,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce32shiftLL", argLength: 3, reg: gp2load, aux: "Int32", asm: "MOVW", typ: "(UInt32,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce8shiftLL", argLength: 4, reg: gp2store, aux: "Int32", asm: "MOVB", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce16shiftLL", argLength: 4, reg: gp2store, aux: "Int32", asm: "MOVH", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce32shiftLL", argLength: 4, reg: gp2store, aux: "Int32", asm: "MOVW", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "DSB", argLength: 1, typ: "Mem", asm: "DSB", hasSideEffects: true},
+		{name: "DMB_ST", argLength: 1, typ: "Mem", asm: "DMB", hasSideEffects: true},
+
+		// pseudo-ops
+		{name: "LoweredNilCheck", argLength: 2, reg: regInfo{inputs: []regMask{gpg}}, nilCheck: true, faultOnNilArg0: true}, // panic if arg0 is nil.  arg1=mem.
+
+		{name: "Equal", argLength: 1, reg: readflags},         // bool, true flags encode x==y false otherwise.
+		{name: "NotEqual", argLength: 1, reg: readflags},      // bool, true flags encode x!=y false otherwise.
+		{name: "LessThan", argLength: 1, reg: readflags},      // bool, true flags encode signed x<y false otherwise.
+		{name: "LessEqual", argLength: 1, reg: readflags},     // bool, true flags encode signed x<=y false otherwise.
+		{name: "GreaterThan", argLength: 1, reg: readflags},   // bool, true flags encode signed x>y false otherwise.
+		{name: "GreaterEqual", argLength: 1, reg: readflags},  // bool, true flags encode signed x>=y false otherwise.
+		{name: "LessThanU", argLength: 1, reg: readflags},     // bool, true flags encode unsigned x<y false otherwise.
+		{name: "LessEqualU", argLength: 1, reg: readflags},    // bool, true flags encode unsigned x<=y false otherwise.
+		{name: "GreaterThanU", argLength: 1, reg: readflags},  // bool, true flags encode unsigned x>y false otherwise.
+		{name: "GreaterEqualU", argLength: 1, reg: readflags}, // bool, true flags encode unsigned x>=y false otherwise.
+
+		// duffzero (must be 4-byte aligned)
+		// arg0 = address of memory to zero (in R1, changed as side effect)
+		// arg1 = value to store (always zero)
+		// arg2 = mem
+		// auxint = offset into duffzero code to start executing
+		// returns mem
+		{
+			name:      "DUFFZERO",
+			aux:       "Int64",
+			argLength: 3,
+			reg: regInfo{
+				inputs:   []regMask{buildReg("R1"), buildReg("R0")},
+				clobbers: buildReg("R1 R14"),
+			},
+			faultOnNilArg0: true,
+		},
+
+		// duffcopy (must be 4-byte aligned)
+		// arg0 = address of dst memory (in R2, changed as side effect)
+		// arg1 = address of src memory (in R1, changed as side effect)
+		// arg2 = mem
+		// auxint = offset into duffcopy code to start executing
+		// returns mem
+		{
+			name:      "DUFFCOPY",
+			aux:       "Int64",
+			argLength: 3,
+			reg: regInfo{
+				inputs:   []regMask{buildReg("R2"), buildReg("R1")},
+				clobbers: buildReg("R0 R1 R2 R14"),
+			},
+			faultOnNilArg0: true,
+			faultOnNilArg1: true,
+		},
+
+		// large or unaligned zeroing
+		// arg0 = address of memory to zero (in R1, changed as side effect)
+		// arg1 = address of the last element to zero
+		// arg2 = value to store (always zero)
+		// arg3 = mem
+		// returns mem
+		//	MOVW.P	Rarg2, 4(R1)
+		//	CMP	R1, Rarg1
+		//	BLE	-2(PC)
+		{
+			name:      "LoweredZero",
+			aux:       "Int64",
+			argLength: 4,
+			reg: regInfo{
+				inputs:   []regMask{buildReg("R1"), gp, gp},
+				clobbers: buildReg("R1"),
+			},
+			clobberFlags:   true,
+			faultOnNilArg0: true,
+		},
+
+		// large or unaligned move
+		// arg0 = address of dst memory (in R2, changed as side effect)
+		// arg1 = address of src memory (in R1, changed as side effect)
+		// arg2 = address of the last element of src
+		// arg3 = mem
+		// returns mem
+		//	MOVW.P	4(R1), Rtmp
+		//	MOVW.P	Rtmp, 4(R2)
+		//	CMP	R1, Rarg2
+		//	BLE	-3(PC)
+		{
+			name:      "LoweredMove",
+			aux:       "Int64",
+			argLength: 4,
+			reg: regInfo{
+				inputs:   []regMask{buildReg("R2"), buildReg("R1"), gp},
+				clobbers: buildReg("R1 R2"),
+			},
+			clobberFlags:   true,
+			faultOnNilArg0: true,
+			faultOnNilArg1: true,
+		},
+
+		// Scheduler ensures LoweredGetClosurePtr occurs only in entry block,
+		// and sorts it to the very beginning of the block to prevent other
+		// use of R11 (thumb.REGCTXT, the closure pointer)
+		{name: "LoweredGetClosurePtr", reg: regInfo{outputs: []regMask{buildReg("R11")}}, zeroWidth: true},
+
+		// LoweredGetCallerSP returns the SP of the caller of the current function.
+		{name: "LoweredGetCallerSP", reg: gp01, rematerializeable: true},
+
+		// LoweredGetCallerPC evaluates to the PC to which its "caller" will return.
+		// I.e., if f calls g "calls" getcallerpc,
+		// the result should be the PC within f that g will return to.
+		// See runtime/stubs.go for a more detailed discussion.
+		{name: "LoweredGetCallerPC", reg: gp01, rematerializeable: true},
+
+		// There are three of these functions so that they can have three different register inputs.
+		// When we check 0 <= c <= cap (A), then 0 <= b <= c (B), then 0 <= a <= b (C), we want the
+		// default registers to match so we don't need to copy registers around unnecessarily.
+		{name: "LoweredPanicBoundsA", argLength: 3, aux: "Int64", reg: regInfo{inputs: []regMask{r2, r3}}, typ: "Mem", call: true}, // arg0=idx, arg1=len, arg2=mem, returns memory. AuxInt contains report code (see PanicBounds in genericOps.go).
+		{name: "LoweredPanicBoundsB", argLength: 3, aux: "Int64", reg: regInfo{inputs: []regMask{r1, r2}}, typ: "Mem", call: true}, // arg0=idx, arg1=len, arg2=mem, returns memory. AuxInt contains report code (see PanicBounds in genericOps.go).
+		{name: "LoweredPanicBoundsC", argLength: 3, aux: "Int64", reg: regInfo{inputs: []regMask{r0, r1}}, typ: "Mem", call: true}, // arg0=idx, arg1=len, arg2=mem, returns memory. AuxInt contains report code (see PanicBounds in genericOps.go).
+		// Extend ops are the same as Bounds ops except the indexes are 64-bit.
+		{name: "LoweredPanicExtendA", argLength: 4, aux: "Int64", reg: regInfo{inputs: []regMask{r4, r2, r3}}, typ: "Mem", call: true}, // arg0=idxHi, arg1=idxLo, arg2=len, arg3=mem, returns memory. AuxInt contains report code (see PanicExtend in genericOps.go).
+		{name: "LoweredPanicExtendB", argLength: 4, aux: "Int64", reg: regInfo{inputs: []regMask{r4, r1, r2}}, typ: "Mem", call: true}, // arg0=idxHi, arg1=idxLo, arg2=len, arg3=mem, returns memory. AuxInt contains report code (see PanicExtend in genericOps.go).
+		{name: "LoweredPanicExtendC", argLength: 4, aux: "Int64", reg: regInfo{inputs: []regMask{r4, r0, r1}}, typ: "Mem", call: true}, // arg0=idxHi, arg1=idxLo, arg2=len, arg3=mem, returns memory. AuxInt contains report code (see PanicExtend in genericOps.go).
+
+		// Constant flag value.
+		// Note: there's an "unordered" outcome for floating-point
+		// comparisons, but we don't use such a beast yet.
+		// This op is for temporary use by rewrite rules. It
+		// cannot appear in the generated assembly.
+		{name: "FlagConstant", aux: "FlagConstant"},
+
+		// (InvertFlags (CMP a b)) == (CMP b a)
+		// InvertFlags is a pseudo-op which can't appear in assembly output.
+		{name: "InvertFlags", argLength: 1}, // reverse direction of arg0
+
+		// LoweredWB invokes runtime.gcWriteBarrier. arg0=destptr, arg1=srcptr, arg2=mem, aux=runtime.gcWriteBarrier
+		// It saves all GP registers if necessary,
+		// but clobbers R14 (LR) because it's a call.
+		{name: "LoweredWB", argLength: 3, reg: regInfo{inputs: []regMask{buildReg("R2"), buildReg("R3")}, clobbers: (callerSave &^ gpg) | buildReg("R14")}, clobberFlags: true, aux: "Sym", symEffect: "None"},
+	}
+
+	blocks := []blockData{
+		{name: "EQ", controls: 1},
+		{name: "NE", controls: 1},
+		{name: "LT", controls: 1},
+		{name: "LE", controls: 1},
+		{name: "GT", controls: 1},
+		{name: "GE", controls: 1},
+		{name: "ULT", controls: 1},
+		{name: "ULE", controls: 1},
+		{name: "UGT", controls: 1},
+		{name: "UGE", controls: 1},
+		{name: "LTnoov", controls: 1}, // 'LT' but without honoring overflow
+		{name: "LEnoov", controls: 1}, // 'LE' but without honoring overflow
+		{name: "GTnoov", controls: 1}, // 'GT' but without honoring overflow
+		{name: "GEnoov", controls: 1}, // 'GE' but without honoring overflow
+	}
+
+	archs = append(archs, arch{
+		name:            "Thumb",
+		pkg:             "cmd/internal/obj/thumb",
+		genfile:         "../../thumb/ssa.go",
+		ops:             ops,
+		blocks:          blocks,
+		regnames:        regNamesThumb,
+		gpregmask:       gp,
+		fpregmask:       fp,
+		framepointerreg: -1, // not used
+		linkreg:         int8(num["R14"]),
+	})
+}
diff --git a/src/cmd/compile/internal/ssa/gen/genericOps.go b/src/cmd/compile/internal/ssa/gen/genericOps.go
index 8cfda35c22..4cbca2c3d9 100644
--- a/src/cmd/compile/internal/ssa/gen/genericOps.go
+++ b/src/cmd/compile/internal/ssa/gen/genericOps.go
@@ -585,6 +585,19 @@ var genericOps = []opData{
 	{name: "AtomicOr8Variant", argLength: 3, typ: "Mem", hasSideEffects: true},                     // *arg0 |= arg1.  arg2=memory.  Returns memory.
 	{name: "AtomicOr32Variant", argLength: 3, typ: "Mem", hasSideEffects: true},                    // *arg0 |= arg1.  arg2=memory.  Returns memory.
 
+	// MMIO operations need for semantically inlining functions in embedded/mmio
+	// package. Both load and store operations on I/O memory can cause side effects.
+	{name: "MMIOLoad32", argLength: 2, typ: "(UInt32,Mem)", hasSideEffects: true},
+	{name: "MMIOLoad16", argLength: 2, typ: "(UInt16,Mem)", hasSideEffects: true},
+	{name: "MMIOLoad8", argLength: 2, typ: "(UInt8,Mem)", hasSideEffects: true},
+	{name: "MMIOStore32", argLength: 3, typ: "Mem", hasSideEffects: true},
+	{name: "MMIOStore16", argLength: 3, typ: "Mem", hasSideEffects: true},
+	{name: "MMIOStore8", argLength: 3, typ: "Mem", hasSideEffects: true},
+	{name: "MMIOMB", argLength: 1, typ: "Mem", hasSideEffects: true},
+
+	// Publication barrier
+	{name: "PublicationBarrier", argLength: 1, typ: "Mem", hasSideEffects: true},
+
 	// Clobber experiment op
 	{name: "Clobber", argLength: 0, typ: "Void", aux: "SymOff", symEffect: "None"}, // write an invalid pointer value to the given pointer slot of a stack variable
 }
diff --git a/src/cmd/compile/internal/ssa/opGen.go b/src/cmd/compile/internal/ssa/opGen.go
index e590f6ba5d..0c014afc48 100644
--- a/src/cmd/compile/internal/ssa/opGen.go
+++ b/src/cmd/compile/internal/ssa/opGen.go
@@ -10,6 +10,7 @@ import (
 	"cmd/internal/obj/ppc64"
 	"cmd/internal/obj/riscv"
 	"cmd/internal/obj/s390x"
+	"cmd/internal/obj/thumb"
 	"cmd/internal/obj/wasm"
 	"cmd/internal/obj/x86"
 )
@@ -143,6 +144,21 @@ const (
 	BlockS390XCLIJ
 	BlockS390XCLGIJ
 
+	BlockThumbEQ
+	BlockThumbNE
+	BlockThumbLT
+	BlockThumbLE
+	BlockThumbGT
+	BlockThumbGE
+	BlockThumbULT
+	BlockThumbULE
+	BlockThumbUGT
+	BlockThumbUGE
+	BlockThumbLTnoov
+	BlockThumbLEnoov
+	BlockThumbGTnoov
+	BlockThumbGEnoov
+
 	BlockPlain
 	BlockIf
 	BlockDefer
@@ -281,6 +297,21 @@ var blockString = [...]string{
 	BlockS390XCLIJ:  "CLIJ",
 	BlockS390XCLGIJ: "CLGIJ",
 
+	BlockThumbEQ:     "EQ",
+	BlockThumbNE:     "NE",
+	BlockThumbLT:     "LT",
+	BlockThumbLE:     "LE",
+	BlockThumbGT:     "GT",
+	BlockThumbGE:     "GE",
+	BlockThumbULT:    "ULT",
+	BlockThumbULE:    "ULE",
+	BlockThumbUGT:    "UGT",
+	BlockThumbUGE:    "UGE",
+	BlockThumbLTnoov: "LTnoov",
+	BlockThumbLEnoov: "LEnoov",
+	BlockThumbGTnoov: "GTnoov",
+	BlockThumbGEnoov: "GEnoov",
+
 	BlockPlain:  "Plain",
 	BlockIf:     "If",
 	BlockDefer:  "Defer",
@@ -2422,6 +2453,242 @@ const (
 	OpS390XLoweredMove
 	OpS390XLoweredZero
 
+	OpThumbADD
+	OpThumbADDconst
+	OpThumbSUB
+	OpThumbSUBconst
+	OpThumbRSB
+	OpThumbRSBconst
+	OpThumbMUL
+	OpThumbHMUL
+	OpThumbHMULU
+	OpThumbDIV
+	OpThumbDIVU
+	OpThumbADDS
+	OpThumbADDSconst
+	OpThumbADC
+	OpThumbADCconst
+	OpThumbSUBS
+	OpThumbSUBSconst
+	OpThumbRSBSconst
+	OpThumbSBC
+	OpThumbSBCconst
+	OpThumbMULLU
+	OpThumbMULA
+	OpThumbMULS
+	OpThumbADDF
+	OpThumbADDD
+	OpThumbSUBF
+	OpThumbSUBD
+	OpThumbMULF
+	OpThumbMULD
+	OpThumbNMULF
+	OpThumbNMULD
+	OpThumbDIVF
+	OpThumbDIVD
+	OpThumbMULAF
+	OpThumbMULAD
+	OpThumbMULSF
+	OpThumbMULSD
+	OpThumbFMULAD
+	OpThumbAND
+	OpThumbANDconst
+	OpThumbOR
+	OpThumbORconst
+	OpThumbORN
+	OpThumbORNconst
+	OpThumbXOR
+	OpThumbXORconst
+	OpThumbBIC
+	OpThumbBICconst
+	OpThumbBFX
+	OpThumbBFXU
+	OpThumbMVN
+	OpThumbNEGF
+	OpThumbNEGD
+	OpThumbSQRTD
+	OpThumbABSD
+	OpThumbCLZ
+	OpThumbREV
+	OpThumbREV16
+	OpThumbRBIT
+	OpThumbSLL
+	OpThumbSLLconst
+	OpThumbSRL
+	OpThumbSRLconst
+	OpThumbSRA
+	OpThumbSRAconst
+	OpThumbSRR
+	OpThumbSRRconst
+	OpThumbADDshiftLL
+	OpThumbADDshiftRL
+	OpThumbADDshiftRA
+	OpThumbSUBshiftLL
+	OpThumbSUBshiftRL
+	OpThumbSUBshiftRA
+	OpThumbRSBshiftLL
+	OpThumbRSBshiftRL
+	OpThumbRSBshiftRA
+	OpThumbANDshiftLL
+	OpThumbANDshiftRL
+	OpThumbANDshiftRA
+	OpThumbORshiftLL
+	OpThumbORshiftRL
+	OpThumbORshiftRA
+	OpThumbORNshiftLL
+	OpThumbORNshiftRL
+	OpThumbORNshiftRA
+	OpThumbXORshiftLL
+	OpThumbXORshiftRL
+	OpThumbXORshiftRA
+	OpThumbXORshiftRR
+	OpThumbBICshiftLL
+	OpThumbBICshiftRL
+	OpThumbBICshiftRA
+	OpThumbMVNshiftLL
+	OpThumbMVNshiftRL
+	OpThumbMVNshiftRA
+	OpThumbADCshiftLL
+	OpThumbADCshiftRL
+	OpThumbADCshiftRA
+	OpThumbSBCshiftLL
+	OpThumbSBCshiftRL
+	OpThumbSBCshiftRA
+	OpThumbADDSshiftLL
+	OpThumbADDSshiftRL
+	OpThumbADDSshiftRA
+	OpThumbSUBSshiftLL
+	OpThumbSUBSshiftRL
+	OpThumbSUBSshiftRA
+	OpThumbRSBSshiftLL
+	OpThumbRSBSshiftRL
+	OpThumbRSBSshiftRA
+	OpThumbCMP
+	OpThumbCMPconst
+	OpThumbCMN
+	OpThumbCMNconst
+	OpThumbTST
+	OpThumbTSTconst
+	OpThumbTEQ
+	OpThumbTEQconst
+	OpThumbCMPF
+	OpThumbCMPD
+	OpThumbCMPshiftLL
+	OpThumbCMPshiftRL
+	OpThumbCMPshiftRA
+	OpThumbCMNshiftLL
+	OpThumbCMNshiftRL
+	OpThumbCMNshiftRA
+	OpThumbTSTshiftLL
+	OpThumbTSTshiftRL
+	OpThumbTSTshiftRA
+	OpThumbTEQshiftLL
+	OpThumbTEQshiftRL
+	OpThumbTEQshiftRA
+	OpThumbCMPF0
+	OpThumbCMPD0
+	OpThumbMOVWconst
+	OpThumbMOVFconst
+	OpThumbMOVDconst
+	OpThumbMOVWaddr
+	OpThumbMOVBload
+	OpThumbMOVBUload
+	OpThumbMOVHload
+	OpThumbMOVHUload
+	OpThumbMOVWload
+	OpThumbMOVFload
+	OpThumbMOVDload
+	OpThumbMOVBstore
+	OpThumbMOVHstore
+	OpThumbMOVWstore
+	OpThumbMOVFstore
+	OpThumbMOVDstore
+	OpThumbMOVWloadidx
+	OpThumbMOVHloadidx
+	OpThumbMOVHUloadidx
+	OpThumbMOVBloadidx
+	OpThumbMOVBUloadidx
+	OpThumbMOVWloadshiftLL
+	OpThumbMOVHloadshiftLL
+	OpThumbMOVHUloadshiftLL
+	OpThumbMOVBloadshiftLL
+	OpThumbMOVBUloadshiftLL
+	OpThumbMOVWstoreidx
+	OpThumbMOVBstoreidx
+	OpThumbMOVHstoreidx
+	OpThumbMOVWstoreshiftLL
+	OpThumbMOVHstoreshiftLL
+	OpThumbMOVBstoreshiftLL
+	OpThumbMOVBreg
+	OpThumbMOVBUreg
+	OpThumbMOVHreg
+	OpThumbMOVHUreg
+	OpThumbMOVWreg
+	OpThumbMOVWnop
+	OpThumbMOVWF
+	OpThumbMOVWD
+	OpThumbMOVWUF
+	OpThumbMOVWUD
+	OpThumbMOVFW
+	OpThumbMOVDW
+	OpThumbMOVFWU
+	OpThumbMOVDWU
+	OpThumbMOVFD
+	OpThumbMOVDF
+	OpThumbCMOVWHSconst
+	OpThumbCMOVWLSconst
+	OpThumbSRAcond
+	OpThumbCALLstatic
+	OpThumbCALLclosure
+	OpThumbCALLinter
+	OpThumbLoadOnce8
+	OpThumbLoadOnce16
+	OpThumbLoadOnce32
+	OpThumbStoreOnce8
+	OpThumbStoreOnce16
+	OpThumbStoreOnce32
+	OpThumbLoadOnce8idx
+	OpThumbLoadOnce16idx
+	OpThumbLoadOnce32idx
+	OpThumbStoreOnce8idx
+	OpThumbStoreOnce16idx
+	OpThumbStoreOnce32idx
+	OpThumbLoadOnce8shiftLL
+	OpThumbLoadOnce16shiftLL
+	OpThumbLoadOnce32shiftLL
+	OpThumbStoreOnce8shiftLL
+	OpThumbStoreOnce16shiftLL
+	OpThumbStoreOnce32shiftLL
+	OpThumbDSB
+	OpThumbDMB_ST
+	OpThumbLoweredNilCheck
+	OpThumbEqual
+	OpThumbNotEqual
+	OpThumbLessThan
+	OpThumbLessEqual
+	OpThumbGreaterThan
+	OpThumbGreaterEqual
+	OpThumbLessThanU
+	OpThumbLessEqualU
+	OpThumbGreaterThanU
+	OpThumbGreaterEqualU
+	OpThumbDUFFZERO
+	OpThumbDUFFCOPY
+	OpThumbLoweredZero
+	OpThumbLoweredMove
+	OpThumbLoweredGetClosurePtr
+	OpThumbLoweredGetCallerSP
+	OpThumbLoweredGetCallerPC
+	OpThumbLoweredPanicBoundsA
+	OpThumbLoweredPanicBoundsB
+	OpThumbLoweredPanicBoundsC
+	OpThumbLoweredPanicExtendA
+	OpThumbLoweredPanicExtendB
+	OpThumbLoweredPanicExtendC
+	OpThumbFlagConstant
+	OpThumbInvertFlags
+	OpThumbLoweredWB
+
 	OpWasmLoweredStaticCall
 	OpWasmLoweredClosureCall
 	OpWasmLoweredInterCall
@@ -2901,6 +3168,14 @@ const (
 	OpAtomicAnd32Variant
 	OpAtomicOr8Variant
 	OpAtomicOr32Variant
+	OpMMIOLoad32
+	OpMMIOLoad16
+	OpMMIOLoad8
+	OpMMIOStore32
+	OpMMIOStore16
+	OpMMIOStore8
+	OpMMIOMB
+	OpPublicationBarrier
 	OpClobber
 )
 
@@ -32619,689 +32894,3941 @@ var opcodeTable = [...]opInfo{
 	},
 
 	{
-		name:    "LoweredStaticCall",
-		auxType: auxCallOff,
-		argLen:  1,
-		call:    true,
-		reg: regInfo{
-			clobbers: 844424930131967, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15 F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31 g
-		},
-	},
-	{
-		name:    "LoweredClosureCall",
-		auxType: auxCallOff,
-		argLen:  3,
-		call:    true,
+		name:         "ADD",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AADD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
-				{1, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
-			clobbers: 844424930131967, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15 F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31 g
-		},
-	},
-	{
-		name:    "LoweredInterCall",
-		auxType: auxCallOff,
-		argLen:  2,
-		call:    true,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
-			clobbers: 844424930131967, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15 F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31 g
 		},
 	},
 	{
-		name:              "LoweredAddr",
-		auxType:           auxSymOff,
-		argLen:            1,
-		rematerializeable: true,
-		symEffect:         SymAddr,
+		name:         "ADDconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AADD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 32639}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "LoweredMove",
-		auxType: auxInt64,
-		argLen:  3,
+		name:         "SUB",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASUB,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
-				{1, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "LoweredZero",
-		auxType: auxInt64,
-		argLen:  2,
+		name:         "SUBconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASUB,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
-		},
-	},
-	{
-		name:   "LoweredGetClosurePtr",
-		argLen: 0,
-		reg: regInfo{
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:              "LoweredGetCallerPC",
-		argLen:            0,
-		rematerializeable: true,
+		name:         "RSB",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ARSB,
 		reg: regInfo{
-			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
-		},
-	},
-	{
-		name:              "LoweredGetCallerSP",
-		argLen:            0,
-		rematerializeable: true,
-		reg: regInfo{
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:           "LoweredNilCheck",
-		argLen:         2,
-		nilCheck:       true,
-		faultOnNilArg0: true,
+		name:         "RSBconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ARSB,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
-		},
-	},
-	{
-		name:      "LoweredWB",
-		auxType:   auxSym,
-		argLen:    3,
-		symEffect: SymNone,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
-				{1, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "LoweredConvert",
-		argLen: 2,
+		name:         "MUL",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AMUL,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "Select",
-		argLen: 3,
-		asm:    wasm.ASelect,
+		name:        "HMUL",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULL,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{2, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Load8U",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AI64Load8U,
+		name:        "HMULU",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULLU,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Load8S",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AI64Load8S,
+		name:   "DIV",
+		argLen: 2,
+		asm:    thumb.ADIV,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Load16U",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AI64Load16U,
+		name:   "DIVU",
+		argLen: 2,
+		asm:    thumb.ADIVU,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Load16S",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AI64Load16S,
+		name:        "ADDS",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AADD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Load32U",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AI64Load32U,
+		name:    "ADDSconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.AADD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Load32S",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AI64Load32S,
+		name:         "ADC",
+		argLen:       3,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AADC,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Load",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AI64Load,
+		name:         "ADCconst",
+		auxType:      auxInt32,
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.AADC,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Store8",
-		auxType: auxInt64,
-		argLen:  3,
-		asm:     wasm.AI64Store8,
+		name:   "SUBS",
+		argLen: 2,
+		asm:    thumb.ASUB,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{1, 281474976776191},  // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
-		},
-	},
-	{
-		name:    "I64Store16",
-		auxType: auxInt64,
-		argLen:  3,
-		asm:     wasm.AI64Store16,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 281474976776191},  // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "I64Store32",
-		auxType: auxInt64,
-		argLen:  3,
-		asm:     wasm.AI64Store32,
+		name:    "SUBSconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ASUB,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{1, 281474976776191},  // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
-		},
-	},
-	{
-		name:    "I64Store",
-		auxType: auxInt64,
-		argLen:  3,
-		asm:     wasm.AI64Store,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 281474976776191},  // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "F32Load",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AF32Load,
+		name:    "RSBSconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ARSB,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "F64Load",
-		auxType: auxInt64,
-		argLen:  2,
-		asm:     wasm.AF64Load,
+		name:         "SBC",
+		argLen:       3,
+		clobberFlags: true,
+		asm:          thumb.ASBC,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "F32Store",
-		auxType: auxInt64,
-		argLen:  3,
-		asm:     wasm.AF32Store,
+		name:         "SBCconst",
+		auxType:      auxInt32,
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASBC,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{1, 4294901760},       // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:    "F64Store",
-		auxType: auxInt64,
-		argLen:  3,
-		asm:     wasm.AF64Store,
+		name:        "MULLU",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULLU,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{1, 281470681743360},  // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
-				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
-		},
-	},
-	{
-		name:              "I64Const",
-		auxType:           auxInt64,
-		argLen:            0,
-		rematerializeable: true,
-		reg: regInfo{
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:              "F32Const",
-		auxType:           auxFloat32,
-		argLen:            0,
-		rematerializeable: true,
+		name:   "MULA",
+		argLen: 3,
+		asm:    thumb.AMULA,
 		reg: regInfo{
-			outputs: []outputInfo{
-				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{2, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
-		},
-	},
-	{
-		name:              "F64Const",
-		auxType:           auxFloat64,
-		argLen:            0,
-		rematerializeable: true,
-		reg: regInfo{
 			outputs: []outputInfo{
-				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "I64Eqz",
-		argLen: 1,
-		asm:    wasm.AI64Eqz,
+		name:   "MULS",
+		argLen: 3,
+		asm:    thumb.AMULS,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{2, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "I64Eq",
-		argLen: 2,
-		asm:    wasm.AI64Eq,
+		name:        "ADDF",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AADDF,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64Ne",
-		argLen: 2,
-		asm:    wasm.AI64Ne,
+		name:        "ADDD",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AADDD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64LtS",
+		name:   "SUBF",
 		argLen: 2,
-		asm:    wasm.AI64LtS,
+		asm:    thumb.ASUBF,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64LtU",
+		name:   "SUBD",
 		argLen: 2,
-		asm:    wasm.AI64LtU,
+		asm:    thumb.ASUBD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64GtS",
-		argLen: 2,
-		asm:    wasm.AI64GtS,
+		name:        "MULF",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULF,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64GtU",
-		argLen: 2,
-		asm:    wasm.AI64GtU,
+		name:        "MULD",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64LeS",
-		argLen: 2,
-		asm:    wasm.AI64LeS,
+		name:        "NMULF",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ANMULF,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64LeU",
-		argLen: 2,
-		asm:    wasm.AI64LeU,
+		name:        "NMULD",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ANMULD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64GeS",
+		name:   "DIVF",
 		argLen: 2,
-		asm:    wasm.AI64GeS,
+		asm:    thumb.ADIVF,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "I64GeU",
+		name:   "DIVD",
 		argLen: 2,
-		asm:    wasm.AI64GeU,
+		asm:    thumb.ADIVD,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
-				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "F32Eq",
-		argLen: 2,
-		asm:    wasm.AF32Eq,
+		name:         "MULAF",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AMULAF,
 		reg: regInfo{
 			inputs: []inputInfo{
 				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "F32Ne",
-		argLen: 2,
-		asm:    wasm.AF32Ne,
+		name:         "MULAD",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AMULAD,
 		reg: regInfo{
 			inputs: []inputInfo{
 				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "F32Lt",
-		argLen: 2,
-		asm:    wasm.AF32Lt,
+		name:         "MULSF",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AMULSF,
 		reg: regInfo{
 			inputs: []inputInfo{
 				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "F32Gt",
-		argLen: 2,
-		asm:    wasm.AF32Gt,
+		name:         "MULSD",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AMULSD,
 		reg: regInfo{
 			inputs: []inputInfo{
 				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "F32Le",
-		argLen: 2,
-		asm:    wasm.AF32Le,
+		name:         "FMULAD",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AFMULAD,
 		reg: regInfo{
 			inputs: []inputInfo{
 				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 			},
 		},
 	},
 	{
-		name:   "F32Ge",
-		argLen: 2,
-		asm:    wasm.AF32Ge,
+		name:         "AND",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AAND,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
-				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "F64Eq",
-		argLen: 2,
-		asm:    wasm.AF64Eq,
+		name:         "ANDconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AAND,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
-				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "F64Ne",
-		argLen: 2,
-		asm:    wasm.AF64Ne,
+		name:         "OR",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AORR,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
-				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "F64Lt",
-		argLen: 2,
-		asm:    wasm.AF64Lt,
+		name:         "ORconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AORR,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
-				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "F64Gt",
-		argLen: 2,
-		asm:    wasm.AF64Gt,
+		name:         "ORN",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.AORN,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
-				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
 			},
 		},
 	},
 	{
-		name:   "F64Le",
-		argLen: 2,
-		asm:    wasm.AF64Le,
+		name:         "ORNconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AORN,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
-				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
 			},
 			outputs: []outputInfo{
-				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "XOR",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "XORconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "BIC",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "BICconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BFX",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ABFX,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BFXU",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ABFXU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "MVN",
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AMVN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "NEGF",
+		argLen: 1,
+		asm:    thumb.ANEGF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "NEGD",
+		argLen: 1,
+		asm:    thumb.ANEGD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "SQRTD",
+		argLen: 1,
+		asm:    thumb.ASQRTD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "ABSD",
+		argLen: 1,
+		asm:    thumb.AABSD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "CLZ",
+		argLen: 1,
+		asm:    thumb.ACLZ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "REV",
+		argLen: 1,
+		asm:    thumb.AREV,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "REV16",
+		argLen: 1,
+		asm:    thumb.AREV16,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "RBIT",
+		argLen: 1,
+		asm:    thumb.ARBIT,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SLL",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASLL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SLLconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASLL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRL",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASRL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRLconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASRL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRA",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASRA,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRAconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASRA,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRR",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASRR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRRconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASRR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ANDshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AAND,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ANDshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AAND,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ANDshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AAND,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORNshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORNshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORNshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "XORshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "XORshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "XORshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "XORshiftRR",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BICshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BICshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BICshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MVNshiftLL",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.AMVN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MVNshiftRL",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.AMVN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MVNshiftRA",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.AMVN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADCshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AADC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADCshiftRL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AADC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADCshiftRA",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AADC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SBCshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.ASBC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SBCshiftRL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.ASBC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SBCshiftRA",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.ASBC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDSshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDSshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDSshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBSshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBSshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBSshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBSshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBSshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBSshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "CMP",
+		argLen: 2,
+		asm:    thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMPconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "CMN",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMNconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "TST",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TSTconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "TEQ",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TEQconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "CMPF",
+		argLen: 2,
+		asm:    thumb.ACMPF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "CMPD",
+		argLen: 2,
+		asm:    thumb.ACMPD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:    "CMPshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMPshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMPshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMNshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMNshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMNshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TSTshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TSTshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TSTshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TEQshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TEQshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TEQshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "CMPF0",
+		argLen: 1,
+		asm:    thumb.ACMPF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "CMPD0",
+		argLen: 1,
+		asm:    thumb.ACMPD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "MOVWconst",
+		auxType:      auxInt32,
+		argLen:       0,
+		clobberFlags: true,
+		asm:          thumb.AMOVW,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:              "MOVFconst",
+		auxType:           auxFloat64,
+		argLen:            0,
+		rematerializeable: true,
+		asm:               thumb.AMOVF,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:              "MOVDconst",
+		auxType:           auxFloat64,
+		argLen:            0,
+		rematerializeable: true,
+		asm:               thumb.AMOVD,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:              "MOVWaddr",
+		auxType:           auxSymOff,
+		argLen:            1,
+		rematerializeable: true,
+		symEffect:         SymAddr,
+		asm:               thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294975488}, // SP SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVBload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVBUload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVHload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVHUload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVWload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVFload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:           "MOVDload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:           "MOVBstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "MOVHstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "MOVWstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "MOVFstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:           "MOVDstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVWloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVHloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVHUloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVBloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVBUloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVWloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVHloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVHUloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVBloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVBUloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVWstoreidx",
+		argLen: 4,
+		asm:    thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:   "MOVBstoreidx",
+		argLen: 4,
+		asm:    thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:   "MOVHstoreidx",
+		argLen: 4,
+		asm:    thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:    "MOVWstoreshiftLL",
+		auxType: auxInt32,
+		argLen:  4,
+		asm:     thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:    "MOVHstoreshiftLL",
+		auxType: auxInt32,
+		argLen:  4,
+		asm:     thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:    "MOVBstoreshiftLL",
+		auxType: auxInt32,
+		argLen:  4,
+		asm:     thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:   "MOVBreg",
+		argLen: 1,
+		asm:    thumb.AMOVBS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVBUreg",
+		argLen: 1,
+		asm:    thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVHreg",
+		argLen: 1,
+		asm:    thumb.AMOVHS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVHUreg",
+		argLen: 1,
+		asm:    thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVWreg",
+		argLen: 1,
+		asm:    thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "MOVWnop",
+		argLen:       1,
+		resultInArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVWF",
+		argLen: 1,
+		asm:    thumb.AMOVWF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVWD",
+		argLen: 1,
+		asm:    thumb.AMOVWD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVWUF",
+		argLen: 1,
+		asm:    thumb.AMOVWF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVWUD",
+		argLen: 1,
+		asm:    thumb.AMOVWD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVFW",
+		argLen: 1,
+		asm:    thumb.AMOVFW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVDW",
+		argLen: 1,
+		asm:    thumb.AMOVDW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVFWU",
+		argLen: 1,
+		asm:    thumb.AMOVFW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVDWU",
+		argLen: 1,
+		asm:    thumb.AMOVDW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVFD",
+		argLen: 1,
+		asm:    thumb.AMOVFD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVDF",
+		argLen: 1,
+		asm:    thumb.AMOVDF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "CMOVWHSconst",
+		auxType:      auxInt32,
+		argLen:       2,
+		resultInArg0: true,
+		asm:          thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "CMOVWLSconst",
+		auxType:      auxInt32,
+		argLen:       2,
+		resultInArg0: true,
+		asm:          thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "SRAcond",
+		argLen: 3,
+		asm:    thumb.ASRA,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "CALLstatic",
+		auxType:      auxCallOff,
+		argLen:       1,
+		clobberFlags: true,
+		call:         true,
+		reg: regInfo{
+			clobbers: 4294926207, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+		},
+	},
+	{
+		name:         "CALLclosure",
+		auxType:      auxCallOff,
+		argLen:       3,
+		clobberFlags: true,
+		call:         true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 2048},  // R11
+				{0, 31615}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 SP R14
+			},
+			clobbers: 4294926207, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+		},
+	},
+	{
+		name:         "CALLinter",
+		auxType:      auxCallOff,
+		argLen:       2,
+		clobberFlags: true,
+		call:         true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 4294926207, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+		},
+	},
+	{
+		name:           "LoadOnce8",
+		auxType:        auxInt32,
+		argLen:         2,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce16",
+		auxType:        auxInt32,
+		argLen:         2,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce32",
+		auxType:        auxInt32,
+		argLen:         2,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "StoreOnce8",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce16",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce32",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "LoadOnce8idx",
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce16idx",
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce32idx",
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "StoreOnce8idx",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce16idx",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce32idx",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "LoadOnce8shiftLL",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce16shiftLL",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce32shiftLL",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "StoreOnce8shiftLL",
+		auxType:        auxInt32,
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce16shiftLL",
+		auxType:        auxInt32,
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce32shiftLL",
+		auxType:        auxInt32,
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "DSB",
+		argLen:         1,
+		hasSideEffects: true,
+		asm:            thumb.ADSB,
+		reg:            regInfo{},
+	},
+	{
+		name:           "DMB_ST",
+		argLen:         1,
+		hasSideEffects: true,
+		asm:            thumb.ADMB,
+		reg:            regInfo{},
+	},
+	{
+		name:           "LoweredNilCheck",
+		argLen:         2,
+		nilCheck:       true,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "Equal",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "NotEqual",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "LessThan",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "LessEqual",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "GreaterThan",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "GreaterEqual",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "LessThanU",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "LessEqualU",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "GreaterThanU",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "GreaterEqualU",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "DUFFZERO",
+		auxType:        auxInt64,
+		argLen:         3,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 2}, // R1
+				{1, 1}, // R0
+			},
+			clobbers: 16386, // R1 R14
+		},
+	},
+	{
+		name:           "DUFFCOPY",
+		auxType:        auxInt64,
+		argLen:         3,
+		faultOnNilArg0: true,
+		faultOnNilArg1: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4}, // R2
+				{1, 2}, // R1
+			},
+			clobbers: 16391, // R0 R1 R2 R14
+		},
+	},
+	{
+		name:           "LoweredZero",
+		auxType:        auxInt64,
+		argLen:         4,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 2},     // R1
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{2, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2, // R1
+		},
+	},
+	{
+		name:           "LoweredMove",
+		auxType:        auxInt64,
+		argLen:         4,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		faultOnNilArg1: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4},     // R2
+				{1, 2},     // R1
+				{2, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 6, // R1 R2
+		},
+	},
+	{
+		name:      "LoweredGetClosurePtr",
+		argLen:    0,
+		zeroWidth: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 2048}, // R11
+			},
+		},
+	},
+	{
+		name:              "LoweredGetCallerSP",
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:              "LoweredGetCallerPC",
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicBoundsA",
+		auxType: auxInt64,
+		argLen:  3,
+		call:    true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4}, // R2
+				{1, 8}, // R3
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicBoundsB",
+		auxType: auxInt64,
+		argLen:  3,
+		call:    true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 2}, // R1
+				{1, 4}, // R2
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicBoundsC",
+		auxType: auxInt64,
+		argLen:  3,
+		call:    true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1}, // R0
+				{1, 2}, // R1
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicExtendA",
+		auxType: auxInt64,
+		argLen:  4,
+		call:    true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 16}, // R4
+				{1, 4},  // R2
+				{2, 8},  // R3
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicExtendB",
+		auxType: auxInt64,
+		argLen:  4,
+		call:    true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 16}, // R4
+				{1, 2},  // R1
+				{2, 4},  // R2
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicExtendC",
+		auxType: auxInt64,
+		argLen:  4,
+		call:    true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 16}, // R4
+				{1, 1},  // R0
+				{2, 2},  // R1
+			},
+		},
+	},
+	{
+		name:    "FlagConstant",
+		auxType: auxFlagConstant,
+		argLen:  0,
+		reg:     regInfo{},
+	},
+	{
+		name:   "InvertFlags",
+		argLen: 1,
+		reg:    regInfo{},
+	},
+	{
+		name:         "LoweredWB",
+		auxType:      auxSym,
+		argLen:       3,
+		clobberFlags: true,
+		symEffect:    SymNone,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4}, // R2
+				{1, 8}, // R3
+			},
+			clobbers: 4294918144, // R14 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+		},
+	},
+
+	{
+		name:    "LoweredStaticCall",
+		auxType: auxCallOff,
+		argLen:  1,
+		call:    true,
+		reg: regInfo{
+			clobbers: 844424930131967, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15 F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31 g
+		},
+	},
+	{
+		name:    "LoweredClosureCall",
+		auxType: auxCallOff,
+		argLen:  3,
+		call:    true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{1, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+			clobbers: 844424930131967, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15 F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31 g
+		},
+	},
+	{
+		name:    "LoweredInterCall",
+		auxType: auxCallOff,
+		argLen:  2,
+		call:    true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+			clobbers: 844424930131967, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15 F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31 g
+		},
+	},
+	{
+		name:              "LoweredAddr",
+		auxType:           auxSymOff,
+		argLen:            1,
+		rematerializeable: true,
+		symEffect:         SymAddr,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "LoweredMove",
+		auxType: auxInt64,
+		argLen:  3,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{1, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "LoweredZero",
+		auxType: auxInt64,
+		argLen:  2,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "LoweredGetClosurePtr",
+		argLen: 0,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:              "LoweredGetCallerPC",
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:              "LoweredGetCallerSP",
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:           "LoweredNilCheck",
+		argLen:         2,
+		nilCheck:       true,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:      "LoweredWB",
+		auxType:   auxSym,
+		argLen:    3,
+		symEffect: SymNone,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+				{1, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "LoweredConvert",
+		argLen: 2,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "Select",
+		argLen: 3,
+		asm:    wasm.ASelect,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{2, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "I64Load8U",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AI64Load8U,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "I64Load8S",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AI64Load8S,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "I64Load16U",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AI64Load16U,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "I64Load16S",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AI64Load16S,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "I64Load32U",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AI64Load32U,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "I64Load32S",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AI64Load32S,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "I64Load",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AI64Load,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:    "I64Store8",
+		auxType: auxInt64,
+		argLen:  3,
+		asm:     wasm.AI64Store8,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 281474976776191},  // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+		},
+	},
+	{
+		name:    "I64Store16",
+		auxType: auxInt64,
+		argLen:  3,
+		asm:     wasm.AI64Store16,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 281474976776191},  // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+		},
+	},
+	{
+		name:    "I64Store32",
+		auxType: auxInt64,
+		argLen:  3,
+		asm:     wasm.AI64Store32,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 281474976776191},  // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+		},
+	},
+	{
+		name:    "I64Store",
+		auxType: auxInt64,
+		argLen:  3,
+		asm:     wasm.AI64Store,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 281474976776191},  // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+		},
+	},
+	{
+		name:    "F32Load",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AF32Load,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:    "F64Load",
+		auxType: auxInt64,
+		argLen:  2,
+		asm:     wasm.AF64Load,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+			},
+		},
+	},
+	{
+		name:    "F32Store",
+		auxType: auxInt64,
+		argLen:  3,
+		asm:     wasm.AF32Store,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 4294901760},       // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+		},
+	},
+	{
+		name:    "F64Store",
+		auxType: auxInt64,
+		argLen:  3,
+		asm:     wasm.AF64Store,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 281470681743360},  // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{0, 1407374883618815}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP SB
+			},
+		},
+	},
+	{
+		name:              "I64Const",
+		auxType:           auxInt64,
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:              "F32Const",
+		auxType:           auxFloat32,
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:              "F64Const",
+		auxType:           auxFloat64,
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+			},
+		},
+	},
+	{
+		name:   "I64Eqz",
+		argLen: 1,
+		asm:    wasm.AI64Eqz,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64Eq",
+		argLen: 2,
+		asm:    wasm.AI64Eq,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64Ne",
+		argLen: 2,
+		asm:    wasm.AI64Ne,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64LtS",
+		argLen: 2,
+		asm:    wasm.AI64LtS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64LtU",
+		argLen: 2,
+		asm:    wasm.AI64LtU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64GtS",
+		argLen: 2,
+		asm:    wasm.AI64GtS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64GtU",
+		argLen: 2,
+		asm:    wasm.AI64GtU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64LeS",
+		argLen: 2,
+		asm:    wasm.AI64LeS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64LeU",
+		argLen: 2,
+		asm:    wasm.AI64LeU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64GeS",
+		argLen: 2,
+		asm:    wasm.AI64GeS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "I64GeU",
+		argLen: 2,
+		asm:    wasm.AI64GeU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+				{1, 281474976776191}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 SP
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F32Eq",
+		argLen: 2,
+		asm:    wasm.AF32Eq,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F32Ne",
+		argLen: 2,
+		asm:    wasm.AF32Ne,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F32Lt",
+		argLen: 2,
+		asm:    wasm.AF32Lt,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F32Gt",
+		argLen: 2,
+		asm:    wasm.AF32Gt,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F32Le",
+		argLen: 2,
+		asm:    wasm.AF32Le,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F32Ge",
+		argLen: 2,
+		asm:    wasm.AF32Ge,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F64Eq",
+		argLen: 2,
+		asm:    wasm.AF64Eq,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F64Ne",
+		argLen: 2,
+		asm:    wasm.AF64Ne,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F64Lt",
+		argLen: 2,
+		asm:    wasm.AF64Lt,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F64Gt",
+		argLen: 2,
+		asm:    wasm.AF64Gt,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
+			},
+		},
+	},
+	{
+		name:   "F64Le",
+		argLen: 2,
+		asm:    wasm.AF64Le,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+				{1, 281470681743360}, // F16 F17 F18 F19 F20 F21 F22 F23 F24 F25 F26 F27 F28 F29 F30 F31
+			},
+			outputs: []outputInfo{
+				{0, 65535}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15
 			},
 		},
 	},
@@ -36115,6 +39642,54 @@ var opcodeTable = [...]opInfo{
 		hasSideEffects: true,
 		generic:        true,
 	},
+	{
+		name:           "MMIOLoad32",
+		argLen:         2,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOLoad16",
+		argLen:         2,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOLoad8",
+		argLen:         2,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOStore32",
+		argLen:         3,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOStore16",
+		argLen:         3,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOStore8",
+		argLen:         3,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOMB",
+		argLen:         1,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "PublicationBarrier",
+		argLen:         1,
+		hasSideEffects: true,
+		generic:        true,
+	},
 	{
 		name:      "Clobber",
 		auxType:   auxSymOff,
@@ -36615,6 +40190,46 @@ var fpRegMaskS390X = regMask(4294901760)
 var specialRegMaskS390X = regMask(0)
 var framepointerRegS390X = int8(-1)
 var linkRegS390X = int8(14)
+var registersThumb = [...]Register{
+	{0, thumb.REG_R0, 0, "R0"},
+	{1, thumb.REG_R1, 1, "R1"},
+	{2, thumb.REG_R2, 2, "R2"},
+	{3, thumb.REG_R3, 3, "R3"},
+	{4, thumb.REG_R4, 4, "R4"},
+	{5, thumb.REG_R5, 5, "R5"},
+	{6, thumb.REG_R6, 6, "R6"},
+	{7, thumb.REG_R7, -1, "R7"},
+	{8, thumb.REG_R8, 7, "R8"},
+	{9, thumb.REG_R9, 8, "R9"},
+	{10, thumb.REGG, -1, "g"},
+	{11, thumb.REG_R11, 9, "R11"},
+	{12, thumb.REG_R12, 10, "R12"},
+	{13, thumb.REGSP, -1, "SP"},
+	{14, thumb.REG_R14, 11, "R14"},
+	{15, thumb.REG_R15, -1, "R15"},
+	{16, thumb.REG_F0, -1, "F0"},
+	{17, thumb.REG_F1, -1, "F1"},
+	{18, thumb.REG_F2, -1, "F2"},
+	{19, thumb.REG_F3, -1, "F3"},
+	{20, thumb.REG_F4, -1, "F4"},
+	{21, thumb.REG_F5, -1, "F5"},
+	{22, thumb.REG_F6, -1, "F6"},
+	{23, thumb.REG_F7, -1, "F7"},
+	{24, thumb.REG_F8, -1, "F8"},
+	{25, thumb.REG_F9, -1, "F9"},
+	{26, thumb.REG_F10, -1, "F10"},
+	{27, thumb.REG_F11, -1, "F11"},
+	{28, thumb.REG_F12, -1, "F12"},
+	{29, thumb.REG_F13, -1, "F13"},
+	{30, thumb.REG_F14, -1, "F14"},
+	{31, thumb.REG_F15, -1, "F15"},
+	{32, 0, -1, "SB"},
+}
+var gpRegMaskThumb = regMask(23423)
+var fpRegMaskThumb = regMask(4294901760)
+var specialRegMaskThumb = regMask(0)
+var framepointerRegThumb = int8(-1)
+var linkRegThumb = int8(14)
 var registersWasm = [...]Register{
 	{0, wasm.REG_R0, 0, "R0"},
 	{1, wasm.REG_R1, 1, "R1"},
diff --git a/src/cmd/compile/internal/ssa/regalloc.go b/src/cmd/compile/internal/ssa/regalloc.go
index 0339b073ae..f4a664fff9 100644
--- a/src/cmd/compile/internal/ssa/regalloc.go
+++ b/src/cmd/compile/internal/ssa/regalloc.go
@@ -596,7 +596,8 @@ func (s *regAllocState) init(f *Func) {
 			// Leaf functions don't save/restore the link register.
 			s.allocatable &^= 1 << uint(s.f.Config.LinkReg)
 		}
-		if s.f.Config.arch == "arm" && objabi.GOARM == 5 {
+		if s.f.Config.arch == "arm" && objabi.GOARM == 5 || s.f.Config.arch == "thumb" && objabi.GOARM&0xF != 0xD {
+			// TODO: handle 32-bit FPU (GOARCH=thumb GOARM=0x7F)
 			// On ARMv5 we insert softfloat calls at each FP instruction.
 			// This clobbers LR almost everywhere. Disable allocating LR
 			// on ARMv5.
@@ -607,7 +608,7 @@ func (s *regAllocState) init(f *Func) {
 		switch s.f.Config.arch {
 		case "amd64":
 			s.allocatable &^= 1 << 15 // R15
-		case "arm":
+		case "arm", "thumb":
 			s.allocatable &^= 1 << 9 // R9
 		case "ppc64le": // R2 already reserved.
 			// nothing to do
diff --git a/src/cmd/compile/internal/ssa/rewrite.go b/src/cmd/compile/internal/ssa/rewrite.go
index 9e5ef68b1e..c0054a2bf2 100644
--- a/src/cmd/compile/internal/ssa/rewrite.go
+++ b/src/cmd/compile/internal/ssa/rewrite.go
@@ -1322,7 +1322,7 @@ func isInlinableMemmove(dst, src *Value, sz int64, c *Config) bool {
 		return sz <= 8
 	case "s390x", "ppc64", "ppc64le":
 		return sz <= 8 || disjoint(dst, sz, src, sz)
-	case "arm", "mips", "mips64", "mipsle", "mips64le":
+	case "arm", "mips", "mips64", "mipsle", "mips64le", "thumb":
 		return sz <= 4
 	}
 	return false
diff --git a/src/cmd/compile/internal/ssa/rewriteThumb.go b/src/cmd/compile/internal/ssa/rewriteThumb.go
new file mode 100644
index 0000000000..fdb0bf1ceb
--- /dev/null
+++ b/src/cmd/compile/internal/ssa/rewriteThumb.go
@@ -0,0 +1,18615 @@
+// Code generated from gen/Thumb.rules; DO NOT EDIT.
+// generated with: cd gen; go run *.go
+
+package ssa
+
+import "cmd/compile/internal/types"
+
+func rewriteValueThumb(v *Value) bool {
+	switch v.Op {
+	case OpAbs:
+		v.Op = OpThumbABSD
+		return true
+	case OpAdd16:
+		v.Op = OpThumbADD
+		return true
+	case OpAdd32:
+		v.Op = OpThumbADD
+		return true
+	case OpAdd32F:
+		v.Op = OpThumbADDF
+		return true
+	case OpAdd32carry:
+		v.Op = OpThumbADDS
+		return true
+	case OpAdd32withcarry:
+		v.Op = OpThumbADC
+		return true
+	case OpAdd64F:
+		v.Op = OpThumbADDD
+		return true
+	case OpAdd8:
+		v.Op = OpThumbADD
+		return true
+	case OpAddPtr:
+		v.Op = OpThumbADD
+		return true
+	case OpAddr:
+		return rewriteValueThumb_OpAddr(v)
+	case OpAnd16:
+		v.Op = OpThumbAND
+		return true
+	case OpAnd32:
+		v.Op = OpThumbAND
+		return true
+	case OpAnd8:
+		v.Op = OpThumbAND
+		return true
+	case OpAndB:
+		v.Op = OpThumbAND
+		return true
+	case OpAvg32u:
+		return rewriteValueThumb_OpAvg32u(v)
+	case OpBitLen32:
+		return rewriteValueThumb_OpBitLen32(v)
+	case OpBswap32:
+		v.Op = OpThumbREV
+		return true
+	case OpClosureCall:
+		v.Op = OpThumbCALLclosure
+		return true
+	case OpCom16:
+		v.Op = OpThumbMVN
+		return true
+	case OpCom32:
+		v.Op = OpThumbMVN
+		return true
+	case OpCom8:
+		v.Op = OpThumbMVN
+		return true
+	case OpConst16:
+		return rewriteValueThumb_OpConst16(v)
+	case OpConst32:
+		return rewriteValueThumb_OpConst32(v)
+	case OpConst32F:
+		return rewriteValueThumb_OpConst32F(v)
+	case OpConst64F:
+		return rewriteValueThumb_OpConst64F(v)
+	case OpConst8:
+		return rewriteValueThumb_OpConst8(v)
+	case OpConstBool:
+		return rewriteValueThumb_OpConstBool(v)
+	case OpConstNil:
+		return rewriteValueThumb_OpConstNil(v)
+	case OpCtz16:
+		return rewriteValueThumb_OpCtz16(v)
+	case OpCtz16NonZero:
+		v.Op = OpCtz32
+		return true
+	case OpCtz32:
+		return rewriteValueThumb_OpCtz32(v)
+	case OpCtz32NonZero:
+		v.Op = OpCtz32
+		return true
+	case OpCtz8:
+		return rewriteValueThumb_OpCtz8(v)
+	case OpCtz8NonZero:
+		v.Op = OpCtz32
+		return true
+	case OpCvt32Fto32:
+		v.Op = OpThumbMOVFW
+		return true
+	case OpCvt32Fto32U:
+		v.Op = OpThumbMOVFWU
+		return true
+	case OpCvt32Fto64F:
+		v.Op = OpThumbMOVFD
+		return true
+	case OpCvt32Uto32F:
+		v.Op = OpThumbMOVWUF
+		return true
+	case OpCvt32Uto64F:
+		v.Op = OpThumbMOVWUD
+		return true
+	case OpCvt32to32F:
+		v.Op = OpThumbMOVWF
+		return true
+	case OpCvt32to64F:
+		v.Op = OpThumbMOVWD
+		return true
+	case OpCvt64Fto32:
+		v.Op = OpThumbMOVDW
+		return true
+	case OpCvt64Fto32F:
+		v.Op = OpThumbMOVDF
+		return true
+	case OpCvt64Fto32U:
+		v.Op = OpThumbMOVDWU
+		return true
+	case OpCvtBoolToUint8:
+		v.Op = OpCopy
+		return true
+	case OpDiv16:
+		return rewriteValueThumb_OpDiv16(v)
+	case OpDiv16u:
+		return rewriteValueThumb_OpDiv16u(v)
+	case OpDiv32:
+		return rewriteValueThumb_OpDiv32(v)
+	case OpDiv32F:
+		v.Op = OpThumbDIVF
+		return true
+	case OpDiv32u:
+		v.Op = OpThumbDIVU
+		return true
+	case OpDiv64F:
+		v.Op = OpThumbDIVD
+		return true
+	case OpDiv8:
+		return rewriteValueThumb_OpDiv8(v)
+	case OpDiv8u:
+		return rewriteValueThumb_OpDiv8u(v)
+	case OpEq16:
+		return rewriteValueThumb_OpEq16(v)
+	case OpEq32:
+		return rewriteValueThumb_OpEq32(v)
+	case OpEq32F:
+		return rewriteValueThumb_OpEq32F(v)
+	case OpEq64F:
+		return rewriteValueThumb_OpEq64F(v)
+	case OpEq8:
+		return rewriteValueThumb_OpEq8(v)
+	case OpEqB:
+		return rewriteValueThumb_OpEqB(v)
+	case OpEqPtr:
+		return rewriteValueThumb_OpEqPtr(v)
+	case OpFMA:
+		return rewriteValueThumb_OpFMA(v)
+	case OpGetCallerPC:
+		v.Op = OpThumbLoweredGetCallerPC
+		return true
+	case OpGetCallerSP:
+		v.Op = OpThumbLoweredGetCallerSP
+		return true
+	case OpGetClosurePtr:
+		v.Op = OpThumbLoweredGetClosurePtr
+		return true
+	case OpHmul32:
+		v.Op = OpThumbHMUL
+		return true
+	case OpHmul32u:
+		v.Op = OpThumbHMULU
+		return true
+	case OpInterCall:
+		v.Op = OpThumbCALLinter
+		return true
+	case OpIsInBounds:
+		return rewriteValueThumb_OpIsInBounds(v)
+	case OpIsNonNil:
+		return rewriteValueThumb_OpIsNonNil(v)
+	case OpIsSliceInBounds:
+		return rewriteValueThumb_OpIsSliceInBounds(v)
+	case OpLeq16:
+		return rewriteValueThumb_OpLeq16(v)
+	case OpLeq16U:
+		return rewriteValueThumb_OpLeq16U(v)
+	case OpLeq32:
+		return rewriteValueThumb_OpLeq32(v)
+	case OpLeq32F:
+		return rewriteValueThumb_OpLeq32F(v)
+	case OpLeq32U:
+		return rewriteValueThumb_OpLeq32U(v)
+	case OpLeq64F:
+		return rewriteValueThumb_OpLeq64F(v)
+	case OpLeq8:
+		return rewriteValueThumb_OpLeq8(v)
+	case OpLeq8U:
+		return rewriteValueThumb_OpLeq8U(v)
+	case OpLess16:
+		return rewriteValueThumb_OpLess16(v)
+	case OpLess16U:
+		return rewriteValueThumb_OpLess16U(v)
+	case OpLess32:
+		return rewriteValueThumb_OpLess32(v)
+	case OpLess32F:
+		return rewriteValueThumb_OpLess32F(v)
+	case OpLess32U:
+		return rewriteValueThumb_OpLess32U(v)
+	case OpLess64F:
+		return rewriteValueThumb_OpLess64F(v)
+	case OpLess8:
+		return rewriteValueThumb_OpLess8(v)
+	case OpLess8U:
+		return rewriteValueThumb_OpLess8U(v)
+	case OpLoad:
+		return rewriteValueThumb_OpLoad(v)
+	case OpLocalAddr:
+		return rewriteValueThumb_OpLocalAddr(v)
+	case OpLsh16x16:
+		return rewriteValueThumb_OpLsh16x16(v)
+	case OpLsh16x32:
+		return rewriteValueThumb_OpLsh16x32(v)
+	case OpLsh16x64:
+		return rewriteValueThumb_OpLsh16x64(v)
+	case OpLsh16x8:
+		return rewriteValueThumb_OpLsh16x8(v)
+	case OpLsh32x16:
+		return rewriteValueThumb_OpLsh32x16(v)
+	case OpLsh32x32:
+		return rewriteValueThumb_OpLsh32x32(v)
+	case OpLsh32x64:
+		return rewriteValueThumb_OpLsh32x64(v)
+	case OpLsh32x8:
+		return rewriteValueThumb_OpLsh32x8(v)
+	case OpLsh8x16:
+		return rewriteValueThumb_OpLsh8x16(v)
+	case OpLsh8x32:
+		return rewriteValueThumb_OpLsh8x32(v)
+	case OpLsh8x64:
+		return rewriteValueThumb_OpLsh8x64(v)
+	case OpLsh8x8:
+		return rewriteValueThumb_OpLsh8x8(v)
+	case OpMMIOLoad16:
+		return rewriteValueThumb_OpMMIOLoad16(v)
+	case OpMMIOLoad32:
+		return rewriteValueThumb_OpMMIOLoad32(v)
+	case OpMMIOLoad8:
+		return rewriteValueThumb_OpMMIOLoad8(v)
+	case OpMMIOMB:
+		v.Op = OpThumbDSB
+		return true
+	case OpMMIOStore16:
+		return rewriteValueThumb_OpMMIOStore16(v)
+	case OpMMIOStore32:
+		return rewriteValueThumb_OpMMIOStore32(v)
+	case OpMMIOStore8:
+		return rewriteValueThumb_OpMMIOStore8(v)
+	case OpMod16:
+		return rewriteValueThumb_OpMod16(v)
+	case OpMod16u:
+		return rewriteValueThumb_OpMod16u(v)
+	case OpMod32:
+		return rewriteValueThumb_OpMod32(v)
+	case OpMod32u:
+		return rewriteValueThumb_OpMod32u(v)
+	case OpMod8:
+		return rewriteValueThumb_OpMod8(v)
+	case OpMod8u:
+		return rewriteValueThumb_OpMod8u(v)
+	case OpMove:
+		return rewriteValueThumb_OpMove(v)
+	case OpMul16:
+		v.Op = OpThumbMUL
+		return true
+	case OpMul32:
+		v.Op = OpThumbMUL
+		return true
+	case OpMul32F:
+		v.Op = OpThumbMULF
+		return true
+	case OpMul32uhilo:
+		v.Op = OpThumbMULLU
+		return true
+	case OpMul64F:
+		v.Op = OpThumbMULD
+		return true
+	case OpMul8:
+		v.Op = OpThumbMUL
+		return true
+	case OpNeg16:
+		return rewriteValueThumb_OpNeg16(v)
+	case OpNeg32:
+		return rewriteValueThumb_OpNeg32(v)
+	case OpNeg32F:
+		v.Op = OpThumbNEGF
+		return true
+	case OpNeg64F:
+		v.Op = OpThumbNEGD
+		return true
+	case OpNeg8:
+		return rewriteValueThumb_OpNeg8(v)
+	case OpNeq16:
+		return rewriteValueThumb_OpNeq16(v)
+	case OpNeq32:
+		return rewriteValueThumb_OpNeq32(v)
+	case OpNeq32F:
+		return rewriteValueThumb_OpNeq32F(v)
+	case OpNeq64F:
+		return rewriteValueThumb_OpNeq64F(v)
+	case OpNeq8:
+		return rewriteValueThumb_OpNeq8(v)
+	case OpNeqB:
+		v.Op = OpThumbXOR
+		return true
+	case OpNeqPtr:
+		return rewriteValueThumb_OpNeqPtr(v)
+	case OpNilCheck:
+		v.Op = OpThumbLoweredNilCheck
+		return true
+	case OpNot:
+		return rewriteValueThumb_OpNot(v)
+	case OpOffPtr:
+		return rewriteValueThumb_OpOffPtr(v)
+	case OpOr16:
+		v.Op = OpThumbOR
+		return true
+	case OpOr32:
+		v.Op = OpThumbOR
+		return true
+	case OpOr8:
+		v.Op = OpThumbOR
+		return true
+	case OpOrB:
+		v.Op = OpThumbOR
+		return true
+	case OpPanicBounds:
+		return rewriteValueThumb_OpPanicBounds(v)
+	case OpPanicExtend:
+		return rewriteValueThumb_OpPanicExtend(v)
+	case OpPublicationBarrier:
+		v.Op = OpThumbDMB_ST
+		return true
+	case OpRotateLeft16:
+		return rewriteValueThumb_OpRotateLeft16(v)
+	case OpRotateLeft32:
+		return rewriteValueThumb_OpRotateLeft32(v)
+	case OpRotateLeft8:
+		return rewriteValueThumb_OpRotateLeft8(v)
+	case OpRound32F:
+		v.Op = OpCopy
+		return true
+	case OpRound64F:
+		v.Op = OpCopy
+		return true
+	case OpRsh16Ux16:
+		return rewriteValueThumb_OpRsh16Ux16(v)
+	case OpRsh16Ux32:
+		return rewriteValueThumb_OpRsh16Ux32(v)
+	case OpRsh16Ux64:
+		return rewriteValueThumb_OpRsh16Ux64(v)
+	case OpRsh16Ux8:
+		return rewriteValueThumb_OpRsh16Ux8(v)
+	case OpRsh16x16:
+		return rewriteValueThumb_OpRsh16x16(v)
+	case OpRsh16x32:
+		return rewriteValueThumb_OpRsh16x32(v)
+	case OpRsh16x64:
+		return rewriteValueThumb_OpRsh16x64(v)
+	case OpRsh16x8:
+		return rewriteValueThumb_OpRsh16x8(v)
+	case OpRsh32Ux16:
+		return rewriteValueThumb_OpRsh32Ux16(v)
+	case OpRsh32Ux32:
+		return rewriteValueThumb_OpRsh32Ux32(v)
+	case OpRsh32Ux64:
+		return rewriteValueThumb_OpRsh32Ux64(v)
+	case OpRsh32Ux8:
+		return rewriteValueThumb_OpRsh32Ux8(v)
+	case OpRsh32x16:
+		return rewriteValueThumb_OpRsh32x16(v)
+	case OpRsh32x32:
+		return rewriteValueThumb_OpRsh32x32(v)
+	case OpRsh32x64:
+		return rewriteValueThumb_OpRsh32x64(v)
+	case OpRsh32x8:
+		return rewriteValueThumb_OpRsh32x8(v)
+	case OpRsh8Ux16:
+		return rewriteValueThumb_OpRsh8Ux16(v)
+	case OpRsh8Ux32:
+		return rewriteValueThumb_OpRsh8Ux32(v)
+	case OpRsh8Ux64:
+		return rewriteValueThumb_OpRsh8Ux64(v)
+	case OpRsh8Ux8:
+		return rewriteValueThumb_OpRsh8Ux8(v)
+	case OpRsh8x16:
+		return rewriteValueThumb_OpRsh8x16(v)
+	case OpRsh8x32:
+		return rewriteValueThumb_OpRsh8x32(v)
+	case OpRsh8x64:
+		return rewriteValueThumb_OpRsh8x64(v)
+	case OpRsh8x8:
+		return rewriteValueThumb_OpRsh8x8(v)
+	case OpSignExt16to32:
+		v.Op = OpThumbMOVHreg
+		return true
+	case OpSignExt8to16:
+		v.Op = OpThumbMOVBreg
+		return true
+	case OpSignExt8to32:
+		v.Op = OpThumbMOVBreg
+		return true
+	case OpSignmask:
+		return rewriteValueThumb_OpSignmask(v)
+	case OpSlicemask:
+		return rewriteValueThumb_OpSlicemask(v)
+	case OpSqrt:
+		v.Op = OpThumbSQRTD
+		return true
+	case OpStaticCall:
+		v.Op = OpThumbCALLstatic
+		return true
+	case OpStore:
+		return rewriteValueThumb_OpStore(v)
+	case OpSub16:
+		v.Op = OpThumbSUB
+		return true
+	case OpSub32:
+		v.Op = OpThumbSUB
+		return true
+	case OpSub32F:
+		v.Op = OpThumbSUBF
+		return true
+	case OpSub32carry:
+		v.Op = OpThumbSUBS
+		return true
+	case OpSub32withcarry:
+		v.Op = OpThumbSBC
+		return true
+	case OpSub64F:
+		v.Op = OpThumbSUBD
+		return true
+	case OpSub8:
+		v.Op = OpThumbSUB
+		return true
+	case OpSubPtr:
+		v.Op = OpThumbSUB
+		return true
+	case OpThumbADC:
+		return rewriteValueThumb_OpThumbADC(v)
+	case OpThumbADCconst:
+		return rewriteValueThumb_OpThumbADCconst(v)
+	case OpThumbADCshiftLL:
+		return rewriteValueThumb_OpThumbADCshiftLL(v)
+	case OpThumbADCshiftRA:
+		return rewriteValueThumb_OpThumbADCshiftRA(v)
+	case OpThumbADCshiftRL:
+		return rewriteValueThumb_OpThumbADCshiftRL(v)
+	case OpThumbADD:
+		return rewriteValueThumb_OpThumbADD(v)
+	case OpThumbADDD:
+		return rewriteValueThumb_OpThumbADDD(v)
+	case OpThumbADDF:
+		return rewriteValueThumb_OpThumbADDF(v)
+	case OpThumbADDS:
+		return rewriteValueThumb_OpThumbADDS(v)
+	case OpThumbADDSshiftLL:
+		return rewriteValueThumb_OpThumbADDSshiftLL(v)
+	case OpThumbADDSshiftRA:
+		return rewriteValueThumb_OpThumbADDSshiftRA(v)
+	case OpThumbADDSshiftRL:
+		return rewriteValueThumb_OpThumbADDSshiftRL(v)
+	case OpThumbADDconst:
+		return rewriteValueThumb_OpThumbADDconst(v)
+	case OpThumbADDshiftLL:
+		return rewriteValueThumb_OpThumbADDshiftLL(v)
+	case OpThumbADDshiftRA:
+		return rewriteValueThumb_OpThumbADDshiftRA(v)
+	case OpThumbADDshiftRL:
+		return rewriteValueThumb_OpThumbADDshiftRL(v)
+	case OpThumbAND:
+		return rewriteValueThumb_OpThumbAND(v)
+	case OpThumbANDconst:
+		return rewriteValueThumb_OpThumbANDconst(v)
+	case OpThumbANDshiftLL:
+		return rewriteValueThumb_OpThumbANDshiftLL(v)
+	case OpThumbANDshiftRA:
+		return rewriteValueThumb_OpThumbANDshiftRA(v)
+	case OpThumbANDshiftRL:
+		return rewriteValueThumb_OpThumbANDshiftRL(v)
+	case OpThumbBFX:
+		return rewriteValueThumb_OpThumbBFX(v)
+	case OpThumbBFXU:
+		return rewriteValueThumb_OpThumbBFXU(v)
+	case OpThumbBIC:
+		return rewriteValueThumb_OpThumbBIC(v)
+	case OpThumbBICconst:
+		return rewriteValueThumb_OpThumbBICconst(v)
+	case OpThumbBICshiftLL:
+		return rewriteValueThumb_OpThumbBICshiftLL(v)
+	case OpThumbBICshiftRA:
+		return rewriteValueThumb_OpThumbBICshiftRA(v)
+	case OpThumbBICshiftRL:
+		return rewriteValueThumb_OpThumbBICshiftRL(v)
+	case OpThumbCMN:
+		return rewriteValueThumb_OpThumbCMN(v)
+	case OpThumbCMNconst:
+		return rewriteValueThumb_OpThumbCMNconst(v)
+	case OpThumbCMNshiftLL:
+		return rewriteValueThumb_OpThumbCMNshiftLL(v)
+	case OpThumbCMNshiftRA:
+		return rewriteValueThumb_OpThumbCMNshiftRA(v)
+	case OpThumbCMNshiftRL:
+		return rewriteValueThumb_OpThumbCMNshiftRL(v)
+	case OpThumbCMOVWHSconst:
+		return rewriteValueThumb_OpThumbCMOVWHSconst(v)
+	case OpThumbCMOVWLSconst:
+		return rewriteValueThumb_OpThumbCMOVWLSconst(v)
+	case OpThumbCMP:
+		return rewriteValueThumb_OpThumbCMP(v)
+	case OpThumbCMPD:
+		return rewriteValueThumb_OpThumbCMPD(v)
+	case OpThumbCMPF:
+		return rewriteValueThumb_OpThumbCMPF(v)
+	case OpThumbCMPconst:
+		return rewriteValueThumb_OpThumbCMPconst(v)
+	case OpThumbCMPshiftLL:
+		return rewriteValueThumb_OpThumbCMPshiftLL(v)
+	case OpThumbCMPshiftRA:
+		return rewriteValueThumb_OpThumbCMPshiftRA(v)
+	case OpThumbCMPshiftRL:
+		return rewriteValueThumb_OpThumbCMPshiftRL(v)
+	case OpThumbDIV:
+		return rewriteValueThumb_OpThumbDIV(v)
+	case OpThumbDIVU:
+		return rewriteValueThumb_OpThumbDIVU(v)
+	case OpThumbEqual:
+		return rewriteValueThumb_OpThumbEqual(v)
+	case OpThumbGreaterEqual:
+		return rewriteValueThumb_OpThumbGreaterEqual(v)
+	case OpThumbGreaterEqualU:
+		return rewriteValueThumb_OpThumbGreaterEqualU(v)
+	case OpThumbGreaterThan:
+		return rewriteValueThumb_OpThumbGreaterThan(v)
+	case OpThumbGreaterThanU:
+		return rewriteValueThumb_OpThumbGreaterThanU(v)
+	case OpThumbLessEqual:
+		return rewriteValueThumb_OpThumbLessEqual(v)
+	case OpThumbLessEqualU:
+		return rewriteValueThumb_OpThumbLessEqualU(v)
+	case OpThumbLessThan:
+		return rewriteValueThumb_OpThumbLessThan(v)
+	case OpThumbLessThanU:
+		return rewriteValueThumb_OpThumbLessThanU(v)
+	case OpThumbLoadOnce16:
+		return rewriteValueThumb_OpThumbLoadOnce16(v)
+	case OpThumbLoadOnce16idx:
+		return rewriteValueThumb_OpThumbLoadOnce16idx(v)
+	case OpThumbLoadOnce32:
+		return rewriteValueThumb_OpThumbLoadOnce32(v)
+	case OpThumbLoadOnce32idx:
+		return rewriteValueThumb_OpThumbLoadOnce32idx(v)
+	case OpThumbLoadOnce8:
+		return rewriteValueThumb_OpThumbLoadOnce8(v)
+	case OpThumbLoadOnce8idx:
+		return rewriteValueThumb_OpThumbLoadOnce8idx(v)
+	case OpThumbMOVBUload:
+		return rewriteValueThumb_OpThumbMOVBUload(v)
+	case OpThumbMOVBUloadidx:
+		return rewriteValueThumb_OpThumbMOVBUloadidx(v)
+	case OpThumbMOVBUloadshiftLL:
+		return rewriteValueThumb_OpThumbMOVBUloadshiftLL(v)
+	case OpThumbMOVBUreg:
+		return rewriteValueThumb_OpThumbMOVBUreg(v)
+	case OpThumbMOVBload:
+		return rewriteValueThumb_OpThumbMOVBload(v)
+	case OpThumbMOVBloadidx:
+		return rewriteValueThumb_OpThumbMOVBloadidx(v)
+	case OpThumbMOVBloadshiftLL:
+		return rewriteValueThumb_OpThumbMOVBloadshiftLL(v)
+	case OpThumbMOVBreg:
+		return rewriteValueThumb_OpThumbMOVBreg(v)
+	case OpThumbMOVBstore:
+		return rewriteValueThumb_OpThumbMOVBstore(v)
+	case OpThumbMOVBstoreidx:
+		return rewriteValueThumb_OpThumbMOVBstoreidx(v)
+	case OpThumbMOVBstoreshiftLL:
+		return rewriteValueThumb_OpThumbMOVBstoreshiftLL(v)
+	case OpThumbMOVDload:
+		return rewriteValueThumb_OpThumbMOVDload(v)
+	case OpThumbMOVDstore:
+		return rewriteValueThumb_OpThumbMOVDstore(v)
+	case OpThumbMOVFload:
+		return rewriteValueThumb_OpThumbMOVFload(v)
+	case OpThumbMOVFstore:
+		return rewriteValueThumb_OpThumbMOVFstore(v)
+	case OpThumbMOVHUload:
+		return rewriteValueThumb_OpThumbMOVHUload(v)
+	case OpThumbMOVHUloadidx:
+		return rewriteValueThumb_OpThumbMOVHUloadidx(v)
+	case OpThumbMOVHUloadshiftLL:
+		return rewriteValueThumb_OpThumbMOVHUloadshiftLL(v)
+	case OpThumbMOVHUreg:
+		return rewriteValueThumb_OpThumbMOVHUreg(v)
+	case OpThumbMOVHload:
+		return rewriteValueThumb_OpThumbMOVHload(v)
+	case OpThumbMOVHloadidx:
+		return rewriteValueThumb_OpThumbMOVHloadidx(v)
+	case OpThumbMOVHloadshiftLL:
+		return rewriteValueThumb_OpThumbMOVHloadshiftLL(v)
+	case OpThumbMOVHreg:
+		return rewriteValueThumb_OpThumbMOVHreg(v)
+	case OpThumbMOVHstore:
+		return rewriteValueThumb_OpThumbMOVHstore(v)
+	case OpThumbMOVHstoreidx:
+		return rewriteValueThumb_OpThumbMOVHstoreidx(v)
+	case OpThumbMOVHstoreshiftLL:
+		return rewriteValueThumb_OpThumbMOVHstoreshiftLL(v)
+	case OpThumbMOVWload:
+		return rewriteValueThumb_OpThumbMOVWload(v)
+	case OpThumbMOVWloadidx:
+		return rewriteValueThumb_OpThumbMOVWloadidx(v)
+	case OpThumbMOVWloadshiftLL:
+		return rewriteValueThumb_OpThumbMOVWloadshiftLL(v)
+	case OpThumbMOVWreg:
+		return rewriteValueThumb_OpThumbMOVWreg(v)
+	case OpThumbMOVWstore:
+		return rewriteValueThumb_OpThumbMOVWstore(v)
+	case OpThumbMOVWstoreidx:
+		return rewriteValueThumb_OpThumbMOVWstoreidx(v)
+	case OpThumbMOVWstoreshiftLL:
+		return rewriteValueThumb_OpThumbMOVWstoreshiftLL(v)
+	case OpThumbMUL:
+		return rewriteValueThumb_OpThumbMUL(v)
+	case OpThumbMULA:
+		return rewriteValueThumb_OpThumbMULA(v)
+	case OpThumbMULD:
+		return rewriteValueThumb_OpThumbMULD(v)
+	case OpThumbMULF:
+		return rewriteValueThumb_OpThumbMULF(v)
+	case OpThumbMULS:
+		return rewriteValueThumb_OpThumbMULS(v)
+	case OpThumbMVN:
+		return rewriteValueThumb_OpThumbMVN(v)
+	case OpThumbMVNshiftLL:
+		return rewriteValueThumb_OpThumbMVNshiftLL(v)
+	case OpThumbMVNshiftRA:
+		return rewriteValueThumb_OpThumbMVNshiftRA(v)
+	case OpThumbMVNshiftRL:
+		return rewriteValueThumb_OpThumbMVNshiftRL(v)
+	case OpThumbNEGD:
+		return rewriteValueThumb_OpThumbNEGD(v)
+	case OpThumbNEGF:
+		return rewriteValueThumb_OpThumbNEGF(v)
+	case OpThumbNMULD:
+		return rewriteValueThumb_OpThumbNMULD(v)
+	case OpThumbNMULF:
+		return rewriteValueThumb_OpThumbNMULF(v)
+	case OpThumbNotEqual:
+		return rewriteValueThumb_OpThumbNotEqual(v)
+	case OpThumbOR:
+		return rewriteValueThumb_OpThumbOR(v)
+	case OpThumbORN:
+		return rewriteValueThumb_OpThumbORN(v)
+	case OpThumbORNconst:
+		return rewriteValueThumb_OpThumbORNconst(v)
+	case OpThumbORNshiftLL:
+		return rewriteValueThumb_OpThumbORNshiftLL(v)
+	case OpThumbORNshiftRA:
+		return rewriteValueThumb_OpThumbORNshiftRA(v)
+	case OpThumbORNshiftRL:
+		return rewriteValueThumb_OpThumbORNshiftRL(v)
+	case OpThumbORconst:
+		return rewriteValueThumb_OpThumbORconst(v)
+	case OpThumbORshiftLL:
+		return rewriteValueThumb_OpThumbORshiftLL(v)
+	case OpThumbORshiftRA:
+		return rewriteValueThumb_OpThumbORshiftRA(v)
+	case OpThumbORshiftRL:
+		return rewriteValueThumb_OpThumbORshiftRL(v)
+	case OpThumbRSB:
+		return rewriteValueThumb_OpThumbRSB(v)
+	case OpThumbRSBSshiftLL:
+		return rewriteValueThumb_OpThumbRSBSshiftLL(v)
+	case OpThumbRSBSshiftRA:
+		return rewriteValueThumb_OpThumbRSBSshiftRA(v)
+	case OpThumbRSBSshiftRL:
+		return rewriteValueThumb_OpThumbRSBSshiftRL(v)
+	case OpThumbRSBconst:
+		return rewriteValueThumb_OpThumbRSBconst(v)
+	case OpThumbRSBshiftLL:
+		return rewriteValueThumb_OpThumbRSBshiftLL(v)
+	case OpThumbRSBshiftRA:
+		return rewriteValueThumb_OpThumbRSBshiftRA(v)
+	case OpThumbRSBshiftRL:
+		return rewriteValueThumb_OpThumbRSBshiftRL(v)
+	case OpThumbSBC:
+		return rewriteValueThumb_OpThumbSBC(v)
+	case OpThumbSBCconst:
+		return rewriteValueThumb_OpThumbSBCconst(v)
+	case OpThumbSBCshiftLL:
+		return rewriteValueThumb_OpThumbSBCshiftLL(v)
+	case OpThumbSBCshiftRA:
+		return rewriteValueThumb_OpThumbSBCshiftRA(v)
+	case OpThumbSBCshiftRL:
+		return rewriteValueThumb_OpThumbSBCshiftRL(v)
+	case OpThumbSLL:
+		return rewriteValueThumb_OpThumbSLL(v)
+	case OpThumbSLLconst:
+		return rewriteValueThumb_OpThumbSLLconst(v)
+	case OpThumbSRA:
+		return rewriteValueThumb_OpThumbSRA(v)
+	case OpThumbSRAcond:
+		return rewriteValueThumb_OpThumbSRAcond(v)
+	case OpThumbSRAconst:
+		return rewriteValueThumb_OpThumbSRAconst(v)
+	case OpThumbSRL:
+		return rewriteValueThumb_OpThumbSRL(v)
+	case OpThumbSRLconst:
+		return rewriteValueThumb_OpThumbSRLconst(v)
+	case OpThumbSUB:
+		return rewriteValueThumb_OpThumbSUB(v)
+	case OpThumbSUBD:
+		return rewriteValueThumb_OpThumbSUBD(v)
+	case OpThumbSUBF:
+		return rewriteValueThumb_OpThumbSUBF(v)
+	case OpThumbSUBS:
+		return rewriteValueThumb_OpThumbSUBS(v)
+	case OpThumbSUBSshiftLL:
+		return rewriteValueThumb_OpThumbSUBSshiftLL(v)
+	case OpThumbSUBSshiftRA:
+		return rewriteValueThumb_OpThumbSUBSshiftRA(v)
+	case OpThumbSUBSshiftRL:
+		return rewriteValueThumb_OpThumbSUBSshiftRL(v)
+	case OpThumbSUBconst:
+		return rewriteValueThumb_OpThumbSUBconst(v)
+	case OpThumbSUBshiftLL:
+		return rewriteValueThumb_OpThumbSUBshiftLL(v)
+	case OpThumbSUBshiftRA:
+		return rewriteValueThumb_OpThumbSUBshiftRA(v)
+	case OpThumbSUBshiftRL:
+		return rewriteValueThumb_OpThumbSUBshiftRL(v)
+	case OpThumbStoreOnce16:
+		return rewriteValueThumb_OpThumbStoreOnce16(v)
+	case OpThumbStoreOnce16idx:
+		return rewriteValueThumb_OpThumbStoreOnce16idx(v)
+	case OpThumbStoreOnce32:
+		return rewriteValueThumb_OpThumbStoreOnce32(v)
+	case OpThumbStoreOnce32idx:
+		return rewriteValueThumb_OpThumbStoreOnce32idx(v)
+	case OpThumbStoreOnce8:
+		return rewriteValueThumb_OpThumbStoreOnce8(v)
+	case OpThumbStoreOnce8idx:
+		return rewriteValueThumb_OpThumbStoreOnce8idx(v)
+	case OpThumbTEQ:
+		return rewriteValueThumb_OpThumbTEQ(v)
+	case OpThumbTEQconst:
+		return rewriteValueThumb_OpThumbTEQconst(v)
+	case OpThumbTEQshiftLL:
+		return rewriteValueThumb_OpThumbTEQshiftLL(v)
+	case OpThumbTEQshiftRA:
+		return rewriteValueThumb_OpThumbTEQshiftRA(v)
+	case OpThumbTEQshiftRL:
+		return rewriteValueThumb_OpThumbTEQshiftRL(v)
+	case OpThumbTST:
+		return rewriteValueThumb_OpThumbTST(v)
+	case OpThumbTSTconst:
+		return rewriteValueThumb_OpThumbTSTconst(v)
+	case OpThumbTSTshiftLL:
+		return rewriteValueThumb_OpThumbTSTshiftLL(v)
+	case OpThumbTSTshiftRA:
+		return rewriteValueThumb_OpThumbTSTshiftRA(v)
+	case OpThumbTSTshiftRL:
+		return rewriteValueThumb_OpThumbTSTshiftRL(v)
+	case OpThumbXOR:
+		return rewriteValueThumb_OpThumbXOR(v)
+	case OpThumbXORconst:
+		return rewriteValueThumb_OpThumbXORconst(v)
+	case OpThumbXORshiftLL:
+		return rewriteValueThumb_OpThumbXORshiftLL(v)
+	case OpThumbXORshiftRA:
+		return rewriteValueThumb_OpThumbXORshiftRA(v)
+	case OpThumbXORshiftRL:
+		return rewriteValueThumb_OpThumbXORshiftRL(v)
+	case OpThumbXORshiftRR:
+		return rewriteValueThumb_OpThumbXORshiftRR(v)
+	case OpTrunc16to8:
+		v.Op = OpCopy
+		return true
+	case OpTrunc32to16:
+		v.Op = OpCopy
+		return true
+	case OpTrunc32to8:
+		v.Op = OpCopy
+		return true
+	case OpWB:
+		v.Op = OpThumbLoweredWB
+		return true
+	case OpXor16:
+		v.Op = OpThumbXOR
+		return true
+	case OpXor32:
+		v.Op = OpThumbXOR
+		return true
+	case OpXor8:
+		v.Op = OpThumbXOR
+		return true
+	case OpZero:
+		return rewriteValueThumb_OpZero(v)
+	case OpZeroExt16to32:
+		v.Op = OpThumbMOVHUreg
+		return true
+	case OpZeroExt8to16:
+		v.Op = OpThumbMOVBUreg
+		return true
+	case OpZeroExt8to32:
+		v.Op = OpThumbMOVBUreg
+		return true
+	case OpZeromask:
+		return rewriteValueThumb_OpZeromask(v)
+	}
+	return false
+}
+func rewriteValueThumb_OpAddr(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (Addr {sym} base)
+	// result: (MOVWaddr {sym} base)
+	for {
+		sym := auxToSym(v.Aux)
+		base := v_0
+		v.reset(OpThumbMOVWaddr)
+		v.Aux = symToAux(sym)
+		v.AddArg(base)
+		return true
+	}
+}
+func rewriteValueThumb_OpAvg32u(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Avg32u <t> x y)
+	// result: (ADD (SRLconst <t> (SUB <t> x y) [1]) y)
+	for {
+		t := v.Type
+		x := v_0
+		y := v_1
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, t)
+		v0.AuxInt = int32ToAuxInt(1)
+		v1 := b.NewValue0(v.Pos, OpThumbSUB, t)
+		v1.AddArg2(x, y)
+		v0.AddArg(v1)
+		v.AddArg2(v0, y)
+		return true
+	}
+}
+func rewriteValueThumb_OpBitLen32(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (BitLen32 <t> x)
+	// result: (RSBconst [32] (CLZ <t> x))
+	for {
+		t := v.Type
+		x := v_0
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(32)
+		v0 := b.NewValue0(v.Pos, OpThumbCLZ, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpConst16(v *Value) bool {
+	// match: (Const16 [val])
+	// result: (MOVWconst [int32(val)])
+	for {
+		val := auxIntToInt16(v.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(val))
+		return true
+	}
+}
+func rewriteValueThumb_OpConst32(v *Value) bool {
+	// match: (Const32 [val])
+	// result: (MOVWconst [int32(val)])
+	for {
+		val := auxIntToInt32(v.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(val))
+		return true
+	}
+}
+func rewriteValueThumb_OpConst32F(v *Value) bool {
+	// match: (Const32F [val])
+	// result: (MOVFconst [float64(val)])
+	for {
+		val := auxIntToFloat32(v.AuxInt)
+		v.reset(OpThumbMOVFconst)
+		v.AuxInt = float64ToAuxInt(float64(val))
+		return true
+	}
+}
+func rewriteValueThumb_OpConst64F(v *Value) bool {
+	// match: (Const64F [val])
+	// result: (MOVDconst [float64(val)])
+	for {
+		val := auxIntToFloat64(v.AuxInt)
+		v.reset(OpThumbMOVDconst)
+		v.AuxInt = float64ToAuxInt(float64(val))
+		return true
+	}
+}
+func rewriteValueThumb_OpConst8(v *Value) bool {
+	// match: (Const8 [val])
+	// result: (MOVWconst [int32(val)])
+	for {
+		val := auxIntToInt8(v.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(val))
+		return true
+	}
+}
+func rewriteValueThumb_OpConstBool(v *Value) bool {
+	// match: (ConstBool [b])
+	// result: (MOVWconst [b2i32(b)])
+	for {
+		b := auxIntToBool(v.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(b))
+		return true
+	}
+}
+func rewriteValueThumb_OpConstNil(v *Value) bool {
+	// match: (ConstNil)
+	// result: (MOVWconst [0])
+	for {
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz16(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Ctz16 <t> x)
+	// result: (CLZ <t> (RBIT <typ.UInt32> (ORconst <typ.UInt32> [0x10000] x)))
+	for {
+		t := v.Type
+		x := v_0
+		v.reset(OpThumbCLZ)
+		v.Type = t
+		v0 := b.NewValue0(v.Pos, OpThumbRBIT, typ.UInt32)
+		v1 := b.NewValue0(v.Pos, OpThumbORconst, typ.UInt32)
+		v1.AuxInt = int32ToAuxInt(0x10000)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz32(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Ctz32 <t> x)
+	// result: (CLZ <t> (RBIT <t> x))
+	for {
+		t := v.Type
+		x := v_0
+		v.reset(OpThumbCLZ)
+		v.Type = t
+		v0 := b.NewValue0(v.Pos, OpThumbRBIT, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz8(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Ctz8 <t> x)
+	// result: (CLZ <t> (RBIT <typ.UInt32> (ORconst <typ.UInt32> [0x100] x)))
+	for {
+		t := v.Type
+		x := v_0
+		v.reset(OpThumbCLZ)
+		v.Type = t
+		v0 := b.NewValue0(v.Pos, OpThumbRBIT, typ.UInt32)
+		v1 := b.NewValue0(v.Pos, OpThumbORconst, typ.UInt32)
+		v1.AuxInt = int32ToAuxInt(0x100)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Div16 [c] x y)
+	// result: (DIV (SignExt16to32 x) (SignExt16to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbDIV)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv16u(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Div16u x y)
+	// result: (DIVU (ZeroExt16to32 x) (ZeroExt16to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbDIVU)
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (Div32 [c] x y)
+	// result: (DIV x y)
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbDIV)
+		v.AddArg2(x, y)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Div8 x y)
+	// result: (DIV (SignExt8to32 x) (SignExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbDIV)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv8u(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Div8u x y)
+	// result: (DIVU (ZeroExt8to32 x) (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbDIVU)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Eq16 x y)
+	// result: (Equal (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Eq32 x y)
+	// result: (Equal (CMP x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq32F(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Eq32F x y)
+	// result: (Equal (CMPF x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq64F(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Eq64F x y)
+	// result: (Equal (CMPD x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Eq8 x y)
+	// result: (Equal (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEqB(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (EqB x y)
+	// result: (XORconst [1] (XOR <typ.Bool> x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(1)
+		v0 := b.NewValue0(v.Pos, OpThumbXOR, typ.Bool)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEqPtr(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (EqPtr x y)
+	// result: (Equal (CMP x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpFMA(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (FMA x y z)
+	// result: (FMULAD z x y)
+	for {
+		x := v_0
+		y := v_1
+		z := v_2
+		v.reset(OpThumbFMULAD)
+		v.AddArg3(z, x, y)
+		return true
+	}
+}
+func rewriteValueThumb_OpIsInBounds(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (IsInBounds idx len)
+	// result: (LessThanU (CMP idx len))
+	for {
+		idx := v_0
+		len := v_1
+		v.reset(OpThumbLessThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(idx, len)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpIsNonNil(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (IsNonNil ptr)
+	// result: (NotEqual (CMPconst [0] ptr))
+	for {
+		ptr := v_0
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(0)
+		v0.AddArg(ptr)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpIsSliceInBounds(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (IsSliceInBounds idx len)
+	// result: (LessEqualU (CMP idx len))
+	for {
+		idx := v_0
+		len := v_1
+		v.reset(OpThumbLessEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(idx, len)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Leq16 x y)
+	// result: (LessEqual (CMP (SignExt16to32 x) (SignExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq16U(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Leq16U x y)
+	// result: (LessEqualU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Leq32 x y)
+	// result: (LessEqual (CMP x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq32F(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Leq32F x y)
+	// result: (GreaterEqual (CMPF y x))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg2(y, x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq32U(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Leq32U x y)
+	// result: (LessEqualU (CMP x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq64F(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Leq64F x y)
+	// result: (GreaterEqual (CMPD y x))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg2(y, x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Leq8 x y)
+	// result: (LessEqual (CMP (SignExt8to32 x) (SignExt8to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq8U(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Leq8U x y)
+	// result: (LessEqualU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Less16 x y)
+	// result: (LessThan (CMP (SignExt16to32 x) (SignExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess16U(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Less16U x y)
+	// result: (LessThanU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Less32 x y)
+	// result: (LessThan (CMP x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess32F(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Less32F x y)
+	// result: (GreaterThan (CMPF y x))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg2(y, x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess32U(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Less32U x y)
+	// result: (LessThanU (CMP x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess64F(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Less64F x y)
+	// result: (GreaterThan (CMPD y x))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg2(y, x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Less8 x y)
+	// result: (LessThan (CMP (SignExt8to32 x) (SignExt8to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess8U(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Less8U x y)
+	// result: (LessThanU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbLessThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLoad(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (Load <t> ptr mem)
+	// cond: t.IsBoolean()
+	// result: (MOVBUload ptr mem)
+	for {
+		t := v.Type
+		ptr := v_0
+		mem := v_1
+		if !(t.IsBoolean()) {
+			break
+		}
+		v.reset(OpThumbMOVBUload)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is8BitInt(t) && isSigned(t))
+	// result: (MOVBload ptr mem)
+	for {
+		t := v.Type
+		ptr := v_0
+		mem := v_1
+		if !(is8BitInt(t) && isSigned(t)) {
+			break
+		}
+		v.reset(OpThumbMOVBload)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is8BitInt(t) && !isSigned(t))
+	// result: (MOVBUload ptr mem)
+	for {
+		t := v.Type
+		ptr := v_0
+		mem := v_1
+		if !(is8BitInt(t) && !isSigned(t)) {
+			break
+		}
+		v.reset(OpThumbMOVBUload)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is16BitInt(t) && isSigned(t))
+	// result: (MOVHload ptr mem)
+	for {
+		t := v.Type
+		ptr := v_0
+		mem := v_1
+		if !(is16BitInt(t) && isSigned(t)) {
+			break
+		}
+		v.reset(OpThumbMOVHload)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is16BitInt(t) && !isSigned(t))
+	// result: (MOVHUload ptr mem)
+	for {
+		t := v.Type
+		ptr := v_0
+		mem := v_1
+		if !(is16BitInt(t) && !isSigned(t)) {
+			break
+		}
+		v.reset(OpThumbMOVHUload)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is32BitInt(t) || isPtr(t))
+	// result: (MOVWload ptr mem)
+	for {
+		t := v.Type
+		ptr := v_0
+		mem := v_1
+		if !(is32BitInt(t) || isPtr(t)) {
+			break
+		}
+		v.reset(OpThumbMOVWload)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: is32BitFloat(t)
+	// result: (MOVFload ptr mem)
+	for {
+		t := v.Type
+		ptr := v_0
+		mem := v_1
+		if !(is32BitFloat(t)) {
+			break
+		}
+		v.reset(OpThumbMOVFload)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: is64BitFloat(t)
+	// result: (MOVDload ptr mem)
+	for {
+		t := v.Type
+		ptr := v_0
+		mem := v_1
+		if !(is64BitFloat(t)) {
+			break
+		}
+		v.reset(OpThumbMOVDload)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpLocalAddr(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (LocalAddr {sym} base _)
+	// result: (MOVWaddr {sym} base)
+	for {
+		sym := auxToSym(v.Aux)
+		base := v_0
+		v.reset(OpThumbMOVWaddr)
+		v.Aux = symToAux(sym)
+		v.AddArg(base)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh16x16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh16x16 x y)
+	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v0.AddArg2(x, v1)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = int32ToAuxInt(256)
+		v2.AddArg(v1)
+		v.AddArg2(v0, v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh16x32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Lsh16x32 x y)
+	// result: (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg2(x, y)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = int32ToAuxInt(256)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh16x64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (Lsh16x64 x (Const64 [c]))
+	// cond: uint64(c) < 16
+	// result: (SLLconst x [int32(c)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 16) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = int32ToAuxInt(int32(c))
+		v.AddArg(x)
+		return true
+	}
+	// match: (Lsh16x64 _ (Const64 [c]))
+	// cond: uint64(c) >= 16
+	// result: (Const16 [0])
+	for {
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 16) {
+			break
+		}
+		v.reset(OpConst16)
+		v.AuxInt = int16ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpLsh16x8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh16x8 x y)
+	// result: (SLL x (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSLL)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg2(x, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh32x16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh32x16 x y)
+	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v0.AddArg2(x, v1)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = int32ToAuxInt(256)
+		v2.AddArg(v1)
+		v.AddArg2(v0, v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh32x32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Lsh32x32 x y)
+	// result: (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg2(x, y)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = int32ToAuxInt(256)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh32x64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (Lsh32x64 x (Const64 [c]))
+	// cond: uint64(c) < 32
+	// result: (SLLconst x [int32(c)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 32) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = int32ToAuxInt(int32(c))
+		v.AddArg(x)
+		return true
+	}
+	// match: (Lsh32x64 _ (Const64 [c]))
+	// cond: uint64(c) >= 32
+	// result: (Const32 [0])
+	for {
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 32) {
+			break
+		}
+		v.reset(OpConst32)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpLsh32x8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh32x8 x y)
+	// result: (SLL x (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSLL)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg2(x, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh8x16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh8x16 x y)
+	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v0.AddArg2(x, v1)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = int32ToAuxInt(256)
+		v2.AddArg(v1)
+		v.AddArg2(v0, v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh8x32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Lsh8x32 x y)
+	// result: (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg2(x, y)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = int32ToAuxInt(256)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh8x64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (Lsh8x64 x (Const64 [c]))
+	// cond: uint64(c) < 8
+	// result: (SLLconst x [int32(c)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 8) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = int32ToAuxInt(int32(c))
+		v.AddArg(x)
+		return true
+	}
+	// match: (Lsh8x64 _ (Const64 [c]))
+	// cond: uint64(c) >= 8
+	// result: (Const8 [0])
+	for {
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 8) {
+			break
+		}
+		v.reset(OpConst8)
+		v.AuxInt = int8ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpLsh8x8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh8x8 x y)
+	// result: (SLL x (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSLL)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg2(x, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOLoad16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MMIOLoad16 ptr mem)
+	// result: (LoadOnce16 [0] ptr mem)
+	for {
+		ptr := v_0
+		mem := v_1
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOLoad32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MMIOLoad32 ptr mem)
+	// result: (LoadOnce32 [0] ptr mem)
+	for {
+		ptr := v_0
+		mem := v_1
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOLoad8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MMIOLoad8 ptr mem)
+	// result: (LoadOnce8 [0] ptr mem)
+	for {
+		ptr := v_0
+		mem := v_1
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOStore16(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MMIOStore16 ptr val mem)
+	// result: (StoreOnce16 [0] ptr val mem)
+	for {
+		ptr := v_0
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOStore32(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MMIOStore32 ptr val mem)
+	// result: (StoreOnce32 [0] ptr val mem)
+	for {
+		ptr := v_0
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOStore8(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MMIOStore8 ptr val mem)
+	// result: (StoreOnce8 [0] ptr val mem)
+	for {
+		ptr := v_0
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Mod16 x y)
+	// result: (Mod32 (SignExt16to32 x) (SignExt16to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpMod32)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod16u(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Mod16u x y)
+	// result: (Mod32u (ZeroExt16to32 x) (ZeroExt16to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpMod32u)
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Mod32 x y)
+	// result: (SUB x (MUL <y.Type> y (DIV <x.Type> x y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSUB)
+		v0 := b.NewValue0(v.Pos, OpThumbMUL, y.Type)
+		v1 := b.NewValue0(v.Pos, OpThumbDIV, x.Type)
+		v1.AddArg2(x, y)
+		v0.AddArg2(y, v1)
+		v.AddArg2(x, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod32u(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Mod32u x y)
+	// result: (SUB x (MUL <y.Type> y (DIVU <x.Type> x y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSUB)
+		v0 := b.NewValue0(v.Pos, OpThumbMUL, y.Type)
+		v1 := b.NewValue0(v.Pos, OpThumbDIVU, x.Type)
+		v1.AddArg2(x, y)
+		v0.AddArg2(y, v1)
+		v.AddArg2(x, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Mod8 x y)
+	// result: (Mod32 (SignExt8to32 x) (SignExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpMod32)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod8u(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Mod8u x y)
+	// result: (Mod32u (ZeroExt8to32 x) (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpMod32u)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpMove(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	config := b.Func.Config
+	typ := &b.Func.Config.Types
+	// match: (Move [0] _ _ mem)
+	// result: mem
+	for {
+		if auxIntToInt64(v.AuxInt) != 0 {
+			break
+		}
+		mem := v_2
+		v.copyOf(mem)
+		return true
+	}
+	// match: (Move [1] dst src mem)
+	// result: (MOVBstore dst (MOVBUload src mem) mem)
+	for {
+		if auxIntToInt64(v.AuxInt) != 1 {
+			break
+		}
+		dst := v_0
+		src := v_1
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v0.AddArg2(src, mem)
+		v.AddArg3(dst, v0, mem)
+		return true
+	}
+	// match: (Move [2] {t} dst src mem)
+	// cond: t.Alignment()%2 == 0
+	// result: (MOVHstore dst (MOVHUload src mem) mem)
+	for {
+		if auxIntToInt64(v.AuxInt) != 2 {
+			break
+		}
+		t := auxToType(v.Aux)
+		dst := v_0
+		src := v_1
+		mem := v_2
+		if !(t.Alignment()%2 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVHUload, typ.UInt16)
+		v0.AddArg2(src, mem)
+		v.AddArg3(dst, v0, mem)
+		return true
+	}
+	// match: (Move [2] dst src mem)
+	// result: (MOVBstore [1] dst (MOVBUload [1] src mem) (MOVBstore dst (MOVBUload src mem) mem))
+	for {
+		if auxIntToInt64(v.AuxInt) != 2 {
+			break
+		}
+		dst := v_0
+		src := v_1
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(1)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v0.AuxInt = int32ToAuxInt(1)
+		v0.AddArg2(src, mem)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v2.AddArg2(src, mem)
+		v1.AddArg3(dst, v2, mem)
+		v.AddArg3(dst, v0, v1)
+		return true
+	}
+	// match: (Move [4] {t} dst src mem)
+	// cond: t.Alignment()%4 == 0
+	// result: (MOVWstore dst (MOVWload src mem) mem)
+	for {
+		if auxIntToInt64(v.AuxInt) != 4 {
+			break
+		}
+		t := auxToType(v.Aux)
+		dst := v_0
+		src := v_1
+		mem := v_2
+		if !(t.Alignment()%4 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVWstore)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWload, typ.UInt32)
+		v0.AddArg2(src, mem)
+		v.AddArg3(dst, v0, mem)
+		return true
+	}
+	// match: (Move [4] {t} dst src mem)
+	// cond: t.Alignment()%2 == 0
+	// result: (MOVHstore [2] dst (MOVHUload [2] src mem) (MOVHstore dst (MOVHUload src mem) mem))
+	for {
+		if auxIntToInt64(v.AuxInt) != 4 {
+			break
+		}
+		t := auxToType(v.Aux)
+		dst := v_0
+		src := v_1
+		mem := v_2
+		if !(t.Alignment()%2 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(2)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVHUload, typ.UInt16)
+		v0.AuxInt = int32ToAuxInt(2)
+		v0.AddArg2(src, mem)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVHstore, types.TypeMem)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVHUload, typ.UInt16)
+		v2.AddArg2(src, mem)
+		v1.AddArg3(dst, v2, mem)
+		v.AddArg3(dst, v0, v1)
+		return true
+	}
+	// match: (Move [4] dst src mem)
+	// result: (MOVBstore [3] dst (MOVBUload [3] src mem) (MOVBstore [2] dst (MOVBUload [2] src mem) (MOVBstore [1] dst (MOVBUload [1] src mem) (MOVBstore dst (MOVBUload src mem) mem))))
+	for {
+		if auxIntToInt64(v.AuxInt) != 4 {
+			break
+		}
+		dst := v_0
+		src := v_1
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(3)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v0.AuxInt = int32ToAuxInt(3)
+		v0.AddArg2(src, mem)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = int32ToAuxInt(2)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v2.AuxInt = int32ToAuxInt(2)
+		v2.AddArg2(src, mem)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v3.AuxInt = int32ToAuxInt(1)
+		v4 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v4.AuxInt = int32ToAuxInt(1)
+		v4.AddArg2(src, mem)
+		v5 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v6 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v6.AddArg2(src, mem)
+		v5.AddArg3(dst, v6, mem)
+		v3.AddArg3(dst, v4, v5)
+		v1.AddArg3(dst, v2, v3)
+		v.AddArg3(dst, v0, v1)
+		return true
+	}
+	// match: (Move [3] dst src mem)
+	// result: (MOVBstore [2] dst (MOVBUload [2] src mem) (MOVBstore [1] dst (MOVBUload [1] src mem) (MOVBstore dst (MOVBUload src mem) mem)))
+	for {
+		if auxIntToInt64(v.AuxInt) != 3 {
+			break
+		}
+		dst := v_0
+		src := v_1
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(2)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v0.AuxInt = int32ToAuxInt(2)
+		v0.AddArg2(src, mem)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = int32ToAuxInt(1)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v2.AuxInt = int32ToAuxInt(1)
+		v2.AddArg2(src, mem)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v4 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v4.AddArg2(src, mem)
+		v3.AddArg3(dst, v4, mem)
+		v1.AddArg3(dst, v2, v3)
+		v.AddArg3(dst, v0, v1)
+		return true
+	}
+	// match: (Move [s] {t} dst src mem)
+	// cond: s%4 == 0 && s > 4 && s <= 512 && t.Alignment()%4 == 0 && !config.noDuffDevice && logLargeCopy(v, s)
+	// result: (DUFFCOPY [8 * (128 - s/4)] dst src mem)
+	for {
+		s := auxIntToInt64(v.AuxInt)
+		t := auxToType(v.Aux)
+		dst := v_0
+		src := v_1
+		mem := v_2
+		if !(s%4 == 0 && s > 4 && s <= 512 && t.Alignment()%4 == 0 && !config.noDuffDevice && logLargeCopy(v, s)) {
+			break
+		}
+		v.reset(OpThumbDUFFCOPY)
+		v.AuxInt = int64ToAuxInt(8 * (128 - s/4))
+		v.AddArg3(dst, src, mem)
+		return true
+	}
+	// match: (Move [s] {t} dst src mem)
+	// cond: ((s > 512 || config.noDuffDevice) || t.Alignment()%4 != 0) && logLargeCopy(v, s)
+	// result: (LoweredMove [t.Alignment()] dst src (ADDconst <src.Type> src [int32(s-moveSize(t.Alignment(), config))]) mem)
+	for {
+		s := auxIntToInt64(v.AuxInt)
+		t := auxToType(v.Aux)
+		dst := v_0
+		src := v_1
+		mem := v_2
+		if !(((s > 512 || config.noDuffDevice) || t.Alignment()%4 != 0) && logLargeCopy(v, s)) {
+			break
+		}
+		v.reset(OpThumbLoweredMove)
+		v.AuxInt = int64ToAuxInt(t.Alignment())
+		v0 := b.NewValue0(v.Pos, OpThumbADDconst, src.Type)
+		v0.AuxInt = int32ToAuxInt(int32(s - moveSize(t.Alignment(), config)))
+		v0.AddArg(src)
+		v.AddArg4(dst, src, v0, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpNeg16(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (Neg16 x)
+	// result: (RSBconst [0] x)
+	for {
+		x := v_0
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeg32(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (Neg32 x)
+	// result: (RSBconst [0] x)
+	for {
+		x := v_0
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeg8(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (Neg8 x)
+	// result: (RSBconst [0] x)
+	for {
+		x := v_0
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Neq16 x y)
+	// result: (NotEqual (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Neq32 x y)
+	// result: (NotEqual (CMP x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq32F(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Neq32F x y)
+	// result: (NotEqual (CMPF x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq64F(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Neq64F x y)
+	// result: (NotEqual (CMPD x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Neq8 x y)
+	// result: (NotEqual (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeqPtr(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (NeqPtr x y)
+	// result: (NotEqual (CMP x y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNot(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (Not x)
+	// result: (XORconst [1] x)
+	for {
+		x := v_0
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(1)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpOffPtr(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (OffPtr [off] ptr:(SP))
+	// result: (MOVWaddr [int32(off)] ptr)
+	for {
+		off := auxIntToInt64(v.AuxInt)
+		ptr := v_0
+		if ptr.Op != OpSP {
+			break
+		}
+		v.reset(OpThumbMOVWaddr)
+		v.AuxInt = int32ToAuxInt(int32(off))
+		v.AddArg(ptr)
+		return true
+	}
+	// match: (OffPtr [off] ptr)
+	// result: (ADDconst [int32(off)] ptr)
+	for {
+		off := auxIntToInt64(v.AuxInt)
+		ptr := v_0
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(int32(off))
+		v.AddArg(ptr)
+		return true
+	}
+}
+func rewriteValueThumb_OpPanicBounds(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (PanicBounds [kind] x y mem)
+	// cond: boundsABI(kind) == 0
+	// result: (LoweredPanicBoundsA [kind] x y mem)
+	for {
+		kind := auxIntToInt64(v.AuxInt)
+		x := v_0
+		y := v_1
+		mem := v_2
+		if !(boundsABI(kind) == 0) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicBoundsA)
+		v.AuxInt = int64ToAuxInt(kind)
+		v.AddArg3(x, y, mem)
+		return true
+	}
+	// match: (PanicBounds [kind] x y mem)
+	// cond: boundsABI(kind) == 1
+	// result: (LoweredPanicBoundsB [kind] x y mem)
+	for {
+		kind := auxIntToInt64(v.AuxInt)
+		x := v_0
+		y := v_1
+		mem := v_2
+		if !(boundsABI(kind) == 1) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicBoundsB)
+		v.AuxInt = int64ToAuxInt(kind)
+		v.AddArg3(x, y, mem)
+		return true
+	}
+	// match: (PanicBounds [kind] x y mem)
+	// cond: boundsABI(kind) == 2
+	// result: (LoweredPanicBoundsC [kind] x y mem)
+	for {
+		kind := auxIntToInt64(v.AuxInt)
+		x := v_0
+		y := v_1
+		mem := v_2
+		if !(boundsABI(kind) == 2) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicBoundsC)
+		v.AuxInt = int64ToAuxInt(kind)
+		v.AddArg3(x, y, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpPanicExtend(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (PanicExtend [kind] hi lo y mem)
+	// cond: boundsABI(kind) == 0
+	// result: (LoweredPanicExtendA [kind] hi lo y mem)
+	for {
+		kind := auxIntToInt64(v.AuxInt)
+		hi := v_0
+		lo := v_1
+		y := v_2
+		mem := v_3
+		if !(boundsABI(kind) == 0) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicExtendA)
+		v.AuxInt = int64ToAuxInt(kind)
+		v.AddArg4(hi, lo, y, mem)
+		return true
+	}
+	// match: (PanicExtend [kind] hi lo y mem)
+	// cond: boundsABI(kind) == 1
+	// result: (LoweredPanicExtendB [kind] hi lo y mem)
+	for {
+		kind := auxIntToInt64(v.AuxInt)
+		hi := v_0
+		lo := v_1
+		y := v_2
+		mem := v_3
+		if !(boundsABI(kind) == 1) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicExtendB)
+		v.AuxInt = int64ToAuxInt(kind)
+		v.AddArg4(hi, lo, y, mem)
+		return true
+	}
+	// match: (PanicExtend [kind] hi lo y mem)
+	// cond: boundsABI(kind) == 2
+	// result: (LoweredPanicExtendC [kind] hi lo y mem)
+	for {
+		kind := auxIntToInt64(v.AuxInt)
+		hi := v_0
+		lo := v_1
+		y := v_2
+		mem := v_3
+		if !(boundsABI(kind) == 2) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicExtendC)
+		v.AuxInt = int64ToAuxInt(kind)
+		v.AddArg4(hi, lo, y, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRotateLeft16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (RotateLeft16 <t> x (MOVWconst [c]))
+	// result: (Or16 (Lsh16x32 <t> x (MOVWconst [c&15])) (Rsh16Ux32 <t> x (MOVWconst [-c&15])))
+	for {
+		t := v.Type
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpOr16)
+		v0 := b.NewValue0(v.Pos, OpLsh16x32, t)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v1.AuxInt = int32ToAuxInt(c & 15)
+		v0.AddArg2(x, v1)
+		v2 := b.NewValue0(v.Pos, OpRsh16Ux32, t)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v3.AuxInt = int32ToAuxInt(-c & 15)
+		v2.AddArg2(x, v3)
+		v.AddArg2(v0, v2)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRotateLeft32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (RotateLeft32 x (MOVWconst [c]))
+	// result: (SRRconst [-c&31] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = int32ToAuxInt(-c & 31)
+		v.AddArg(x)
+		return true
+	}
+	// match: (RotateLeft32 x y)
+	// result: (SRR x (RSBconst [0] <y.Type> y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRR)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBconst, y.Type)
+		v0.AuxInt = int32ToAuxInt(0)
+		v0.AddArg(y)
+		v.AddArg2(x, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpRotateLeft8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (RotateLeft8 <t> x (MOVWconst [c]))
+	// result: (Or8 (Lsh8x32 <t> x (MOVWconst [c&7])) (Rsh8Ux32 <t> x (MOVWconst [-c&7])))
+	for {
+		t := v.Type
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpOr8)
+		v0 := b.NewValue0(v.Pos, OpLsh8x32, t)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v1.AuxInt = int32ToAuxInt(c & 7)
+		v0.AddArg2(x, v1)
+		v2 := b.NewValue0(v.Pos, OpRsh8Ux32, t)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v3.AuxInt = int32ToAuxInt(-c & 7)
+		v2.AddArg2(x, v3)
+		v.AddArg2(v0, v2)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh16Ux16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16Ux16 x y)
+	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v3 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v3.AuxInt = int32ToAuxInt(256)
+		v3.AddArg(v2)
+		v.AddArg2(v0, v3)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16Ux32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16Ux32 x y)
+	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) y) (CMPconst [256] y) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg2(v1, y)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = int32ToAuxInt(256)
+		v2.AddArg(y)
+		v.AddArg2(v0, v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16Ux64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16Ux64 x (Const64 [c]))
+	// cond: uint64(c) < 16
+	// result: (SRLconst (SLLconst <typ.UInt32> x [16]) [int32(c+16)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 16) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(int32(c + 16))
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(16)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (Rsh16Ux64 _ (Const64 [c]))
+	// cond: uint64(c) >= 16
+	// result: (Const16 [0])
+	for {
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 16) {
+			break
+		}
+		v.reset(OpConst16)
+		v.AuxInt = int16ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh16Ux8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16Ux8 x y)
+	// result: (SRL (ZeroExt16to32 x) (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRL)
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16x16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16x16 x y)
+	// result: (SRAcond (SignExt16to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = int32ToAuxInt(256)
+		v2.AddArg(v1)
+		v.AddArg3(v0, v1, v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16x32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16x32 x y)
+	// result: (SRAcond (SignExt16to32 x) y (CMPconst [256] y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = int32ToAuxInt(256)
+		v1.AddArg(y)
+		v.AddArg3(v0, y, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16x64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16x64 x (Const64 [c]))
+	// cond: uint64(c) < 16
+	// result: (SRAconst (SLLconst <typ.UInt32> x [16]) [int32(c+16)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 16) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(int32(c + 16))
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(16)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (Rsh16x64 x (Const64 [c]))
+	// cond: uint64(c) >= 16
+	// result: (SRAconst (SLLconst <typ.UInt32> x [16]) [31])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 16) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(31)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(16)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh16x8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16x8 x y)
+	// result: (SRA (SignExt16to32 x) (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRA)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32Ux16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh32Ux16 x y)
+	// result: (CMOVWHSconst (SRL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v0.AddArg2(x, v1)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = int32ToAuxInt(256)
+		v2.AddArg(v1)
+		v.AddArg2(v0, v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32Ux32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Rsh32Ux32 x y)
+	// result: (CMOVWHSconst (SRL <x.Type> x y) (CMPconst [256] y) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v0.AddArg2(x, y)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = int32ToAuxInt(256)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32Ux64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (Rsh32Ux64 x (Const64 [c]))
+	// cond: uint64(c) < 32
+	// result: (SRLconst x [int32(c)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 32) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(int32(c))
+		v.AddArg(x)
+		return true
+	}
+	// match: (Rsh32Ux64 _ (Const64 [c]))
+	// cond: uint64(c) >= 32
+	// result: (Const32 [0])
+	for {
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 32) {
+			break
+		}
+		v.reset(OpConst32)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh32Ux8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh32Ux8 x y)
+	// result: (SRL x (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRL)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg2(x, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32x16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh32x16 x y)
+	// result: (SRAcond x (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v0.AddArg(y)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = int32ToAuxInt(256)
+		v1.AddArg(v0)
+		v.AddArg3(x, v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32x32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Rsh32x32 x y)
+	// result: (SRAcond x y (CMPconst [256] y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(256)
+		v0.AddArg(y)
+		v.AddArg3(x, y, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32x64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (Rsh32x64 x (Const64 [c]))
+	// cond: uint64(c) < 32
+	// result: (SRAconst x [int32(c)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 32) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(int32(c))
+		v.AddArg(x)
+		return true
+	}
+	// match: (Rsh32x64 x (Const64 [c]))
+	// cond: uint64(c) >= 32
+	// result: (SRAconst x [31])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 32) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(31)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh32x8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh32x8 x y)
+	// result: (SRA x (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRA)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg2(x, v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8Ux16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8Ux16 x y)
+	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg2(v1, v2)
+		v3 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v3.AuxInt = int32ToAuxInt(256)
+		v3.AddArg(v2)
+		v.AddArg2(v0, v3)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8Ux32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8Ux32 x y)
+	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) y) (CMPconst [256] y) [0])
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(0)
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg2(v1, y)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = int32ToAuxInt(256)
+		v2.AddArg(y)
+		v.AddArg2(v0, v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8Ux64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8Ux64 x (Const64 [c]))
+	// cond: uint64(c) < 8
+	// result: (SRLconst (SLLconst <typ.UInt32> x [24]) [int32(c+24)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 8) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(int32(c + 24))
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(24)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (Rsh8Ux64 _ (Const64 [c]))
+	// cond: uint64(c) >= 8
+	// result: (Const8 [0])
+	for {
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 8) {
+			break
+		}
+		v.reset(OpConst8)
+		v.AuxInt = int8ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh8Ux8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8Ux8 x y)
+	// result: (SRL (ZeroExt8to32 x) (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRL)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8x16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8x16 x y)
+	// result: (SRAcond (SignExt8to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = int32ToAuxInt(256)
+		v2.AddArg(v1)
+		v.AddArg3(v0, v1, v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8x32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8x32 x y)
+	// result: (SRAcond (SignExt8to32 x) y (CMPconst [256] y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = int32ToAuxInt(256)
+		v1.AddArg(y)
+		v.AddArg3(v0, y, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8x64(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8x64 x (Const64 [c]))
+	// cond: uint64(c) < 8
+	// result: (SRAconst (SLLconst <typ.UInt32> x [24]) [int32(c+24)])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) < 8) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(int32(c + 24))
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(24)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (Rsh8x64 x (Const64 [c]))
+	// cond: uint64(c) >= 8
+	// result: (SRAconst (SLLconst <typ.UInt32> x [24]) [31])
+	for {
+		x := v_0
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := auxIntToInt64(v_1.AuxInt)
+		if !(uint64(c) >= 8) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(31)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(24)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh8x8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8x8 x y)
+	// result: (SRA (SignExt8to32 x) (ZeroExt8to32 y))
+	for {
+		x := v_0
+		y := v_1
+		v.reset(OpThumbSRA)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg2(v0, v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpSignmask(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (Signmask x)
+	// result: (SRAconst x [31])
+	for {
+		x := v_0
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(31)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpSlicemask(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (Slicemask <t> x)
+	// result: (SRAconst (RSBconst <t> [0] x) [31])
+	for {
+		t := v.Type
+		x := v_0
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(31)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBconst, t)
+		v0.AuxInt = int32ToAuxInt(0)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpStore(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (Store {t} ptr val mem)
+	// cond: t.Size() == 1
+	// result: (MOVBstore ptr val mem)
+	for {
+		t := auxToType(v.Aux)
+		ptr := v_0
+		val := v_1
+		mem := v_2
+		if !(t.Size() == 1) {
+			break
+		}
+		v.reset(OpThumbMOVBstore)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (Store {t} ptr val mem)
+	// cond: t.Size() == 2
+	// result: (MOVHstore ptr val mem)
+	for {
+		t := auxToType(v.Aux)
+		ptr := v_0
+		val := v_1
+		mem := v_2
+		if !(t.Size() == 2) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (Store {t} ptr val mem)
+	// cond: t.Size() == 4 && !is32BitFloat(val.Type)
+	// result: (MOVWstore ptr val mem)
+	for {
+		t := auxToType(v.Aux)
+		ptr := v_0
+		val := v_1
+		mem := v_2
+		if !(t.Size() == 4 && !is32BitFloat(val.Type)) {
+			break
+		}
+		v.reset(OpThumbMOVWstore)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (Store {t} ptr val mem)
+	// cond: t.Size() == 4 && is32BitFloat(val.Type)
+	// result: (MOVFstore ptr val mem)
+	for {
+		t := auxToType(v.Aux)
+		ptr := v_0
+		val := v_1
+		mem := v_2
+		if !(t.Size() == 4 && is32BitFloat(val.Type)) {
+			break
+		}
+		v.reset(OpThumbMOVFstore)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (Store {t} ptr val mem)
+	// cond: t.Size() == 8 && is64BitFloat(val.Type)
+	// result: (MOVDstore ptr val mem)
+	for {
+		t := auxToType(v.Aux)
+		ptr := v_0
+		val := v_1
+		mem := v_2
+		if !(t.Size() == 8 && is64BitFloat(val.Type)) {
+			break
+		}
+		v.reset(OpThumbMOVDstore)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADC(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ADC (MOVWconst [c]) x flags)
+	// result: (ADCconst [c] x flags)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_0.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_0.AuxInt)
+			x := v_1
+			flags := v_2
+			v.reset(OpThumbADCconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, flags)
+			return true
+		}
+		break
+	}
+	// match: (ADC x (SLLconst [c] y) flags)
+	// result: (ADCshiftLL x y [c] flags)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			flags := v_2
+			v.reset(OpThumbADCshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg3(x, y, flags)
+			return true
+		}
+		break
+	}
+	// match: (ADC x (SRLconst [c] y) flags)
+	// result: (ADCshiftRL x y [c] flags)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			flags := v_2
+			v.reset(OpThumbADCshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg3(x, y, flags)
+			return true
+		}
+		break
+	}
+	// match: (ADC x (SRAconst [c] y) flags)
+	// result: (ADCshiftRA x y [c] flags)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			flags := v_2
+			v.reset(OpThumbADCshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg3(x, y, flags)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADCconst(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ADCconst [c] (ADDconst [d] x) flags)
+	// result: (ADCconst [c+d] x flags)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		flags := v_1
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int32ToAuxInt(c + d)
+		v.AddArg2(x, flags)
+		return true
+	}
+	// match: (ADCconst [c] (SUBconst [d] x) flags)
+	// result: (ADCconst [c-d] x flags)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		flags := v_1
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int32ToAuxInt(c - d)
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADCshiftLL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADCshiftLL (MOVWconst [c]) x [d] flags)
+	// result: (ADCconst [c] (SLLconst <x.Type> x [d]) flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		flags := v_2
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg2(v0, flags)
+		return true
+	}
+	// match: (ADCshiftLL x (MOVWconst [c]) [d] flags)
+	// result: (ADCconst x [c<<uint64(d)] flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		flags := v_2
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADCshiftRA(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADCshiftRA (MOVWconst [c]) x [d] flags)
+	// result: (ADCconst [c] (SRAconst <x.Type> x [d]) flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		flags := v_2
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg2(v0, flags)
+		return true
+	}
+	// match: (ADCshiftRA x (MOVWconst [c]) [d] flags)
+	// result: (ADCconst x [c>>uint64(d)] flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		flags := v_2
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADCshiftRL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADCshiftRL (MOVWconst [c]) x [d] flags)
+	// result: (ADCconst [c] (SRLconst <x.Type> x [d]) flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		flags := v_2
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg2(v0, flags)
+		return true
+	}
+	// match: (ADCshiftRL x (MOVWconst [c]) [d] flags)
+	// result: (ADCconst x [int32(uint32(c)>>uint64(d))] flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		flags := v_2
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADD(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADD x (MOVWconst [c]))
+	// result: (ADDconst [c] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbADDconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (ADD x (SLLconst [c] y))
+	// result: (ADDshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbADDshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (ADD x (SRLconst [c] y))
+	// result: (ADDshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbADDshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (ADD x (SRAconst [c] y))
+	// result: (ADDshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbADDshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (ADD x (RSBconst [0] y))
+	// result: (SUB x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbRSBconst || auxIntToInt32(v_1.AuxInt) != 0 {
+				continue
+			}
+			y := v_1.Args[0]
+			v.reset(OpThumbSUB)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (ADD <t> (RSBconst [c] x) (RSBconst [d] y))
+	// result: (RSBconst [c+d] (ADD <t> x y))
+	for {
+		t := v.Type
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_0.Op != OpThumbRSBconst {
+				continue
+			}
+			c := auxIntToInt32(v_0.AuxInt)
+			x := v_0.Args[0]
+			if v_1.Op != OpThumbRSBconst {
+				continue
+			}
+			d := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbRSBconst)
+			v.AuxInt = int32ToAuxInt(c + d)
+			v0 := b.NewValue0(v.Pos, OpThumbADD, t)
+			v0.AddArg2(x, y)
+			v.AddArg(v0)
+			return true
+		}
+		break
+	}
+	// match: (ADD (MUL x y) a)
+	// result: (MULA x y a)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_0.Op != OpThumbMUL {
+				continue
+			}
+			y := v_0.Args[1]
+			x := v_0.Args[0]
+			a := v_1
+			v.reset(OpThumbMULA)
+			v.AddArg3(x, y, a)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDD(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ADDD a (MULD x y))
+	// cond: a.Uses == 1
+	// result: (MULAD a x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			a := v_0
+			if v_1.Op != OpThumbMULD {
+				continue
+			}
+			y := v_1.Args[1]
+			x := v_1.Args[0]
+			if !(a.Uses == 1) {
+				continue
+			}
+			v.reset(OpThumbMULAD)
+			v.AddArg3(a, x, y)
+			return true
+		}
+		break
+	}
+	// match: (ADDD a (NMULD x y))
+	// cond: a.Uses == 1
+	// result: (MULSD a x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			a := v_0
+			if v_1.Op != OpThumbNMULD {
+				continue
+			}
+			y := v_1.Args[1]
+			x := v_1.Args[0]
+			if !(a.Uses == 1) {
+				continue
+			}
+			v.reset(OpThumbMULSD)
+			v.AddArg3(a, x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDF(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ADDF a (MULF x y))
+	// cond: a.Uses == 1
+	// result: (MULAF a x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			a := v_0
+			if v_1.Op != OpThumbMULF {
+				continue
+			}
+			y := v_1.Args[1]
+			x := v_1.Args[0]
+			if !(a.Uses == 1) {
+				continue
+			}
+			v.reset(OpThumbMULAF)
+			v.AddArg3(a, x, y)
+			return true
+		}
+		break
+	}
+	// match: (ADDF a (NMULF x y))
+	// cond: a.Uses == 1
+	// result: (MULSF a x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			a := v_0
+			if v_1.Op != OpThumbNMULF {
+				continue
+			}
+			y := v_1.Args[1]
+			x := v_1.Args[0]
+			if !(a.Uses == 1) {
+				continue
+			}
+			v.reset(OpThumbMULSF)
+			v.AddArg3(a, x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDS(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ADDS x (MOVWconst [c]))
+	// result: (ADDSconst [c] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbADDSconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (ADDS x (SLLconst [c] y))
+	// result: (ADDSshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbADDSshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (ADDS x (SRLconst [c] y))
+	// result: (ADDSshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbADDSshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (ADDS x (SRAconst [c] y))
+	// result: (ADDSshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbADDSshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDSshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADDSshiftLL (MOVWconst [c]) x [d])
+	// result: (ADDSconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDSshiftLL x (MOVWconst [c]) [d])
+	// result: (ADDSconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDSshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADDSshiftRA (MOVWconst [c]) x [d])
+	// result: (ADDSconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDSshiftRA x (MOVWconst [c]) [d])
+	// result: (ADDSconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDSshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADDSshiftRL (MOVWconst [c]) x [d])
+	// result: (ADDSconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDSshiftRL x (MOVWconst [c]) [d])
+	// result: (ADDSconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (ADDconst [off1] (MOVWaddr [off2] {sym} ptr))
+	// result: (MOVWaddr [off1+off2] {sym} ptr)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVWaddr)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg(ptr)
+		return true
+	}
+	// match: (ADDconst [0] x)
+	// result: x
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		x := v_0
+		v.copyOf(x)
+		return true
+	}
+	// match: (ADDconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [c+d])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c + d)
+		return true
+	}
+	// match: (ADDconst [c] (ADDconst [d] x))
+	// result: (ADDconst [c+d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c + d)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDconst [c] (SUBconst [d] x))
+	// result: (ADDconst [c-d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c - d)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDconst [c] (RSBconst [d] x))
+	// result: (RSBconst [c+d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c + d)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (ADDshiftLL (MOVWconst [c]) x [d])
+	// result: (ADDconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDshiftLL x (MOVWconst [c]) [d])
+	// result: (ADDconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDshiftLL [c] (SRLconst x [32-c]) x)
+	// result: (SRRconst [32-c] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSRLconst || auxIntToInt32(v_0.AuxInt) != 32-c {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = int32ToAuxInt(32 - c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDshiftLL <typ.UInt16> [8] (BFXU <typ.UInt16> [int32(armBFAuxInt(8, 8))] x) x)
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 || auxIntToInt32(v.AuxInt) != 8 || v_0.Op != OpThumbBFXU || v_0.Type != typ.UInt16 || auxIntToInt32(v_0.AuxInt) != int32(armBFAuxInt(8, 8)) {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDshiftLL <typ.UInt16> [8] (SRLconst <typ.UInt16> [24] (SLLconst [16] x)) x)
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 || auxIntToInt32(v.AuxInt) != 8 || v_0.Op != OpThumbSRLconst || v_0.Type != typ.UInt16 || auxIntToInt32(v_0.AuxInt) != 24 {
+			break
+		}
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbSLLconst || auxIntToInt32(v_0_0.AuxInt) != 16 {
+			break
+		}
+		x := v_0_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADDshiftRA (MOVWconst [c]) x [d])
+	// result: (ADDconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDshiftRA x (MOVWconst [c]) [d])
+	// result: (ADDconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ADDshiftRL (MOVWconst [c]) x [d])
+	// result: (ADDconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDshiftRL x (MOVWconst [c]) [d])
+	// result: (ADDconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDshiftRL [c] (SLLconst x [32-c]) x)
+	// result: (SRRconst [ c] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSLLconst || auxIntToInt32(v_0.AuxInt) != 32-c {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbAND(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (AND x (MOVWconst [c]))
+	// result: (ANDconst [c] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbANDconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (AND x (SLLconst [c] y))
+	// result: (ANDshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbANDshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (AND x (SRLconst [c] y))
+	// result: (ANDshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbANDshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (AND x (SRAconst [c] y))
+	// result: (ANDshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbANDshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (AND x x)
+	// result: x
+	for {
+		x := v_0
+		if x != v_1 {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (AND x (MVN y))
+	// result: (BIC x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMVN {
+				continue
+			}
+			y := v_1.Args[0]
+			v.reset(OpThumbBIC)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (AND x (MVNshiftLL y [c]))
+	// result: (BICshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMVNshiftLL {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbBICshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (AND x (MVNshiftRL y [c]))
+	// result: (BICshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMVNshiftRL {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbBICshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (AND x (MVNshiftRA y [c]))
+	// result: (BICshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMVNshiftRA {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbBICshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbANDconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (ANDconst [c] y:(MOVBUreg _))
+	// cond: c&0xFF == 0xFF
+	// result: y
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		y := v_0
+		if y.Op != OpThumbMOVBUreg || !(c&0xFF == 0xFF) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	// match: (ANDconst [c] y:(MOVHUreg _))
+	// cond: c&0xFFFF == 0xFFFF
+	// result: y
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		y := v_0
+		if y.Op != OpThumbMOVHUreg || !(c&0xFFFF == 0xFFFF) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	// match: (ANDconst [c] (MOVBUreg x))
+	// result: (ANDconst [c&0xFF] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVBUreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c & 0xFF)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDconst [c] (MOVHUreg x))
+	// result: (ANDconst [c&0xFFFF] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVHUreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c & 0xFFFF)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDconst [0] _)
+	// result: (MOVWconst [0])
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	// match: (ANDconst [c] x)
+	// cond: int32(c)==-1
+	// result: x
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if !(int32(c) == -1) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (ANDconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [c&d])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c & d)
+		return true
+	}
+	// match: (ANDconst [c] (ANDconst [d] x))
+	// result: (ANDconst [c&d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c & d)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbANDshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ANDshiftLL (MOVWconst [c]) x [d])
+	// result: (ANDconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ANDshiftLL x (MOVWconst [c]) [d])
+	// result: (ANDconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDshiftLL x y:(SLLconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		y := v_1
+		if y.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(y.AuxInt)
+		if x != y.Args[0] || !(c == d) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbANDshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ANDshiftRA (MOVWconst [c]) x [d])
+	// result: (ANDconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ANDshiftRA x (MOVWconst [c]) [d])
+	// result: (ANDconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDshiftRA x y:(SRAconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		y := v_1
+		if y.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(y.AuxInt)
+		if x != y.Args[0] || !(c == d) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbANDshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ANDshiftRL (MOVWconst [c]) x [d])
+	// result: (ANDconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ANDshiftRL x (MOVWconst [c]) [d])
+	// result: (ANDconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDshiftRL x y:(SRLconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		y := v_1
+		if y.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(y.AuxInt)
+		if x != y.Args[0] || !(c == d) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBFX(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (BFX [c] (MOVWconst [d]))
+	// result: (MOVWconst [d<<(32-uint32(c&0xff)-uint32(c>>8))>>(32-uint32(c>>8))])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(d << (32 - uint32(c&0xff) - uint32(c>>8)) >> (32 - uint32(c>>8)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBFXU(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (BFXU [c] (MOVWconst [d]))
+	// result: (MOVWconst [int32(uint32(d)<<(32-uint32(c&0xff)-uint32(c>>8))>>(32-uint32(c>>8)))])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(d) << (32 - uint32(c&0xff) - uint32(c>>8)) >> (32 - uint32(c>>8))))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBIC(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (BIC x (MOVWconst [c]))
+	// result: (BICconst [c] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (BIC x (SLLconst [c] y))
+	// result: (BICshiftLL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (BIC x (SRLconst [c] y))
+	// result: (BICshiftRL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (BIC x (SRAconst [c] y))
+	// result: (BICshiftRA x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (BIC x x)
+	// result: (MOVWconst [0])
+	for {
+		x := v_0
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBICconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (BICconst [0] x)
+	// result: x
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		x := v_0
+		v.copyOf(x)
+		return true
+	}
+	// match: (BICconst [c] _)
+	// cond: int32(c)==-1
+	// result: (MOVWconst [0])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	// match: (BICconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [d&^c])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(d &^ c)
+		return true
+	}
+	// match: (BICconst [c] (BICconst [d] x))
+	// result: (BICconst [c|d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbBICconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int32ToAuxInt(c | d)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBICshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (BICshiftLL x (MOVWconst [c]) [d])
+	// result: (BICconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (BICshiftLL x (SLLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBICshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (BICshiftRA x (MOVWconst [c]) [d])
+	// result: (BICconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (BICshiftRA x (SRAconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBICshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (BICshiftRL x (MOVWconst [c]) [d])
+	// result: (BICconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (BICshiftRL x (SRLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMN(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (CMN x (MOVWconst [c]))
+	// result: (CMNconst [c] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbCMNconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (CMN x (SLLconst [c] y))
+	// result: (CMNshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbCMNshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (CMN x (SRLconst [c] y))
+	// result: (CMNshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbCMNshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (CMN x (SRAconst [c] y))
+	// result: (CMNshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbCMNshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMNconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (CMNconst (MOVWconst [x]) [y])
+	// result: (FlagConstant [addFlags32(x,y)])
+	for {
+		y := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbFlagConstant)
+		v.AuxInt = flagConstantToAuxInt(addFlags32(x, y))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMNshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (CMNshiftLL (MOVWconst [c]) x [d])
+	// result: (CMNconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMNshiftLL x (MOVWconst [c]) [d])
+	// result: (CMNconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMNshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (CMNshiftRA (MOVWconst [c]) x [d])
+	// result: (CMNconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMNshiftRA x (MOVWconst [c]) [d])
+	// result: (CMNconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMNshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (CMNshiftRL (MOVWconst [c]) x [d])
+	// result: (CMNconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMNshiftRL x (MOVWconst [c]) [d])
+	// result: (CMNconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMOVWHSconst(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (CMOVWHSconst _ (FlagConstant [fc]) [c])
+	// cond: fc.uge()
+	// result: (MOVWconst [c])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_1.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_1.AuxInt)
+		if !(fc.uge()) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c)
+		return true
+	}
+	// match: (CMOVWHSconst x (FlagConstant [fc]) [c])
+	// cond: fc.ult()
+	// result: x
+	for {
+		x := v_0
+		if v_1.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_1.AuxInt)
+		if !(fc.ult()) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (CMOVWHSconst x (InvertFlags flags) [c])
+	// result: (CMOVWLSconst x flags [c])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbInvertFlags {
+			break
+		}
+		flags := v_1.Args[0]
+		v.reset(OpThumbCMOVWLSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMOVWLSconst(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (CMOVWLSconst _ (FlagConstant [fc]) [c])
+	// cond: fc.ule()
+	// result: (MOVWconst [c])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_1.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_1.AuxInt)
+		if !(fc.ule()) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c)
+		return true
+	}
+	// match: (CMOVWLSconst x (FlagConstant [fc]) [c])
+	// cond: fc.ugt()
+	// result: x
+	for {
+		x := v_0
+		if v_1.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_1.AuxInt)
+		if !(fc.ugt()) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (CMOVWLSconst x (InvertFlags flags) [c])
+	// result: (CMOVWHSconst x flags [c])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbInvertFlags {
+			break
+		}
+		flags := v_1.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMP(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (CMP x (MOVWconst [c]))
+	// result: (CMPconst [c] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbCMPconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (CMP (MOVWconst [c]) x)
+	// result: (InvertFlags (CMPconst [c] x))
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(c)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMP x y)
+	// cond: x.ID > y.ID
+	// result: (InvertFlags (CMP y x))
+	for {
+		x := v_0
+		y := v_1
+		if !(x.ID > y.ID) {
+			break
+		}
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg2(y, x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMP x (SLLconst [c] y))
+	// result: (CMPshiftLL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbCMPshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (CMP (SLLconst [c] y) x)
+	// result: (InvertFlags (CMPshiftLL x y [c]))
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(c)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMP x (SRLconst [c] y))
+	// result: (CMPshiftRL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbCMPshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (CMP (SRLconst [c] y) x)
+	// result: (InvertFlags (CMPshiftRL x y [c]))
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(c)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMP x (SRAconst [c] y))
+	// result: (CMPshiftRA x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbCMPshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (CMP (SRAconst [c] y) x)
+	// result: (InvertFlags (CMPshiftRA x y [c]))
+	for {
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(c)
+		v0.AddArg2(x, y)
+		v.AddArg(v0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPD(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (CMPD x (MOVDconst [0]))
+	// result: (CMPD0 x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVDconst || auxIntToFloat64(v_1.AuxInt) != 0 {
+			break
+		}
+		v.reset(OpThumbCMPD0)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPF(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (CMPF x (MOVFconst [0]))
+	// result: (CMPF0 x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVFconst || auxIntToFloat64(v_1.AuxInt) != 0 {
+			break
+		}
+		v.reset(OpThumbCMPF0)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (CMPconst (MOVWconst [x]) [y])
+	// result: (FlagConstant [subFlags32(x,y)])
+	for {
+		y := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbFlagConstant)
+		v.AuxInt = flagConstantToAuxInt(subFlags32(x, y))
+		return true
+	}
+	// match: (CMPconst (MOVBUreg _) [c])
+	// cond: 0xff < c
+	// result: (FlagConstant [subFlags32(0, 1)])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVBUreg || !(0xff < c) {
+			break
+		}
+		v.reset(OpThumbFlagConstant)
+		v.AuxInt = flagConstantToAuxInt(subFlags32(0, 1))
+		return true
+	}
+	// match: (CMPconst (MOVHUreg _) [c])
+	// cond: 0xffff < c
+	// result: (FlagConstant [subFlags32(0, 1)])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVHUreg || !(0xffff < c) {
+			break
+		}
+		v.reset(OpThumbFlagConstant)
+		v.AuxInt = flagConstantToAuxInt(subFlags32(0, 1))
+		return true
+	}
+	// match: (CMPconst (ANDconst _ [m]) [n])
+	// cond: 0 <= m && m < n
+	// result: (FlagConstant [subFlags32(0, 1)])
+	for {
+		n := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		m := auxIntToInt32(v_0.AuxInt)
+		if !(0 <= m && m < n) {
+			break
+		}
+		v.reset(OpThumbFlagConstant)
+		v.AuxInt = flagConstantToAuxInt(subFlags32(0, 1))
+		return true
+	}
+	// match: (CMPconst (SRLconst _ [c]) [n])
+	// cond: 0 <= n && 0 < c && c <= 32 && (1<<uint32(32-c)) <= uint32(n)
+	// result: (FlagConstant [subFlags32(0, 1)])
+	for {
+		n := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		if !(0 <= n && 0 < c && c <= 32 && (1<<uint32(32-c)) <= uint32(n)) {
+			break
+		}
+		v.reset(OpThumbFlagConstant)
+		v.AuxInt = flagConstantToAuxInt(subFlags32(0, 1))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (CMPshiftLL (MOVWconst [c]) x [d])
+	// result: (InvertFlags (CMPconst [c] (SLLconst <x.Type> x [d])))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(c)
+		v1 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v1.AuxInt = int32ToAuxInt(d)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMPshiftLL x (MOVWconst [c]) [d])
+	// result: (CMPconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbCMPconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (CMPshiftRA (MOVWconst [c]) x [d])
+	// result: (InvertFlags (CMPconst [c] (SRAconst <x.Type> x [d])))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(c)
+		v1 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v1.AuxInt = int32ToAuxInt(d)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMPshiftRA x (MOVWconst [c]) [d])
+	// result: (CMPconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbCMPconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (CMPshiftRL (MOVWconst [c]) x [d])
+	// result: (InvertFlags (CMPconst [c] (SRLconst <x.Type> x [d])))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = int32ToAuxInt(c)
+		v1 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v1.AuxInt = int32ToAuxInt(d)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMPshiftRL x (MOVWconst [c]) [d])
+	// result: (CMPconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbCMPconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbDIV(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (DIV x (MOVWconst [1]))
+	// result: x
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst || auxIntToInt32(v_1.AuxInt) != 1 {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (DIV x (MOVWconst [c]))
+	// cond: isPowerOfTwo32(c)
+	// result: (SRAconst [int32(log32(c))] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if !(isPowerOfTwo32(c)) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(int32(log32(c)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (DIV (MOVWconst [c]) (MOVWconst [d]))
+	// cond: d != 0
+	// result: (MOVWconst [c/d])
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_1.AuxInt)
+		if !(d != 0) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c / d)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbDIVU(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (DIVU x (MOVWconst [1]))
+	// result: x
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst || auxIntToInt32(v_1.AuxInt) != 1 {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (DIVU x (MOVWconst [c]))
+	// cond: isPowerOfTwo32(c)
+	// result: (SRLconst [int32(log32(c))] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if !(isPowerOfTwo32(c)) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(int32(log32(c)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (DIVU (MOVWconst [c]) (MOVWconst [d]))
+	// cond: d != 0
+	// result: (MOVWconst [int32(uint32(c)/uint32(d))])
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_1.AuxInt)
+		if !(d != 0) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) / uint32(d)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbEqual(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (Equal (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.eq())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.eq()))
+		return true
+	}
+	// match: (Equal (InvertFlags x))
+	// result: (Equal x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbEqual)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbGreaterEqual(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (GreaterEqual (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.ge())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.ge()))
+		return true
+	}
+	// match: (GreaterEqual (InvertFlags x))
+	// result: (LessEqual x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbLessEqual)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbGreaterEqualU(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (GreaterEqualU (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.uge())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.uge()))
+		return true
+	}
+	// match: (GreaterEqualU (InvertFlags x))
+	// result: (LessEqualU x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbLessEqualU)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbGreaterThan(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (GreaterThan (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.gt())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.gt()))
+		return true
+	}
+	// match: (GreaterThan (InvertFlags x))
+	// result: (LessThan x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbLessThan)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbGreaterThanU(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (GreaterThanU (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.ugt())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.ugt()))
+		return true
+	}
+	// match: (GreaterThanU (InvertFlags x))
+	// result: (LessThanU x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbLessThanU)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLessEqual(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (LessEqual (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.le())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.le()))
+		return true
+	}
+	// match: (LessEqual (InvertFlags x))
+	// result: (GreaterEqual x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLessEqualU(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (LessEqualU (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.ule())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.ule()))
+		return true
+	}
+	// match: (LessEqualU (InvertFlags x))
+	// result: (GreaterEqualU x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbGreaterEqualU)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLessThan(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (LessThan (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.lt())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.lt()))
+		return true
+	}
+	// match: (LessThan (InvertFlags x))
+	// result: (GreaterThan x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLessThanU(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (LessThanU (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.ult())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.ult()))
+		return true
+	}
+	// match: (LessThanU (InvertFlags x))
+	// result: (GreaterThanU x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbGreaterThanU)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce16(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (LoadOnce16 [off1] (ADDconst [off2] ptr) mem)
+	// result: (LoadOnce16 [off1+off2] ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce16 [off1] (SUBconst [off2] ptr) mem)
+	// result: (LoadOnce16 [off1-off2] ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce16 [0] (ADD ptr idx) mem)
+	// result: (LoadOnce16idx ptr idx mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce16idx)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (LoadOnce16 [0] (ADDshiftLL ptr idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce16shiftLL ptr idx [c] mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce16shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce16idx(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (LoadOnce16idx ptr (MOVWconst [c]) mem)
+	// result: (LoadOnce16 [c] ptr mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce16idx (MOVWconst [c]) ptr mem)
+	// result: (LoadOnce16 [c] ptr mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		mem := v_2
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce16idx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce16shiftLL ptr idx [c] mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce16shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce32(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (LoadOnce32 [off1] (ADDconst [off2] ptr) mem)
+	// result: (LoadOnce32 [off1+off2] ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce32 [off1] (SUBconst [off2] ptr) mem)
+	// result: (LoadOnce32 [off1-off2] ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce32 [0] (ADD ptr idx) mem)
+	// result: (LoadOnce32idx ptr idx mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce32idx)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (LoadOnce32 [0] (ADDshiftLL ptr idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce32shiftLL ptr idx [c] mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce32shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce32idx(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (LoadOnce32idx ptr (MOVWconst [c]) mem)
+	// result: (LoadOnce32 [c] ptr mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce32idx (MOVWconst [c]) ptr mem)
+	// result: (LoadOnce32 [c] ptr mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		mem := v_2
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce32idx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce32shiftLL ptr idx [c] mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce32shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce8(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (LoadOnce8 [off1] (ADDconst [off2] ptr) mem)
+	// result: (LoadOnce8 [off1+off2] ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce8 [off1] (SUBconst [off2] ptr) mem)
+	// result: (LoadOnce8 [off1-off2] ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce8 [0] (ADD ptr idx) mem)
+	// result: (LoadOnce8idx ptr idx mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbLoadOnce8idx)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (LoadOnce8 [0] (ADDshiftLL ptr idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce8shiftLL ptr idx [c] mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce8shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce8idx(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (LoadOnce8idx ptr (MOVWconst [c]) mem)
+	// result: (LoadOnce8 [c] ptr mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce8idx (MOVWconst [c]) ptr mem)
+	// result: (LoadOnce8 [c] ptr mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		mem := v_2
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (LoadOnce8idx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce8shiftLL ptr idx [c] mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce8shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBUload(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBUload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// result: (MOVBUload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBUload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// result: (MOVBUload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBUload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVBUload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBUload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: (MOVBUreg x)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVBstore {
+			break
+		}
+		off2 := auxIntToInt32(v_1.AuxInt)
+		sym2 := auxToSym(v_1.Aux)
+		x := v_1.Args[1]
+		ptr2 := v_1.Args[0]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVBUloadidx ptr idx mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVBUloadidx)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVBUload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVBUloadshiftLL ptr idx [c] mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBUloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVBUload [off] {sym} (SB) _)
+	// cond: symIsRO(sym)
+	// result: (MOVWconst [int32(read8(sym, int64(off)))])
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpSB || !(symIsRO(sym)) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(read8(sym, int64(off))))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBUloadidx(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBUloadidx ptr idx (MOVBstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: (MOVBUreg x)
+	for {
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVBstoreidx {
+			break
+		}
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUloadidx ptr (MOVWconst [c]) mem)
+	// result: (MOVBUload [c] ptr mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBUloadidx (MOVWconst [c]) ptr mem)
+	// result: (MOVBUload [c] ptr mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		mem := v_2
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBUloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVBUloadshiftLL ptr idx [c] mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBUloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVBUloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVBUloadshiftLL ptr idx [c] mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBUloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBUloadshiftLL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBUloadshiftLL ptr idx [c] (MOVBstoreshiftLL ptr2 idx [d] x _))
+	// cond: c==d && isSamePtr(ptr, ptr2)
+	// result: (MOVBUreg x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVBstoreshiftLL {
+			break
+		}
+		d := auxIntToInt32(v_2.AuxInt)
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(c == d && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUloadshiftLL ptr (MOVWconst [c]) [d] mem)
+	// result: (MOVBUload [int32(uint32(c)<<uint64(d))] ptr mem)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) << uint64(d)))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBUreg(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (MOVBUreg y:(ANDconst [c] _))
+	// cond: uint64(c) <= 0xFF
+	// result: y
+	for {
+		y := v_0
+		if y.Op != OpThumbANDconst {
+			break
+		}
+		c := auxIntToInt32(y.AuxInt)
+		if !(uint64(c) <= 0xFF) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	// match: (MOVBUreg (MOVBreg x))
+	// result: (MOVBUreg x)
+	for {
+		if v_0.Op != OpThumbMOVBreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg (MOVBUreg x))
+	// result: (MOVBUreg x)
+	for {
+		if v_0.Op != OpThumbMOVBUreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg <t> (SRLconst [c] (MOVBUreg x)))
+	// result: (SRLconst [c] (MOVBUreg <t> x))
+	for {
+		t := v.Type
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbMOVBUreg {
+			break
+		}
+		x := v_0_0.Args[0]
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUreg, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MOVBUreg (BFXU [c] x))
+	// cond: (c>>8 <= 8)
+	// result: (BFXU [c] x)
+	for {
+		if v_0.Op != OpThumbBFXU {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c>>8 <= 8) {
+			break
+		}
+		v.reset(OpThumbBFXU)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg (SRLconst [c] x))
+	// cond: c>=24
+	// result: (SRLconst [c] x)
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c >= 24) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg x:(MOVBUload _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUload {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg x:(MOVBUloadidx _ _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUloadidx {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg (ANDconst [c] x))
+	// result: (ANDconst [c&0xff] x)
+	for {
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c & 0xff)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg x:(MOVBUreg _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg (MOVWconst [c]))
+	// result: (MOVWconst [int32(uint8(c))])
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(uint8(c)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBload(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// result: (MOVBload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// result: (MOVBload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVBload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: (MOVBreg x)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVBstore {
+			break
+		}
+		off2 := auxIntToInt32(v_1.AuxInt)
+		sym2 := auxToSym(v_1.Aux)
+		x := v_1.Args[1]
+		ptr2 := v_1.Args[0]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVBloadidx ptr idx mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVBloadidx)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVBload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVBloadshiftLL ptr idx [c] mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBloadidx(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBloadidx ptr idx (MOVBstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: (MOVBreg x)
+	for {
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVBstoreidx {
+			break
+		}
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBloadidx ptr (MOVWconst [c]) mem)
+	// result: (MOVBload [c] ptr mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBloadidx (MOVWconst [c]) ptr mem)
+	// result: (MOVBload [c] ptr mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		mem := v_2
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVBloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVBloadshiftLL ptr idx [c] mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVBloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVBloadshiftLL ptr idx [c] mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBloadshiftLL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBloadshiftLL ptr idx [c] (MOVBstoreshiftLL ptr2 idx [d] x _))
+	// cond: c==d && isSamePtr(ptr, ptr2)
+	// result: (MOVBreg x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVBstoreshiftLL {
+			break
+		}
+		d := auxIntToInt32(v_2.AuxInt)
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(c == d && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBloadshiftLL ptr (MOVWconst [c]) [d] mem)
+	// result: (MOVBload [int32(uint32(c)<<uint64(d))] ptr mem)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) << uint64(d)))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBreg(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (MOVBreg (MOVBUreg x))
+	// result: (MOVBreg x)
+	for {
+		if v_0.Op != OpThumbMOVBUreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg (MOVBreg x))
+	// result: (MOVBreg x)
+	for {
+		if v_0.Op != OpThumbMOVBreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg <t> (SRAconst [c] (MOVBreg x)))
+	// result: (SRAconst [c] (MOVBreg <t> x))
+	for {
+		t := v.Type
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbMOVBreg {
+			break
+		}
+		x := v_0_0.Args[0]
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBreg, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MOVBreg (BFX [c] x))
+	// cond: (c>>8 <= 8)
+	// result: (BFX [c] x)
+	for {
+		if v_0.Op != OpThumbBFX {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c>>8 <= 8) {
+			break
+		}
+		v.reset(OpThumbBFX)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg (SRLconst [c] x))
+	// cond: c>24
+	// result: (SRLconst [c] x)
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c > 24) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg (SRLconst [c] x))
+	// cond: c==24
+	// result: (SRAconst [c] x)
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c == 24) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg (SRAconst [c] x))
+	// cond: c>=24
+	// result: (SRAconst [c] x)
+	for {
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c >= 24) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg x:(MOVBload _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBload {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg x:(MOVBloadidx _ _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBloadidx {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg (ANDconst [c] x))
+	// cond: c & 0x80 == 0
+	// result: (ANDconst [c&0x7f] x)
+	for {
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c&0x80 == 0) {
+			break
+		}
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c & 0x7f)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg x:(MOVBreg _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg (MOVWconst [c]))
+	// result: (MOVWconst [int32(int8(c))])
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(int8(c)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBstore(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// result: (MOVBstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVBstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// result: (MOVBstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVBstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVBstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVBstore [off] {sym} ptr (MOVBreg x) mem)
+	// result: (MOVBstore [off] {sym} ptr x mem)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVBreg {
+			break
+		}
+		x := v_1.Args[0]
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(off)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, x, mem)
+		return true
+	}
+	// match: (MOVBstore [off] {sym} ptr (MOVBUreg x) mem)
+	// result: (MOVBstore [off] {sym} ptr x mem)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVBUreg {
+			break
+		}
+		x := v_1.Args[0]
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(off)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, x, mem)
+		return true
+	}
+	// match: (MOVBstore [off] {sym} ptr (MOVHreg x) mem)
+	// result: (MOVBstore [off] {sym} ptr x mem)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVHreg {
+			break
+		}
+		x := v_1.Args[0]
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(off)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, x, mem)
+		return true
+	}
+	// match: (MOVBstore [off] {sym} ptr (MOVHUreg x) mem)
+	// result: (MOVBstore [off] {sym} ptr x mem)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVHUreg {
+			break
+		}
+		x := v_1.Args[0]
+		mem := v_2
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(off)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, x, mem)
+		return true
+	}
+	// match: (MOVBstore [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (MOVBstoreidx ptr idx val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVBstoreidx)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (MOVBstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVBstoreshiftLL ptr idx [c] val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBstoreidx(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBstoreidx ptr (MOVWconst [c]) val mem)
+	// result: (MOVBstore [c] ptr val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVBstoreidx (MOVWconst [c]) ptr val mem)
+	// result: (MOVBstore [c] ptr val mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVBstoreidx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (MOVBstoreshiftLL ptr idx [c] val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (MOVBstoreidx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (MOVBstoreshiftLL ptr idx [c] val mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBstoreshiftLL(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVBstoreshiftLL ptr (MOVWconst [c]) [d] val mem)
+	// result: (MOVBstore [int32(uint32(c)<<uint64(d))] ptr val mem)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) << uint64(d)))
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVDload(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVDload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// result: (MOVDload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVDload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVDload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// result: (MOVDload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVDload)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVDload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVDload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVDload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVDload [off] {sym} ptr (MOVDstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVDstore {
+			break
+		}
+		off2 := auxIntToInt32(v_1.AuxInt)
+		sym2 := auxToSym(v_1.Aux)
+		x := v_1.Args[1]
+		ptr2 := v_1.Args[0]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVDstore(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVDstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// result: (MOVDstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVDstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVDstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// result: (MOVDstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVDstore)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVDstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVDstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVDstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVFload(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVFload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// result: (MOVFload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVFload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVFload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// result: (MOVFload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVFload)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVFload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVFload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVFload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVFload [off] {sym} ptr (MOVFstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVFstore {
+			break
+		}
+		off2 := auxIntToInt32(v_1.AuxInt)
+		sym2 := auxToSym(v_1.Aux)
+		x := v_1.Args[1]
+		ptr2 := v_1.Args[0]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVFstore(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVFstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// result: (MOVFstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVFstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVFstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// result: (MOVFstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVFstore)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVFstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVFstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVFstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHUload(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	config := b.Func.Config
+	// match: (MOVHUload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// result: (MOVHUload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHUload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// result: (MOVHUload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHUload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVHUload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHUload [off] {sym} ptr (MOVHstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: (MOVHUreg x)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVHstore {
+			break
+		}
+		off2 := auxIntToInt32(v_1.AuxInt)
+		sym2 := auxToSym(v_1.Aux)
+		x := v_1.Args[1]
+		ptr2 := v_1.Args[0]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVHUloadidx ptr idx mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVHUloadidx)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVHUload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVHUloadshiftLL ptr idx [c] mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHUloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVHUload [off] {sym} (SB) _)
+	// cond: symIsRO(sym)
+	// result: (MOVWconst [int32(read16(sym, int64(off), config.ctxt.Arch.ByteOrder))])
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpSB || !(symIsRO(sym)) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(read16(sym, int64(off), config.ctxt.Arch.ByteOrder)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHUloadidx(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVHUloadidx ptr idx (MOVHstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: (MOVHUreg x)
+	for {
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVHstoreidx {
+			break
+		}
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUloadidx ptr (MOVWconst [c]) mem)
+	// result: (MOVHUload [c] ptr mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHUloadidx (MOVWconst [c]) ptr mem)
+	// result: (MOVHUload [c] ptr mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		mem := v_2
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHUloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVHUloadshiftLL ptr idx [c] mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHUloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVHUloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVHUloadshiftLL ptr idx [c] mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHUloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHUloadshiftLL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVHUloadshiftLL ptr idx [c] (MOVHstoreshiftLL ptr2 idx [d] x _))
+	// cond: c==d && isSamePtr(ptr, ptr2)
+	// result: (MOVHUreg x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVHstoreshiftLL {
+			break
+		}
+		d := auxIntToInt32(v_2.AuxInt)
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(c == d && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUloadshiftLL ptr (MOVWconst [c]) [d] mem)
+	// result: (MOVHUload [int32(uint32(c)<<uint64(d))] ptr mem)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) << uint64(d)))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHUreg(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (MOVHUreg y:(ANDconst [c] _))
+	// cond: uint64(c) <= 0xFFFF
+	// result: y
+	for {
+		y := v_0
+		if y.Op != OpThumbANDconst {
+			break
+		}
+		c := auxIntToInt32(y.AuxInt)
+		if !(uint64(c) <= 0xFFFF) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	// match: (MOVHUreg (MOVBUreg x))
+	// result: (MOVBUreg x)
+	for {
+		if v_0.Op != OpThumbMOVBUreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg (MOVHUreg x))
+	// result: (MOVHUreg x)
+	for {
+		if v_0.Op != OpThumbMOVHUreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVHUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg (MOVHreg x))
+	// result: (MOVHUreg x)
+	for {
+		if v_0.Op != OpThumbMOVHreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVHUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg <t> (SRLconst [c] (MOVBUreg x)))
+	// result: (SRLconst [c] (MOVBUreg <t> x))
+	for {
+		t := v.Type
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbMOVBUreg {
+			break
+		}
+		x := v_0_0.Args[0]
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUreg, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MOVHUreg <t> (SRLconst [c] (MOVHUreg x)))
+	// result: (SRLconst [c] (MOVHUreg <t> x))
+	for {
+		t := v.Type
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbMOVHUreg {
+			break
+		}
+		x := v_0_0.Args[0]
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVHUreg, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MOVHUreg (BFXU [c] x))
+	// cond: (c>>8 <= 8)
+	// result: (BFXU [c] x)
+	for {
+		if v_0.Op != OpThumbBFXU {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c>>8 <= 8) {
+			break
+		}
+		v.reset(OpThumbBFXU)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg (BFXU [c] x))
+	// cond: (c>>8 <= 16)
+	// result: (BFXU [c] x)
+	for {
+		if v_0.Op != OpThumbBFXU {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c>>8 <= 16) {
+			break
+		}
+		v.reset(OpThumbBFXU)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg (SRLconst [c] x))
+	// cond: c>=16
+	// result: (SRLconst [c] x)
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c >= 16) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVBUload _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUload {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVHUload _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVHUload {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVBUloadidx _ _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUloadidx {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVHUloadidx _ _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVHUloadidx {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg (ANDconst [c] x))
+	// result: (ANDconst [c&0xffff] x)
+	for {
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c & 0xffff)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVBUreg _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVHUreg _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVHUreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg (MOVWconst [c]))
+	// result: (MOVWconst [int32(uint16(c))])
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(uint16(c)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHload(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVHload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// result: (MOVHload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// result: (MOVHload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVHload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHload [off] {sym} ptr (MOVHstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: (MOVHreg x)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVHstore {
+			break
+		}
+		off2 := auxIntToInt32(v_1.AuxInt)
+		sym2 := auxToSym(v_1.Aux)
+		x := v_1.Args[1]
+		ptr2 := v_1.Args[0]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVHloadidx ptr idx mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVHloadidx)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVHload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVHloadshiftLL ptr idx [c] mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHloadidx(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVHloadidx ptr idx (MOVHstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: (MOVHreg x)
+	for {
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVHstoreidx {
+			break
+		}
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHloadidx ptr (MOVWconst [c]) mem)
+	// result: (MOVHload [c] ptr mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHloadidx (MOVWconst [c]) ptr mem)
+	// result: (MOVHload [c] ptr mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		mem := v_2
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVHloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVHloadshiftLL ptr idx [c] mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVHloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVHloadshiftLL ptr idx [c] mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHloadshiftLL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVHloadshiftLL ptr idx [c] (MOVHstoreshiftLL ptr2 idx [d] x _))
+	// cond: c==d && isSamePtr(ptr, ptr2)
+	// result: (MOVHreg x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVHstoreshiftLL {
+			break
+		}
+		d := auxIntToInt32(v_2.AuxInt)
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(c == d && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHloadshiftLL ptr (MOVWconst [c]) [d] mem)
+	// result: (MOVHload [int32(uint32(c)<<uint64(d))] ptr mem)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) << uint64(d)))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHreg(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (MOVHreg (MOVBreg x))
+	// result: (MOVBreg x)
+	for {
+		if v_0.Op != OpThumbMOVBreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (MOVHreg x))
+	// result: (MOVHreg x)
+	for {
+		if v_0.Op != OpThumbMOVHreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVHreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (MOVHUreg x))
+	// result: (MOVHreg x)
+	for {
+		if v_0.Op != OpThumbMOVHUreg {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMOVHreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg <t> (SRAconst [c] (MOVBreg x)))
+	// result: (SRAconst [c] (MOVBreg <t> x))
+	for {
+		t := v.Type
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbMOVBreg {
+			break
+		}
+		x := v_0_0.Args[0]
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBreg, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MOVHreg <t> (SRAconst [c] (MOVHreg x)))
+	// result: (SRAconst [c] (MOVHreg <t> x))
+	for {
+		t := v.Type
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbMOVHreg {
+			break
+		}
+		x := v_0_0.Args[0]
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVHreg, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MOVHreg (BFX [c] x))
+	// cond: (c>>8 <= 8)
+	// result: (BFX [c] x)
+	for {
+		if v_0.Op != OpThumbBFX {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c>>8 <= 8) {
+			break
+		}
+		v.reset(OpThumbBFX)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (BFX [c] x))
+	// cond: (c>>8 <= 16)
+	// result: (BFX [c] x)
+	for {
+		if v_0.Op != OpThumbBFX {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c>>8 <= 16) {
+			break
+		}
+		v.reset(OpThumbBFX)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (SRLconst [c] x))
+	// cond: c>16
+	// result: (SRLconst [c] x)
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c > 16) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (SRAconst [c] x))
+	// cond: c>=16
+	// result: (SRAconst [c] x)
+	for {
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c >= 16) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (SRLconst [c] x))
+	// cond: c==16
+	// result: (SRAconst [c] x)
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c == 16) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBload _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBload {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBUload _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUload {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVHload _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVHload {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBloadidx _ _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBloadidx {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBUloadidx _ _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUloadidx {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVHloadidx _ _ _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVHloadidx {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (ANDconst [c] x))
+	// cond: c & 0x8000 == 0
+	// result: (ANDconst [c&0x7fff] x)
+	for {
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(c&0x8000 == 0) {
+			break
+		}
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int32ToAuxInt(c & 0x7fff)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBreg _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBUreg _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVBUreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVHreg _))
+	// result: (MOVWreg x)
+	for {
+		x := v_0
+		if x.Op != OpThumbMOVHreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (MOVWconst [c]))
+	// result: (MOVWconst [int32(int16(c))])
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(int16(c)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHstore(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVHstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// result: (MOVHstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVHstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// result: (MOVHstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVHstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVHstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVHstore [off] {sym} ptr (MOVHreg x) mem)
+	// result: (MOVHstore [off] {sym} ptr x mem)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVHreg {
+			break
+		}
+		x := v_1.Args[0]
+		mem := v_2
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(off)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, x, mem)
+		return true
+	}
+	// match: (MOVHstore [off] {sym} ptr (MOVHUreg x) mem)
+	// result: (MOVHstore [off] {sym} ptr x mem)
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVHUreg {
+			break
+		}
+		x := v_1.Args[0]
+		mem := v_2
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(off)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, x, mem)
+		return true
+	}
+	// match: (MOVHstore [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (MOVHstoreidx ptr idx val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVHstoreidx)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (MOVHstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVHstoreshiftLL ptr idx [c] val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHstoreidx(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVHstoreidx ptr (MOVWconst [c]) val mem)
+	// result: (MOVHstore [c] ptr val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVHstoreidx (MOVWconst [c]) ptr val mem)
+	// result: (MOVHstore [c] ptr val mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVHstoreidx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (MOVHstoreshiftLL ptr idx [c] val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (MOVHstoreidx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (MOVHstoreshiftLL ptr idx [c] val mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHstoreshiftLL(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVHstoreshiftLL ptr (MOVWconst [c]) [d] val mem)
+	// result: (MOVHstore [int32(uint32(c)<<uint64(d))] ptr val mem)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) << uint64(d)))
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWload(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	config := b.Func.Config
+	// match: (MOVWload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// result: (MOVWload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVWload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// result: (MOVWload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		mem := v_1
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVWload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVWload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVWload [off] {sym} ptr (MOVWstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWstore {
+			break
+		}
+		off2 := auxIntToInt32(v_1.AuxInt)
+		sym2 := auxToSym(v_1.Aux)
+		x := v_1.Args[1]
+		ptr2 := v_1.Args[0]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (MOVWload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVWloadidx ptr idx mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVWloadidx)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVWload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVWloadshiftLL ptr idx [c] mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		mem := v_1
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVWload [off] {sym} (SB) _)
+	// cond: symIsRO(sym)
+	// result: (MOVWconst [int32(read32(sym, int64(off), config.ctxt.Arch.ByteOrder))])
+	for {
+		off := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpSB || !(symIsRO(sym)) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(read32(sym, int64(off), config.ctxt.Arch.ByteOrder)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWloadidx(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVWloadidx ptr idx (MOVWstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVWstoreidx {
+			break
+		}
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (MOVWloadidx ptr (MOVWconst [c]) mem)
+	// result: (MOVWload [c] ptr mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVWloadidx (MOVWconst [c]) ptr mem)
+	// result: (MOVWload [c] ptr mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		mem := v_2
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	// match: (MOVWloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVWloadshiftLL ptr idx [c] mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	// match: (MOVWloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVWloadshiftLL ptr idx [c] mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWloadshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, idx, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWloadshiftLL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVWloadshiftLL ptr idx [c] (MOVWstoreshiftLL ptr2 idx [d] x _))
+	// cond: c==d && isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		idx := v_1
+		if v_2.Op != OpThumbMOVWstoreshiftLL {
+			break
+		}
+		d := auxIntToInt32(v_2.AuxInt)
+		x := v_2.Args[2]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] || !(c == d && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (MOVWloadshiftLL ptr (MOVWconst [c]) [d] mem)
+	// result: (MOVWload [int32(uint32(c)<<uint64(d))] ptr mem)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		mem := v_2
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) << uint64(d)))
+		v.AddArg2(ptr, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWreg(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (MOVWreg x)
+	// cond: x.Uses == 1
+	// result: (MOVWnop x)
+	for {
+		x := v_0
+		if !(x.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMOVWnop)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVWreg (MOVWconst [c]))
+	// result: (MOVWconst [c])
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWstore(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVWstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// result: (MOVWstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVWstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// result: (MOVWstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.Aux = symToAux(sym)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVWstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVWstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		sym1 := auxToSym(v.Aux)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym2 := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.Aux = symToAux(mergeSym(sym1, sym2))
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVWstore [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (MOVWstoreidx ptr idx val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVWstoreidx)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (MOVWstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVWstoreshiftLL ptr idx [c] val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		sym := auxToSym(v.Aux)
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWstoreidx(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVWstoreidx ptr (MOVWconst [c]) val mem)
+	// result: (MOVWstore [c] ptr val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVWstoreidx (MOVWconst [c]) ptr val mem)
+	// result: (MOVWstore [c] ptr val mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (MOVWstoreidx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (MOVWstoreshiftLL ptr idx [c] val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (MOVWstoreidx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (MOVWstoreshiftLL ptr idx [c] val mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWstoreshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWstoreshiftLL(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MOVWstoreshiftLL ptr (MOVWconst [c]) [d] val mem)
+	// result: (MOVWstore [int32(uint32(c)<<uint64(d))] ptr val mem)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) << uint64(d)))
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMUL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (MUL x (MOVWconst [c]))
+	// cond: int32(c) == -1
+	// result: (RSBconst [0] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			if !(int32(c) == -1) {
+				continue
+			}
+			v.reset(OpThumbRSBconst)
+			v.AuxInt = int32ToAuxInt(0)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (MUL _ (MOVWconst [0]))
+	// result: (MOVWconst [0])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_1.Op != OpThumbMOVWconst || auxIntToInt32(v_1.AuxInt) != 0 {
+				continue
+			}
+			v.reset(OpThumbMOVWconst)
+			v.AuxInt = int32ToAuxInt(0)
+			return true
+		}
+		break
+	}
+	// match: (MUL x (MOVWconst [1]))
+	// result: x
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst || auxIntToInt32(v_1.AuxInt) != 1 {
+				continue
+			}
+			v.copyOf(x)
+			return true
+		}
+		break
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: isPowerOfTwo32(c)
+	// result: (SLLconst [int32(log32(c))] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			if !(isPowerOfTwo32(c)) {
+				continue
+			}
+			v.reset(OpThumbSLLconst)
+			v.AuxInt = int32ToAuxInt(int32(log32(c)))
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: isPowerOfTwo32(c-1) && c >= 3
+	// result: (ADDshiftLL x x [int32(log32(c-1))])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			if !(isPowerOfTwo32(c-1) && c >= 3) {
+				continue
+			}
+			v.reset(OpThumbADDshiftLL)
+			v.AuxInt = int32ToAuxInt(int32(log32(c - 1)))
+			v.AddArg2(x, x)
+			return true
+		}
+		break
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: isPowerOfTwo32(c+1) && c >= 7
+	// result: (RSBshiftLL x x [int32(log32(c+1))])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			if !(isPowerOfTwo32(c+1) && c >= 7) {
+				continue
+			}
+			v.reset(OpThumbRSBshiftLL)
+			v.AuxInt = int32ToAuxInt(int32(log32(c + 1)))
+			v.AddArg2(x, x)
+			return true
+		}
+		break
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: c%3 == 0 && isPowerOfTwo32(c/3)
+	// result: (SLLconst [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1]))
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			if !(c%3 == 0 && isPowerOfTwo32(c/3)) {
+				continue
+			}
+			v.reset(OpThumbSLLconst)
+			v.AuxInt = int32ToAuxInt(int32(log32(c / 3)))
+			v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+			v0.AuxInt = int32ToAuxInt(1)
+			v0.AddArg2(x, x)
+			v.AddArg(v0)
+			return true
+		}
+		break
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: c%5 == 0 && isPowerOfTwo32(c/5)
+	// result: (SLLconst [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2]))
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			if !(c%5 == 0 && isPowerOfTwo32(c/5)) {
+				continue
+			}
+			v.reset(OpThumbSLLconst)
+			v.AuxInt = int32ToAuxInt(int32(log32(c / 5)))
+			v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+			v0.AuxInt = int32ToAuxInt(2)
+			v0.AddArg2(x, x)
+			v.AddArg(v0)
+			return true
+		}
+		break
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: c%7 == 0 && isPowerOfTwo32(c/7)
+	// result: (SLLconst [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3]))
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			if !(c%7 == 0 && isPowerOfTwo32(c/7)) {
+				continue
+			}
+			v.reset(OpThumbSLLconst)
+			v.AuxInt = int32ToAuxInt(int32(log32(c / 7)))
+			v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+			v0.AuxInt = int32ToAuxInt(3)
+			v0.AddArg2(x, x)
+			v.AddArg(v0)
+			return true
+		}
+		break
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: c%9 == 0 && isPowerOfTwo32(c/9)
+	// result: (SLLconst [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3]))
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			if !(c%9 == 0 && isPowerOfTwo32(c/9)) {
+				continue
+			}
+			v.reset(OpThumbSLLconst)
+			v.AuxInt = int32ToAuxInt(int32(log32(c / 9)))
+			v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+			v0.AuxInt = int32ToAuxInt(3)
+			v0.AddArg2(x, x)
+			v.AddArg(v0)
+			return true
+		}
+		break
+	}
+	// match: (MUL (MOVWconst [c]) (MOVWconst [d]))
+	// result: (MOVWconst [c*d])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_0.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_0.AuxInt)
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			d := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbMOVWconst)
+			v.AuxInt = int32ToAuxInt(c * d)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULA(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c == -1
+	// result: (SUB a x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c == -1) {
+			break
+		}
+		v.reset(OpThumbSUB)
+		v.AddArg2(a, x)
+		return true
+	}
+	// match: (MULA _ (MOVWconst [0]) a)
+	// result: a
+	for {
+		if v_1.Op != OpThumbMOVWconst || auxIntToInt32(v_1.AuxInt) != 0 {
+			break
+		}
+		a := v_2
+		v.copyOf(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [1]) a)
+	// result: (ADD x a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst || auxIntToInt32(v_1.AuxInt) != 1 {
+			break
+		}
+		a := v_2
+		v.reset(OpThumbADD)
+		v.AddArg2(x, a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo32(c)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c))] x) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(isPowerOfTwo32(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c)))
+		v0.AddArg(x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo32(c-1) && c >= 3
+	// result: (ADD (ADDshiftLL <x.Type> x x [int32(log32(c-1))]) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(isPowerOfTwo32(c-1) && c >= 3) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c - 1)))
+		v0.AddArg2(x, x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo32(c+1) && c >= 7
+	// result: (ADD (RSBshiftLL <x.Type> x x [int32(log32(c+1))]) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(isPowerOfTwo32(c+1) && c >= 7) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c + 1)))
+		v0.AddArg2(x, x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c%3 == 0 && isPowerOfTwo32(c/3)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1])) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c%3 == 0 && isPowerOfTwo32(c/3)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 3)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(1)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c%5 == 0 && isPowerOfTwo32(c/5)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2])) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c%5 == 0 && isPowerOfTwo32(c/5)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 5)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(2)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c%7 == 0 && isPowerOfTwo32(c/7)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3])) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c%7 == 0 && isPowerOfTwo32(c/7)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 7)))
+		v1 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(3)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c%9 == 0 && isPowerOfTwo32(c/9)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3])) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c%9 == 0 && isPowerOfTwo32(c/9)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 9)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(3)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c == -1
+	// result: (SUB a x)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c == -1) {
+			break
+		}
+		v.reset(OpThumbSUB)
+		v.AddArg2(a, x)
+		return true
+	}
+	// match: (MULA (MOVWconst [0]) _ a)
+	// result: a
+	for {
+		if v_0.Op != OpThumbMOVWconst || auxIntToInt32(v_0.AuxInt) != 0 {
+			break
+		}
+		a := v_2
+		v.copyOf(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [1]) x a)
+	// result: (ADD x a)
+	for {
+		if v_0.Op != OpThumbMOVWconst || auxIntToInt32(v_0.AuxInt) != 1 {
+			break
+		}
+		x := v_1
+		a := v_2
+		v.reset(OpThumbADD)
+		v.AddArg2(x, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo32(c)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c))] x) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(isPowerOfTwo32(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c)))
+		v0.AddArg(x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo32(c-1) && c >= 3
+	// result: (ADD (ADDshiftLL <x.Type> x x [int32(log32(c-1))]) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(isPowerOfTwo32(c-1) && c >= 3) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c - 1)))
+		v0.AddArg2(x, x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo32(c+1) && c >= 7
+	// result: (ADD (RSBshiftLL <x.Type> x x [int32(log32(c+1))]) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(isPowerOfTwo32(c+1) && c >= 7) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c + 1)))
+		v0.AddArg2(x, x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c%3 == 0 && isPowerOfTwo32(c/3)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1])) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c%3 == 0 && isPowerOfTwo32(c/3)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 3)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(1)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c%5 == 0 && isPowerOfTwo32(c/5)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2])) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c%5 == 0 && isPowerOfTwo32(c/5)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 5)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(2)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c%7 == 0 && isPowerOfTwo32(c/7)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3])) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c%7 == 0 && isPowerOfTwo32(c/7)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 7)))
+		v1 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(3)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c%9 == 0 && isPowerOfTwo32(c/9)
+	// result: (ADD (SLLconst <x.Type> [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3])) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c%9 == 0 && isPowerOfTwo32(c/9)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 9)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(3)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) (MOVWconst [d]) a)
+	// result: (ADDconst [c*d] a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c * d)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULD(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MULD (NEGD x) y)
+	// result: (NMULD x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_0.Op != OpThumbNEGD {
+				continue
+			}
+			x := v_0.Args[0]
+			y := v_1
+			v.reset(OpThumbNMULD)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULF(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (MULF (NEGF x) y)
+	// result: (NMULF x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_0.Op != OpThumbNEGF {
+				continue
+			}
+			x := v_0.Args[0]
+			y := v_1
+			v.reset(OpThumbNMULF)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULS(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c == -1
+	// result: (ADD a x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c == -1) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v.AddArg2(a, x)
+		return true
+	}
+	// match: (MULS _ (MOVWconst [0]) a)
+	// result: a
+	for {
+		if v_1.Op != OpThumbMOVWconst || auxIntToInt32(v_1.AuxInt) != 0 {
+			break
+		}
+		a := v_2
+		v.copyOf(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [1]) a)
+	// result: (RSB x a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst || auxIntToInt32(v_1.AuxInt) != 1 {
+			break
+		}
+		a := v_2
+		v.reset(OpThumbRSB)
+		v.AddArg2(x, a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo32(c)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c))] x) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(isPowerOfTwo32(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c)))
+		v0.AddArg(x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo32(c-1) && c >= 3
+	// result: (RSB (ADDshiftLL <x.Type> x x [int32(log32(c-1))]) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(isPowerOfTwo32(c-1) && c >= 3) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c - 1)))
+		v0.AddArg2(x, x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo32(c+1) && c >= 7
+	// result: (RSB (RSBshiftLL <x.Type> x x [int32(log32(c+1))]) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(isPowerOfTwo32(c+1) && c >= 7) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c + 1)))
+		v0.AddArg2(x, x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c%3 == 0 && isPowerOfTwo32(c/3)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1])) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c%3 == 0 && isPowerOfTwo32(c/3)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 3)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(1)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c%5 == 0 && isPowerOfTwo32(c/5)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2])) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c%5 == 0 && isPowerOfTwo32(c/5)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 5)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(2)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c%7 == 0 && isPowerOfTwo32(c/7)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3])) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c%7 == 0 && isPowerOfTwo32(c/7)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 7)))
+		v1 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(3)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c%9 == 0 && isPowerOfTwo32(c/9)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3])) a)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		if !(c%9 == 0 && isPowerOfTwo32(c/9)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 9)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(3)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c == -1
+	// result: (ADD a x)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c == -1) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v.AddArg2(a, x)
+		return true
+	}
+	// match: (MULS (MOVWconst [0]) _ a)
+	// result: a
+	for {
+		if v_0.Op != OpThumbMOVWconst || auxIntToInt32(v_0.AuxInt) != 0 {
+			break
+		}
+		a := v_2
+		v.copyOf(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [1]) x a)
+	// result: (RSB x a)
+	for {
+		if v_0.Op != OpThumbMOVWconst || auxIntToInt32(v_0.AuxInt) != 1 {
+			break
+		}
+		x := v_1
+		a := v_2
+		v.reset(OpThumbRSB)
+		v.AddArg2(x, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo32(c)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c))] x) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(isPowerOfTwo32(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c)))
+		v0.AddArg(x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo32(c-1) && c >= 3
+	// result: (RSB (ADDshiftLL <x.Type> x x [int32(log32(c-1))]) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(isPowerOfTwo32(c-1) && c >= 3) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c - 1)))
+		v0.AddArg2(x, x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo32(c+1) && c >= 7
+	// result: (RSB (RSBshiftLL <x.Type> x x [int32(log32(c+1))]) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(isPowerOfTwo32(c+1) && c >= 7) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c + 1)))
+		v0.AddArg2(x, x)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c%3 == 0 && isPowerOfTwo32(c/3)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c/3))] (ADDshiftLL <x.Type> x x [1])) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c%3 == 0 && isPowerOfTwo32(c/3)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 3)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(1)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c%5 == 0 && isPowerOfTwo32(c/5)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c/5))] (ADDshiftLL <x.Type> x x [2])) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c%5 == 0 && isPowerOfTwo32(c/5)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 5)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(2)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c%7 == 0 && isPowerOfTwo32(c/7)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c/7))] (RSBshiftLL <x.Type> x x [3])) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c%7 == 0 && isPowerOfTwo32(c/7)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 7)))
+		v1 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(3)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c%9 == 0 && isPowerOfTwo32(c/9)
+	// result: (RSB (SLLconst <x.Type> [int32(log32(c/9))] (ADDshiftLL <x.Type> x x [3])) a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		a := v_2
+		if !(c%9 == 0 && isPowerOfTwo32(c/9)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(int32(log32(c / 9)))
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = int32ToAuxInt(3)
+		v1.AddArg2(x, x)
+		v0.AddArg(v1)
+		v.AddArg2(v0, a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) (MOVWconst [d]) a)
+	// result: (SUBconst [c*d] a)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_1.AuxInt)
+		a := v_2
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(c * d)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMVN(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (MVN (MOVWconst [c]))
+	// result: (MOVWconst [^c])
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(^c)
+		return true
+	}
+	// match: (MVN (SLLconst [c] x))
+	// result: (MVNshiftLL x [c])
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbMVNshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MVN (SRLconst [c] x))
+	// result: (MVNshiftRL x [c])
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbMVNshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MVN (SRAconst [c] x))
+	// result: (MVNshiftRA x [c])
+	for {
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbMVNshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMVNshiftLL(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (MVNshiftLL (MOVWconst [c]) [d])
+	// result: (MOVWconst [^(c<<uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(^(c << uint64(d)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMVNshiftRA(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (MVNshiftRA (MOVWconst [c]) [d])
+	// result: (MOVWconst [^(int32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(^(int32(c) >> uint64(d)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMVNshiftRL(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (MVNshiftRL (MOVWconst [c]) [d])
+	// result: (MOVWconst [^int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(^int32(uint32(c) >> uint64(d)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNEGD(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (NEGD (MULD x y))
+	// result: (NMULD x y)
+	for {
+		if v_0.Op != OpThumbMULD {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		v.reset(OpThumbNMULD)
+		v.AddArg2(x, y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNEGF(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (NEGF (MULF x y))
+	// result: (NMULF x y)
+	for {
+		if v_0.Op != OpThumbMULF {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		v.reset(OpThumbNMULF)
+		v.AddArg2(x, y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNMULD(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (NMULD (NEGD x) y)
+	// result: (MULD x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_0.Op != OpThumbNEGD {
+				continue
+			}
+			x := v_0.Args[0]
+			y := v_1
+			v.reset(OpThumbMULD)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNMULF(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (NMULF (NEGF x) y)
+	// result: (MULF x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			if v_0.Op != OpThumbNEGF {
+				continue
+			}
+			x := v_0.Args[0]
+			y := v_1
+			v.reset(OpThumbMULF)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNotEqual(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (NotEqual (FlagConstant [fc]))
+	// result: (MOVWconst [b2i32(fc.ne())])
+	for {
+		if v_0.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(b2i32(fc.ne()))
+		return true
+	}
+	// match: (NotEqual (InvertFlags x))
+	// result: (NotEqual x)
+	for {
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbNotEqual)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbOR(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (OR x (MOVWconst [c]))
+	// result: (ORconst [c] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbORconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (OR x (SLLconst [c] y))
+	// result: (ORshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbORshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (OR x (SRLconst [c] y))
+	// result: (ORshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbORshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (OR x (SRAconst [c] y))
+	// result: (ORshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbORshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (OR x x)
+	// result: x
+	for {
+		x := v_0
+		if x != v_1 {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (OR x (MVN y))
+	// result: (ORN x y)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMVN {
+				continue
+			}
+			y := v_1.Args[0]
+			v.reset(OpThumbORN)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORN(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ORN x (MOVWconst [c]))
+	// result: (ORNconst [c] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbORNconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORN x (SLLconst [c] y))
+	// result: (ORNshiftLL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbORNshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (ORN x (SRLconst [c] y))
+	// result: (ORNshiftRL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbORNshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (ORN x (SRAconst [c] y))
+	// result: (ORNshiftRA x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbORNshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (ORN x x)
+	// result: (MOVWconst [-1])
+	for {
+		x := v_0
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(-1)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORNconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (ORNconst [0] _)
+	// result: (MOVWconst [-1])
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(-1)
+		return true
+	}
+	// match: (ORNconst [c] x)
+	// cond: int32(c)==-1
+	// result: x
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if !(int32(c) == -1) {
+			break
+		}
+		v.copyOf(x)
+		return true
+	}
+	// match: (ORNconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [^c|d])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(^c | d)
+		return true
+	}
+	// match: (ORNconst [c] (ORNconst [d] x))
+	// result: (ORconst [^c|^d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbORNconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbORconst)
+		v.AuxInt = int32ToAuxInt(^c | ^d)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORNshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ORNshiftLL x (MOVWconst [c]) [d])
+	// result: (ORNconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbORNconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORNshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ORNshiftRA x (MOVWconst [c]) [d])
+	// result: (ORNconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbORNconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORNshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (ORNshiftRL x (MOVWconst [c]) [d])
+	// result: (ORNconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbORNconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (ORconst [0] x)
+	// result: x
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		x := v_0
+		v.copyOf(x)
+		return true
+	}
+	// match: (ORconst [c] _)
+	// cond: int32(c)==-1
+	// result: (MOVWconst [-1])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(-1)
+		return true
+	}
+	// match: (ORconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [c|d])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c | d)
+		return true
+	}
+	// match: (ORconst [c] (ORconst [d] x))
+	// result: (ORconst [c|d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbORconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbORconst)
+		v.AuxInt = int32ToAuxInt(c | d)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (ORshiftLL (MOVWconst [c]) x [d])
+	// result: (ORconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbORconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ORshiftLL x (MOVWconst [c]) [d])
+	// result: (ORconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbORconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: ( ORshiftLL [c] (SRLconst x [32-c]) x)
+	// result: (SRRconst [32-c] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSRLconst || auxIntToInt32(v_0.AuxInt) != 32-c {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = int32ToAuxInt(32 - c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftLL <typ.UInt16> [8] (BFXU <typ.UInt16> [int32(armBFAuxInt(8, 8))] x) x)
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 || auxIntToInt32(v.AuxInt) != 8 || v_0.Op != OpThumbBFXU || v_0.Type != typ.UInt16 || auxIntToInt32(v_0.AuxInt) != int32(armBFAuxInt(8, 8)) {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftLL <typ.UInt16> [8] (SRLconst <typ.UInt16> [24] (SLLconst [16] x)) x)
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 || auxIntToInt32(v.AuxInt) != 8 || v_0.Op != OpThumbSRLconst || v_0.Type != typ.UInt16 || auxIntToInt32(v_0.AuxInt) != 24 {
+			break
+		}
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbSLLconst || auxIntToInt32(v_0_0.AuxInt) != 16 {
+			break
+		}
+		x := v_0_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftLL x y:(SLLconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		y := v_1
+		if y.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(y.AuxInt)
+		if x != y.Args[0] || !(c == d) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ORshiftRA (MOVWconst [c]) x [d])
+	// result: (ORconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbORconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ORshiftRA x (MOVWconst [c]) [d])
+	// result: (ORconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbORconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftRA x y:(SRAconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		y := v_1
+		if y.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(y.AuxInt)
+		if x != y.Args[0] || !(c == d) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (ORshiftRL (MOVWconst [c]) x [d])
+	// result: (ORconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbORconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ORshiftRL x (MOVWconst [c]) [d])
+	// result: (ORconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbORconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: ( ORshiftRL [c] (SLLconst x [32-c]) x)
+	// result: (SRRconst [ c] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSLLconst || auxIntToInt32(v_0.AuxInt) != 32-c {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftRL x y:(SRLconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		y := v_1
+		if y.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(y.AuxInt)
+		if x != y.Args[0] || !(c == d) {
+			break
+		}
+		v.copyOf(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSB(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (RSB (MOVWconst [c]) x)
+	// result: (SUBconst [c] x)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSB x (MOVWconst [c]))
+	// result: (RSBconst [c] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSB x (SLLconst [c] y))
+	// result: (RSBshiftLL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbRSBshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (RSB (SLLconst [c] y) x)
+	// result: (SUBshiftLL x y [c])
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbSUBshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (RSB x (SRLconst [c] y))
+	// result: (RSBshiftRL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbRSBshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (RSB (SRLconst [c] y) x)
+	// result: (SUBshiftRL x y [c])
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbSUBshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (RSB x (SRAconst [c] y))
+	// result: (RSBshiftRA x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbRSBshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (RSB (SRAconst [c] y) x)
+	// result: (SUBshiftRA x y [c])
+	for {
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbSUBshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (RSB x x)
+	// result: (MOVWconst [0])
+	for {
+		x := v_0
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	// match: (RSB (MUL x y) a)
+	// result: (MULS x y a)
+	for {
+		if v_0.Op != OpThumbMUL {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		a := v_1
+		v.reset(OpThumbMULS)
+		v.AddArg3(x, y, a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBSshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (RSBSshiftLL (MOVWconst [c]) x [d])
+	// result: (SUBSconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBSshiftLL x (MOVWconst [c]) [d])
+	// result: (RSBSconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBSshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (RSBSshiftRA (MOVWconst [c]) x [d])
+	// result: (SUBSconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBSshiftRA x (MOVWconst [c]) [d])
+	// result: (RSBSconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBSshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (RSBSshiftRL (MOVWconst [c]) x [d])
+	// result: (SUBSconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBSshiftRL x (MOVWconst [c]) [d])
+	// result: (RSBSconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (RSBconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [c-d])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c - d)
+		return true
+	}
+	// match: (RSBconst [c] (RSBconst [d] x))
+	// result: (ADDconst [c-d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(c - d)
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBconst [c] (ADDconst [d] x))
+	// result: (RSBconst [c-d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c - d)
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBconst [c] (SUBconst [d] x))
+	// result: (RSBconst [c+d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c + d)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (RSBshiftLL (MOVWconst [c]) x [d])
+	// result: (SUBconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBshiftLL x (MOVWconst [c]) [d])
+	// result: (RSBconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBshiftLL x (SLLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (RSBshiftRA (MOVWconst [c]) x [d])
+	// result: (SUBconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBshiftRA x (MOVWconst [c]) [d])
+	// result: (RSBconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBshiftRA x (SRAconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (RSBshiftRL (MOVWconst [c]) x [d])
+	// result: (SUBconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBshiftRL x (MOVWconst [c]) [d])
+	// result: (RSBconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBshiftRL x (SRLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBC(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SBC x (MOVWconst [c]) flags)
+	// result: (SBCconst [c] x flags)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		flags := v_2
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, flags)
+		return true
+	}
+	// match: (SBC x (SLLconst [c] y) flags)
+	// result: (SBCshiftLL x y [c] flags)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		flags := v_2
+		v.reset(OpThumbSBCshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(x, y, flags)
+		return true
+	}
+	// match: (SBC x (SRLconst [c] y) flags)
+	// result: (SBCshiftRL x y [c] flags)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		flags := v_2
+		v.reset(OpThumbSBCshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(x, y, flags)
+		return true
+	}
+	// match: (SBC x (SRAconst [c] y) flags)
+	// result: (SBCshiftRA x y [c] flags)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		flags := v_2
+		v.reset(OpThumbSBCshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(x, y, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBCconst(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SBCconst [c] (ADDconst [d] x) flags)
+	// result: (SBCconst [c-d] x flags)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		flags := v_1
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int32ToAuxInt(c - d)
+		v.AddArg2(x, flags)
+		return true
+	}
+	// match: (SBCconst [c] (SUBconst [d] x) flags)
+	// result: (SBCconst [c+d] x flags)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		flags := v_1
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int32ToAuxInt(c + d)
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBCshiftLL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SBCshiftLL x (MOVWconst [c]) [d] flags)
+	// result: (SBCconst x [c<<uint64(d)] flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		flags := v_2
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBCshiftRA(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SBCshiftRA x (MOVWconst [c]) [d] flags)
+	// result: (SBCconst x [c>>uint64(d)] flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		flags := v_2
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBCshiftRL(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SBCshiftRL x (MOVWconst [c]) [d] flags)
+	// result: (SBCconst x [int32(uint32(c)>>uint64(d))] flags)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		flags := v_2
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg2(x, flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SLL x (MOVWconst [c]))
+	// result: (SLLconst x [c&31])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = int32ToAuxInt(c & 31)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSLLconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (SLLconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [d<<uint64(c)])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(d << uint64(c))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SRA x (MOVWconst [c]))
+	// result: (SRAconst x [c&31])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(c & 31)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRAcond(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SRAcond x _ (FlagConstant [fc]))
+	// cond: fc.uge()
+	// result: (SRAconst x [31])
+	for {
+		x := v_0
+		if v_2.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_2.AuxInt)
+		if !(fc.uge()) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(31)
+		v.AddArg(x)
+		return true
+	}
+	// match: (SRAcond x y (FlagConstant [fc]))
+	// cond: fc.ult()
+	// result: (SRA x y)
+	for {
+		x := v_0
+		y := v_1
+		if v_2.Op != OpThumbFlagConstant {
+			break
+		}
+		fc := auxIntToFlagConstant(v_2.AuxInt)
+		if !(fc.ult()) {
+			break
+		}
+		v.reset(OpThumbSRA)
+		v.AddArg2(x, y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRAconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (SRAconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [d>>uint64(c)])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(d >> uint64(c))
+		return true
+	}
+	// match: (SRAconst (SLLconst x [c]) [d])
+	// cond: uint64(d)>=uint64(c) && uint64(d)<=31
+	// result: (BFX [(d-c)|(32-d)<<8] x)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(uint64(d) >= uint64(c) && uint64(d) <= 31) {
+			break
+		}
+		v.reset(OpThumbBFX)
+		v.AuxInt = int32ToAuxInt((d - c) | (32-d)<<8)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SRL x (MOVWconst [c]))
+	// result: (SRLconst x [c&31])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = int32ToAuxInt(c & 31)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRLconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (SRLconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [int32(uint32(d)>>uint64(c))])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(d) >> uint64(c)))
+		return true
+	}
+	// match: (SRLconst (SLLconst x [c]) [d])
+	// cond: uint64(d)>=uint64(c) && uint64(d)<=31
+	// result: (BFXU [(d-c)|(32-d)<<8] x)
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		if !(uint64(d) >= uint64(c) && uint64(d) <= 31) {
+			break
+		}
+		v.reset(OpThumbBFXU)
+		v.AuxInt = int32ToAuxInt((d - c) | (32-d)<<8)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUB(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SUB (MOVWconst [c]) x)
+	// result: (RSBconst [c] x)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUB x (MOVWconst [c]))
+	// result: (SUBconst [c] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUB x (SLLconst [c] y))
+	// result: (SUBshiftLL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUB (SLLconst [c] y) x)
+	// result: (RSBshiftLL x y [c])
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbRSBshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUB x (SRLconst [c] y))
+	// result: (SUBshiftRL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUB (SRLconst [c] y) x)
+	// result: (RSBshiftRL x y [c])
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbRSBshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUB x (SRAconst [c] y))
+	// result: (SUBshiftRA x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUB (SRAconst [c] y) x)
+	// result: (RSBshiftRA x y [c])
+	for {
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbRSBshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUB x x)
+	// result: (MOVWconst [0])
+	for {
+		x := v_0
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	// match: (SUB a (MUL x y))
+	// result: (MULS x y a)
+	for {
+		a := v_0
+		if v_1.Op != OpThumbMUL {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		v.reset(OpThumbMULS)
+		v.AddArg3(x, y, a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBD(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SUBD a (MULD x y))
+	// cond: a.Uses == 1
+	// result: (MULSD a x y)
+	for {
+		a := v_0
+		if v_1.Op != OpThumbMULD {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULSD)
+		v.AddArg3(a, x, y)
+		return true
+	}
+	// match: (SUBD a (NMULD x y))
+	// cond: a.Uses == 1
+	// result: (MULAD a x y)
+	for {
+		a := v_0
+		if v_1.Op != OpThumbNMULD {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULAD)
+		v.AddArg3(a, x, y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBF(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SUBF a (MULF x y))
+	// cond: a.Uses == 1
+	// result: (MULSF a x y)
+	for {
+		a := v_0
+		if v_1.Op != OpThumbMULF {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULSF)
+		v.AddArg3(a, x, y)
+		return true
+	}
+	// match: (SUBF a (NMULF x y))
+	// cond: a.Uses == 1
+	// result: (MULAF a x y)
+	for {
+		a := v_0
+		if v_1.Op != OpThumbNMULF {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULAF)
+		v.AddArg3(a, x, y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBS(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (SUBS x (MOVWconst [c]))
+	// result: (SUBSconst [c] x)
+	for {
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBS x (SLLconst [c] y))
+	// result: (SUBSshiftLL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBSshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUBS (SLLconst [c] y) x)
+	// result: (RSBSshiftLL x y [c])
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbRSBSshiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUBS x (SRLconst [c] y))
+	// result: (SUBSshiftRL x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBSshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUBS (SRLconst [c] y) x)
+	// result: (RSBSshiftRL x y [c])
+	for {
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbRSBSshiftRL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUBS x (SRAconst [c] y))
+	// result: (SUBSshiftRA x y [c])
+	for {
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBSshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	// match: (SUBS (SRAconst [c] y) x)
+	// result: (RSBSshiftRA x y [c])
+	for {
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		y := v_0.Args[0]
+		x := v_1
+		v.reset(OpThumbRSBSshiftRA)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg2(x, y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBSshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (SUBSshiftLL (MOVWconst [c]) x [d])
+	// result: (RSBSconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBSshiftLL x (MOVWconst [c]) [d])
+	// result: (SUBSconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBSshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (SUBSshiftRA (MOVWconst [c]) x [d])
+	// result: (RSBSconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBSshiftRA x (MOVWconst [c]) [d])
+	// result: (SUBSconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBSshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (SUBSshiftRL (MOVWconst [c]) x [d])
+	// result: (RSBSconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBSshiftRL x (MOVWconst [c]) [d])
+	// result: (SUBSconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (SUBconst [off1] (MOVWaddr [off2] {sym} ptr))
+	// result: (MOVWaddr [off2-off1] {sym} ptr)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		sym := auxToSym(v_0.Aux)
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVWaddr)
+		v.AuxInt = int32ToAuxInt(off2 - off1)
+		v.Aux = symToAux(sym)
+		v.AddArg(ptr)
+		return true
+	}
+	// match: (SUBconst [0] x)
+	// result: x
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		x := v_0
+		v.copyOf(x)
+		return true
+	}
+	// match: (SUBconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [d-c])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(d - c)
+		return true
+	}
+	// match: (SUBconst [c] (SUBconst [d] x))
+	// result: (ADDconst [-c-d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(-c - d)
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBconst [c] (ADDconst [d] x))
+	// result: (ADDconst [-c+d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int32ToAuxInt(-c + d)
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBconst [c] (RSBconst [d] x))
+	// result: (RSBconst [-c+d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(-c + d)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (SUBshiftLL (MOVWconst [c]) x [d])
+	// result: (RSBconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBshiftLL x (MOVWconst [c]) [d])
+	// result: (SUBconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBshiftLL x (SLLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (SUBshiftRA (MOVWconst [c]) x [d])
+	// result: (RSBconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBshiftRA x (MOVWconst [c]) [d])
+	// result: (SUBconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBshiftRA x (SRAconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (SUBshiftRL (MOVWconst [c]) x [d])
+	// result: (RSBconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBshiftRL x (MOVWconst [c]) [d])
+	// result: (SUBconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBshiftRL x (SRLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce16(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (StoreOnce16 [off1] (ADDconst [off2] ptr) val mem)
+	// result: (StoreOnce16 [off1+off2] ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce16 [off1] (SUBconst [off2] ptr) val mem)
+	// result: (StoreOnce16 [off1-off2] ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce16 [0] (ADD ptr idx) val mem)
+	// result: (StoreOnce16idx ptr idx val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce16idx)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (StoreOnce16 [0] (ADDshiftLL ptr idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce16shiftLL ptr idx [c] val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce16shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce16idx(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (StoreOnce16idx ptr (MOVWconst [c]) val mem)
+	// result: (StoreOnce16 [c] ptr val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce16idx (MOVWconst [c]) ptr val mem)
+	// result: (StoreOnce16 [c] ptr val mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce16idx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce16shiftLL ptr idx [c] val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce16shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (StoreOnce16idx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (StoreOnce16shiftLL ptr idx [c] val mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce16shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce32(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (StoreOnce32 [off1] (ADDconst [off2] ptr) val mem)
+	// result: (StoreOnce32 [off1+off2] ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce32 [off1] (SUBconst [off2] ptr) val mem)
+	// result: (StoreOnce32 [off1-off2] ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce32 [0] (ADD ptr idx) val mem)
+	// result: (StoreOnce32idx ptr idx val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce32idx)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (StoreOnce32 [0] (ADDshiftLL ptr idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce32shiftLL ptr idx [c] val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce32shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce32idx(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (StoreOnce32idx ptr (MOVWconst [c]) val mem)
+	// result: (StoreOnce32 [c] ptr val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce32idx (MOVWconst [c]) ptr val mem)
+	// result: (StoreOnce32 [c] ptr val mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce32idx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce32shiftLL ptr idx [c] val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce32shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (StoreOnce32idx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (StoreOnce32shiftLL ptr idx [c] val mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce32shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce8(v *Value) bool {
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (StoreOnce8 [off1] (ADDconst [off2] ptr) val mem)
+	// result: (StoreOnce8 [off1+off2] ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = int32ToAuxInt(off1 + off2)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce8 [off1] (SUBconst [off2] ptr) val mem)
+	// result: (StoreOnce8 [off1-off2] ptr val mem)
+	for {
+		off1 := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := auxIntToInt32(v_0.AuxInt)
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = int32ToAuxInt(off1 - off2)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce8 [0] (ADD ptr idx) val mem)
+	// result: (StoreOnce8idx ptr idx val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		v.reset(OpThumbStoreOnce8idx)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (StoreOnce8 [0] (ADDshiftLL ptr idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce8shiftLL ptr idx [c] val mem)
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 || v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v_1
+		mem := v_2
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce8shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce8idx(v *Value) bool {
+	v_3 := v.Args[3]
+	v_2 := v.Args[2]
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (StoreOnce8idx ptr (MOVWconst [c]) val mem)
+	// result: (StoreOnce8 [c] ptr val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce8idx (MOVWconst [c]) ptr val mem)
+	// result: (StoreOnce8 [c] ptr val mem)
+	for {
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg3(ptr, val, mem)
+		return true
+	}
+	// match: (StoreOnce8idx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce8shiftLL ptr idx [c] val mem)
+	for {
+		ptr := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		idx := v_1.Args[0]
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce8shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	// match: (StoreOnce8idx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (StoreOnce8shiftLL ptr idx [c] val mem)
+	for {
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		idx := v_0.Args[0]
+		ptr := v_1
+		val := v_2
+		mem := v_3
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce8shiftLL)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg4(ptr, idx, val, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQ(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (TEQ x (MOVWconst [c]))
+	// result: (TEQconst [c] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbTEQconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (TEQ x (SLLconst [c] y))
+	// result: (TEQshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbTEQshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (TEQ x (SRLconst [c] y))
+	// result: (TEQshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbTEQshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (TEQ x (SRAconst [c] y))
+	// result: (TEQshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbTEQshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (TEQconst (MOVWconst [x]) [y])
+	// result: (FlagConstant [logicFlags32(x^y)])
+	for {
+		y := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbFlagConstant)
+		v.AuxInt = flagConstantToAuxInt(logicFlags32(x ^ y))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (TEQshiftLL (MOVWconst [c]) x [d])
+	// result: (TEQconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TEQshiftLL x (MOVWconst [c]) [d])
+	// result: (TEQconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (TEQshiftRA (MOVWconst [c]) x [d])
+	// result: (TEQconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TEQshiftRA x (MOVWconst [c]) [d])
+	// result: (TEQconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (TEQshiftRL (MOVWconst [c]) x [d])
+	// result: (TEQconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TEQshiftRL x (MOVWconst [c]) [d])
+	// result: (TEQconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTST(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (TST x (MOVWconst [c]))
+	// result: (TSTconst [c] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbTSTconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (TST x (SLLconst [c] y))
+	// result: (TSTshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbTSTshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (TST x (SRLconst [c] y))
+	// result: (TSTshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbTSTshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (TST x (SRAconst [c] y))
+	// result: (TSTshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbTSTshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTSTconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (TSTconst (MOVWconst [x]) [y])
+	// result: (FlagConstant [logicFlags32(x&y)])
+	for {
+		y := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbFlagConstant)
+		v.AuxInt = flagConstantToAuxInt(logicFlags32(x & y))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTSTshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (TSTshiftLL (MOVWconst [c]) x [d])
+	// result: (TSTconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TSTshiftLL x (MOVWconst [c]) [d])
+	// result: (TSTconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTSTshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (TSTshiftRA (MOVWconst [c]) x [d])
+	// result: (TSTconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TSTshiftRA x (MOVWconst [c]) [d])
+	// result: (TSTconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTSTshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (TSTshiftRL (MOVWconst [c]) x [d])
+	// result: (TSTconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TSTshiftRL x (MOVWconst [c]) [d])
+	// result: (TSTconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXOR(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	// match: (XOR x (MOVWconst [c]))
+	// result: (XORconst [c] x)
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbMOVWconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			v.reset(OpThumbXORconst)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg(x)
+			return true
+		}
+		break
+	}
+	// match: (XOR x (SLLconst [c] y))
+	// result: (XORshiftLL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSLLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbXORshiftLL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (XOR x (SRLconst [c] y))
+	// result: (XORshiftRL x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRLconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbXORshiftRL)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (XOR x (SRAconst [c] y))
+	// result: (XORshiftRA x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRAconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbXORshiftRA)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (XOR x (SRRconst [c] y))
+	// result: (XORshiftRR x y [c])
+	for {
+		for _i0 := 0; _i0 <= 1; _i0, v_0, v_1 = _i0+1, v_1, v_0 {
+			x := v_0
+			if v_1.Op != OpThumbSRRconst {
+				continue
+			}
+			c := auxIntToInt32(v_1.AuxInt)
+			y := v_1.Args[0]
+			v.reset(OpThumbXORshiftRR)
+			v.AuxInt = int32ToAuxInt(c)
+			v.AddArg2(x, y)
+			return true
+		}
+		break
+	}
+	// match: (XOR x x)
+	// result: (MOVWconst [0])
+	for {
+		x := v_0
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORconst(v *Value) bool {
+	v_0 := v.Args[0]
+	// match: (XORconst [0] x)
+	// result: x
+	for {
+		if auxIntToInt32(v.AuxInt) != 0 {
+			break
+		}
+		x := v_0
+		v.copyOf(x)
+		return true
+	}
+	// match: (XORconst [c] (MOVWconst [d]))
+	// result: (MOVWconst [c^d])
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(c ^ d)
+		return true
+	}
+	// match: (XORconst [c] (XORconst [d] x))
+	// result: (XORconst [c^d] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbXORconst {
+			break
+		}
+		d := auxIntToInt32(v_0.AuxInt)
+		x := v_0.Args[0]
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(c ^ d)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORshiftLL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (XORshiftLL (MOVWconst [c]) x [d])
+	// result: (XORconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (XORshiftLL x (MOVWconst [c]) [d])
+	// result: (XORconst x [c<<uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(c << uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftLL [c] (SRLconst x [32-c]) x)
+	// result: (SRRconst [32-c] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSRLconst || auxIntToInt32(v_0.AuxInt) != 32-c {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = int32ToAuxInt(32 - c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftLL <typ.UInt16> [8] (BFXU <typ.UInt16> [int32(armBFAuxInt(8, 8))] x) x)
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 || auxIntToInt32(v.AuxInt) != 8 || v_0.Op != OpThumbBFXU || v_0.Type != typ.UInt16 || auxIntToInt32(v_0.AuxInt) != int32(armBFAuxInt(8, 8)) {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftLL <typ.UInt16> [8] (SRLconst <typ.UInt16> [24] (SLLconst [16] x)) x)
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 || auxIntToInt32(v.AuxInt) != 8 || v_0.Op != OpThumbSRLconst || v_0.Type != typ.UInt16 || auxIntToInt32(v_0.AuxInt) != 24 {
+			break
+		}
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbSLLconst || auxIntToInt32(v_0_0.AuxInt) != 16 {
+			break
+		}
+		x := v_0_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftLL x (SLLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORshiftRA(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (XORshiftRA (MOVWconst [c]) x [d])
+	// result: (XORconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (XORshiftRA x (MOVWconst [c]) [d])
+	// result: (XORconst x [c>>uint64(d)])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(c >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftRA x (SRAconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORshiftRL(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (XORshiftRL (MOVWconst [c]) x [d])
+	// result: (XORconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (XORshiftRL x (MOVWconst [c]) [d])
+	// result: (XORconst x [int32(uint32(c)>>uint64(d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftRL [c] (SLLconst x [32-c]) x)
+	// result: (SRRconst [ c] x)
+	for {
+		c := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbSLLconst || auxIntToInt32(v_0.AuxInt) != 32-c {
+			break
+		}
+		x := v_0.Args[0]
+		if x != v_1 {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftRL x (SRLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		if x != v_1.Args[0] || !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int32ToAuxInt(0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORshiftRR(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	// match: (XORshiftRR (MOVWconst [c]) x [d])
+	// result: (XORconst [c] (SRRconst <x.Type> x [d]))
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_0.AuxInt)
+		x := v_1
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(c)
+		v0 := b.NewValue0(v.Pos, OpThumbSRRconst, x.Type)
+		v0.AuxInt = int32ToAuxInt(d)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (XORshiftRR x (MOVWconst [c]) [d])
+	// result: (XORconst x [int32(uint32(c)>>uint64(d)|uint32(c)<<uint64(32-d))])
+	for {
+		d := auxIntToInt32(v.AuxInt)
+		x := v_0
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := auxIntToInt32(v_1.AuxInt)
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int32ToAuxInt(int32(uint32(c)>>uint64(d) | uint32(c)<<uint64(32-d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpZero(v *Value) bool {
+	v_1 := v.Args[1]
+	v_0 := v.Args[0]
+	b := v.Block
+	config := b.Func.Config
+	typ := &b.Func.Config.Types
+	// match: (Zero [0] _ mem)
+	// result: mem
+	for {
+		if auxIntToInt64(v.AuxInt) != 0 {
+			break
+		}
+		mem := v_1
+		v.copyOf(mem)
+		return true
+	}
+	// match: (Zero [1] ptr mem)
+	// result: (MOVBstore ptr (MOVWconst [0]) mem)
+	for {
+		if auxIntToInt64(v.AuxInt) != 1 {
+			break
+		}
+		ptr := v_0
+		mem := v_1
+		v.reset(OpThumbMOVBstore)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(0)
+		v.AddArg3(ptr, v0, mem)
+		return true
+	}
+	// match: (Zero [2] {t} ptr mem)
+	// cond: t.Alignment()%2 == 0
+	// result: (MOVHstore ptr (MOVWconst [0]) mem)
+	for {
+		if auxIntToInt64(v.AuxInt) != 2 {
+			break
+		}
+		t := auxToType(v.Aux)
+		ptr := v_0
+		mem := v_1
+		if !(t.Alignment()%2 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(0)
+		v.AddArg3(ptr, v0, mem)
+		return true
+	}
+	// match: (Zero [2] ptr mem)
+	// result: (MOVBstore [1] ptr (MOVWconst [0]) (MOVBstore [0] ptr (MOVWconst [0]) mem))
+	for {
+		if auxIntToInt64(v.AuxInt) != 2 {
+			break
+		}
+		ptr := v_0
+		mem := v_1
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(1)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = int32ToAuxInt(0)
+		v1.AddArg3(ptr, v0, mem)
+		v.AddArg3(ptr, v0, v1)
+		return true
+	}
+	// match: (Zero [4] {t} ptr mem)
+	// cond: t.Alignment()%4 == 0
+	// result: (MOVWstore ptr (MOVWconst [0]) mem)
+	for {
+		if auxIntToInt64(v.AuxInt) != 4 {
+			break
+		}
+		t := auxToType(v.Aux)
+		ptr := v_0
+		mem := v_1
+		if !(t.Alignment()%4 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVWstore)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(0)
+		v.AddArg3(ptr, v0, mem)
+		return true
+	}
+	// match: (Zero [4] {t} ptr mem)
+	// cond: t.Alignment()%2 == 0
+	// result: (MOVHstore [2] ptr (MOVWconst [0]) (MOVHstore [0] ptr (MOVWconst [0]) mem))
+	for {
+		if auxIntToInt64(v.AuxInt) != 4 {
+			break
+		}
+		t := auxToType(v.Aux)
+		ptr := v_0
+		mem := v_1
+		if !(t.Alignment()%2 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = int32ToAuxInt(2)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVHstore, types.TypeMem)
+		v1.AuxInt = int32ToAuxInt(0)
+		v1.AddArg3(ptr, v0, mem)
+		v.AddArg3(ptr, v0, v1)
+		return true
+	}
+	// match: (Zero [4] ptr mem)
+	// result: (MOVBstore [3] ptr (MOVWconst [0]) (MOVBstore [2] ptr (MOVWconst [0]) (MOVBstore [1] ptr (MOVWconst [0]) (MOVBstore [0] ptr (MOVWconst [0]) mem))))
+	for {
+		if auxIntToInt64(v.AuxInt) != 4 {
+			break
+		}
+		ptr := v_0
+		mem := v_1
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(3)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = int32ToAuxInt(2)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v2.AuxInt = int32ToAuxInt(1)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v3.AuxInt = int32ToAuxInt(0)
+		v3.AddArg3(ptr, v0, mem)
+		v2.AddArg3(ptr, v0, v3)
+		v1.AddArg3(ptr, v0, v2)
+		v.AddArg3(ptr, v0, v1)
+		return true
+	}
+	// match: (Zero [3] ptr mem)
+	// result: (MOVBstore [2] ptr (MOVWconst [0]) (MOVBstore [1] ptr (MOVWconst [0]) (MOVBstore [0] ptr (MOVWconst [0]) mem)))
+	for {
+		if auxIntToInt64(v.AuxInt) != 3 {
+			break
+		}
+		ptr := v_0
+		mem := v_1
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = int32ToAuxInt(2)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = int32ToAuxInt(1)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v2.AuxInt = int32ToAuxInt(0)
+		v2.AddArg3(ptr, v0, mem)
+		v1.AddArg3(ptr, v0, v2)
+		v.AddArg3(ptr, v0, v1)
+		return true
+	}
+	// match: (Zero [s] {t} ptr mem)
+	// cond: s%4 == 0 && s > 4 && s <= 512 && t.Alignment()%4 == 0 && !config.noDuffDevice
+	// result: (DUFFZERO [4 * (128 - s/4)] ptr (MOVWconst [0]) mem)
+	for {
+		s := auxIntToInt64(v.AuxInt)
+		t := auxToType(v.Aux)
+		ptr := v_0
+		mem := v_1
+		if !(s%4 == 0 && s > 4 && s <= 512 && t.Alignment()%4 == 0 && !config.noDuffDevice) {
+			break
+		}
+		v.reset(OpThumbDUFFZERO)
+		v.AuxInt = int64ToAuxInt(4 * (128 - s/4))
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = int32ToAuxInt(0)
+		v.AddArg3(ptr, v0, mem)
+		return true
+	}
+	// match: (Zero [s] {t} ptr mem)
+	// cond: (s > 512 || config.noDuffDevice) || t.Alignment()%4 != 0
+	// result: (LoweredZero [t.Alignment()] ptr (ADDconst <ptr.Type> ptr [int32(s-moveSize(t.Alignment(), config))]) (MOVWconst [0]) mem)
+	for {
+		s := auxIntToInt64(v.AuxInt)
+		t := auxToType(v.Aux)
+		ptr := v_0
+		mem := v_1
+		if !((s > 512 || config.noDuffDevice) || t.Alignment()%4 != 0) {
+			break
+		}
+		v.reset(OpThumbLoweredZero)
+		v.AuxInt = int64ToAuxInt(t.Alignment())
+		v0 := b.NewValue0(v.Pos, OpThumbADDconst, ptr.Type)
+		v0.AuxInt = int32ToAuxInt(int32(s - moveSize(t.Alignment(), config)))
+		v0.AddArg(ptr)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v1.AuxInt = int32ToAuxInt(0)
+		v.AddArg4(ptr, v0, v1, mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpZeromask(v *Value) bool {
+	v_0 := v.Args[0]
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Zeromask x)
+	// result: (SRAconst (RSBshiftRL <typ.Int32> x x [1]) [31])
+	for {
+		x := v_0
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = int32ToAuxInt(31)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftRL, typ.Int32)
+		v0.AuxInt = int32ToAuxInt(1)
+		v0.AddArg2(x, x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteBlockThumb(b *Block) bool {
+	switch b.Kind {
+	case BlockThumbEQ:
+		// match: (EQ (FlagConstant [fc]) yes no)
+		// cond: fc.eq()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.eq()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (EQ (FlagConstant [fc]) yes no)
+		// cond: !fc.eq()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.eq()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (EQ (InvertFlags cmp) yes no)
+		// result: (EQ cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbEQ, cmp)
+			return true
+		}
+		// match: (EQ (CMP x (RSBconst [0] y)))
+		// result: (EQ (CMN x y))
+		for b.Controls[0].Op == OpThumbCMP {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			x := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+				break
+			}
+			y := v_0_1.Args[0]
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMN x (RSBconst [0] y)))
+		// result: (EQ (CMP x y))
+		for b.Controls[0].Op == OpThumbCMN {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			v_0_0 := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
+				x := v_0_0
+				if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+					continue
+				}
+				y := v_0_1.Args[0]
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbEQ, v0)
+				return true
+			}
+			break
+		}
+		// match: (EQ (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMP x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMP a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMPconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMPshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMPshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMPshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMN x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbEQ, v0)
+				return true
+			}
+			break
+		}
+		// match: (EQ (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMN a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMNconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMNshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMNshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMNshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TST x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTST, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbEQ, v0)
+				return true
+			}
+			break
+		}
+		// match: (EQ (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TSTconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TSTshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TSTshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TSTshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQ x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTEQ, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbEQ, v0)
+				return true
+			}
+			break
+		}
+		// match: (EQ (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbEQ, v0)
+			return true
+		}
+	case BlockThumbGE:
+		// match: (GE (FlagConstant [fc]) yes no)
+		// cond: fc.ge()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.ge()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (GE (FlagConstant [fc]) yes no)
+		// cond: !fc.ge()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.ge()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GE (InvertFlags cmp) yes no)
+		// result: (LE cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbLE, cmp)
+			return true
+		}
+		// match: (GE (CMP x (RSBconst [0] y)))
+		// result: (GE (CMN x y))
+		for b.Controls[0].Op == OpThumbCMP {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			x := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+				break
+			}
+			y := v_0_1.Args[0]
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGE, v0)
+			return true
+		}
+		// match: (GE (CMN x (RSBconst [0] y)))
+		// result: (GE (CMP x y))
+		for b.Controls[0].Op == OpThumbCMN {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			v_0_0 := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
+				x := v_0_0
+				if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+					continue
+				}
+				y := v_0_1.Args[0]
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbGE, v0)
+				return true
+			}
+			break
+		}
+		// match: (GE (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMP x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMP a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMPconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMPshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMPshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMPshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMN x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbGEnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (GE (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMN a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMNconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMNshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMNshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (CMNshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TST x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTST, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbGEnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (GE (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TSTconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TSTshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TSTshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TSTshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TEQ x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTEQ, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbGEnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (GE (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TEQconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TEQshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TEQshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GEnoov (TEQshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGEnoov, v0)
+			return true
+		}
+	case BlockThumbGEnoov:
+		// match: (GEnoov (FlagConstant [fc]) yes no)
+		// cond: fc.geNoov()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.geNoov()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (GEnoov (FlagConstant [fc]) yes no)
+		// cond: !fc.geNoov()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.geNoov()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GEnoov (InvertFlags cmp) yes no)
+		// result: (LEnoov cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbLEnoov, cmp)
+			return true
+		}
+	case BlockThumbGT:
+		// match: (GT (FlagConstant [fc]) yes no)
+		// cond: fc.gt()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.gt()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (GT (FlagConstant [fc]) yes no)
+		// cond: !fc.gt()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.gt()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GT (InvertFlags cmp) yes no)
+		// result: (LT cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbLT, cmp)
+			return true
+		}
+		// match: (GT (CMP x (RSBconst [0] y)))
+		// result: (GT (CMN x y))
+		for b.Controls[0].Op == OpThumbCMP {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			x := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+				break
+			}
+			y := v_0_1.Args[0]
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGT, v0)
+			return true
+		}
+		// match: (GT (CMN x (RSBconst [0] y)))
+		// result: (GT (CMP x y))
+		for b.Controls[0].Op == OpThumbCMN {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			v_0_0 := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
+				x := v_0_0
+				if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+					continue
+				}
+				y := v_0_1.Args[0]
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbGT, v0)
+				return true
+			}
+			break
+		}
+		// match: (GT (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMP x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMP a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMPconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMPshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMPshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMPshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMN x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbGTnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (GT (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMNconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMNshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMNshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMNshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (CMN a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TST x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTST, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbGTnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (GT (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TSTconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TSTshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TSTshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TSTshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TEQ x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTEQ, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbGTnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (GT (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TEQconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TEQshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TEQshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GTnoov (TEQshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbGTnoov, v0)
+			return true
+		}
+	case BlockThumbGTnoov:
+		// match: (GTnoov (FlagConstant [fc]) yes no)
+		// cond: fc.gtNoov()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.gtNoov()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (GTnoov (FlagConstant [fc]) yes no)
+		// cond: !fc.gtNoov()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.gtNoov()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GTnoov (InvertFlags cmp) yes no)
+		// result: (LTnoov cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbLTnoov, cmp)
+			return true
+		}
+	case BlockIf:
+		// match: (If (Equal cc) yes no)
+		// result: (EQ cc yes no)
+		for b.Controls[0].Op == OpThumbEqual {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbEQ, cc)
+			return true
+		}
+		// match: (If (NotEqual cc) yes no)
+		// result: (NE cc yes no)
+		for b.Controls[0].Op == OpThumbNotEqual {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbNE, cc)
+			return true
+		}
+		// match: (If (LessThan cc) yes no)
+		// result: (LT cc yes no)
+		for b.Controls[0].Op == OpThumbLessThan {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbLT, cc)
+			return true
+		}
+		// match: (If (LessThanU cc) yes no)
+		// result: (ULT cc yes no)
+		for b.Controls[0].Op == OpThumbLessThanU {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbULT, cc)
+			return true
+		}
+		// match: (If (LessEqual cc) yes no)
+		// result: (LE cc yes no)
+		for b.Controls[0].Op == OpThumbLessEqual {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbLE, cc)
+			return true
+		}
+		// match: (If (LessEqualU cc) yes no)
+		// result: (ULE cc yes no)
+		for b.Controls[0].Op == OpThumbLessEqualU {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbULE, cc)
+			return true
+		}
+		// match: (If (GreaterThan cc) yes no)
+		// result: (GT cc yes no)
+		for b.Controls[0].Op == OpThumbGreaterThan {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbGT, cc)
+			return true
+		}
+		// match: (If (GreaterThanU cc) yes no)
+		// result: (UGT cc yes no)
+		for b.Controls[0].Op == OpThumbGreaterThanU {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbUGT, cc)
+			return true
+		}
+		// match: (If (GreaterEqual cc) yes no)
+		// result: (GE cc yes no)
+		for b.Controls[0].Op == OpThumbGreaterEqual {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbGE, cc)
+			return true
+		}
+		// match: (If (GreaterEqualU cc) yes no)
+		// result: (UGE cc yes no)
+		for b.Controls[0].Op == OpThumbGreaterEqualU {
+			v_0 := b.Controls[0]
+			cc := v_0.Args[0]
+			b.resetWithControl(BlockThumbUGE, cc)
+			return true
+		}
+		// match: (If cond yes no)
+		// result: (NE (CMPconst [0] cond) yes no)
+		for {
+			cond := b.Controls[0]
+			v0 := b.NewValue0(cond.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(0)
+			v0.AddArg(cond)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+	case BlockThumbLE:
+		// match: (LE (FlagConstant [fc]) yes no)
+		// cond: fc.le()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.le()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (LE (FlagConstant [fc]) yes no)
+		// cond: !fc.le()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.le()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LE (InvertFlags cmp) yes no)
+		// result: (GE cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbGE, cmp)
+			return true
+		}
+		// match: (LE (CMP x (RSBconst [0] y)))
+		// result: (LE (CMN x y))
+		for b.Controls[0].Op == OpThumbCMP {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			x := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+				break
+			}
+			y := v_0_1.Args[0]
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLE, v0)
+			return true
+		}
+		// match: (LE (CMN x (RSBconst [0] y)))
+		// result: (LE (CMP x y))
+		for b.Controls[0].Op == OpThumbCMN {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			v_0_0 := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
+				x := v_0_0
+				if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+					continue
+				}
+				y := v_0_1.Args[0]
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbLE, v0)
+				return true
+			}
+			break
+		}
+		// match: (LE (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMP x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMP a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMPconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMPshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMPshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMPshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMN x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbLEnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (LE (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMN a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMNconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMNshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMNshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (CMNshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TST x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTST, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbLEnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (LE (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TSTconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TSTshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TSTshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TSTshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TEQ x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTEQ, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbLEnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (LE (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TEQconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TEQshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TEQshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LEnoov (TEQshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLEnoov, v0)
+			return true
+		}
+	case BlockThumbLEnoov:
+		// match: (LEnoov (FlagConstant [fc]) yes no)
+		// cond: fc.leNoov()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.leNoov()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (LEnoov (FlagConstant [fc]) yes no)
+		// cond: !fc.leNoov()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.leNoov()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LEnoov (InvertFlags cmp) yes no)
+		// result: (GEnoov cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbGEnoov, cmp)
+			return true
+		}
+	case BlockThumbLT:
+		// match: (LT (FlagConstant [fc]) yes no)
+		// cond: fc.lt()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.lt()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (LT (FlagConstant [fc]) yes no)
+		// cond: !fc.lt()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.lt()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LT (InvertFlags cmp) yes no)
+		// result: (GT cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbGT, cmp)
+			return true
+		}
+		// match: (LT (CMP x (RSBconst [0] y)))
+		// result: (LT (CMN x y))
+		for b.Controls[0].Op == OpThumbCMP {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			x := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+				break
+			}
+			y := v_0_1.Args[0]
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLT, v0)
+			return true
+		}
+		// match: (LT (CMN x (RSBconst [0] y)))
+		// result: (LT (CMP x y))
+		for b.Controls[0].Op == OpThumbCMN {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			v_0_0 := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
+				x := v_0_0
+				if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+					continue
+				}
+				y := v_0_1.Args[0]
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbLT, v0)
+				return true
+			}
+			break
+		}
+		// match: (LT (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMP x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMP a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMPconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMPshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMPshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMPshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMN x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbLTnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (LT (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMN a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMNconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMNshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMNshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (CMNshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TST x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTST, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbLTnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (LT (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TSTconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TSTshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TSTshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TSTshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TEQ x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTEQ, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbLTnoov, v0)
+				return true
+			}
+			break
+		}
+		// match: (LT (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TEQconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TEQshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TEQshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LTnoov (TEQshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbLTnoov, v0)
+			return true
+		}
+	case BlockThumbLTnoov:
+		// match: (LTnoov (FlagConstant [fc]) yes no)
+		// cond: fc.ltNoov()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.ltNoov()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (LTnoov (FlagConstant [fc]) yes no)
+		// cond: !fc.ltNoov()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.ltNoov()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LTnoov (InvertFlags cmp) yes no)
+		// result: (GTnoov cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbGTnoov, cmp)
+			return true
+		}
+	case BlockThumbNE:
+		// match: (NE (CMPconst [0] (Equal cc)) yes no)
+		// result: (EQ cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbEqual {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbEQ, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (NotEqual cc)) yes no)
+		// result: (NE cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbNotEqual {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbNE, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (LessThan cc)) yes no)
+		// result: (LT cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbLessThan {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbLT, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (LessThanU cc)) yes no)
+		// result: (ULT cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbLessThanU {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbULT, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (LessEqual cc)) yes no)
+		// result: (LE cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbLessEqual {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbLE, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (LessEqualU cc)) yes no)
+		// result: (ULE cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbLessEqualU {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbULE, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (GreaterThan cc)) yes no)
+		// result: (GT cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbGreaterThan {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbGT, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (GreaterThanU cc)) yes no)
+		// result: (UGT cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbGreaterThanU {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbUGT, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (GreaterEqual cc)) yes no)
+		// result: (GE cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbGreaterEqual {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbGE, cc)
+			return true
+		}
+		// match: (NE (CMPconst [0] (GreaterEqualU cc)) yes no)
+		// result: (UGE cc yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			v_0_0 := v_0.Args[0]
+			if v_0_0.Op != OpThumbGreaterEqualU {
+				break
+			}
+			cc := v_0_0.Args[0]
+			b.resetWithControl(BlockThumbUGE, cc)
+			return true
+		}
+		// match: (NE (FlagConstant [fc]) yes no)
+		// cond: fc.ne()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.ne()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (NE (FlagConstant [fc]) yes no)
+		// cond: !fc.ne()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.ne()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (NE (InvertFlags cmp) yes no)
+		// result: (NE cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbNE, cmp)
+			return true
+		}
+		// match: (NE (CMP x (RSBconst [0] y)))
+		// result: (NE (CMN x y))
+		for b.Controls[0].Op == OpThumbCMP {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			x := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+				break
+			}
+			y := v_0_1.Args[0]
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMN x (RSBconst [0] y)))
+		// result: (NE (CMP x y))
+		for b.Controls[0].Op == OpThumbCMN {
+			v_0 := b.Controls[0]
+			_ = v_0.Args[1]
+			v_0_0 := v_0.Args[0]
+			v_0_1 := v_0.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, v_0_0, v_0_1 = _i0+1, v_0_1, v_0_0 {
+				x := v_0_0
+				if v_0_1.Op != OpThumbRSBconst || auxIntToInt32(v_0_1.AuxInt) != 0 {
+					continue
+				}
+				y := v_0_1.Args[0]
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbNE, v0)
+				return true
+			}
+			break
+		}
+		// match: (NE (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMP x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMP a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMP, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMPconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMPshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMPshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMPshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMN x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbNE, v0)
+				return true
+			}
+			break
+		}
+		// match: (NE (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMN a (MUL <x.Type> x y)) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMN, types.TypeFlags)
+			v1 := b.NewValue0(v_0.Pos, OpThumbMUL, x.Type)
+			v1.AddArg2(x, y)
+			v0.AddArg2(a, v1)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMNconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMNshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMNshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMNshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TST x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTST, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbNE, v0)
+				return true
+			}
+			break
+		}
+		// match: (NE (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TSTconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TSTshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TSTshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TSTshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQ x y) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			_ = l.Args[1]
+			l_0 := l.Args[0]
+			l_1 := l.Args[1]
+			for _i0 := 0; _i0 <= 1; _i0, l_0, l_1 = _i0+1, l_1, l_0 {
+				x := l_0
+				y := l_1
+				if !(l.Uses == 1) {
+					continue
+				}
+				v0 := b.NewValue0(v_0.Pos, OpThumbTEQ, types.TypeFlags)
+				v0.AddArg2(x, y)
+				b.resetWithControl(BlockThumbNE, v0)
+				return true
+			}
+			break
+		}
+		// match: (NE (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQconst [c] x) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg(x)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQshiftLL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQshiftRL x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQshiftRA x y [c]) yes no)
+		for b.Controls[0].Op == OpThumbCMPconst {
+			v_0 := b.Controls[0]
+			if auxIntToInt32(v_0.AuxInt) != 0 {
+				break
+			}
+			l := v_0.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := auxIntToInt32(l.AuxInt)
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			v0 := b.NewValue0(v_0.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = int32ToAuxInt(c)
+			v0.AddArg2(x, y)
+			b.resetWithControl(BlockThumbNE, v0)
+			return true
+		}
+	case BlockThumbUGE:
+		// match: (UGE (FlagConstant [fc]) yes no)
+		// cond: fc.uge()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.uge()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (UGE (FlagConstant [fc]) yes no)
+		// cond: !fc.uge()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.uge()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (UGE (InvertFlags cmp) yes no)
+		// result: (ULE cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbULE, cmp)
+			return true
+		}
+	case BlockThumbUGT:
+		// match: (UGT (FlagConstant [fc]) yes no)
+		// cond: fc.ugt()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.ugt()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (UGT (FlagConstant [fc]) yes no)
+		// cond: !fc.ugt()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.ugt()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (UGT (InvertFlags cmp) yes no)
+		// result: (ULT cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbULT, cmp)
+			return true
+		}
+	case BlockThumbULE:
+		// match: (ULE (FlagConstant [fc]) yes no)
+		// cond: fc.ule()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.ule()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (ULE (FlagConstant [fc]) yes no)
+		// cond: !fc.ule()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.ule()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (ULE (InvertFlags cmp) yes no)
+		// result: (UGE cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbUGE, cmp)
+			return true
+		}
+	case BlockThumbULT:
+		// match: (ULT (FlagConstant [fc]) yes no)
+		// cond: fc.ult()
+		// result: (First yes no)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(fc.ult()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			return true
+		}
+		// match: (ULT (FlagConstant [fc]) yes no)
+		// cond: !fc.ult()
+		// result: (First no yes)
+		for b.Controls[0].Op == OpThumbFlagConstant {
+			v_0 := b.Controls[0]
+			fc := auxIntToFlagConstant(v_0.AuxInt)
+			if !(!fc.ult()) {
+				break
+			}
+			b.Reset(BlockFirst)
+			b.swapSuccessors()
+			return true
+		}
+		// match: (ULT (InvertFlags cmp) yes no)
+		// result: (UGT cmp yes no)
+		for b.Controls[0].Op == OpThumbInvertFlags {
+			v_0 := b.Controls[0]
+			cmp := v_0.Args[0]
+			b.resetWithControl(BlockThumbUGT, cmp)
+			return true
+		}
+	}
+	return false
+}
diff --git a/src/cmd/compile/internal/ssa/schedule.go b/src/cmd/compile/internal/ssa/schedule.go
index 8facb91100..181c09083d 100644
--- a/src/cmd/compile/internal/ssa/schedule.go
+++ b/src/cmd/compile/internal/ssa/schedule.go
@@ -79,7 +79,7 @@ func (op Op) isLoweredGetClosurePtr() bool {
 	switch op {
 	case OpAMD64LoweredGetClosurePtr, OpPPC64LoweredGetClosurePtr, OpARMLoweredGetClosurePtr, OpARM64LoweredGetClosurePtr,
 		Op386LoweredGetClosurePtr, OpMIPS64LoweredGetClosurePtr, OpS390XLoweredGetClosurePtr, OpMIPSLoweredGetClosurePtr,
-		OpRISCV64LoweredGetClosurePtr, OpWasmLoweredGetClosurePtr:
+		OpRISCV64LoweredGetClosurePtr, OpThumbLoweredGetClosurePtr, OpWasmLoweredGetClosurePtr:
 		return true
 	}
 	return false
@@ -128,7 +128,8 @@ func schedule(f *Func) {
 				v.Op == OpARMLoweredNilCheck || v.Op == OpARM64LoweredNilCheck ||
 				v.Op == Op386LoweredNilCheck || v.Op == OpMIPS64LoweredNilCheck ||
 				v.Op == OpS390XLoweredNilCheck || v.Op == OpMIPSLoweredNilCheck ||
-				v.Op == OpRISCV64LoweredNilCheck || v.Op == OpWasmLoweredNilCheck:
+				v.Op == OpRISCV64LoweredNilCheck || v.Op == OpThumbLoweredNilCheck ||
+				v.Op == OpWasmLoweredNilCheck:
 				// Nil checks must come before loads from the same address.
 				score[v.ID] = ScoreNilCheck
 			case v.Op == OpPhi:
diff --git a/src/cmd/compile/internal/thumb/galign.go b/src/cmd/compile/internal/thumb/galign.go
new file mode 100644
index 0000000000..af7292fbec
--- /dev/null
+++ b/src/cmd/compile/internal/thumb/galign.go
@@ -0,0 +1,26 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumb
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/compile/internal/ssa"
+	"cmd/internal/obj/thumb"
+	"cmd/internal/objabi"
+)
+
+func Init(arch *gc.Arch) {
+	arch.LinkArch = &thumb.Link
+	arch.REGSP = thumb.REGSP
+	arch.MAXWIDTH = (1 << 32) - 1
+	arch.SoftFloat = objabi.GOARM&0xF != 0xD // TODO: handle GOARM==0x7F (32-bit FPU)
+	arch.ZeroRange = zerorange
+	arch.Ginsnop = ginsnop      // used as inline mark
+	arch.Ginsnopdefer = ginsnop // for stack trace to show right line number for deffered calls
+
+	arch.SSAMarkMoves = func(s *gc.SSAGenState, b *ssa.Block) {}
+	arch.SSAGenValue = ssaGenValue
+	arch.SSAGenBlock = ssaGenBlock
+}
diff --git a/src/cmd/compile/internal/thumb/ggen.go b/src/cmd/compile/internal/thumb/ggen.go
new file mode 100644
index 0000000000..574f6e860f
--- /dev/null
+++ b/src/cmd/compile/internal/thumb/ggen.go
@@ -0,0 +1,52 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumb
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj"
+	"cmd/internal/obj/thumb"
+)
+
+func zerorange(pp *gc.Progs, p *obj.Prog, off, cnt int64, r0 *uint32) *obj.Prog {
+	if cnt == 0 {
+		return p
+	}
+	if *r0 == 0 {
+		p = pp.Appendpp(p, thumb.AMOVW, obj.TYPE_CONST, 0, 0, obj.TYPE_REG, thumb.REG_R0, 0)
+		*r0 = 1
+	}
+
+	if cnt < int64(4*gc.Widthptr) {
+		for i := int64(0); i < cnt; i += int64(gc.Widthptr) {
+			p = pp.Appendpp(p, thumb.AMOVW, obj.TYPE_REG, thumb.REG_R0, 0, obj.TYPE_MEM, thumb.REGSP, 4+off+i)
+		}
+	} else if cnt <= int64(128*gc.Widthptr) {
+		p = pp.Appendpp(p, thumb.AADD, obj.TYPE_CONST, 0, 4+off, obj.TYPE_REG, thumb.REG_R1, 0)
+		p.Reg = thumb.REGSP
+		p = pp.Appendpp(p, obj.ADUFFZERO, obj.TYPE_NONE, 0, 0, obj.TYPE_MEM, 0, 0)
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.Duffzero
+		p.To.Offset = 4 * (128 - cnt/int64(gc.Widthptr))
+	} else {
+		p = pp.Appendpp(p, thumb.AADD, obj.TYPE_CONST, 0, 4+off, obj.TYPE_REG, thumb.REG_R1, 0)
+		p.Reg = thumb.REGSP
+		p = pp.Appendpp(p, thumb.AADD, obj.TYPE_CONST, 0, cnt, obj.TYPE_REG, thumb.REG_R2, 0)
+		p.Reg = thumb.REG_R1
+		p = pp.Appendpp(p, thumb.AMOVW, obj.TYPE_REG, thumb.REG_R0, 0, obj.TYPE_MEM, thumb.REG_R1, 4)
+		p1 := p
+		p.Scond |= thumb.C_PBIT
+		p = pp.Appendpp(p, thumb.ACMP, obj.TYPE_REG, thumb.REG_R1, 0, obj.TYPE_NONE, 0, 0)
+		p.Reg = thumb.REG_R2
+		p = pp.Appendpp(p, thumb.ABNE, obj.TYPE_NONE, 0, 0, obj.TYPE_BRANCH, 0, 0)
+		gc.Patch(p, p1)
+	}
+
+	return p
+}
+
+func ginsnop(pp *gc.Progs) *obj.Prog {
+	return pp.Prog(thumb.ANOP2)
+}
diff --git a/src/cmd/compile/internal/thumb/ssa.go b/src/cmd/compile/internal/thumb/ssa.go
new file mode 100644
index 0000000000..6e6b0d384c
--- /dev/null
+++ b/src/cmd/compile/internal/thumb/ssa.go
@@ -0,0 +1,926 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumb
+
+import (
+	"fmt"
+	"math"
+	"math/bits"
+
+	"cmd/compile/internal/gc"
+	"cmd/compile/internal/logopt"
+	"cmd/compile/internal/ssa"
+	"cmd/compile/internal/types"
+	"cmd/internal/obj"
+	"cmd/internal/obj/thumb"
+	"cmd/internal/objabi"
+)
+
+// loadByType returns the load instruction of the given type.
+func loadByType(t *types.Type) obj.As {
+	if t.IsFloat() {
+		switch t.Size() {
+		case 4:
+			return thumb.AMOVF
+		case 8:
+			return thumb.AMOVD
+		}
+	} else {
+		switch t.Size() {
+		case 1:
+			if t.IsSigned() {
+				return thumb.AMOVB
+			} else {
+				return thumb.AMOVBU
+			}
+		case 2:
+			if t.IsSigned() {
+				return thumb.AMOVH
+			} else {
+				return thumb.AMOVHU
+			}
+		case 4:
+			return thumb.AMOVW
+		}
+	}
+	panic("bad load type")
+}
+
+// storeByType returns the store instruction of the given type.
+func storeByType(t *types.Type) obj.As {
+	if t.IsFloat() {
+		switch t.Size() {
+		case 4:
+			return thumb.AMOVF
+		case 8:
+			return thumb.AMOVD
+		}
+	} else {
+		switch t.Size() {
+		case 1:
+			return thumb.AMOVB
+		case 2:
+			return thumb.AMOVH
+		case 4:
+			return thumb.AMOVW
+		}
+	}
+	panic("bad store type")
+}
+
+// shift type is used as Offset in obj.TYPE_SHIFT operands to encode shifted register operands
+type shift int64
+
+// copied from ../../../internal/obj/util.go:/TYPE_SHIFT
+func (v shift) String() string {
+	op := "<<>>->@>"[((v>>5)&3)<<1:]
+	if v&(1<<4) != 0 {
+		// register shift
+		return fmt.Sprintf("R%d%c%cR%d", v&15, op[0], op[1], (v>>8)&15)
+	} else {
+		// constant shift
+		return fmt.Sprintf("R%d%c%c%d", v&15, op[0], op[1], (v>>7)&31)
+	}
+}
+
+// makeshift encodes a register shifted by a constant
+func makeshift(reg int16, typ int64, s int64) shift {
+	return shift(int64(reg&0xf) | typ | (s&31)<<7)
+}
+
+// genshift generates a Prog for r = r0 op (r1 shifted by n)
+func genshift(s *gc.SSAGenState, as obj.As, r0, r1, r int16, typ int64, n int64) *obj.Prog {
+	p := s.Prog(as)
+	p.From.Type = obj.TYPE_SHIFT
+	p.From.Offset = int64(makeshift(r1, typ, n))
+	p.Reg = r0
+	if r != 0 {
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	}
+	return p
+}
+
+// find a (lsb, width) pair for BFC
+// lsb must be in [0, 31], width must be in [1, 32 - lsb]
+// return (0xffffffff, 0) if v is not a binary like 0...01...10...0
+func getBFC(v uint32) (uint32, uint32) {
+	var m, l uint32
+	// BFC is not applicable with zero
+	if v == 0 {
+		return 0xffffffff, 0
+	}
+	// find the lowest set bit, for example l=2 for 0x3ffffffc
+	l = uint32(bits.TrailingZeros32(v))
+	// m-1 represents the highest set bit index, for example m=30 for 0x3ffffffc
+	m = 32 - uint32(bits.LeadingZeros32(v))
+	// check if v is a binary like 0...01...10...0
+	if (1<<m)-(1<<l) == v {
+		// it must be m > l for non-zero v
+		return l, m - l
+	}
+	// invalid
+	return 0xffffffff, 0
+}
+
+func ssaGenValue(s *gc.SSAGenState, v *ssa.Value) {
+	switch v.Op {
+	case ssa.OpCopy, ssa.OpThumbMOVWreg:
+		if v.Type.IsMemory() {
+			return
+		}
+		x := v.Args[0].Reg()
+		y := v.Reg()
+		if x == y {
+			return
+		}
+		as := thumb.AMOVW
+		if v.Type.IsFloat() {
+			switch v.Type.Size() {
+			case 4:
+				as = thumb.AMOVF
+			case 8:
+				as = thumb.AMOVD
+			default:
+				panic("bad float size")
+			}
+		}
+		p := s.Prog(as)
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = x
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = y
+	case ssa.OpThumbMOVWnop:
+		if v.Reg() != v.Args[0].Reg() {
+			v.Fatalf("input[0] and output not in same register %s", v.LongString())
+		}
+		// nothing to do
+	case ssa.OpLoadReg:
+		if v.Type.IsFlags() {
+			v.Fatalf("load flags not implemented: %v", v.LongString())
+			return
+		}
+		p := s.Prog(loadByType(v.Type))
+		gc.AddrAuto(&p.From, v.Args[0])
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpStoreReg:
+		if v.Type.IsFlags() {
+			v.Fatalf("store flags not implemented: %v", v.LongString())
+			return
+		}
+		p := s.Prog(storeByType(v.Type))
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		gc.AddrAuto(&p.To, v)
+	case ssa.OpThumbADD,
+		ssa.OpThumbADC,
+		ssa.OpThumbSUB,
+		ssa.OpThumbSBC,
+		ssa.OpThumbRSB,
+		ssa.OpThumbAND,
+		ssa.OpThumbOR,
+		ssa.OpThumbORN,
+		ssa.OpThumbXOR,
+		ssa.OpThumbBIC,
+		ssa.OpThumbMUL,
+		ssa.OpThumbDIV,
+		ssa.OpThumbDIVU,
+		ssa.OpThumbADDF,
+		ssa.OpThumbADDD,
+		ssa.OpThumbSUBF,
+		ssa.OpThumbSUBD,
+		ssa.OpThumbSLL,
+		ssa.OpThumbSRL,
+		ssa.OpThumbSRA,
+		ssa.OpThumbSRR,
+		ssa.OpThumbMULF,
+		ssa.OpThumbMULD,
+		ssa.OpThumbNMULF,
+		ssa.OpThumbNMULD,
+		ssa.OpThumbDIVF,
+		ssa.OpThumbDIVD:
+		r := v.Reg()
+		r1 := v.Args[0].Reg()
+		r2 := v.Args[1].Reg()
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = r2
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	case ssa.OpThumbMULAF, ssa.OpThumbMULAD, ssa.OpThumbMULSF, ssa.OpThumbMULSD, ssa.OpThumbFMULAD:
+		r := v.Reg()
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+		if r != r0 {
+			v.Fatalf("result and addend are not in the same register: %v", v.LongString())
+		}
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = r2
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	case ssa.OpThumbADDS,
+		ssa.OpThumbSUBS:
+		r := v.Reg0()
+		r1 := v.Args[0].Reg()
+		r2 := v.Args[1].Reg()
+		p := s.Prog(v.Op.Asm())
+		p.Scond = thumb.C_SBIT
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = r2
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	case ssa.OpThumbSRAcond:
+		// Thumb shift instructions uses only the low-order byte of the shift amount
+		// generate conditional instructions to deal with large shifts
+		// flag is already set
+		// SRA.HS	$31, Rarg0, Rdst // shift 31 bits to get the sign bit
+		// SRA.LO	Rarg1, Rarg0, Rdst
+		r := v.Reg()
+		r1 := v.Args[0].Reg()
+		r2 := v.Args[1].Reg()
+		p := s.Prog(thumb.ASRA)
+		p.Scond = thumb.C_SCOND_HS
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 31
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+		p = s.Prog(thumb.ASRA)
+		p.Scond = thumb.C_SCOND_LO
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = r2
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	case ssa.OpThumbBFX, ssa.OpThumbBFXU:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt >> 8
+		p.SetFrom3(obj.Addr{Type: obj.TYPE_CONST, Offset: v.AuxInt & 0xff})
+		p.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbANDconst, ssa.OpThumbBICconst:
+		// try to optimize ANDconst and BICconst to BFC, which saves bytes and ticks
+		// BFC is only available on ARMv7, and its result and source are in the same register
+		if objabi.GOARM >= 7 && v.Reg() == v.Args[0].Reg() {
+			var val uint32
+			if v.Op == ssa.OpThumbANDconst {
+				val = ^uint32(v.AuxInt)
+			} else { // BICconst
+				val = uint32(v.AuxInt)
+			}
+			lsb, width := getBFC(val)
+			// omit BFC for ARM's imm12
+			if 8 < width && width < 24 {
+				p := s.Prog(thumb.ABFC)
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = int64(width)
+				p.SetFrom3(obj.Addr{Type: obj.TYPE_CONST, Offset: int64(lsb)})
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = v.Reg()
+				break
+			}
+		}
+		// fall back to ordinary form
+		fallthrough
+	case ssa.OpThumbADDconst,
+		ssa.OpThumbADCconst,
+		ssa.OpThumbSUBconst,
+		ssa.OpThumbSBCconst,
+		ssa.OpThumbRSBconst,
+		ssa.OpThumbORconst,
+		ssa.OpThumbORNconst,
+		ssa.OpThumbXORconst,
+		ssa.OpThumbSLLconst,
+		ssa.OpThumbSRLconst,
+		ssa.OpThumbSRAconst:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbADDSconst,
+		ssa.OpThumbSUBSconst,
+		ssa.OpThumbRSBSconst:
+		p := s.Prog(v.Op.Asm())
+		p.Scond = thumb.C_SBIT
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg0()
+	case ssa.OpThumbSRRconst:
+		genshift(s, thumb.AMOVW, 0, v.Args[0].Reg(), v.Reg(), thumb.SHIFT_RR, v.AuxInt)
+	case ssa.OpThumbADDshiftLL,
+		ssa.OpThumbADCshiftLL,
+		ssa.OpThumbSUBshiftLL,
+		ssa.OpThumbSBCshiftLL,
+		ssa.OpThumbRSBshiftLL,
+		ssa.OpThumbANDshiftLL,
+		ssa.OpThumbORshiftLL,
+		ssa.OpThumbORNshiftLL,
+		ssa.OpThumbXORshiftLL,
+		ssa.OpThumbBICshiftLL:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg(), thumb.SHIFT_LL, v.AuxInt)
+	case ssa.OpThumbADDSshiftLL,
+		ssa.OpThumbSUBSshiftLL,
+		ssa.OpThumbRSBSshiftLL:
+		p := genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg0(), thumb.SHIFT_LL, v.AuxInt)
+		p.Scond = thumb.C_SBIT
+	case ssa.OpThumbADDshiftRL,
+		ssa.OpThumbADCshiftRL,
+		ssa.OpThumbSUBshiftRL,
+		ssa.OpThumbSBCshiftRL,
+		ssa.OpThumbRSBshiftRL,
+		ssa.OpThumbANDshiftRL,
+		ssa.OpThumbORshiftRL,
+		ssa.OpThumbORNshiftRL,
+		ssa.OpThumbXORshiftRL,
+		ssa.OpThumbBICshiftRL:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg(), thumb.SHIFT_LR, v.AuxInt)
+	case ssa.OpThumbADDSshiftRL,
+		ssa.OpThumbSUBSshiftRL,
+		ssa.OpThumbRSBSshiftRL:
+		p := genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg0(), thumb.SHIFT_LR, v.AuxInt)
+		p.Scond = thumb.C_SBIT
+	case ssa.OpThumbADDshiftRA,
+		ssa.OpThumbADCshiftRA,
+		ssa.OpThumbSUBshiftRA,
+		ssa.OpThumbSBCshiftRA,
+		ssa.OpThumbRSBshiftRA,
+		ssa.OpThumbANDshiftRA,
+		ssa.OpThumbORshiftRA,
+		ssa.OpThumbORNshiftRA,
+		ssa.OpThumbXORshiftRA,
+		ssa.OpThumbBICshiftRA:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg(), thumb.SHIFT_AR, v.AuxInt)
+	case ssa.OpThumbADDSshiftRA,
+		ssa.OpThumbSUBSshiftRA,
+		ssa.OpThumbRSBSshiftRA:
+		p := genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg0(), thumb.SHIFT_AR, v.AuxInt)
+		p.Scond = thumb.C_SBIT
+	case ssa.OpThumbXORshiftRR:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg(), thumb.SHIFT_RR, v.AuxInt)
+	case ssa.OpThumbMVNshiftLL:
+		genshift(s, v.Op.Asm(), 0, v.Args[0].Reg(), v.Reg(), thumb.SHIFT_LL, v.AuxInt)
+	case ssa.OpThumbMVNshiftRL:
+		genshift(s, v.Op.Asm(), 0, v.Args[0].Reg(), v.Reg(), thumb.SHIFT_LR, v.AuxInt)
+	case ssa.OpThumbMVNshiftRA:
+		genshift(s, v.Op.Asm(), 0, v.Args[0].Reg(), v.Reg(), thumb.SHIFT_AR, v.AuxInt)
+	case ssa.OpThumbHMUL,
+		ssa.OpThumbHMULU:
+		// 32-bit high multiplication
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.Reg = v.Args[1].Reg()
+		p.To.Type = obj.TYPE_REGREG
+		p.To.Reg = v.Reg()
+		p.To.Offset = thumb.REGTMP // throw away low 32-bit into tmp register
+	case ssa.OpThumbMULLU:
+		// 32-bit multiplication, results 64-bit, high 32-bit in out0, low 32-bit in out1
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.Reg = v.Args[1].Reg()
+		p.To.Type = obj.TYPE_REGREG
+		p.To.Reg = v.Reg0()           // high 32-bit
+		p.To.Offset = int64(v.Reg1()) // low 32-bit
+	case ssa.OpThumbMULA, ssa.OpThumbMULS:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.Reg = v.Args[1].Reg()
+		p.To.Type = obj.TYPE_REGREG2
+		p.To.Reg = v.Reg()                   // result
+		p.To.Offset = int64(v.Args[2].Reg()) // addend
+	case ssa.OpThumbMOVWconst:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbMOVFconst,
+		ssa.OpThumbMOVDconst:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_FCONST
+		p.From.Val = math.Float64frombits(uint64(v.AuxInt))
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbCMP,
+		ssa.OpThumbCMN,
+		ssa.OpThumbTST,
+		ssa.OpThumbTEQ,
+		ssa.OpThumbCMPF,
+		ssa.OpThumbCMPD:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		// Special layout in ARM assembly
+		// Comparing to x86, the operands of ARM's CMP are reversed.
+		p.From.Reg = v.Args[1].Reg()
+		p.Reg = v.Args[0].Reg()
+	case ssa.OpThumbCMPconst,
+		ssa.OpThumbCMNconst,
+		ssa.OpThumbTSTconst,
+		ssa.OpThumbTEQconst:
+		// Special layout in ARM assembly
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.Reg = v.Args[0].Reg()
+	case ssa.OpThumbCMPF0,
+		ssa.OpThumbCMPD0:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+	case ssa.OpThumbCMPshiftLL, ssa.OpThumbCMNshiftLL, ssa.OpThumbTSTshiftLL, ssa.OpThumbTEQshiftLL:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), 0, thumb.SHIFT_LL, v.AuxInt)
+	case ssa.OpThumbCMPshiftRL, ssa.OpThumbCMNshiftRL, ssa.OpThumbTSTshiftRL, ssa.OpThumbTEQshiftRL:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), 0, thumb.SHIFT_LR, v.AuxInt)
+	case ssa.OpThumbCMPshiftRA, ssa.OpThumbCMNshiftRA, ssa.OpThumbTSTshiftRA, ssa.OpThumbTEQshiftRA:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), 0, thumb.SHIFT_AR, v.AuxInt)
+	case ssa.OpThumbMOVWaddr:
+		p := s.Prog(thumb.AMOVW)
+		p.From.Type = obj.TYPE_ADDR
+		p.From.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+
+		var wantreg string
+		// MOVW $sym+off(base), R
+		// the assembler expands it as the following:
+		// - base is SP: add constant offset to SP (R13)
+		//               when constant is large, tmp register (R11) may be used
+		// - base is SB: load external address from constant pool (use relocation)
+		switch v.Aux.(type) {
+		default:
+			v.Fatalf("aux is of unknown type %T", v.Aux)
+		case *obj.LSym:
+			wantreg = "SB"
+			gc.AddAux(&p.From, v)
+		case *gc.Node:
+			wantreg = "SP"
+			gc.AddAux(&p.From, v)
+		case nil:
+			// No sym, just MOVW $off(SP), R
+			wantreg = "SP"
+			p.From.Offset = v.AuxInt
+		}
+		if reg := v.Args[0].RegName(); reg != wantreg {
+			v.Fatalf("bad reg %s for symbol type %T, want %s", reg, v.Aux, wantreg)
+		}
+	case ssa.OpThumbMOVWload,
+		ssa.OpThumbMOVHload,
+		ssa.OpThumbMOVHUload,
+		ssa.OpThumbMOVBload,
+		ssa.OpThumbMOVBUload,
+		ssa.OpThumbMOVDload,
+		ssa.OpThumbMOVFload,
+		ssa.OpThumbLoadOnce32,
+		ssa.OpThumbLoadOnce16,
+		ssa.OpThumbLoadOnce8:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = v.Args[0].Reg()
+		gc.AddAux(&p.From, v)
+		p.To.Type = obj.TYPE_REG
+		if _, ok := v.Block.Func.RegAlloc[v.ID].(ssa.LocPair); ok {
+			p.To.Reg = v.Reg0()
+		} else {
+			p.To.Reg = v.Reg()
+		}
+	case ssa.OpThumbMOVWstore,
+		ssa.OpThumbMOVHstore,
+		ssa.OpThumbMOVBstore,
+		ssa.OpThumbMOVDstore,
+		ssa.OpThumbMOVFstore,
+		ssa.OpThumbStoreOnce32,
+		ssa.OpThumbStoreOnce16,
+		ssa.OpThumbStoreOnce8:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[1].Reg()
+		p.To.Type = obj.TYPE_MEM
+		p.To.Reg = v.Args[0].Reg()
+		gc.AddAux(&p.To, v)
+	case ssa.OpThumbMOVWloadidx,
+		ssa.OpThumbMOVHUloadidx,
+		ssa.OpThumbMOVHloadidx,
+		ssa.OpThumbMOVBUloadidx,
+		ssa.OpThumbMOVBloadidx,
+		ssa.OpThumbLoadOnce32idx,
+		ssa.OpThumbLoadOnce16idx,
+		ssa.OpThumbLoadOnce8idx:
+		// this is just shift 0 bits
+		fallthrough
+	case ssa.OpThumbMOVWloadshiftLL,
+		ssa.OpThumbMOVHUloadshiftLL,
+		ssa.OpThumbMOVHloadshiftLL,
+		ssa.OpThumbMOVBUloadshiftLL,
+		ssa.OpThumbMOVBloadshiftLL,
+		ssa.OpThumbLoadOnce32shiftLL,
+		ssa.OpThumbLoadOnce16shiftLL,
+		ssa.OpThumbLoadOnce8shiftLL:
+		var toreg int16
+		if _, ok := v.Block.Func.RegAlloc[v.ID].(ssa.LocPair); ok {
+			toreg = v.Reg0()
+		} else {
+			toreg = v.Reg()
+		}
+		p := genshift(s, v.Op.Asm(), 0, v.Args[1].Reg(), toreg, thumb.SHIFT_LL, v.AuxInt)
+		p.From.Reg = v.Args[0].Reg()
+	case ssa.OpThumbMOVWstoreidx,
+		ssa.OpThumbMOVHstoreidx,
+		ssa.OpThumbMOVBstoreidx,
+		ssa.OpThumbStoreOnce32idx,
+		ssa.OpThumbStoreOnce16idx,
+		ssa.OpThumbStoreOnce8idx:
+		// this is just shift 0 bits
+		fallthrough
+	case ssa.OpThumbMOVWstoreshiftLL,
+		ssa.OpThumbMOVHstoreshiftLL,
+		ssa.OpThumbMOVBstoreshiftLL,
+		ssa.OpThumbStoreOnce32shiftLL,
+		ssa.OpThumbStoreOnce16shiftLL,
+		ssa.OpThumbStoreOnce8shiftLL:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[2].Reg()
+		p.To.Type = obj.TYPE_SHIFT
+		p.To.Reg = v.Args[0].Reg()
+		p.To.Offset = int64(makeshift(v.Args[1].Reg(), thumb.SHIFT_LL, v.AuxInt))
+	case ssa.OpThumbMOVBreg,
+		ssa.OpThumbMOVBUreg,
+		ssa.OpThumbMOVHreg,
+		ssa.OpThumbMOVHUreg:
+		a := v.Args[0]
+		for a.Op == ssa.OpCopy || a.Op == ssa.OpThumbMOVWreg || a.Op == ssa.OpThumbMOVWnop {
+			a = a.Args[0]
+		}
+		if a.Op == ssa.OpLoadReg {
+			t := a.Type
+			switch {
+			case v.Op == ssa.OpThumbMOVBreg && t.Size() == 1 && t.IsSigned(),
+				v.Op == ssa.OpThumbMOVBUreg && t.Size() == 1 && !t.IsSigned(),
+				v.Op == ssa.OpThumbMOVHreg && t.Size() == 2 && t.IsSigned(),
+				v.Op == ssa.OpThumbMOVHUreg && t.Size() == 2 && !t.IsSigned():
+				// arg is a proper-typed load, already zero/sign-extended, don't extend again
+				if v.Reg() == v.Args[0].Reg() {
+					return
+				}
+				p := s.Prog(thumb.AMOVW)
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = v.Args[0].Reg()
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = v.Reg()
+				return
+			default:
+			}
+		}
+		fallthrough
+	case ssa.OpThumbMVN,
+		ssa.OpThumbCLZ,
+		ssa.OpThumbREV,
+		ssa.OpThumbREV16,
+		ssa.OpThumbRBIT,
+		ssa.OpThumbSQRTD,
+		ssa.OpThumbNEGF,
+		ssa.OpThumbNEGD,
+		ssa.OpThumbABSD,
+		ssa.OpThumbMOVWF,
+		ssa.OpThumbMOVWD,
+		ssa.OpThumbMOVFW,
+		ssa.OpThumbMOVDW,
+		ssa.OpThumbMOVFD,
+		ssa.OpThumbMOVDF:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbMOVWUF,
+		ssa.OpThumbMOVWUD,
+		ssa.OpThumbMOVFWU,
+		ssa.OpThumbMOVDWU:
+		p := s.Prog(v.Op.Asm())
+		p.Scond = thumb.C_UBIT
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbCMOVWHSconst:
+		p := s.Prog(thumb.AMOVW)
+		p.Scond = thumb.C_SCOND_HS
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbCMOVWLSconst:
+		p := s.Prog(thumb.AMOVW)
+		p.Scond = thumb.C_SCOND_LS
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbCALLstatic, ssa.OpThumbCALLclosure, ssa.OpThumbCALLinter:
+		s.Call(v)
+	case ssa.OpThumbLoweredWB:
+		p := s.Prog(obj.ACALL)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = v.Aux.(*obj.LSym)
+	case ssa.OpThumbLoweredPanicBoundsA, ssa.OpThumbLoweredPanicBoundsB, ssa.OpThumbLoweredPanicBoundsC:
+		p := s.Prog(obj.ACALL)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.BoundsCheckFunc[v.AuxInt]
+		s.UseArgs(8) // space used in callee args area by assembly stubs
+	case ssa.OpThumbLoweredPanicExtendA, ssa.OpThumbLoweredPanicExtendB, ssa.OpThumbLoweredPanicExtendC:
+		p := s.Prog(obj.ACALL)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.ExtendCheckFunc[v.AuxInt]
+		s.UseArgs(12) // space used in callee args area by assembly stubs
+	case ssa.OpThumbDUFFZERO:
+		p := s.Prog(obj.ADUFFZERO)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.Duffzero
+		p.To.Offset = v.AuxInt
+	case ssa.OpThumbDUFFCOPY:
+		p := s.Prog(obj.ADUFFCOPY)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.Duffcopy
+		p.To.Offset = v.AuxInt
+	case ssa.OpThumbLoweredNilCheck:
+		if objabi.GOOS == "noos" {
+			// BUG: avoid nil check because of MMIO
+		} else {
+			// Issue a load which will fault if arg is nil.
+			p := s.Prog(thumb.AMOVBU)
+			p.From.Type = obj.TYPE_MEM
+			p.From.Reg = v.Args[0].Reg()
+			gc.AddAux(&p.From, v)
+			p.To.Type = obj.TYPE_REG
+			p.To.Reg = thumb.REGTMP
+			if logopt.Enabled() {
+				logopt.LogOpt(v.Pos, "nilcheck", "genssa", v.Block.Func.Name)
+			}
+			if gc.Debug_checknil != 0 && v.Pos.Line() > 1 { // v.Pos.Line()==1 in generated wrappers
+				gc.Warnl(v.Pos, "generated nil check")
+			}
+		}
+	case ssa.OpThumbLoweredZero:
+		// MOVW.P	Rarg2, 4(R1)
+		// CMP	Rarg1, R1
+		// BLE	-2(PC)
+		// arg1 is the address of the last element to zero
+		// arg2 is known to be zero
+		// auxint is alignment
+		var sz int64
+		var mov obj.As
+		switch {
+		case v.AuxInt%4 == 0:
+			sz = 4
+			mov = thumb.AMOVW
+		case v.AuxInt%2 == 0:
+			sz = 2
+			mov = thumb.AMOVH
+		default:
+			sz = 1
+			mov = thumb.AMOVB
+		}
+		p := s.Prog(mov)
+		p.Scond = thumb.C_PBIT
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[2].Reg()
+		p.To.Type = obj.TYPE_MEM
+		p.To.Reg = thumb.REG_R1
+		p.To.Offset = sz
+		p2 := s.Prog(thumb.ACMP)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = v.Args[1].Reg()
+		p2.Reg = thumb.REG_R1
+		p3 := s.Prog(thumb.ABLE)
+		p3.To.Type = obj.TYPE_BRANCH
+		gc.Patch(p3, p)
+	case ssa.OpThumbLoweredMove:
+		// MOVW.P	4(R1), Rtmp
+		// MOVW.P	Rtmp, 4(R2)
+		// CMP	Rarg2, R1
+		// BLE	-3(PC)
+		// arg2 is the address of the last element of src
+		// auxint is alignment
+		var sz int64
+		var mov obj.As
+		switch {
+		case v.AuxInt%4 == 0:
+			sz = 4
+			mov = thumb.AMOVW
+		case v.AuxInt%2 == 0:
+			sz = 2
+			mov = thumb.AMOVH
+		default:
+			sz = 1
+			mov = thumb.AMOVB
+		}
+		p := s.Prog(mov)
+		p.Scond = thumb.C_PBIT
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = thumb.REG_R1
+		p.From.Offset = sz
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = thumb.REGTMP
+		p2 := s.Prog(mov)
+		p2.Scond = thumb.C_PBIT
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = thumb.REGTMP
+		p2.To.Type = obj.TYPE_MEM
+		p2.To.Reg = thumb.REG_R2
+		p2.To.Offset = sz
+		p3 := s.Prog(thumb.ACMP)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = v.Args[2].Reg()
+		p3.Reg = thumb.REG_R1
+		p4 := s.Prog(thumb.ABLE)
+		p4.To.Type = obj.TYPE_BRANCH
+		gc.Patch(p4, p)
+	case ssa.OpThumbEqual,
+		ssa.OpThumbNotEqual,
+		ssa.OpThumbLessThan,
+		ssa.OpThumbLessEqual,
+		ssa.OpThumbGreaterThan,
+		ssa.OpThumbGreaterEqual,
+		ssa.OpThumbLessThanU,
+		ssa.OpThumbLessEqualU,
+		ssa.OpThumbGreaterThanU,
+		ssa.OpThumbGreaterEqualU:
+		// generate boolean values
+		// use conditional move, preserve flags
+		scond := condBits[v.Op] | thumb.C_PBIT
+		p := s.Prog(thumb.AMOVW)
+		p.Scond = scond ^ 1
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 0
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+		p = s.Prog(thumb.AMOVW)
+		p.Scond = scond
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbLoweredGetClosurePtr:
+		// Closure pointer is R11 (thumb.REGCTXT).
+		gc.CheckLoweredGetClosurePtr(v)
+	case ssa.OpThumbLoweredGetCallerSP:
+		// caller's SP is FixedFrameSize below the address of the first arg
+		p := s.Prog(thumb.AMOVW)
+		p.From.Type = obj.TYPE_ADDR
+		p.From.Offset = -gc.Ctxt.FixedFrameSize()
+		p.From.Name = obj.NAME_PARAM
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbLoweredGetCallerPC:
+		p := s.Prog(obj.AGETCALLERPC)
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbFlagConstant:
+		v.Fatalf("FlagConstant op should never make it to codegen %v", v.LongString())
+	case ssa.OpThumbInvertFlags:
+		v.Fatalf("InvertFlags should never make it to codegen %v", v.LongString())
+	case ssa.OpClobber:
+		// TODO: implement for clobberdead experiment. Nop is ok for now.
+	case ssa.OpThumbDSB:
+		s.Prog(thumb.ADSB)
+	case ssa.OpThumbDMB_ST:
+		p := s.Prog(thumb.ADMB)
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = thumb.REG_MB_ST
+	default:
+		v.Fatalf("genValue not implemented: %s", v.LongString())
+	}
+}
+
+var condBits = map[ssa.Op]uint8{
+	ssa.OpThumbEqual:         thumb.C_SCOND_EQ,
+	ssa.OpThumbNotEqual:      thumb.C_SCOND_NE,
+	ssa.OpThumbLessThan:      thumb.C_SCOND_LT,
+	ssa.OpThumbLessThanU:     thumb.C_SCOND_LO,
+	ssa.OpThumbLessEqual:     thumb.C_SCOND_LE,
+	ssa.OpThumbLessEqualU:    thumb.C_SCOND_LS,
+	ssa.OpThumbGreaterThan:   thumb.C_SCOND_GT,
+	ssa.OpThumbGreaterThanU:  thumb.C_SCOND_HI,
+	ssa.OpThumbGreaterEqual:  thumb.C_SCOND_GE,
+	ssa.OpThumbGreaterEqualU: thumb.C_SCOND_HS,
+}
+
+var blockJump = map[ssa.BlockKind]struct {
+	asm, invasm obj.As
+}{
+	ssa.BlockThumbEQ:     {thumb.ABEQ, thumb.ABNE},
+	ssa.BlockThumbNE:     {thumb.ABNE, thumb.ABEQ},
+	ssa.BlockThumbLT:     {thumb.ABLT, thumb.ABGE},
+	ssa.BlockThumbGE:     {thumb.ABGE, thumb.ABLT},
+	ssa.BlockThumbLE:     {thumb.ABLE, thumb.ABGT},
+	ssa.BlockThumbGT:     {thumb.ABGT, thumb.ABLE},
+	ssa.BlockThumbULT:    {thumb.ABLO, thumb.ABHS},
+	ssa.BlockThumbUGE:    {thumb.ABHS, thumb.ABLO},
+	ssa.BlockThumbUGT:    {thumb.ABHI, thumb.ABLS},
+	ssa.BlockThumbULE:    {thumb.ABLS, thumb.ABHI},
+	ssa.BlockThumbLTnoov: {thumb.ABMI, thumb.ABPL},
+	ssa.BlockThumbGEnoov: {thumb.ABPL, thumb.ABMI},
+}
+
+// To model a 'LEnoov' ('<=' without overflow checking) branching
+var leJumps = [2][2]gc.IndexJump{
+	{{Jump: thumb.ABEQ, Index: 0}, {Jump: thumb.ABPL, Index: 1}}, // next == b.Succs[0]
+	{{Jump: thumb.ABMI, Index: 0}, {Jump: thumb.ABEQ, Index: 0}}, // next == b.Succs[1]
+}
+
+// To model a 'GTnoov' ('>' without overflow checking) branching
+var gtJumps = [2][2]gc.IndexJump{
+	{{Jump: thumb.ABMI, Index: 1}, {Jump: thumb.ABEQ, Index: 1}}, // next == b.Succs[0]
+	{{Jump: thumb.ABEQ, Index: 1}, {Jump: thumb.ABPL, Index: 0}}, // next == b.Succs[1]
+}
+
+func ssaGenBlock(s *gc.SSAGenState, b, next *ssa.Block) {
+	switch b.Kind {
+	case ssa.BlockPlain:
+		if b.Succs[0].Block() != next {
+			p := s.Prog(obj.AJMP)
+			p.To.Type = obj.TYPE_BRANCH
+			s.Branches = append(s.Branches, gc.Branch{P: p, B: b.Succs[0].Block()})
+		}
+
+	case ssa.BlockDefer:
+		// defer returns in R0:
+		// 0 if we should continue executing
+		// 1 if we should jump to deferreturn call
+		p := s.Prog(thumb.ACMP)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 0
+		p.Reg = thumb.REG_R0
+		p = s.Prog(thumb.ABNE)
+		p.To.Type = obj.TYPE_BRANCH
+		s.Branches = append(s.Branches, gc.Branch{P: p, B: b.Succs[1].Block()})
+		if b.Succs[0].Block() != next {
+			p := s.Prog(obj.AJMP)
+			p.To.Type = obj.TYPE_BRANCH
+			s.Branches = append(s.Branches, gc.Branch{P: p, B: b.Succs[0].Block()})
+		}
+
+	case ssa.BlockExit:
+
+	case ssa.BlockRet:
+		s.Prog(obj.ARET)
+
+	case ssa.BlockRetJmp:
+		p := s.Prog(obj.ARET)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = b.Aux.(*obj.LSym)
+
+	case ssa.BlockThumbEQ, ssa.BlockThumbNE,
+		ssa.BlockThumbLT, ssa.BlockThumbGE,
+		ssa.BlockThumbLE, ssa.BlockThumbGT,
+		ssa.BlockThumbULT, ssa.BlockThumbUGT,
+		ssa.BlockThumbULE, ssa.BlockThumbUGE,
+		ssa.BlockThumbLTnoov, ssa.BlockThumbGEnoov:
+		jmp := blockJump[b.Kind]
+		switch next {
+		case b.Succs[0].Block():
+			s.Br(jmp.invasm, b.Succs[1].Block())
+		case b.Succs[1].Block():
+			s.Br(jmp.asm, b.Succs[0].Block())
+		default:
+			if b.Likely != ssa.BranchUnlikely {
+				s.Br(jmp.asm, b.Succs[0].Block())
+				s.Br(obj.AJMP, b.Succs[1].Block())
+			} else {
+				s.Br(jmp.invasm, b.Succs[1].Block())
+				s.Br(obj.AJMP, b.Succs[0].Block())
+			}
+		}
+
+	case ssa.BlockThumbLEnoov:
+		s.CombJump(b, next, &leJumps)
+
+	case ssa.BlockThumbGTnoov:
+		s.CombJump(b, next, &gtJumps)
+
+	default:
+		b.Fatalf("branch not implemented: %s", b.LongString())
+	}
+}
diff --git a/src/cmd/compile/main.go b/src/cmd/compile/main.go
index 3aa64a5ce2..a51759687f 100644
--- a/src/cmd/compile/main.go
+++ b/src/cmd/compile/main.go
@@ -14,6 +14,7 @@ import (
 	"cmd/compile/internal/ppc64"
 	"cmd/compile/internal/riscv64"
 	"cmd/compile/internal/s390x"
+	"cmd/compile/internal/thumb"
 	"cmd/compile/internal/wasm"
 	"cmd/compile/internal/x86"
 	"cmd/internal/objabi"
@@ -35,6 +36,7 @@ var archInits = map[string]func(*gc.Arch){
 	"ppc64le":  ppc64.Init,
 	"riscv64":  riscv64.Init,
 	"s390x":    s390x.Init,
+	"thumb":    thumb.Init,
 	"wasm":     wasm.Init,
 }
 
diff --git a/src/cmd/dist/build.go b/src/cmd/dist/build.go
index c8c3212d16..dfc20280be 100644
--- a/src/cmd/dist/build.go
+++ b/src/cmd/dist/build.go
@@ -71,6 +71,7 @@ var okgoarch = []string{
 	"ppc64le",
 	"riscv64",
 	"s390x",
+	"thumb",
 	"sparc64",
 	"wasm",
 }
@@ -92,6 +93,7 @@ var okgoos = []string{
 	"plan9",
 	"windows",
 	"aix",
+	"noos",
 }
 
 // find reports the first index of p in l[0:n], or else -1.
@@ -1173,7 +1175,7 @@ func cmdenv() {
 	xprintf(format, "GOROOT", goroot)
 	xprintf(format, "GOTMPDIR", os.Getenv("GOTMPDIR"))
 	xprintf(format, "GOTOOLDIR", tooldir)
-	if goarch == "arm" {
+	if goarch == "arm" || goarch == "thumb" {
 		xprintf(format, "GOARM", goarm)
 	}
 	if goarch == "386" {
@@ -1551,6 +1553,7 @@ var cgoEnabled = map[string]bool{
 	"linux/mips64le":  true,
 	"linux/riscv64":   true,
 	"linux/s390x":     true,
+	"linux/thumb":     false,
 	"linux/sparc64":   true,
 	"android/386":     true,
 	"android/amd64":   true,
@@ -1575,6 +1578,8 @@ var cgoEnabled = map[string]bool{
 	"windows/386":     true,
 	"windows/amd64":   true,
 	"windows/arm":     false,
+	"noos/thumb":      false,
+	"noos/riscv64":    false,
 }
 
 // List of platforms which are supported but not complete yet. These get
diff --git a/src/cmd/dist/buildtool.go b/src/cmd/dist/buildtool.go
index cf85f2ac8e..669f6c5749 100644
--- a/src/cmd/dist/buildtool.go
+++ b/src/cmd/dist/buildtool.go
@@ -49,6 +49,7 @@ var bootstrapDirs = []string{
 	"cmd/compile/internal/s390x",
 	"cmd/compile/internal/ssa",
 	"cmd/compile/internal/syntax",
+	"cmd/compile/internal/thumb",
 	"cmd/compile/internal/types",
 	"cmd/compile/internal/x86",
 	"cmd/compile/internal/wasm",
@@ -66,6 +67,7 @@ var bootstrapDirs = []string{
 	"cmd/internal/obj/ppc64",
 	"cmd/internal/obj/riscv",
 	"cmd/internal/obj/s390x",
+	"cmd/internal/obj/thumb",
 	"cmd/internal/obj/x86",
 	"cmd/internal/obj/wasm",
 	"cmd/internal/pkgpath",
@@ -88,6 +90,7 @@ var bootstrapDirs = []string{
 	"cmd/link/internal/riscv64",
 	"cmd/link/internal/s390x",
 	"cmd/link/internal/sym",
+	"cmd/link/internal/thumb",
 	"cmd/link/internal/x86",
 	"compress/flate",
 	"compress/zlib",
@@ -121,6 +124,8 @@ var ignoreSuffixes = []string{
 	"_arm64.go",
 	"_riscv64.s",
 	"_riscv64.go",
+	"_thumb.s",
+	"_thumb.go",
 	"_wasm.s",
 	"_wasm.go",
 	"_test.s",
diff --git a/src/cmd/go/internal/cfg/cfg.go b/src/cmd/go/internal/cfg/cfg.go
index c48904eacc..f42fded6dc 100644
--- a/src/cmd/go/internal/cfg/cfg.go
+++ b/src/cmd/go/internal/cfg/cfg.go
@@ -254,7 +254,7 @@ var (
 	GOMODCACHE   = envOr("GOMODCACHE", gopathDir("pkg/mod"))
 
 	// Used in envcmd.MkEnv and build ID computations.
-	GOARM    = envOr("GOARM", fmt.Sprint(objabi.GOARM))
+	GOARM    = envOr("GOARM", fmt.Sprintf("%x", objabi.GOARM))
 	GO386    = envOr("GO386", objabi.GO386)
 	GOMIPS   = envOr("GOMIPS", objabi.GOMIPS)
 	GOMIPS64 = envOr("GOMIPS64", objabi.GOMIPS64)
diff --git a/src/cmd/go/testdata/script/build_thumb.txt b/src/cmd/go/testdata/script/build_thumb.txt
new file mode 100644
index 0000000000..e35d5adbcd
--- /dev/null
+++ b/src/cmd/go/testdata/script/build_thumb.txt
@@ -0,0 +1,12 @@
+[short] skip 'skipping cross-compile in short mode'
+
+env GOARCH=thumb
+env GOOS=linux
+
+go build hello.go
+! stderr 'unable to find math.a'
+
+-- hello.go --
+package main
+
+func main() {}
\ No newline at end of file
diff --git a/src/cmd/internal/obj/link.go b/src/cmd/internal/obj/link.go
index 8c8ff587ff..a72bccffdb 100644
--- a/src/cmd/internal/obj/link.go
+++ b/src/cmd/internal/obj/link.go
@@ -416,6 +416,7 @@ const (
 	ABaseMIPS
 	ABaseRISCV
 	ABaseS390X
+	ABaseThumb
 	ABaseWasm
 
 	AllowedOpCodes = 1 << 11            // The number of opcodes available for any given architecture.
@@ -629,6 +630,9 @@ const (
 	// ContentAddressable indicates this is a content-addressable symbol.
 	AttrContentAddressable
 
+	// Generate an interrupt entry/exit prologue/epilogue.
+	AttrISR
+
 	// attrABIBase is the value at which the ABI is encoded in
 	// Attribute. This must be last; all bits after this are
 	// assumed to be an ABI value.
@@ -654,6 +658,7 @@ func (a Attribute) TopFrame() bool           { return a&AttrTopFrame != 0 }
 func (a Attribute) Indexed() bool            { return a&AttrIndexed != 0 }
 func (a Attribute) UsedInIface() bool        { return a&AttrUsedInIface != 0 }
 func (a Attribute) ContentAddressable() bool { return a&AttrContentAddressable != 0 }
+func (a Attribute) ISR() bool                { return a&AttrISR != 0 }
 
 func (a *Attribute) Set(flag Attribute, value bool) {
 	if value {
diff --git a/src/cmd/internal/obj/plist.go b/src/cmd/internal/obj/plist.go
index 2b096996f7..9da4efd486 100644
--- a/src/cmd/internal/obj/plist.go
+++ b/src/cmd/internal/obj/plist.go
@@ -137,6 +137,7 @@ func (ctxt *Link) InitTextSym(s *LSym, flag int) {
 	s.Set(AttrNeedCtxt, flag&NEEDCTXT != 0)
 	s.Set(AttrNoFrame, flag&NOFRAME != 0)
 	s.Set(AttrTopFrame, flag&TOPFRAME != 0)
+	s.Set(AttrISR, flag&ISR != 0)
 	s.Type = objabi.STEXT
 	ctxt.Text = append(ctxt.Text, s)
 
diff --git a/src/cmd/internal/obj/riscv/obj.go b/src/cmd/internal/obj/riscv/obj.go
index 9257a6453a..a4ea740137 100644
--- a/src/cmd/internal/obj/riscv/obj.go
+++ b/src/cmd/internal/obj/riscv/obj.go
@@ -552,6 +552,16 @@ func preprocess(ctxt *obj.Link, cursym *obj.LSym, newprog obj.ProgAlloc) {
 		prologue.To = obj.Addr{Type: obj.TYPE_REG, Reg: REG_SP}
 		prologue.Spadj = int32(stacksize)
 
+		if ctxt.Headtype == objabi.Hnoos {
+			// If we are on handler stack the nested trap can clober LR saved
+			// before (required by async preemption). Save LR one more time
+			// after decrementing SP.
+			prologue = obj.Appendp(prologue, newprog)
+			prologue.As = AMOV
+			prologue.From = obj.Addr{Type: obj.TYPE_REG, Reg: REG_LR}
+			prologue.To = obj.Addr{Type: obj.TYPE_MEM, Reg: REG_SP}
+		}
+
 		prologue = ctxt.EndUnsafePoint(prologue, newprog, -1)
 	}
 
diff --git a/src/cmd/internal/obj/textflag.go b/src/cmd/internal/obj/textflag.go
index d2cec734b1..1b9975754b 100644
--- a/src/cmd/internal/obj/textflag.go
+++ b/src/cmd/internal/obj/textflag.go
@@ -51,4 +51,7 @@ const (
 	// Function is the top of the call stack. Call stack unwinders should stop
 	// at this function.
 	TOPFRAME = 2048
+
+	// Generate interrupt handler prologue / epilogue.
+	ISR = 4096
 )
diff --git a/src/cmd/internal/obj/thumb/a.out.go b/src/cmd/internal/obj/thumb/a.out.go
new file mode 100644
index 0000000000..79e47f3e9e
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/a.out.go
@@ -0,0 +1,511 @@
+// Inferno utils/5c/5.out.h
+// https://bitbucket.org/inferno-os/inferno-os/src/master/utils/5c/5.out.h
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import "cmd/internal/obj"
+
+const (
+	// must be 16-aligned
+	REG_R0 = obj.RBaseThumb + iota
+	REG_R1
+	REG_R2
+	REG_R3
+	REG_R4
+	REG_R5
+	REG_R6
+	REG_R7
+	REG_R8
+	REG_R9
+	REG_R10
+	REG_R11
+	REG_R12
+	REG_R13
+	REG_R14
+	REG_R15
+
+	// must be 16-aligned
+	REG_F0
+	REG_F1
+	REG_F2
+	REG_F3
+	REG_F4
+	REG_F5
+	REG_F6
+	REG_F7
+	REG_F8
+	REG_F9
+	REG_F10
+	REG_F11
+	REG_F12
+	REG_F13
+	REG_F14
+	REG_F15
+
+	// must be 32-aligned
+	REG_APSR        // 0
+	REG_IAPSR       // 1
+	REG_EAPSR       // 2
+	REG_XPSR        // 3
+	_               // 4
+	REG_IPSR        // 5
+	REG_EPSR        // 6
+	REG_IEPSR       // 7
+	REG_MSP         // 8
+	REG_PSP         // 9
+	_               // 10
+	_               // 11
+	_               // 12
+	_               // 13
+	_               // 14
+	_               // 15
+	REG_PRIMASK     // 16
+	REG_BASEPRI     // 17
+	REG_BASEPRI_MAX // 18
+	REG_FAULTMASK   // 19
+	REG_CONTROL     // 20
+
+	REG_FPSCR
+
+	// Use R7 as REGTMP insetad of R11 to raise the chance to generate 16-bit
+	// instructions. R11 seems to be twice as often used as R7 in arm code:
+	// for i in 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15; do
+	//   echo -n "R$i "
+	//   go tool objdump compile |grep -v 00000000 |grep "R$i[ ,]" |wc -l
+	// done
+	// R0 461185
+	// R1 315395
+	// R2 209245
+	// R3 164614
+	// R4 111672
+	// R5 77637
+	// R6 58855
+	// R7 58084
+	// R8 40492
+	// R9 35562
+	// R10 6508
+	// R11 100948
+	// R12 29360
+	// R13 23314
+	// R14 44902
+	// R15 6227
+	// Drawback: incompatible with arm assembly that use R11.
+
+	REGTMP  = REG_R7  // reserved for compiler and linker
+	REGG    = REG_R10 // pointer to goroutine structure (g)
+	REGCTXT = REG_R11 // closure pointer
+	REGSP   = REG_R13
+	REGLINK = REG_R14
+	REGPC   = REG_R15
+
+	FREGTMP = REG_F15 // reserved for compiler and linker
+)
+
+const pseudoBase = obj.RBaseThumb + 1<<8
+
+const (
+	// Pseudo-registers that encode options for DMB, DSB, ISB (must be 16-aligned).
+	REG_MB_OSHST = pseudoBase + 2
+	REG_MB_OSH   = pseudoBase + 3
+	REG_MB_NSHST = pseudoBase + 6
+	REG_MB_NSH   = pseudoBase + 7
+	REG_MB_ISHST = pseudoBase + 10
+	REG_MB_ISH   = pseudoBase + 11
+	REG_MB_ST    = pseudoBase + 14
+	REG_MB_SY    = pseudoBase + 15
+
+	// Pseudo-registers that encode firstcond for IT (must be 16-aligned).
+	REG_EQ = pseudoBase + 16 // equal (Z==1)
+	REG_NE = pseudoBase + 17 // not equal (Z == 0)
+	REG_HS = pseudoBase + 18 // unsigned higher or same (C == 1)
+	REG_CS                   // carry set (C == 1)
+	REG_LO = pseudoBase + 19 // unsigned lower (C == 0)
+	REG_CC                   // carry clear (C == 0)
+	REG_MI = pseudoBase + 20 // minus/negative (N == 1)
+	REG_PL = pseudoBase + 21 // plus/positive or zero (N == 0)
+	REG_VS = pseudoBase + 22 // overflow (V==1)
+	REG_VC = pseudoBase + 23 // no overflow (V==0)
+	REG_HI = pseudoBase + 24 // unsigned higher (C==1 && Z==0)
+	REG_LS = pseudoBase + 25 // unsigned lower or same (C==0 || Z==1)
+	REG_GE = pseudoBase + 26 // signed greater than or equal (N == V)
+	REG_LT = pseudoBase + 27 // signed less than (N != V)
+	REG_GT = pseudoBase + 28 // signed greater than (Z==0 && N==V)
+	REG_LE = pseudoBase + 29 // signed less than or equal (Z==1 || N!=V)
+	REG_AL = pseudoBase + 30 // always
+
+	MAXREG
+)
+
+type Aclass uint8
+
+//go:generate stringer -type Aclass
+
+const (
+	C_NONE Aclass = iota
+
+	// Do not fragment or reorder these register/shift classes, or match/aclass/oplook will break.
+	C_RLO      // R0-R7
+	C_SP       // R13
+	C_PC       // R15
+	C_REG      // R0-R15
+	C_SHIFTILO // register shift R>>x (R0-R7)
+	C_SHIFTI   // register shift R>>x
+
+	C_REGREG
+	C_REGREG2 // multiply accumulate dest regs
+
+	// Do not fragment or reorder these registerlist classes, or match/aclass/oplook will break.
+	C_LISTLO   // R0-R7
+	C_LISTLOLR // R0-R7,LR
+	C_LISTLOPC // R0-R7,PC
+	C_LIST
+
+	C_SHIFTR   // register shift R>>R
+	C_SHIFTRLO // register shift R>>R (R0-R7)
+	C_FREG
+	C_FCR
+	C_SPEC // special register
+	C_MB   // memory barier option (pseudo-register)
+	C_IT   // firstcond for IT
+
+	// Do not fragment or reorder these C_*CON classes, or match/aclass/oplook will break.
+
+	C_ZCON
+	C_U1CON2
+	C_U6CON2
+	C_U8CON1_4 // not contain C_U8CON2
+	C_U8CON5_8
+	C_U7CON2 // T16 ADD  u7<<2, R13
+	C_U8CON2 // T16 ADD  u8<<2, R13, Rd
+	C_U3CON  // T16 ADD  u3, Rn, Rd
+	C_U8CON  // T16 ADD  u8, Rdn
+	C_U12CON // T32 ADD  u12, Rn, Rd
+	C_U16CON // T32 MOVW u16, Rd
+	C_E32CON // T32 ADD  e32, Rn, Rd
+	C_LCON
+
+	C_ZFCON
+	C_SFCON // T32 MOVF f8, Fd
+	C_LFCON
+
+	// Do not fragment or reorder these C_*BRA classes, or match/aclass/brlook will break.
+	C_U6BRA  // T16 CBZ   Rn, u6<<1
+	C_S8BRA  // T16 Bcond i8<<1
+	C_S11BRA // T16 B     i11<<1
+	C_S20BRA // T32 Bcond ji20<<1
+	C_S24BRA // T32 B     ji24<<1
+
+	// Do not fragment or reorder these C_*OR* classes, or match/aclass/oplook will break.
+	C_BORLO   // T16 MOVB       u5(Rn), Rt
+	C_HORLO   // T16 MOVH       u5<<1(Rn), Rt
+	C_WORLO   // T16 MOVW       u5<<2(Rn), Rt
+	C_WOSP    // T16 MOVW       u8<<2(R13), Rt
+	C_WOPC    // T16 MOVW       u8<<2(PC), Rt
+	C_UOREG   // T32 MOV{B,H,W} u12(Rn), Rt
+	C_SOREG   // T32 MOV{B,H,W} ±u8(Rn), Rt
+	C_SOPC    // T32 MOVW       ±u12(R15), Rt
+	C_FOREG   // T32 MOV{F,D}   ±u8<<2(Rn), Rt (MOVW ±u8<<2(Rn), (Rta, Rtb))
+	C_ZORLO   // T16 MOVM.IA.W  (Rn), reglist
+	C_ZOSP    // T16 MOVM.IA.W  (R13), reglist
+	C_ZOREG   // T16 JMP        (Rn)
+	C_LOREG   // long offset
+	C_LORLO   // long offset, Rn <= R7
+	C_U0ORLO  // (Rn)        C_BORLO, C_HORLO, C_WORLO, C_UOREG, C_SOREG, C_FOREG, ZORLO, ZOREG
+	C_U3ORLO2 // u3<<2(Rn)   C_BORLO, C_HORLO, C_WORLO, C_UOREG, C_SOREG, C_FOREG
+	C_U4ORLO2 // u4<<2(Rn)   C_HORLO, C_WORLO, C_UOREG, C_SOREG, C_FOREG
+	C_U5ORLO2 // u5<<2(Rn)   C_WORLO, C_UOREG, C_SOREG, C_FOREG
+	C_U4ORLO1 // u4<<1(Rn)   C_BORLO, C_HORLO, C_UOREG, C_SOREG
+	C_U5ORLO1 // u5<<1(Rn)   C_HORLO, C_UOREG, C_SOREG
+	C_U5ORLO  // u5(Rn)      C_BORLO, C_UOREG, C_SOREG
+	C_U0OPC   // (R15)       C_WOPC,  C_SOPC,  C_UOREG, C_SOREG, C_FOREG, ZOREG
+	C_U6OPC2  // u6<<2(R15)  C_WOPC,  C_SOPC,  C_UOREG, C_SOREG, C_FOREG
+	C_U8OPC2  // u8<<2(R15)  C_WOPC,  C_SOPC,  C_UOREG, C_FOREG
+	C_U12OPC  // u12(R15)    C_SOPC,  C_UOREG
+	C_S8OPC   // -u8(R15)    C_SOPC,  C_SOREG
+	C_S8OPC2  // -u8<<2(R15) C_SOPC,  C_FOREG
+	C_S12OPC  // -u12(R15)   C_SOPC
+	C_U0OSP   // (R13)       C_WOSP,  C_UOREG, C_SOREG, C_FOREG, ZOSP, ZOREG
+	C_U6OSP2  // u6<<2(R13)  C_WOSP,  C_UOREG, C_SOREG, C_FOREG
+	C_U8OSP2  // u8<<2(R13)  C_WOSP,  C_UOREG, C_FOREG
+	C_U0OREG  // (Rn)        C_UOREG, C_SOREG, C_FOREG, ZOREG
+	C_U6OREG2 // u6<<2(Rn)   C_UOREG, C_SOREG, C_FOREG
+	C_U8OREG2 // u8<<2(Rn)   C_UOREG, C_FOREG
+	C_U8OREG  // u8(Rn)      C_UOREG, C_SOREG
+	C_U12OREG // u12(Rn)     C_UOREG
+	C_S6OREG2 // -u6<<2(Rn)  C_SOREG, C_FOREG
+	C_S8OREG2 // -u8<<2(Rn)  C_FOREG
+	C_S8OREG  // -u8(Rn)     C_SOREG
+
+	C_ROREG // (Rn)(Rm*x)
+	C_RORLO // (Rn)(Rm)
+
+	C_TEXTSIZE
+
+	C_GOK
+)
+
+//go:generate go run ../stringer.go -i $GOFILE -o anames.go -p thumb
+
+const (
+	AAND = obj.ABaseThumb + obj.A_ARCHSPECIFIC + iota
+	AEOR
+	ASUB
+	ARSB
+	AADD
+	AADC
+	ASBC
+	ATST
+	ATEQ
+	ACMP
+	ACMN
+	AORR
+	ABIC
+
+	AMVN
+	AORN
+
+	AMUL
+	ADIV
+	ADIVU
+
+	AMULL
+	AMULLU
+	AMULAL
+	AMULALU
+	AMULA
+	AMULS
+	AMULAWB
+	AMULAWT
+
+	ACLZ
+	AREV
+	AREV16
+	ARBIT
+	AREVSH
+	ASEL
+	ABFX
+	ABFXU
+	ABFC
+	ABFI
+
+	// Do not reorder or fragment the conditional branch
+	// opcodes, or the predication code will break.
+	ABEQ
+	ABNE
+	ABCS
+	ABHS
+	ABCC
+	ABLO
+	ABMI
+	ABPL
+	ABVS
+	ABVC
+	ABHI
+	ABLS
+	ABGE
+	ABLT
+	ABGT
+	ABLE
+	ACBZ
+	ACBNZ
+
+	ATBB
+	ATBH
+
+	// do not reorder or split AITxyz
+	AITTTT
+	AITTT
+	AITTTE
+	AITT
+	AITTET
+	AITTE
+	AITTEE
+	AIT
+	AITETT
+	AITET
+	AITETE
+	AITE
+	AITEET
+	AITEE
+	AITEEE
+
+	// do not reorder or split NOP-compatible opcodes
+	ANOP2
+	AYIELD
+	AWFE
+	AWFI
+	ASEV
+
+	ACPSID
+	ACPSIE
+
+	// do not reorder or split memory barrier opcodes
+	ADSB
+	ADMB
+	AISB
+
+	AMOVB
+	AMOVBU
+	AMOVH
+	AMOVHU
+	AMOVW
+	AMOVM
+	AMOVT
+
+	ACMPF
+	ACMPD
+
+	ASQRTF
+	ASQRTD
+	AMOVF
+	AMOVD
+
+	ASWI
+	ABKPT
+
+	ALDREX
+	ALDREXB
+	ALDREXH
+	ASTREX
+	ASTREXB
+	ASTREXH
+	ACLREX
+
+	ANOP4
+
+	// do not reorder shift opcodes
+	ASLL
+	ASRL
+	ASRA
+	ASRR
+
+	// not implemented
+	AMMULA
+	AMMULS
+	AMULABB
+
+	AADDF
+	AADDD
+	ASUBF
+	ASUBD
+	AMULF
+	AMULD
+	AMULAF
+	AMULAD
+	AMULSF
+	AMULSD
+	AFMULAF
+	AFMULAD
+	AFNMULAF
+	AFNMULAD
+	AFMULSF
+	AFMULSD
+	AFNMULSF
+	AFNMULSD
+	AABSF
+	AABSD
+	ANEGF
+	ANEGD
+	ANMULF
+	ANMULD
+	ADIVF
+	ADIVD
+	AMOVWF
+	AMOVWD
+	AMOVFW
+	AMOVDW
+	AMOVFD
+	AMOVDF
+
+	AWORD
+	AHWORD
+
+	ALAST
+
+	// aliases
+	AB     = obj.AJMP  // B, BX
+	ABL    = obj.ACALL // BL, BLX
+	AMOVBS = AMOVB
+	AMOVHS = AMOVH
+)
+
+// scond byte
+const (
+	C_SCOND = (1 << 4) - 1
+	C_SBIT  = 1 << 4
+	C_PBIT  = 1 << 5
+	C_WBIT  = 1 << 6
+	C_FBIT  = 1 << 7 /* psr flags-only */
+	C_UBIT  = 1 << 7 /* up bit, unsigned bit */
+	C_IA    = C_UBIT
+	C_DB    = C_PBIT
+
+	// These constants are the ARM condition codes encodings,
+	// XORed with 14 so that C_SCOND_NONE has value 0,
+	// so that a zeroed Prog.scond means "always execute".
+	C_SCOND_XOR = 14
+
+	C_SCOND_EQ   = 0 ^ C_SCOND_XOR
+	C_SCOND_NE   = 1 ^ C_SCOND_XOR
+	C_SCOND_HS   = 2 ^ C_SCOND_XOR // CS
+	C_SCOND_LO   = 3 ^ C_SCOND_XOR // CC
+	C_SCOND_MI   = 4 ^ C_SCOND_XOR
+	C_SCOND_PL   = 5 ^ C_SCOND_XOR
+	C_SCOND_VS   = 6 ^ C_SCOND_XOR
+	C_SCOND_VC   = 7 ^ C_SCOND_XOR
+	C_SCOND_HI   = 8 ^ C_SCOND_XOR
+	C_SCOND_LS   = 9 ^ C_SCOND_XOR
+	C_SCOND_GE   = 10 ^ C_SCOND_XOR
+	C_SCOND_LT   = 11 ^ C_SCOND_XOR
+	C_SCOND_GT   = 12 ^ C_SCOND_XOR
+	C_SCOND_LE   = 13 ^ C_SCOND_XOR
+	C_SCOND_NONE = 14 ^ C_SCOND_XOR
+	C_SCOND_NV   = 15 ^ C_SCOND_XOR
+
+	/* D_SHIFT type */
+	SHIFT_LL = 0 << 5
+	SHIFT_LR = 1 << 5
+	SHIFT_AR = 2 << 5
+	SHIFT_RR = 3 << 5
+)
+
+// http://infocenter.arm.com/help/topic/com.arm.doc.ihi0040b/IHI0040B_aadwarf.pdf
+var ARMDWARFRegisters = map[int16]int16{}
+
+func init() {
+	// f assigns dwarfregisters[from:to] = (base):(step*(to-from)+base)
+	f := func(from, to, base, step int16) {
+		for r := int16(from); r <= to; r++ {
+			ARMDWARFRegisters[r] = step*(r-from) + base
+		}
+	}
+	f(REG_R0, REG_R15, 0, 1)
+	f(REG_F0, REG_F15, 64, 2) // Use d0 through D15, aka S0, S2, ..., S30
+}
diff --git a/src/cmd/internal/obj/thumb/aclass_string.go b/src/cmd/internal/obj/thumb/aclass_string.go
new file mode 100644
index 0000000000..a77381a32f
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/aclass_string.go
@@ -0,0 +1,16 @@
+// Code generated by "stringer -type Aclass"; DO NOT EDIT.
+
+package thumb
+
+import "strconv"
+
+const _Aclass_name = "C_NONEC_RLOC_SPC_PCC_REGC_SHIFTILOC_SHIFTIC_REGREGC_REGREG2C_LISTLOC_LISTLOLRC_LISTLOPCC_LISTC_SHIFTRC_SHIFTRLOC_FREGC_FCRC_SPECC_MBC_ITC_ZCONC_U1CON2C_U6CON2C_U8CON1_4C_U8CON5_8C_U7CON2C_U8CON2C_U3CONC_U8CONC_U12CONC_U16CONC_E32CONC_LCONC_ZFCONC_SFCONC_LFCONC_U6BRAC_S8BRAC_S11BRAC_S20BRAC_S24BRAC_BORLOC_HORLOC_WORLOC_WOSPC_WOPCC_UOREGC_SOREGC_SOPCC_FOREGC_ZORLOC_ZOSPC_ZOREGC_LOREGC_LORLOC_U0ORLOC_U3ORLO2C_U4ORLO2C_U5ORLO2C_U4ORLO1C_U5ORLO1C_U5ORLOC_U0OPCC_U6OPC2C_U8OPC2C_U12OPCC_S8OPCC_S8OPC2C_S12OPCC_U0OSPC_U6OSP2C_U8OSP2C_U0OREGC_U6OREG2C_U8OREG2C_U8OREGC_U12OREGC_S6OREG2C_S8OREG2C_S8OREGC_ROREGC_RORLOC_TEXTSIZEC_GOK"
+
+var _Aclass_index = [...]uint16{0, 6, 11, 15, 19, 24, 34, 42, 50, 59, 67, 77, 87, 93, 101, 111, 117, 122, 128, 132, 136, 142, 150, 158, 168, 178, 186, 194, 201, 208, 216, 224, 232, 238, 245, 252, 259, 266, 273, 281, 289, 297, 304, 311, 318, 324, 330, 337, 344, 350, 357, 364, 370, 377, 384, 391, 399, 408, 417, 426, 435, 444, 452, 459, 467, 475, 483, 490, 498, 506, 513, 521, 529, 537, 546, 555, 563, 572, 581, 590, 598, 605, 612, 622, 627}
+
+func (i Aclass) String() string {
+	if i >= Aclass(len(_Aclass_index)-1) {
+		return "Aclass(" + strconv.FormatInt(int64(i), 10) + ")"
+	}
+	return _Aclass_name[_Aclass_index[i]:_Aclass_index[i+1]]
+}
diff --git a/src/cmd/internal/obj/thumb/anames.go b/src/cmd/internal/obj/thumb/anames.go
new file mode 100644
index 0000000000..6e0dd5bcfc
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/anames.go
@@ -0,0 +1,154 @@
+// Code generated by stringer -i a.out.go -o anames.go -p thumb; DO NOT EDIT.
+
+package thumb
+
+import "cmd/internal/obj"
+
+var Anames = []string{
+	obj.A_ARCHSPECIFIC: "AND",
+	"EOR",
+	"SUB",
+	"RSB",
+	"ADD",
+	"ADC",
+	"SBC",
+	"TST",
+	"TEQ",
+	"CMP",
+	"CMN",
+	"ORR",
+	"BIC",
+	"MVN",
+	"ORN",
+	"MUL",
+	"DIV",
+	"DIVU",
+	"MULL",
+	"MULLU",
+	"MULAL",
+	"MULALU",
+	"MULA",
+	"MULS",
+	"MULAWB",
+	"MULAWT",
+	"CLZ",
+	"REV",
+	"REV16",
+	"RBIT",
+	"REVSH",
+	"SEL",
+	"BFX",
+	"BFXU",
+	"BFC",
+	"BFI",
+	"BEQ",
+	"BNE",
+	"BCS",
+	"BHS",
+	"BCC",
+	"BLO",
+	"BMI",
+	"BPL",
+	"BVS",
+	"BVC",
+	"BHI",
+	"BLS",
+	"BGE",
+	"BLT",
+	"BGT",
+	"BLE",
+	"CBZ",
+	"CBNZ",
+	"TBB",
+	"TBH",
+	"ITTTT",
+	"ITTT",
+	"ITTTE",
+	"ITT",
+	"ITTET",
+	"ITTE",
+	"ITTEE",
+	"IT",
+	"ITETT",
+	"ITET",
+	"ITETE",
+	"ITE",
+	"ITEET",
+	"ITEE",
+	"ITEEE",
+	"NOP2",
+	"YIELD",
+	"WFE",
+	"WFI",
+	"SEV",
+	"CPSID",
+	"CPSIE",
+	"DSB",
+	"DMB",
+	"ISB",
+	"MOVB",
+	"MOVBU",
+	"MOVH",
+	"MOVHU",
+	"MOVW",
+	"MOVM",
+	"MOVT",
+	"CMPF",
+	"CMPD",
+	"SQRTF",
+	"SQRTD",
+	"MOVF",
+	"MOVD",
+	"SWI",
+	"BKPT",
+	"LDREX",
+	"LDREXB",
+	"LDREXH",
+	"STREX",
+	"STREXB",
+	"STREXH",
+	"CLREX",
+	"NOP4",
+	"SLL",
+	"SRL",
+	"SRA",
+	"SRR",
+	"MMULA",
+	"MMULS",
+	"MULABB",
+	"ADDF",
+	"ADDD",
+	"SUBF",
+	"SUBD",
+	"MULF",
+	"MULD",
+	"MULAF",
+	"MULAD",
+	"MULSF",
+	"MULSD",
+	"FMULAF",
+	"FMULAD",
+	"FNMULAF",
+	"FNMULAD",
+	"FMULSF",
+	"FMULSD",
+	"FNMULSF",
+	"FNMULSD",
+	"ABSF",
+	"ABSD",
+	"NEGF",
+	"NEGD",
+	"NMULF",
+	"NMULD",
+	"DIVF",
+	"DIVD",
+	"MOVWF",
+	"MOVWD",
+	"MOVFW",
+	"MOVDW",
+	"MOVFD",
+	"MOVDF",
+	"WORD",
+	"HWORD",
+	"LAST",
+}
diff --git a/src/cmd/internal/obj/thumb/asm.go b/src/cmd/internal/obj/thumb/asm.go
new file mode 100644
index 0000000000..50a1ff6d98
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/asm.go
@@ -0,0 +1,1182 @@
+// Inferno utils/5l/span.c
+// https://bitbucket.org/inferno-os/inferno-os/src/master/utils/5l/span.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"fmt"
+	"math"
+
+	"cmd/internal/obj"
+	"cmd/internal/objabi"
+)
+
+type Ctx struct {
+	ctxt     *obj.Link
+	newprog  obj.ProgAlloc
+	cursym   *obj.LSym
+	blitrl   *obj.Prog
+	elitrl   *obj.Prog
+	it       *obj.Prog
+	beforeit *obj.Prog
+	autosize int // frame size: auto + ret
+	pool     struct {
+		start int
+		size  int
+	}
+}
+
+func regclass(r int16) Aclass {
+	if REG_R0 <= r && r <= REG_R7 {
+		return C_RLO
+	}
+	if REG_R0 <= r && r <= REG_R15 {
+		switch r {
+		case REGSP:
+			return C_SP
+		case REGPC:
+			return C_PC
+		}
+		return C_REG
+	}
+	if REG_F0 <= r && r <= REG_F15 {
+		return C_FREG
+	}
+	if REG_APSR <= r && r <= REG_CONTROL {
+		return C_SPEC
+	}
+	if r == REG_FPSCR {
+		return C_FCR
+	}
+	if REG_MB_OSHST <= r && r <= REG_MB_SY {
+		return C_MB
+	}
+	if REG_EQ <= r && r <= REG_AL {
+		return C_IT
+	}
+	return C_GOK
+}
+
+func rol(x uint32, k int) uint32 {
+	m := uint(k) & 31
+	return x<<m | x>>(32-m)
+}
+
+func mic(u uint32) bool {
+	if u>>8 == 0 {
+		return true // 00000000_00000000_00000000_abcdefgh
+	}
+	for n := 31; n >= 8; n-- {
+		if rol(u, n)&^0xFF == 0 {
+			return true // rotated 00000000_00000000_00000000_1bcdefgh
+		}
+	}
+	if hi, lo := u>>16, u&0xFFFF; hi == lo {
+		hi = lo >> 8
+		lo &= 0xFF
+		switch {
+		case hi == 0:
+			return true // 00000000_abcdefgh_00000000_abcdefgh
+		case lo == 0:
+			return true // abcdefgh_00000000_abcdefgh_00000000
+		case hi == lo:
+			return true // abcdefgh_abcdefgh_abcdefgh_abcdefgh
+		}
+	}
+	return false
+}
+
+func conclass(u uint32) Aclass {
+	if u&3 == 0 {
+		switch {
+		case u == 0:
+			return C_ZCON
+		case u < 1<<3:
+			return C_U1CON2
+		case u < 1<<8:
+			return C_U6CON2
+		case u < 1<<9:
+			return C_U7CON2
+		case u < 1<<10:
+			return C_U8CON2
+		}
+	} else {
+		switch {
+		case u < 1<<3:
+			return C_U3CON
+		case u < 1<<8:
+			return C_U8CON
+		}
+	}
+	for n := uint(1); n <= 8; n++ {
+		if u&(1<<n-1) == 0 && u>>n <= 0xFF {
+			if n <= 4 {
+				return C_U8CON1_4
+			}
+			return C_U8CON5_8
+		}
+	}
+	switch {
+	case u < 1<<12:
+		return C_U12CON
+	case u < 1<<16:
+		return C_U16CON
+	case mic(u):
+		return C_E32CON
+	}
+	return C_LCON
+}
+
+func (c *Ctx) offset(a *obj.Addr) int64 {
+	switch a.Name {
+	case obj.NAME_PARAM:
+		return a.Offset + int64(c.autosize+prasize)
+	case obj.NAME_AUTO:
+		return a.Offset + int64(c.autosize)
+	}
+	return a.Offset
+}
+
+func (c *Ctx) chipzero(e float64) int {
+	switch objabi.GOARM {
+	case 0x7F, 0x7D: // ARMv7-M with floating point extension
+		break
+	default:
+		return -1
+	}
+	if math.Float64bits(e) == 0 {
+		return 0
+	}
+	return -1
+}
+
+func (c *Ctx) chipfloat(e float64) int {
+	switch objabi.GOARM {
+	case 0x7F, 0x7D: // ARMv7-M with floating point extension
+		break
+	default:
+		return -1
+	}
+	ei := math.Float64bits(e)
+	l := uint32(ei)
+	h := uint32(ei >> 32)
+
+	if l != 0 || h&0xffff != 0 {
+		return -1
+	}
+	h1 := h & 0x7fc00000
+	if h1 != 0x40000000 && h1 != 0x3fc00000 {
+		return -1
+	}
+	n := 0
+
+	// sign bit (a)
+	if h&0x80000000 != 0 {
+		n |= 1 << 7
+	}
+
+	// exp sign bit (b)
+	if h1 == 0x3fc00000 {
+		n |= 1 << 6
+	}
+
+	// rest of exp and mantissa (cd-efgh)
+	n |= int((h >> 16) & 0x3f)
+
+	//print("match %.8lux %.8lux %d\n", l, h, n);
+	return n
+}
+
+func (c *Ctx) aclass(a *obj.Addr, as *obj.As) Aclass {
+	switch a.Type {
+	case obj.TYPE_NONE:
+		return C_NONE
+
+	case obj.TYPE_REG:
+		return regclass(a.Reg)
+
+	case obj.TYPE_REGREG:
+		return C_REGREG
+
+	case obj.TYPE_REGREG2:
+		return C_REGREG2
+
+	case obj.TYPE_REGLIST:
+		if a.Offset&^0xC0FF == 0 {
+			switch a.Offset >> 12 {
+			case 0:
+				return C_LISTLO
+			case 4:
+				return C_LISTLOLR
+			case 8:
+				return C_LISTLOPC
+			}
+		}
+		return C_LIST
+
+	case obj.TYPE_SHIFT:
+		if a.Reg == 0 {
+			rlo := a.Offset&15 <= 7
+			if a.Offset>>4&1 == 0 {
+				// R>>i
+				if rlo && a.Offset>>5&3 != 3 {
+					// rlo and not @>
+					return C_SHIFTILO
+				}
+				return C_SHIFTI
+			} else {
+				// R>>R
+				if rlo && a.Offset>>8&15 <= 7 {
+					return C_SHIFTRLO
+				}
+				return C_SHIFTR
+			}
+		}
+
+	case obj.TYPE_MEM:
+		switch a.Name {
+		case obj.NAME_STATIC, obj.NAME_EXTERN:
+			if a.Sym == nil || a.Sym.Name == "" {
+				return C_GOK
+			}
+			return C_LORLO // MOVW litoffset(PC), R7 and use (R7) as address
+
+		case obj.NAME_NONE, obj.NAME_AUTO, obj.NAME_PARAM:
+			if a.Index != 0 {
+				if a.Reg <= REG_R7 && a.Index <= REG_R7 && uint(a.Scale) <= 1 {
+					return C_RORLO
+				}
+				return C_ROREG
+			}
+			if a.Name == obj.NAME_AUTO || a.Name == obj.NAME_PARAM {
+				a.Reg = REGSP
+			}
+			offset := c.offset(a)
+			switch {
+			case offset > 0:
+				switch {
+				case a.Reg <= REG_R7:
+					switch {
+					case offset&3 != 0:
+						break
+					case offset < 1<<5:
+						return C_U3ORLO2
+					case offset < 1<<6:
+						return C_U4ORLO2
+					case offset < 1<<7:
+						return C_U5ORLO2
+					}
+					switch {
+					case offset&1 != 0:
+						break
+					case offset < 1<<5:
+						return C_U4ORLO1
+					case offset < 1<<6:
+						return C_U5ORLO1
+					}
+					if offset < 1<<5 {
+						return C_U5ORLO
+					}
+				case a.Reg == REGSP:
+					switch {
+					case offset&3 != 0:
+						break
+					case offset < 1<<8:
+						return C_U6OSP2
+					case offset < 1<<10:
+						return C_U8OSP2
+					}
+				case a.Reg == REGPC:
+					switch {
+					case offset&3 != 0:
+						break
+					case offset < 1<<8:
+						return C_U6OPC2
+					case offset < 1<<10:
+						return C_U8OPC2
+					}
+					if offset < 1<<12 {
+						return C_U12OPC
+					}
+					return C_LOREG // there is no wider class than U12
+				}
+				switch {
+				case offset&3 != 0:
+					break
+				case offset < 1<<8:
+					return C_U6OREG2
+				case offset < 1<<10:
+					return C_U8OREG2
+				}
+				switch {
+				case offset < 1<<8:
+					return C_U8OREG
+				case offset < 1<<12:
+					return C_U12OREG
+				}
+			case offset == 0:
+				switch {
+				case a.Reg <= REG_R7:
+					return C_U0ORLO
+				case a.Reg == REG_R13:
+					return C_U0OSP
+				case a.Reg == REG_R15:
+					return C_U0OPC
+				}
+				return C_U0OREG
+			default: // offset < 0
+				offset = -offset
+				if a.Reg == REGPC {
+					switch {
+					case offset&3 != 0:
+						break
+					case offset < 1<<10:
+						return C_S8OPC2
+					}
+					switch {
+					case offset < 1<<8:
+						return C_S8OPC
+					case offset < 1<<12:
+						return C_S12OPC
+					}
+					return C_LOREG // there is no wider class than S12
+				}
+				switch {
+				case offset&3 != 0:
+					break
+				case offset < 1<<8:
+					return C_S6OREG2
+				case offset < 1<<10:
+					return C_S8OREG2
+				}
+				if offset < 1<<8 {
+					return C_S8OREG
+				}
+			}
+			if a.Reg <= REG_R7 {
+				return C_LORLO
+			}
+			return C_LOREG
+		}
+
+	case obj.TYPE_FCONST:
+		if c.chipzero(a.Val.(float64)) >= 0 {
+			return C_ZFCON
+		}
+		if c.chipfloat(a.Val.(float64)) >= 0 {
+			return C_SFCON
+		}
+		return C_LFCON
+
+	case obj.TYPE_TEXTSIZE:
+		return C_TEXTSIZE
+
+	case obj.TYPE_CONST:
+		if a.Offset > 4294967295 || a.Offset < -2147483648 {
+			return C_GOK // doesn't fit in uint32 or int32
+		}
+		con := uint32(a.Offset)
+		cc := conclass(con)
+		if as != nil && cc >= C_E32CON {
+			switch *as {
+			case AADD, ASUB, ACMP, ACMN:
+				con = -con
+				nc := conclass(con)
+				if nc >= cc {
+					break
+				}
+				cc = nc
+				a.Offset = int64(con)
+				switch *as {
+				case AADD:
+					*as = ASUB
+				case ASUB:
+					*as = AADD
+				case ACMP:
+					*as = ACMN
+				default: // ACMN
+					*as = ACMP
+				}
+			case AAND, ABIC, AORR, AORN, AMOVW, AMVN:
+				con = ^con
+				nc := conclass(con)
+				if nc >= cc {
+					break
+				}
+				cc = nc
+				a.Offset = int64(con)
+				switch *as {
+				case AAND:
+					*as = ABIC
+				case ABIC:
+					*as = AAND
+				case AORR:
+					*as = AORN
+				case AORN:
+					*as = AORR
+				case AMOVW:
+					*as = AMVN
+				default: // AMVN
+					*as = AMOVW
+				}
+			}
+		}
+		return cc
+
+	case obj.TYPE_ADDR:
+		switch a.Name {
+		case obj.NAME_STATIC, obj.NAME_EXTERN:
+			if a.Sym == nil || a.Sym.Name == "" {
+				return C_GOK
+			}
+			return C_LCON
+		}
+		// MOV with NAME_NONE, NAME_PARAM, NAME_AUTO is converted to ADD with TYPE_CONST
+
+	case obj.TYPE_BRANCH:
+		// can't determine branch length here so assume some tentative value
+		switch *as {
+		case AB, ABL:
+			return C_S24BRA
+		case ACBZ, ACBNZ:
+			return C_U6BRA
+		default: // Bcond
+			return C_S20BRA
+		}
+	}
+
+	return C_GOK
+}
+
+func itstate(p *obj.Prog) uint16 {
+	mask := int(p.As-AITTTT) + 1
+	// IT      mask = 0b1000
+	// IT T    mask = 0b0100
+	// IT E    mask = 0b1100
+	// IT TT   mask = 0b0010
+	// IT ET   mask = 0b1010
+	// IT TE   mask = 0b0110
+	// IT EE   mask = 0b1110
+	// IT TTT  mask = 0b0001
+	// IT ETT  mask = 0b1001
+	// IT TET  mask = 0b0101
+	// IT EET  mask = 0b1101
+	// IT TTE  mask = 0b0011
+	// IT ETE  mask = 0b1011
+	// IT TEE  mask = 0b0111
+	// IT EEE  mask = 0b1111
+	if fc0 := int(p.Scond) & 1; fc0 != 0 {
+		n := uint(0)
+		for mask>>n&1 == 0 {
+			n++ // count trailing zeros
+		}
+		for n++; n < 4; n++ {
+			mask ^= fc0 << n
+		}
+	}
+	return uint16(mask)
+}
+
+func (c *Ctx) itclose() {
+	c.it.As = AITTTT + obj.As(c.it.Mark) - 1
+	c.it.Mark = uint16(c.it.Scond<<4) | itstate(c.it)
+	c.it = nil
+}
+
+func (c *Ctx) oplook(p *obj.Prog) (ret *Optab) {
+	if k := int(p.Optab) - 1; k >= 0 {
+		return &oprange[p.As&obj.AMask][k]
+	}
+	a1 := Aclass(p.From.Class)
+	if a1 == 0 {
+		a1 = c.aclass(&p.From, &p.As) + 1
+		p.From.Class = int8(a1)
+	}
+	a1--
+	a3 := Aclass(p.To.Class)
+	if a3 == 0 {
+		a3 = c.aclass(&p.To, &p.As)
+		if a1 == C_SHIFTRLO && p.As == AMOVW && int(p.From.Offset)&7 == int(p.To.Reg)&7 {
+			switch a3 {
+			case C_RLO:
+				a3 = C_NONE // special case to generate 16bit `MOVW Rdn<v>Rm, Rdn`
+			case C_NONE:
+				a3 = C_GOK
+			}
+		}
+		a3++
+		p.To.Class = int8(a3)
+	}
+	a3--
+	a2 := C_NONE
+	if p.Reg != 0 {
+		a2 = regclass(p.Reg)
+	}
+
+	//fmt.Printf("\t%-12v %-12v %-12v %04b\n", a1, a2, a3, p.Scond>>4)
+
+	autoit := c.cursym.Func().Text.Mark&AUTOIT != 0
+	cond := p.Scond & 0xF
+again:
+	if autoit {
+		if c.it != nil {
+			if (cond^C_SCOND_XOR)>>1 != c.it.Scond>>1 {
+				// no condition or condition does not match the current it block
+				c.itclose()
+			}
+		}
+		if c.it == nil {
+			if cond != C_SCOND_NONE {
+				if p.As == AB && p.To.Type == obj.TYPE_BRANCH && p.To.Name == obj.NAME_NONE {
+					// convert `B.cond label` to `Bcond label`
+					switch cond {
+					case C_SCOND_EQ:
+						p.As = ABEQ
+					case C_SCOND_NE:
+						p.As = ABNE
+					case C_SCOND_HS:
+						p.As = ABHS
+					case C_SCOND_LO:
+						p.As = ABLO
+					case C_SCOND_MI:
+						p.As = ABMI
+					case C_SCOND_PL:
+						p.As = ABPL
+					case C_SCOND_VS:
+						p.As = ABVS
+					case C_SCOND_VC:
+						p.As = ABVC
+					case C_SCOND_HI:
+						p.As = ABHI
+					case C_SCOND_LS:
+						p.As = ABLS
+					case C_SCOND_GE:
+						p.As = ABGE
+					case C_SCOND_LT:
+						p.As = ABLT
+					case C_SCOND_GT:
+						p.As = ABGT
+					case C_SCOND_LE:
+						p.As = ABLE
+					default:
+						c.ctxt.Diag("invalid flags: %v", p)
+					}
+					p.Scond &^= 0xF
+					a3 = C_S20BRA
+					p.To.Class = int8(C_S20BRA + 1)
+				} else {
+					// insert IT by replacing p to avoid jumping in middle of IT block
+					// BUG: this only works for one instruction IT blocks (jump target check is need)
+					q := c.newprog()
+					*q = *p
+					p.Link = q
+					p.As = AIT
+					p.Spadj = 0
+					p.Scond = cond ^ C_SCOND_XOR
+					p.Mark = 0x10 // invalid (empty IT block)
+					p.Reg = 4     // number of free slots in IT block
+					p.Optab = 1
+					c.it = p
+					return &oprange[AIT&obj.AMask][0]
+				}
+			}
+		}
+	} else {
+		if c.it != nil {
+			switch {
+			case AITTTT <= p.As && p.As <= AITEEE:
+				c.ctxt.Diag("IT in previous IT block: %v", p)
+			case ABEQ <= p.As && p.As <= ACBNZ:
+				c.ctxt.Diag("not allowed in IT block: %v", p)
+			case c.it.RegTo2<<1&0xF != 0 && (p.As == AB || p.As == ABL):
+				// BUG: take into account other cases when PC is modified
+				c.ctxt.Diag("only allowed as the last instruction in IT block: %v", p)
+			case int16(cond^C_SCOND_XOR) != c.it.RegTo2>>4:
+				c.ctxt.Diag("condition code does not match IT code: %v", p)
+			}
+		} else if cond != 0 {
+			c.ctxt.Diag("condition code outisde IT block: %v", p)
+		}
+	}
+	spuw := p.Scond &^ 0xF
+	var spxor uint8
+	if spuw&(C_SBIT|C_PBIT) == 0 {
+		spxor = C_SBIT
+	} else if c.it != nil {
+		spxor = C_SBIT | C_PBIT
+	}
+	ops := oprange[p.As&obj.AMask]
+	for k := range ops {
+		op := &ops[k]
+		if op.flag&NOIT != 0 && c.it != nil {
+			continue
+		}
+		rscond := op.rscond
+		oscond := op.oscond
+		if rscond&C_SBIT != 0 {
+			// 16-bit data processing instruction that sets flags outside IT block
+			rscond ^= spxor
+			oscond ^= spxor
+		}
+		if spuw&rscond == rscond && spuw&oscond == spuw &&
+			(op.a2 == a2 || op.a2 == C_REG && (C_RLO <= a2 && a2 < C_REG)) &&
+			match(op.a1, a1) && match(op.a3, a3) {
+
+			p.Optab = uint16(k + 1)
+			/*fmt.Printf(
+				"\t%-12v %-12v %-12v %04b %04b\n",
+				op.a1, op.a2, op.a3, rscond>>4, oscond>>4,
+			)*/
+			ret = op
+			break
+		}
+	}
+	if ret == nil {
+		c.ctxt.Diag(
+			"illegal combination %v; %v %v %v; from %v %v; to %v %v",
+			p, a1, a2, a3, p.From.Type, p.From.Name, p.To.Type, p.To.Name,
+		)
+		return &Optab{as: obj.AUNDEF}
+	}
+	if autoit {
+		if c.it != nil {
+			n := int(ret.size) >> 4
+			if n > int(c.it.Reg) {
+				// can not fit n instructions in current it block
+				c.itclose()
+				p.Optab = 0
+				ret = nil
+				goto again
+			}
+			cond ^= C_SCOND_XOR
+			for ; n > 0; n-- {
+				c.it.Reg--
+				ito := uint16(1 << uint(c.it.Reg))
+				if cond&1 == c.it.Scond&1 {
+					c.it.Mark -= ito
+				} else {
+					c.it.Mark += ito
+				}
+			}
+			if c.it.Reg == 0 || p.As == AB || p.As == ABL {
+				c.itclose()
+			}
+		}
+	} else {
+		if AITTTT <= p.As && p.As <= AITEEE {
+			p.Scond = uint8(p.From.Reg - REG_EQ)
+			p.Mark = itstate(p)
+			p.RegTo2 = int16(int(p.Scond)<<4 | int(p.Mark))
+			c.it = p
+		} else if c.it != nil {
+			c.it.RegTo2 = c.it.RegTo2&^0x1F | c.it.RegTo2<<(ret.size>>4)&0x1F
+			if c.it.RegTo2&0xF == 0 {
+				c.it = nil
+			}
+		}
+	}
+	return ret
+}
+
+func (c *Ctx) brlook(p *obj.Prog) *Optab {
+	v := (p.To.Target().Pc - p.Pc - 4) >> 1
+	var a3 Aclass
+	switch {
+	case 0 <= v && v < 1<<6:
+		a3 = C_U6BRA
+	case -1<<7 <= v && v < 1<<7:
+		a3 = C_S8BRA
+	case -1<<10 <= v && v < 1<<10:
+		a3 = C_S11BRA
+	case -1<<19 <= v && v < 1<<19:
+		a3 = C_S20BRA
+	case -1<<23 <= v && v < 1<<23:
+		a3 = C_S24BRA
+	default:
+		c.ctxt.Diag("branch too long: %d %v", v*2, p)
+		return &Optab{as: obj.AUNDEF}
+	}
+	p.To.Class = int8(a3) + 1
+	ops := oprange[p.As&obj.AMask]
+	for k := range ops {
+		op := &ops[k]
+		if a3 <= op.a3 && op.a3 <= C_S24BRA {
+			p.Optab = uint16(k + 1)
+			return op
+		}
+	}
+	c.ctxt.Diag("illegal branch combination %v; %v; to %v %v", p, a3, p.To.Type, p.To.Name)
+	return &Optab{as: obj.AUNDEF}
+}
+
+func debug(p *obj.Prog) {
+	fmt.Printf("\n%v\n", p)
+	if p.Reg != 0 {
+		fmt.Printf(
+			"\t%02d %-5v %v(%v), R%d, %v(%v)\n\n",
+			p.Pc, p.As, p.From.Type, p.From.Name, p.Reg-obj.RBaseThumb, p.To.Type, p.To.Name,
+		)
+	} else {
+		fmt.Printf(
+			"\t%02d %-5v %v(%v), %v(%v)\n\n",
+			p.Pc, p.As, p.From.Type, p.From.Name, p.To.Type, p.To.Name,
+		)
+	}
+}
+
+func (c *Ctx) litoffset(p *obj.Prog, reglo bool, pcdiff int) (offset int, short bool) {
+	v := int(p.Pool.Pc - p.Pc&^3 - 4)
+	if v > 0 {
+		v += pcdiff // forward literal
+	}
+	if v <= -1<<12 || 1<<12 <= v {
+		c.ctxt.Diag("|literal offset| >= 1<<12: %d %v", v, p)
+		return 0, false
+	}
+	return v, reglo && uint(v) < 1<<10
+}
+
+func span(ctxt *obj.Link, cursym *obj.LSym, newprog obj.ProgAlloc) {
+	if ctxt.Retpoline {
+		ctxt.Diag("-spectre=ret not supported on thumb")
+		ctxt.Retpoline = false // don't keep printing
+	}
+	p := cursym.Func().Text
+	if p == nil || p.Link == nil {
+		return // external functions or ELF section symbol
+	}
+	if oprange[AAND&obj.AMask] == nil {
+		ctxt.Diag("thumb ops not initialized, call thumb.buildop first")
+	}
+	c := &Ctx{ctxt: ctxt, newprog: newprog, cursym: cursym, autosize: int(p.To.Offset) + rasize}
+	p.Pc = 0
+	pc := 0
+	dbg := false
+	if false && p.From.Sym.Name == "\"\".main" {
+		dbg = true
+		fmt.Println("-->", p)
+	}
+	for p.Link != nil {
+		prev := p
+		p = p.Link
+		if p.As == AWORD && pc&3 != 0 {
+			pc += 2
+		}
+		p.Pc = int64(pc)
+		if dbg {
+			debug(p)
+		}
+		o := c.oplook(p)
+		if o.as == AIT {
+			c.beforeit = prev
+		}
+		switch o.flag & (LFROM | LTO) {
+		case LFROM:
+			c.addpool(p, &p.From, pc)
+		case LTO:
+			c.addpool(p, &p.To, pc)
+		}
+		pc += int(o.size) & 0xF
+		if c.blitrl != nil {
+			if p.Link == nil {
+				p, pc = c.flushpool(p, pc, false)
+			} else {
+				p, pc = c.checkpool(p, pc)
+				if dbg && p != prev.Link {
+					if c.it == nil {
+						fmt.Printf("\tpool flushed\n")
+					} else {
+						fmt.Printf("\tpool flushed before last IT - restarting\n")
+					}
+				}
+			}
+		}
+	}
+
+	// determine exect instruction sizes
+	// TODO: jump across functions needs reloc and always uses 32-bit instruction
+
+	for {
+		p = c.cursym.Func().Text
+		pc = 0
+		changed := false
+		for p.Link != nil {
+			p = p.Link
+			if p.As == AWORD && pc&3 != 0 {
+				pc += 2
+			}
+			pcdiff := int(int64(pc) - p.Pc)
+			if pcdiff != 0 {
+				changed = true
+				p.Pc = int64(pc)
+			}
+			var o *Optab
+			var m int
+			if p.To.Type == obj.TYPE_BRANCH && p.To.Target() != nil && p.To.Sym == nil {
+				o = c.brlook(p)
+				m = int(o.size) & 0xF
+			} else {
+				o = c.oplook(p)
+				m = int(o.size) & 0xF
+				if o.flag&(LFROM|LTO) != 0 {
+					// in most cases the literal is loaded into R7 (REGTMP)
+					loreg := true
+					// but in the following cases the literal is loaded straight into destination register
+					if p.As == AMOVW || p.As == AMVN {
+						if p.From.Type == obj.TYPE_CONST || p.From.Type == obj.TYPE_ADDR &&
+							(p.From.Name == obj.NAME_STATIC || p.From.Name == obj.NAME_EXTERN) {
+							loreg = p.To.Reg <= REG_R7
+						}
+					}
+					if _, short := c.litoffset(p, loreg, pcdiff); short {
+						// optab assumees 32-bit load literal instruction;
+						// here we are correcting this assumption
+						m -= 2
+						//fmt.Println(-2, loreg)
+					}
+					if p.As == AMOVF || p.As == AMOVD {
+						name := p.From.Name
+						if o.flag&LTO != 0 {
+							name = p.To.Name
+						}
+						if name == obj.NAME_STATIC || name == obj.NAME_EXTERN {
+							// optab assumes the offset(regBase) requires
+							// 16-bit ADD regBase,REGTMP instruction but
+							// in this case the offset is absolute addres so
+							// there is no ADD instruction
+							m -= 2
+						}
+					}
+				}
+			}
+			//fmt.Println(m, p)
+			pc += m
+		}
+		if !changed {
+			break
+		}
+	}
+	c.cursym.Size = int64(pc)
+
+	// lay out the code
+
+	c.cursym.Grow(c.cursym.Size)
+	bp := c.cursym.P
+	var out [8]uint16
+	p = c.cursym.Func().Text
+	pc = int(p.Pc)
+	for p.Link != nil {
+		p = p.Link
+		var o *Optab
+		if p.To.Type == obj.TYPE_BRANCH && p.To.Target() != nil {
+			o = c.brlook(p)
+		} else {
+			o = c.oplook(p)
+		}
+		if o.asmout == nil {
+			continue
+		}
+		m := o.asmout(c, p, out[:])
+		if m == 0 {
+			return
+		}
+		if int64(pc) > p.Pc {
+			ctxt.Diag("PC padding invalid: want %#d, has %#d: %v", p.Pc, pc, p)
+			return
+		}
+		for int64(pc) != p.Pc {
+			// emit NOP
+			bp[1] = 0xBF
+			bp[0] = 0x00
+			bp = bp[2:]
+			pc += 2
+		}
+		for _, v := range out[:m/2] {
+			bp[0] = byte(v)
+			bp[1] = byte(v >> 8)
+			bp = bp[2:]
+		}
+		pc += m
+		// unset base register in case of SP/FP for better printing
+		var a *obj.Addr
+		if p.From.Type == obj.TYPE_MEM || p.From.Type == obj.TYPE_ADDR {
+			a = &p.From
+		} else if p.To.Type == obj.TYPE_MEM || p.To.Type == obj.TYPE_ADDR {
+			a = &p.To
+		} else {
+			continue
+		}
+		if a.Reg == REGSP && (a.Name == obj.NAME_AUTO || a.Name == obj.NAME_PARAM) {
+			a.Reg = obj.REG_NONE
+		}
+	}
+}
+
+func (c *Ctx) addpool(p *obj.Prog, a *obj.Addr, pc int) {
+	t := c.newprog()
+	t.As = AWORD
+	switch a.Name {
+	case obj.NAME_EXTERN, obj.NAME_STATIC:
+		t.To.Type = a.Type
+		t.To.Name = a.Name
+		t.To.Sym = a.Sym
+		t.To.Offset = a.Offset
+	default:
+		t.To.Type = obj.TYPE_CONST
+		lit := c.offset(a)
+		if p.As == AMVN {
+			lit = ^lit // MVN lit, Rd is implemented as MOVW litoffset(PC), Rd
+		}
+		t.To.Offset = lit
+	}
+	for q := c.blitrl; q != nil; q = q.Link {
+		if q.Rel == nil && q.To == t.To {
+			p.Pool = q
+			return
+		}
+	}
+	if c.blitrl == nil {
+		c.blitrl = t
+		c.pool.start = pc
+	} else {
+		c.elitrl.Link = t
+	}
+	c.elitrl = t
+	c.pool.size += 4
+	p.Pool = t // store the link to the pool entry in Pool
+}
+
+func (c *Ctx) flushpool(p *obj.Prog, pc int, skip bool) (*obj.Prog, int) {
+	if skip {
+		q := c.newprog()
+		q.As = AB
+		q.To.Type = obj.TYPE_BRANCH
+		q.To.SetTarget(p.Link)
+		q.Pos = p.Pos
+		q.Link = c.blitrl
+		c.blitrl = q
+	}
+	if pc&3 != 0 {
+		pc += 2
+	}
+	for q := c.blitrl; q != nil; q = q.Link {
+		q.Pos = p.Pos // the line number of the preceding instruction (no deltas in the pcln)
+		q.Pc = int64(pc)
+		pc += 4
+	}
+	c.pool.size = 0
+	c.pool.start = 0
+	c.elitrl.Link = p.Link
+	p.Link = c.blitrl
+	c.blitrl = nil // BUG: should refer back to values until out-of-range
+	return c.elitrl, pc
+}
+
+func (c *Ctx) checkpool(p *obj.Prog, pc int) (*obj.Prog, int) {
+	poolLast := pc
+	skip := !(p.As == AB && p.Scond&0xF == 0)
+	if skip {
+		poolLast += 4 // the AB instruction to jump around the pool
+	}
+	poolLast += c.pool.size - 4 // the offset of the last pool entry
+	refpc := c.pool.start       // PC of the first pool reference
+	v := poolLast - refpc - 4   // PC-relative offset
+	if v >= 0xFF0 {
+		goto flush
+	}
+	if !skip && v >= 0xFD0 {
+		if q := p.Link.Link; q != nil && q.Link != nil {
+			goto flush
+		}
+	}
+	return p, pc
+flush:
+	if c.it == nil {
+		return c.flushpool(p, pc, skip)
+	}
+	// cannot flush literal pool inside IT block
+	q := c.beforeit
+	skip = !(q.As == AB && q.Scond&0xF == 0)
+	return c.flushpool(q, int(c.it.Pc), skip)
+}
+
+var xcmporeg = [...][14]byte{
+	//B H  W  W  W  U  S  S  F  Z  Z  Z  L  L
+	//O O  O  O  O  O  O  O  O  O  O  O  O  O
+	//R R  R  S  P  R  R  P  R  R  S  R  R  R
+	//L L  L  P  C  E  E  C  E  L  P  E  E  L
+	//O O  O        G  G     G  O     G  G  O
+
+	{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1}, // C_LORLO
+	{1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1}, // C_U0ORLO
+	{1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1}, // C_U3ORLO2
+	{0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1}, // C_U4ORLO2
+	{0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1}, // C_U5ORLO2
+	{1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1}, // C_U4ORLO1
+	{0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1}, // C_U5ORLO1
+	{1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1}, // C_U5ORLO
+	{0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0}, // C_U0OPC
+	{0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0}, // C_U6OPC2
+	{0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0}, // C_U8OPC2
+	{0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0}, // C_U12OPC
+	{0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0}, // C_S8OPC
+	{0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0}, // C_S8OPC2
+	{0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0}, // C_S12OPC
+	{0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0}, // C_U0OSP
+	{0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0}, // C_U6OSP2
+	{0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0}, // C_U8OSP2
+	{0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0}, // C_U0OREG
+	{0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0}, // C_U6OREG2
+	{0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0}, // C_U8OREG2
+	{0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0}, // C_U8OREG
+	{0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0}, // C_U12OREG
+	{0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0}, // C_S6OREG2
+	{0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0}, // C_S8OREG2
+	{0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0}, // C_S8OREG
+}
+
+var xcmpcon = [...][8]byte{
+	//U U  U  U  U  U  E  L
+	//7 8  3  8  1  1  3
+	//<<<<       2  6  2
+	//2 2
+
+	{1, 1, 1, 1, 1, 1, 1, 1}, // C_ZCON
+	{1, 1, 1, 1, 1, 1, 1, 1}, // C_U1CON2
+	{1, 1, 0, 1, 1, 1, 1, 1}, // C_U6CON2
+	{0, 0, 0, 0, 1, 1, 1, 1}, // C_U8CON1_4 // not contain C_U8CON2
+	{0, 0, 0, 0, 0, 1, 1, 1}, // C_U8CON5_8
+	{1, 1, 0, 0, 1, 1, 1, 1}, // C_U7CON2
+	{0, 1, 0, 0, 1, 1, 1, 1}, // C_U8CON2
+	{0, 0, 1, 1, 1, 1, 1, 1}, // C_U3CON
+	{0, 0, 0, 1, 1, 1, 1, 1}, // C_U8CON
+	{0, 0, 0, 0, 1, 1, 0, 1}, // C_U12CON
+	{0, 0, 0, 0, 0, 1, 0, 1}, // C_U16CON
+	{0, 0, 0, 0, 0, 0, 1, 1}, // C_E32CON
+}
+
+var xcmpreg = [...][6]byte{
+	//R S  P  R  S  S
+	//L P  C  E  I  I
+	//O       G  L
+
+	{1, 0, 0, 1, 1, 1}, // C_RLO
+	{0, 1, 0, 1, 0, 1}, // C_SP
+	{0, 0, 1, 1, 0, 1}, // C_PC
+	{0, 0, 0, 1, 0, 1}, // C_REG
+	{0, 0, 0, 0, 1, 1}, // C_SIFTILO
+}
+
+func match(op, code Aclass) bool {
+	if op == code {
+		return true
+	}
+	switch {
+	case C_RLO <= code && code <= C_SHIFTILO:
+		if op < C_RLO || C_SHIFTI < op {
+			return false
+		}
+		return xcmpreg[code-C_RLO][op-C_RLO] != 0
+	case C_LORLO <= code && code <= C_S8OREG:
+		if op < C_BORLO || C_LORLO < op {
+			return false
+		}
+		return xcmporeg[code-C_LORLO][op-C_BORLO] != 0
+	case C_ZCON <= code && code <= C_E32CON:
+		if op < C_U7CON2 || C_LCON < op {
+			return false
+		}
+		return xcmpcon[code-C_ZCON][op-C_U7CON2] != 0
+	case op == C_ROREG && code == C_RORLO:
+		return true
+	case op == C_SHIFTR && code == C_SHIFTRLO:
+		return true
+	case op == C_LIST && (C_LISTLO <= code && code <= C_LISTLOPC):
+		return true
+	case (op == C_LISTLOLR || op == C_LISTLOPC) && code == C_LISTLO:
+		return true
+	}
+	return false
+}
+
+var oprange [ALAST & obj.AMask][]Optab
+
+var (
+	deferreturn *obj.LSym
+	symdiv      *obj.LSym
+	symdivu     *obj.LSym
+	symmod      *obj.LSym
+	symmodu     *obj.LSym
+)
+
+func buildop(ctxt *obj.Link) {
+	if oprange[AAND&obj.AMask] != nil {
+		// Already initialized; stop now. This happens in the
+		// cmd/asm tests, each of which re-initializes the arch.
+		return
+	}
+
+	deferreturn = ctxt.LookupABI("runtime.deferreturn", obj.ABIInternal)
+
+	symdiv = ctxt.Lookup("runtime._div")
+	symdivu = ctxt.Lookup("runtime._divu")
+	symmod = ctxt.Lookup("runtime._mod")
+	symmodu = ctxt.Lookup("runtime._modu")
+
+	for i := range optab {
+		oi := &optab[i]
+		if !(oi.size != 0 || oi.a1 != 0 || oi.a2 != 0 || oi.a3 != 0) {
+			continue
+		}
+		oi.oscond |= oi.rscond
+	}
+
+	for i := 0; i < len(optab); {
+		r := optab[i].as
+		if r == obj.AXXX {
+			continue
+		}
+		r0 := r & obj.AMask
+		start := i
+		for i++; i < len(optab) && optab[i].as == r; i++ {
+		}
+		opt := optab[start:i]
+		oprange[r0] = append(oprange[r0], opt...)
+		for i < len(optab) {
+			oi := &optab[i]
+			if oi.size != 0 || oi.a1 != 0 || oi.a2 != 0 || oi.a3 != 0 || oi.flag != 0 {
+				break
+			}
+			op := &oprange[oi.as&obj.AMask]
+			*op = append(*op, opt...)
+			i++
+		}
+	}
+}
diff --git a/src/cmd/internal/obj/thumb/asm.md b/src/cmd/internal/obj/thumb/asm.md
new file mode 100644
index 0000000000..2b4f682f56
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/asm.md
@@ -0,0 +1,34 @@
+The Thumb2 assembler is intended to generate code for ARMv7-M ISA. It can be used for other variants of ARMv7 ISA as long as the access to special registers or coprocessors is not need (however, you can always use WORD, HWORD to generate not supported instructions).
+
+For the rest of this document thumb means the Thumb2 assembler, arm means the ARM assembler.
+
+### Differences in relation to arm
+
+1. The lack of .S suffix does not mean that generated data processing instruction does not modify flags. In such case thumb generates shortest instruction encoding that may (but may not) modify flags. Use .P suffix to preserve flags.
+
+2. Thumb uses R7 as temporary register (REGTMP) to promote shortest encodings (arm uses R11).
+
+3. Thumb requires the hardware division. ARMv7-M ISA supports it, but in case of ARMv7-A the hardware division is optional.
+
+4. Thumb requires support for unalligned memory access by MOVW, MOVH instructions (ARMv7 supports it if ARMv7-A SCTLR.A=0, ARMv7-M CCR.UNALIGN_TRP=0).
+
+5. Thumb does not support .IB (increment before), .DA (decrement after) suffixes.
+
+### Thumb only features
+
+1. As long as a function has not any explicit IT instruction thumb automatically adds IT instructions before group of 1 to 4 instructions with condition suffixes to make them conditional. Any explicit IT instruction disables this auto-IT mode for the entrie function.
+
+2. In auto-IT mode thumb converts `B.cond label` to `Bcond label` (if not the last instruction in IT block).
+
+### Portable code
+
+To write portable code for arm and thumb:
+
+1. Check condition flags immediately after setting them, do not use .P to preserve flags.
+
+2. Be careful when using R7, R11 registers.
+
+3. Do not use IT instruction.
+
+4. Do not use thumb-only instructions like CBZ, CBNZ.
+
diff --git a/src/cmd/internal/obj/thumb/instr_encoding.md b/src/cmd/internal/obj/thumb/instr_encoding.md
new file mode 100644
index 0000000000..8e44386b89
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/instr_encoding.md
@@ -0,0 +1,43 @@
+# Thumb2 assembler, instruction encoding
+
+## Definitions
+
+### Register specifiers
+
+Rm - first register operand, Rn - second register operand, Rd - destination register, Rdm - combined first operand and destination register, Rdn - combined second operand and destination register, Rt - transfered register.
+
+### Shift type
+
+	vv  <v>  Descr
+	-------------------------
+	00  <<   logical left
+	01  >>   logical right
+	10  ->   arithmetic right
+	11  @>   rotate right (u5==0: rotate right with extend)
+	
+### Modified immediate constant (e32)
+
+    xxxx xexx xxxx xxxx  xeee xxxx eeee eeee
+          i               iii      abcd efgh
+    
+    iiiia  const
+    ------------------------------------------
+    0000x  00000000 00000000 00000000 abcdefgh
+    0001x  00000000 abcdefgh 00000000 abcdefgh
+    0010x  abcdefgh 00000000 abcdefgh 00000000
+    0011x  abcdefgh abcdefgh abcdefgh abcdefgh
+    01000  1bcdefgh 00000000 00000000 00000000
+    01001  01bcdefg h0000000 00000000 00000000
+    01010  001bcdef gh000000 00000000 00000000
+    .....  ........ ........ ........ ........
+    11111  00000000 00000000 00000001 bcdefgh0
+	
+## 16 and 32-bit instructions
+
+Bits [15:11] of the least significant half-word determine 16/32-bit instruction. 32-bit instructions have the following format:
+
+	1110 1xxx xxxx xxxx  xxxx xxxx xxxx xxxx
+	1111 0xxx xxxx xxxx  xxxx xxxx xxxx xxxx
+	1111 1xxx xxxx xxxx  xxxx xxxx xxxx xxxx
+	
+Almost all 16 and 32-bit instructions require condition suffix inside IT block. Most 16-bit instructions sets flags outside IT block.
\ No newline at end of file
diff --git a/src/cmd/internal/obj/thumb/instr_group.txt b/src/cmd/internal/obj/thumb/instr_group.txt
new file mode 100644
index 0000000000..c526a8b6d8
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/instr_group.txt
@@ -0,0 +1,304 @@
+	0100 0100 dmmm mddd                       .ADD       Rm, Rdn         ; does not set flags
+	1010 1ddd uuuu uuuu                       .ADD       u8<<2, R13, Rd
+	1010 0ddd uuuu uuuu                       .ADD       u8<<2, R15, Rd
+
+	0001 100m mmnn nddd                       .ADD       Rm, Rn, Rd
+	0001 101m mmnn nddd                        SUB       Rm, Rn, Rd
+	1110 1011 000s nnnn  0uuu dddd uuvv mmmm  .ADD.s     Rm<v>u5, Rn, Rd
+	1110 1011 101s nnnn  0uuu dddd uuvv mmmm   SUB.s     Rm<v>u5, Rn, Rd
+	0001 110u uunn nddd                       .ADD       u3, Rn, Rd
+	0001 111u uunn nddd                        SUB       u3, Rn, Rd
+	0011 0ddd uuuu uuuu                       .ADD       u8, Rdn
+	0011 1ddd uuuu uuuu                        SUB       u8, Rdn
+	1011 0000 0uuu uuuu                       .ADD       u7<<2, R13
+	1011 0000 1uuu uuuu                        SUB       u7<<2, R13
+	1111 0u10 0000 nnnn  0uuu dddd uuuu uuuu  .ADD       u12, Rn, Rd
+	1111 0u10 1010 nnnn  0uuu dddd uuuu uuuu   SUB       u12, Rn, Rd
+	1111 0e01 000s nnnn  0eee dddd eeee eeee  .ADD.s     e32, Rn, Rd
+	1111 0e01 101s nnnn  0eee dddd eeee eeee   SUB.s     e32, Rn, Rd
+
+	0100 0000 00mm mddd                       .AND       Rm, Rdn
+	0100 0001 01mm mddd                        ADC       Rm, Rdn
+	0100 0011 10mm mddd                        BIC       Rm, Rdn
+	0100 0000 01mm mddd                        EOR       Rm, Rdn
+	0100 0011 00mm mddd                        ORR       Rm, Rdn
+	0100 0001 10mm mddd                        SBC       Rm, Rdn
+	0100 0011 01mm mddd                        MUL       Rm, Rdn
+	0100 0011 11mm mddd                        MVN       Rm, Rd
+
+	0100 0010 01nn nddd                       .RSB       $0, Rn, Rd
+
+	1110 1010 000s nnnn  0uuu dddd uuvv mmmm  .AND.s     Rm<v>u5, Rn, Rd
+	1110 1011 010s nnnn  0uuu dddd uuvv mmmm   ADC.s     Rm<v>u5, Rn, Rd
+	1110 1010 001s nnnn  0uuu dddd uuvv mmmm   BIC.s     Rm<v>u5, Rn, Rd
+	1110 1010 100s nnnn  0uuu dddd uuvv mmmm   EOR.s     Rm<v>u5, Rn, Rd
+	1110 1010 010s nnnn  0uuu dddd uuvv mmmm   ORR.s     Rm<v>u5, Rn, Rd
+	1110 1011 110s nnnn  0uuu dddd uuvv mmmm   RSB.s     Rm<v>u5, Rn, Rd
+	1110 1011 011s nnnn  0uuu dddd uuvv mmmm   SBC.s     Rm<v>u5, Rn, Rd
+	1110 1010 011s nnnn  0uuu dddd uuvv mmmm   ORN.s     Rm<v>u5, Rn, Rd
+	1111 0e00 000s nnnn  0eee dddd eeee eeee  .AND.s     e32, Rn, Rd
+	1111 0e01 010s nnnn  0eee dddd eeee eeee   ADC.s     e32, Rn, Rd
+	1111 0e00 001s nnnn  0eee dddd eeee eeee   BIC.s     e32, Rn, Rd
+	1111 0e00 100s nnnn  0eee dddd eeee eeee   EOR.s     e32, Rn, Rd
+	1111 0e00 010s nnnn  0eee dddd eeee eeee   ORR.s     e32, Rn, Rd
+	1111 0e01 110s nnnn  0eee dddd eeee eeee   RSB.s     e32, Rn, Rd
+	1111 0e01 011s nnnn  0eee dddd eeee eeee   SBC.s     e32, Rn, Rd
+	1111 0e00 011s nnnn  0eee dddd eeee eeee   ORN.s     e32, Rn, Rd
+
+	0100 0110 dmmm mddd                       .MOVW      Rm, Rd       ; preserves flags
+	0000 0000 00mm mddd                       .MOVW      Rm, Rd       ; not permitted in IT
+	000v vuuu uumm mddd                       .MOVW      Rm<v>u5, Rd  ; vv != 11
+	0100 0000 10mm mddd                       .MOVW      Rdn<<Rm, Rdn
+	0100 0000 11mm mddd                       .MOVW      Rdn>>Rm, Rdn
+	0100 0001 00mm mddd                       .MOVW      Rdn->Rm, Rdn
+	0100 0001 11mm mddd                       .MOVW      Rdn@>Rm, Rdn
+	1111 1010 0vvs nnnn  1111 dddd 0000 mmmm  .MOVW.s    Rn<v>Rm, Rd
+
+	0010 0ddd uuuu uuuu                       .MOVW      u8, Rd
+	1111 0y10 0100 uuuu  0zzz dddd zzzz zzzz  .MOVW      uyz16, Rd
+
+	1110 1010 011s 1111  0uuu dddd uuvv mmmm  .MVN.s     Rm<v>u5, Rd
+	1110 1010 010s 1111  0uuu dddd uuvv mmmm   MOVW.s    Rm<v>u5, Rd
+	1111 0e00 011s 1111  0eee dddd eeee eeee  .MVN.s     e32, Rd
+	1111 0e00 010s 1111  0eee dddd eeee eeee   MOVW.s    e32, Rd
+
+	1111 0y10 1100 uuuu  0zzz dddd zzzz zzzz  .MOVT      uyz16, Rd
+
+	1011 0010 00mm mddd	                      .MOVH      Rm, Rd  ; ARM SXTH
+	1011 0010 01mm mddd                        MOVB      Rm, Rd  ; ARM SXTB
+	1011 0010 10mm mddd                        MOVHU     Rm, Rd  ; ARM UXTH
+	1011 0010 11mm mddd                        MOVBU     Rm, Rd  ; ARM UXTB
+	1111 1010 0000 1111  1111 dddd 10rr mmmm  .MOVH      Rm@>rot, Rd  ; ARM SXTH
+	1111 1010 0100 1111  1111 dddd 10rr mmmm   MOVB      Rm@>rot, Rd  ; ARM SXTB
+	1111 1010 0001 1111  1111 dddd 10rr mmmm   MOVHU     Rm@>rot, Rd  ; ARM UXTH
+	1111 1010 0101 1111  1111 dddd 10rr mmmm   MOVBU     Rm@>rot, Rd  ; ARM UXTB
+
+	1111 0011 1110 1111  1000 dddd mmmm mmmm   MOVW      SYSm, Rd
+	1111 0011 1000 nnnn  1000 mm00 mmmm mmmm   MOVW      Rn, SYSm
+
+	1110 1110 1111 0001  tttt 1010 0001 0000   MOVW      FPSCR, Rt
+	1110 1110 1110 0001  tttt 1010 0001 0000   MOVW      Rt, FPSCR
+
+	1111 1011 0000 nnnn  1111 dddd 0000 mmmm  .MUL       Rm, Rn, Rd
+	1111 1011 1001 nnnn  1111 dddd 1111 mmmm   DIV       Rm, Rn, Rd
+	1111 1011 1011 nnnn  1111 dddd 1111 mmmm   DIVU      Rm, Rn, Rd
+
+	1111 1011 1000 nnnn  llll hhhh 0000 mmmm  .MULL      Rm, Rn, (Rdh, Rdl)
+	1111 1011 1010 nnnn  llll hhhh 0000 mmmm   MULLU     Rm, Rn, (Rdh, Rdl)
+	1111 1011 1100 nnnn  llll hhhh 0000 mmmm   MULAL     Rm, Rn, (Rdh, Rdl)  ; Rd+=int64(Rn)*int64(Rm)
+	1111 1011 1110 nnnn  llll hhhh 0000 mmmm   MULALU    Rm, Rn, (Rdh, Rdl)  ; Rd+=uint64(Rn)*uint64(Rm)
+
+	1111 1011 0000 nnnn  aaaa dddd 0000 mmmm  .MULA      Rm, Rn, Ra, Rd  ; Rd=Ra+Rn*Rm
+	1111 1011 0000 nnnn  aaaa dddd 0001 mmmm   MULS      Rm, Rn, Ra, Rd  ; Rd=Ra-Rn*Rm
+	1111 1011 0011 nnnn  aaaa dddd 0000 mmmm   MULAWB    Rm, Rn, Ra, Rd
+	1111 1011 0011 nnnn  aaaa dddd 0001 mmmm   MULAWT    Rm, Rn, Ra, Rd
+
+	1011 1010 00mm mddd                       .REV       Rm, Rd
+	1011 1010 01mm mddd                        REV16     Rm, Rd
+	1011 1010 11mm mddd                        REVSH     Rm, Rd
+
+	1111 1010 1011 mmmm  1111 dddd 1000 mmmm  .CLZ       Rm, Rd
+	1111 1010 1001 mmmm  1111 dddd 1000 mmmm   REV       Rm, Rd
+	1111 1010 1001 mmmm  1111 dddd 1001 mmmm   REV16     Rm, Rd
+	1111 1010 1001 mmmm  1111 dddd 1010 mmmm   RBIT      Rm, Rd
+	1111 1010 1001 mmmm  1111 dddd 1011 mmmm   REVSH     Rm, Rd
+
+	1111 1010 1010 nnnn  1111 dddd 1000 mmmm  .SEL       Rm, Rn, Rd  ; DSP extension
+
+	1111 0011 0100 nnnn  0uuu dddd uu0w wwww  .BFX       width, ulsb, Rn, Rd  ; wwww = width-1
+	1111 0011 1100 nnnn  0uuu dddd uu0w wwww   BFXU      width, ulsb, Rn, Rd  ; wwww = width-1
+	1111 0011 0110 nnnn  0uuu dddd uu0k kkkk   BFI       width, ulsb, Rn, Rd  ; kkkk = ulsb+width-1
+	1111 0011 0110 1111  0uuu dddd uu0k kkkk   BFC       width, ulsb, Rd      ; kkkk = ulsb+width-1
+
+	0100 0010 00mm mnnn                       .TST       Rm, Rn
+	0100 0010 10mm mnnn                        CMP       Rm, Rn
+	0100 0010 11mm mnnn                        CMN       Rm, Rn
+
+	0100 0101 nmmm mnnn                       .CMP       Rm, Rn
+	0010 1nnn uuuu uuuu                        CMP       u8, Rn
+
+	1110 1010 0001 nnnn  0uuu 1111 uuvv mmmm  .TST       Rm<v>u5, Rn
+	1110 1010 1001 nnnn  0uuu 1111 uuvv mmmm   TEQ       Rm<v>u5, Rn
+	1110 1011 0001 nnnn  0uuu 1111 uuvv mmmm   CMN       Rm<v>u5, Rn
+	1110 1011 1011 nnnn  0uuu 1111 uuvv mmmm   CMP       Rm<v>u5, Rn
+	1111 0e00 0001 nnnn  0eee 1111 eeee eeee  .TST       e32, Rn
+	1111 0e00 1001 nnnn  0eee 1111 eeee eeee   TEQ       e32, Rn
+	1111 0e01 0001 nnnn  0eee 1111 eeee eeee   CMN       e32, Rn
+	1111 0e01 1011 nnnn  0eee 1111 eeee eeee   CMP       e32, Rn
+
+	0110 1uuu uunn nttt                       .MOVW      u5<<2(Rn), Rt
+	1001 1ttt uuuu uuuu                       .MOVW      u8<<2(R13), Rt
+	0100 1ttt uuuu uuuu                       .MOVW      u8<<2(R15), Rt
+	1000 1uuu uunn nttt                       .MOVHU     u5<<1(Rn), Rt
+	0111 1uuu uunn nttt                       .MOVBU     u5(Rn), Rt
+
+	0110 0uuu uunn nttt                       .MOVW      Rt, u5<<2(Rn)
+	1001 0ttt uuuu uuuu                       .MOVW      Rt, u8<<2(R13)
+	1000 0uuu uunn nttt                       .MOVH      Rt, u5<<1(Rn)
+	0111 0uuu uunn nttt                       .MOVB      Rt, u5(Rn)
+
+	0101 100m mmnn nttt                       .MOVW      (Rn)(Rm), Rt
+	0101 111m mmnn nttt                        MOVH      (Rn)(Rm), Rt
+	0101 101m mmnn nttt                        MOVHU     (Rn)(Rm), Rt
+	0101 011m mmnn nttt                        MOVB      (Rn)(Rm), Rt
+	0101 110m mmnn nttt                        MOVBU     (Rn)(Rm), Rt
+	1111 1000 0101 nnnn  tttt 0000 00uu mmmm  .MOVW      (Rn)(Rm*1<<u2), Rt
+	1111 1001 0011 nnnn  tttt 0000 00uu mmmm   MOVH      (Rn)(Rm*1<<u2), Rt
+	1111 1000 0011 nnnn  tttt 0000 00uu mmmm   MOVHU     (Rn)(Rm*1<<u2), Rt
+	1111 1001 0001 nnnn  tttt 0000 00uu mmmm   MOVB      (Rn)(Rm*1<<u2), Rt
+	1111 1000 0001 nnnn  tttt 0000 00uu mmmm   MOVBU     (Rn)(Rm*1<<u2), Rt
+	1111 1000 ±101 1111  tttt uuuu uuuu uuuu  .MOVW      ±u12(R15), Rt
+	1111 1001 ±011 1111  tttt uuuu uuuu uuuu   MOVH      ±u12(R15), Rt
+	1111 1000 ±011 1111  tttt uuuu uuuu uuuu   MOVHU     ±u12(R15), Rt
+	1111 1001 ±001 1111  tttt uuuu uuuu uuuu   MOVB      ±u12(R15), Rt
+	1111 1000 ±001 1111  tttt uuuu uuuu uuuu   MOVBU     ±u12(R15), Rt
+	1111 1000 1101 nnnn  tttt uuuu uuuu uuuu  .MOVW      u12(Rn), Rt
+	1111 1001 1011 nnnn  tttt uuuu uuuu uuuu   MOVH      u12(Rn), Rt
+	1111 1000 1011 nnnn  tttt uuuu uuuu uuuu   MOVHU     u12(Rn), Rt
+	1111 1001 1001 nnnn  tttt uuuu uuuu uuuu   MOVB      u12(Rn), Rt
+	1111 1000 1001 nnnn  tttt uuuu uuuu uuuu   MOVBU     u12(Rn), Rt
+	1111 1000 0101 nnnn  tttt 1p±w uuuu uuuu  .MOVW.p.w  ±u8(Rn), Rt
+	1111 1001 0011 nnnn  tttt 1p±w uuuu uuuu   MOVH.p.w  ±u8(Rn), Rt
+	1111 1000 0011 nnnn  tttt 1p±w uuuu uuuu   MOVHU.p.w ±u8(Rn), Rt
+	1111 1001 0001 nnnn  tttt 1p±w uuuu uuuu   MOVB.p.w  ±u8(Rn), Rt
+	1111 1000 0001 nnnn  tttt 1p±w uuuu uuuu   MOVBU.p.w ±u8(Rn), Rt
+
+	0101 000m mmnn nttt                       .MOVW      Rt, (Rn)(Rm)
+	0101 001m mmnn nttt                        MOVH      Rt, (Rn)(Rm)
+	0101 010m mmnn nttt                        MOVB      Rt, (Rn)(Rm)
+	1111 1000 0100 nnnn  tttt 0000 00uu mmmm  .MOVW      Rt, (Rn)(Rm*1<<u2)
+	1111 1000 0010 nnnn  tttt 0000 00uu mmmm   MOVH      Rt, (Rn)(Rm*1<<u2)
+	1111 1000 0000 nnnn  tttt 0000 00uu mmmm   MOVB      Rt, (Rn)(Rm*1<<u2)
+	1111 1000 1100 nnnn  tttt uuuu uuuu uuuu  .MOVW      Rt, u12(Rn)
+	1111 1000 1010 nnnn  tttt uuuu uuuu uuuu   MOVH      Rt, u12(Rn)
+	1111 1000 1000 nnnn  tttt uuuu uuuu uuuu   MOVB      Rt, u12(Rn)
+	1111 1000 0100 nnnn  tttt 1p±w uuuu uuuu  .MOVW.p.w  Rt, ±u8(Rn)
+	1111 1000 0010 nnnn  tttt 1p±w uuuu uuuu   MOVH.p.w  Rt, ±u8(Rn)
+	1111 1000 0000 nnnn  tttt 1p±w uuuu uuuu   MOVB.p.w  Rt, ±u8(Rn)
+
+	1110 1110 0d11 nnnn  dddd 1010 n0m0 mmmm  .ADDF      Fm, Fn, Fd
+	1110 1110 0d11 nnnn  dddd 1011 n0m0 mmmm   ADDD      Fm, Fn, Fd
+	1110 1110 0d11 nnnn  dddd 1010 n1m0 mmmm   SUBF      Fm, Fn, Fd
+	1110 1110 0d11 nnnn  dddd 1011 n1m0 mmmm   SUBD      Fm, Fn, Fd
+	1110 1110 0d10 nnnn  dddd 1010 n0m0 mmmm   MULF      Fm, Fn, Fd
+	1110 1110 0d10 nnnn  dddd 1011 n0m0 mmmm   MULD      Fm, Fn, Fd
+	1110 1110 1d00 nnnn  dddd 1010 n0m0 mmmm   DIVF      Fm, Fn, Fd
+	1110 1110 1d00 nnnn  dddd 1011 n0m0 mmmm   DIVD      Fm, Fn, Fd
+	1110 1110 0d00 nnnn  dddd 1010 n0m0 mmmm   MULAF     Fm, Fn, Fd
+	1110 1110 0d00 nnnn  dddd 1011 n0m0 mmmm   MULAD     Fm, Fn, Fd
+	1110 1110 0d00 nnnn  dddd 1010 n1m0 mmmm   MULSF     Fm, Fn, Fd
+	1110 1110 0d00 nnnn  dddd 1011 n1m0 mmmm   MULSD     Fm, Fn, Fd
+	1110 1110 0d10 nnnn  dddd 1010 n1m0 mmmm   NMULF     Fm, Fn, Fd
+	1110 1110 0d10 nnnn  dddd 1011 n1m0 mmmm   NMULD     Fm, Fn, Fd
+
+	1110 1110 1d11 0001  dddd 1010 11m0 mmmm  .SQRTF     Fm, Fd
+	1110 1110 1d11 0001  dddd 1011 11m0 mmmm   SQRTD     Fm, Fd
+	1110 1110 1d11 0001  dddd 1010 01m0 mmmm   NEGF      Fm, Fd
+	1110 1110 1d11 0001  dddd 1011 01m0 mmmm   NEGD      Fm, Fd
+	1110 1110 1d11 0000  dddd 1010 01m0 mmmm   MOVF      Fm, Fd
+	1110 1110 1d11 0000  dddd 1011 01m0 mmmm   MOVD      Fm, Fd
+	1110 1110 1d11 0000  dddd 1010 11m0 mmmm   ABSF      Fm, Fd
+	1110 1110 1d11 0000  dddd 1011 11m0 mmmm   ABSD      Fm, Fd
+	1110 1110 1d11 0111  dddd 1010 11m0 mmmm   MOVFD     Fm, Fd
+	1110 1110 1d11 0111  dddd 1011 11m0 mmmm   MOVDF     Fm, Fd
+
+	1110 1110 00h1 nnnn  tttt 1011 n001 0000  .MOVW      Fn, Rt
+	1110 1110 00h0 dddd  tttt 1011 d001 0000  .MOVW      Rt, Fd
+
+	1111 1110 1d11 1101  dddd 1010 11m0 mmmm  .MOVFW     Fm, Fd
+	1111 1110 1d11 1100  dddd 1010 11m0 mmmm   MOVFW.U   Fm, Fd
+	1111 1110 1d11 1101  dddd 1011 11m0 mmmm   MOVDW     Fm, Fd
+	1111 1110 1d11 1100  dddd 1011 11m0 mmmm   MOVDW.U   Fm, Fd
+	1111 1110 1d11 1000  dddd 1010 11m0 mmmm   MOVWF     Fm, Fd
+	1111 1110 1d11 1000  dddd 1010 01m0 mmmm   MOVWF.U   Fm, Fd
+	1111 1110 1d11 1000  dddd 1011 11m0 mmmm   MOVWD     Fm, Fd
+	1111 1110 1d11 1000  dddd 1011 01m0 mmmm   MOVWD.U   Fm, Fd
+
+	1110 1110 1d11 0100  dddd 1010 e1m0 mmmm  .CMPF      Fm, Fd
+	1110 1110 1d11 0100  dddd 1011 e1m0 mmmm   CMPD      Fm, Fd
+	1110 1110 1d11 0101  dddd 1010 e100 0000  .CMPF      Fd
+	1110 1110 1d11 0101  dddd 1010 e100 0000   CMPD      Fd
+
+	1110 1101 ±d01 nnnn  dddd 1010 uuuu uuuu  .MOVF      ±u8<<2(Rn), Fd
+	1110 1101 ±d01 nnnn  dddd 1011 uuuu uuuu   MOVD      ±u8<<2(Rn), Fd
+	1110 1101 ±d00 nnnn  dddd 1010 uuuu uuuu  .MOVF      Fd, ±u8<<2(Rn)
+	1110 1101 ±d00 nnnn  dddd 1011 uuuu uuuu   MOVD      Fd, ±u8<<2(Rn)
+	1110 1110 1d11 ffff  dddd 1010 0000 ffff  .MOVF      f8, Fd
+	1110 1110 1d11 ffff  dddd 1011 0000 ffff   MOVD      f8, Fd
+
+	1011 1111 cccc mmmm                       .ITmask    firstcond
+
+	1110 0iii iiii iiii                       .B         i11<<1   ; outside/last in IT
+
+	1011 00u1 uuuu unnn                       .CBZ       Rn, u6<<1  ; outside IT
+	1011 10u1 uuuu unnn                        CBNZ      Rn, u6<<1  ; outside IT
+
+	0100 0111 0mmm m000                       .B         (Rm)  ; ARM: BX Rm
+	0100 0111 1mmm m000                        BL        (Rm)  ; ARM: BLX Rm
+	1111 0jii iiii iiii  10j1 jiii iiii iiii  .B         ji24<<1  ; outside/last in IT
+	1111 0jii iiii iiii  11j1 jiii iiii iiii   BL        ji24<<1  ; outside/last in IT
+
+	1101 cccc iiii iiii                       .Bcond     i8<<1    ; outside IT
+	1111 0jcc ccii iiii  10j0 jiii iiii iiii  .Bcond     ji20<<1  ; outside IT
+
+	1110 1000 1101 nnnn  1111 0000 0000 mmmm  .TBB	     Rm, Rn
+	1110 1000 1101 nnnn  1111 0000 0001 mmmm   TBH	     Rm, Rn
+
+	1101 1111 uuuu uuuu                       .SWI       u8  ; SVC
+	1011 1110 uuuu uuuu                        BKPT      u8
+	1101 1110 uuuu uuuu                        UNDEF     u8
+
+	1011 010r rrrr rrrr                       .MOVM.DB.W reglist, (R13)  ; PUSH [R0-R7,R14]
+	1100 0nnn rrrr rrrr                       .MOVM.IA.W reglist, (Rn)
+	1110 1000 10w0 nnnn  0r0r rrrr rrrr rrrr  .MOVM.IA.w reglist, (Rn)
+	1110 1001 00w0 nnnn  0r0r rrrr rrrr rrrr  .MOVM.DB.w reglist, (Rn)   ; .W,n=13 -> PUSH
+
+	1011 110r rrrr rrrr                       .MOVM.IA.W (R13), reglist  ; POP [R0-R7,R14]
+	1100 1nnn rrrr rrrr                       .MOVM.IA.W (Rn), reglist
+	1110 1000 10w1 nnnn  rr0r rrrr rrrr rrrr  .MOVM.IA.w (Rn), reglist   ; .W,n=13 -> POP
+	1110 1001 00w1 nnnn  rr0r rrrr rrrr rrrr  .MOVM.DB.w (Rn), reglist
+
+	1110 1000 0101 nnnn  tttt 1111 uuuu uuuu  .LDREX     u8<<2(Rn), Rt
+	1110 1000 0100 nnnn  tttt dddd uuuu uuuu  .STREX     Rt, u8<<2(Rn), Rd
+
+	1110 1000 1101 nnnn  tttt 1111 0100 1111  .LDREXB    (Rn), Rt
+	1110 1000 1101 nnnn  tttt 1111 0101 1111   LDREXH    (Rn), Rt
+
+	1110 1000 1100 nnnn  tttt 1111 0100 dddd  .STREXB    Rt, (Rn), Rd
+	1110 1000 1100 nnnn  tttt 1111 0101 dddd   STREXH    Rt, (Rn), Rd
+
+	1011 1111 0000 0000  .NOP2
+	1011 1111 0001 0000   YIELD
+	1011 1111 0010 0000   WFE
+	1011 1111 0011 0000   WFI
+	1011 1111 0100 0000   SEV
+
+	1111 0011 1010 1111  1000 0000 0000 0000  .NOP4
+	1111 0011 1011 1111  1000 1111 0010 1111   CLREX
+
+	1111 0011 1011 1111  1000 1111 0100 oooo  .DSB       opt
+	1111 0011 1011 1111  1000 1111 0101 oooo   DMB       opt
+	1111 0011 1011 1111  1000 1111 0110 oooo   ISB       opt
+
+	1011 0110 0110 0010                        CPSIE     i       ; outside IT
+	1011 0110 0111 0010                        CPSID     i       ; outside IT
+
+-- not implemented
+
+	1110 100p ±1w0 nnnn  aaaa bbbb uuuu uuuu   MOVW.p.w  (Rta, Rtb), ±u8<<2(Rn)
+	1110 100p ±1w1 nnnn  aaaa bbbb uuuu uuuu   MOVW.p.w  ±u8<<2(Rn), (Rta, Rtb)
+
+	1111 1010 1000 nnnn  1111 dddd 0000 mmmm   ADD8      Rm, Rn, Rd
+	1111 1010 1000 nnnn  1111 dddd 0100 mmmm   ADD8U     Rm, Rn, Rd
+	1111 1010 1001 nnnn  1111 dddd 0000 mmmm   ADD16     Rm, Rn, Rd
+	1111 1010 1001 nnnn  1111 dddd 0100 mmmm   ADD16U    Rm, Rn, Rd
+
+	1111 0011 1010 1111  1000 0000 0000 0000   NOP4
+	1111 0011 1010 1111  1000 0000 0000 0001   YIELD
+	1111 0011 1010 1111  1000 0000 0000 0010   WFE
+	1111 0011 1010 1111  1000 0000 0000 0011   WFI
+	1111 0011 1010 1111  1000 0000 0000 0100   SEV
+
+	1111 0111 1111 uuuu  1010 uuuu uuuu uuuu   UNDEF     u16
+
+	... and more
diff --git a/src/cmd/internal/obj/thumb/list.go b/src/cmd/internal/obj/thumb/list.go
new file mode 100644
index 0000000000..e5bf925908
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/list.go
@@ -0,0 +1,176 @@
+// Inferno utils/5c/list.c
+// https://bitbucket.org/inferno-os/inferno-os/src/master/utils/5c/list.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"cmd/internal/obj"
+	"fmt"
+)
+
+func init() {
+	obj.RegisterRegister(obj.RBaseThumb, MAXREG, rconv)
+	obj.RegisterOpcode(obj.ABaseThumb, Anames)
+	obj.RegisterRegisterList(obj.RegListARMLo, obj.RegListARMHi, rlconv)
+	obj.RegisterOpSuffix("thumb", cconv)
+}
+
+var condCode = []string{
+	".EQ",
+	".NE",
+	".CS",
+	".CC",
+	".MI",
+	".PL",
+	".VS",
+	".VC",
+	".HI",
+	".LS",
+	".GE",
+	".LT",
+	".GT",
+	".LE",
+	"",
+	".NV",
+}
+
+func cconv(s uint8) string {
+	sc := condCode[(s&C_SCOND)^C_SCOND_XOR]
+	if s&C_SBIT != 0 {
+		sc += ".S"
+	}
+	if s&C_PBIT != 0 {
+		sc += ".P"
+	}
+	if s&C_WBIT != 0 {
+		sc += ".W"
+	}
+	if s&C_UBIT != 0 { /* ambiguous with FBIT */
+		sc += ".U"
+	}
+	return sc
+}
+
+func rconv(r int) string {
+	if r == 0 {
+		return "NONE"
+	}
+	if r == REGG {
+		// Special case.
+		return "g"
+	}
+	if REG_R0 <= r && r <= REG_R15 {
+		return fmt.Sprintf("R%d", r-REG_R0)
+	}
+	if REG_F0 <= r && r <= REG_F15 {
+		return fmt.Sprintf("F%d", r-REG_F0)
+	}
+	if REG_EQ <= r && r <= REG_AL {
+		r = (r - REG_EQ) * 2
+		return "EQNEHSLOMIPLVSVCHILSGELTGTLEAL"[r : r+2]
+	}
+
+	switch r {
+	case REG_APSR:
+		return "APSR"
+	case REG_IAPSR:
+		return "IAPSR"
+	case REG_EAPSR:
+		return "EAPSR"
+	case REG_XPSR:
+		return "XPSR"
+	case REG_IPSR:
+		return "IPSR"
+	case REG_EPSR:
+		return "EPSR"
+	case REG_IEPSR:
+		return "IEPSR"
+
+	case REG_FPSCR:
+		return "FPSCR"
+
+	case REG_MSP:
+		return "MSP"
+	case REG_PSP:
+		return "PSP"
+
+	case REG_PRIMASK:
+		return "PRIMASK"
+	case REG_BASEPRI:
+		return "BASEPRI"
+	case REG_BASEPRI_MAX:
+		return "BASEPRI_MAX"
+	case REG_FAULTMASK:
+		return "FAULTMASK"
+	case REG_CONTROL:
+		return "CONTROL"
+
+	case REG_MB_SY:
+		return "MB_SY"
+	case REG_MB_ST:
+		return "MB_ST"
+	case REG_MB_ISH:
+		return "MB_ISH"
+	case REG_MB_ISHST:
+		return "MB_ISHST"
+	case REG_MB_NSH:
+		return "MB_NSH"
+	case REG_MB_NSHST:
+		return "MB_NSHST"
+	case REG_MB_OSH:
+		return "MB_OSH"
+	case REG_MB_OSHST:
+		return "MB_OSHST"
+	}
+
+	return fmt.Sprintf("Rgok(%d)", r-obj.RBaseThumb)
+}
+
+func rlconv(list int64) string {
+	str := ""
+	for i := 0; i < 16; i++ {
+		if list&(1<<uint(i)) != 0 {
+			if str == "" {
+				str += "["
+			} else {
+				str += ","
+			}
+			// This is ARM-specific; R10 is g.
+			if i == REGG-REG_R0 {
+				str += "g"
+			} else {
+				str += fmt.Sprintf("R%d", i)
+			}
+		}
+	}
+
+	str += "]"
+	return str
+}
diff --git a/src/cmd/internal/obj/thumb/obj.go b/src/cmd/internal/obj/thumb/obj.go
new file mode 100644
index 0000000000..07a324fc79
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/obj.go
@@ -0,0 +1,877 @@
+// Derived from Inferno utils/5c/swt.c
+// https://bitbucket.org/inferno-os/inferno-os/src/master/utils/5c/swt.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+package thumb
+
+import (
+	"cmd/internal/obj"
+	"cmd/internal/objabi"
+	"cmd/internal/sys"
+)
+
+const (
+	LEAF = 1 << iota
+	AUTOIT
+)
+
+// return address size (on stack frame)
+const (
+	prasize = 4 // parent function
+	rasize  = 4 // current function
+)
+
+func preprocess(ctxt *obj.Link, cursym *obj.LSym, newprog obj.ProgAlloc) {
+	if cursym.Func().Text == nil || cursym.Func().Text.Link == nil {
+		return
+	}
+
+	c := Ctx{ctxt: ctxt, cursym: cursym, newprog: newprog}
+
+	p := c.cursym.Func().Text
+	autoffset := int(p.To.Offset)
+	if autoffset == -4 {
+		// Historical way to mark NOFRAME.
+		p.From.Sym.Set(obj.AttrNoFrame, true)
+		autoffset = 0
+	}
+	if autoffset < 0 || autoffset%4 != 0 {
+		c.ctxt.Diag("frame size %d not 0 or a positive multiple of 4", autoffset)
+	}
+	if p.From.Sym.NoFrame() {
+		if autoffset != 0 {
+			c.ctxt.Diag("NOFRAME functions must have a frame size of 0, not %d", autoffset)
+		}
+	}
+
+	cursym.Func().Locals = int32(autoffset)
+	cursym.Func().Args = p.To.Val.(int32)
+	cursym.Func().Text.Mark = AUTOIT
+	if !cursym.Func().Text.From.Sym.ISR() {
+		cursym.Func().Text.Mark |= LEAF
+	}
+
+	for p := cursym.Func().Text; p != nil; p = p.Link {
+		switch {
+		case AITTTT <= p.As && p.As <= AITEEE:
+			cursym.Func().Text.Mark &^= AUTOIT
+		case p.As == ABL:
+			cursym.Func().Text.Mark &^= LEAF
+		}
+	}
+
+	var autosize int
+	for p := cursym.Func().Text; p != nil; p = p.Link {
+		if cursym.Func().Text.Mark&AUTOIT != 0 && p.To.Type == obj.TYPE_BRANCH &&
+			ABEQ <= p.As && p.As <= ABLE {
+			// convert `Bcond label` to `B.cond label`
+			cond := byte(C_SCOND_LE)
+			switch p.As {
+			case ABEQ:
+				cond = C_SCOND_EQ
+			case ABNE:
+				cond = C_SCOND_NE
+			case ABCS, ABHS:
+				cond = C_SCOND_HS
+			case ABCC, ABLO:
+				cond = C_SCOND_LO
+			case ABMI:
+				cond = C_SCOND_MI
+			case ABPL:
+				cond = C_SCOND_PL
+			case ABVS:
+				cond = C_SCOND_VS
+			case ABVC:
+				cond = C_SCOND_VC
+			case ABHI:
+				cond = C_SCOND_HI
+			case ABLS:
+				cond = C_SCOND_LS
+			case ABGE:
+				cond = C_SCOND_GE
+			case ABLT:
+				cond = C_SCOND_LT
+			case ABGT:
+				cond = C_SCOND_GT
+			}
+			p.As = AB
+			p.Scond = p.Scond&^C_SCOND | cond
+		}
+
+		switch p.As {
+		case obj.ATEXT:
+			autosize = autoffset
+
+			if p.Mark&LEAF != 0 && autosize == 0 {
+				// A leaf function with no locals has no frame.
+				p.From.Sym.Set(obj.AttrNoFrame, true)
+			}
+
+			if !p.From.Sym.NoFrame() {
+				// If there is a stack frame at all, it includes
+				// space to save the LR.
+				autosize += rasize
+			}
+
+			if autosize == 0 && cursym.Func().Text.Mark&LEAF == 0 {
+				// A very few functions that do not return to their caller
+				// are not identified as leaves but still have no frame.
+				if ctxt.Debugvlog {
+					ctxt.Logf("save suppressed in: %s\n", cursym.Name)
+				}
+				cursym.Func().Text.Mark |= LEAF
+			}
+
+			c.autosize = autosize
+			p.To.Offset = int64(autosize - rasize) // FP offsets need an updated p.To.Offset.
+
+			if cursym.Func().Text.From.Sym.ISR() {
+				// Emit the interrupt handler prologue:
+				//
+				//  PUSH  [R4-R11]
+				//  TST   $0x10, LR
+				//  BNE   nofpuctx
+				//	MOVW  CONTROL, R0
+				//  CPSID i
+				//  VPUSH [D8-D15]    // sets CONTROL.FPCA
+				//  MOVW  R0, CONTROL // clears FPCA
+				//  CPSIE i
+				// nofpuctx:
+				//
+				// The current Go ABI in the presence of the FPU makes the
+				// exception entry and exit very inefficient. If a thread uses
+				// FPU the above code saves the whole FPU context at exception
+				// entry even if the exception handler does not use the FPU. To
+				// prevent the higher priority exception to save the same FPU
+				// context again the above code clears CONTROL.FPCA just after
+				// VPUSH. An another approach can be disabling the FPU at
+				// exception entry and re-enabling it only if the UsageFault
+				// occurs as descibed in ARM Application Note 298. The undoubted
+				// advantages of the ARMv7-M exception model deffinitely require
+				// to follow AAPCS.
+				//
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVM
+				p.Scond |= C_DB | C_WBIT
+				p.From.Type = obj.TYPE_REGLIST
+				p.From.Offset = 0xFF0
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REGSP
+				p = obj.Appendp(p, newprog)
+				p.As = ATST
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0x10
+				p.Reg = REGLINK
+				bne := obj.Appendp(p, newprog)
+				bne.As = ABNE
+				bne.To.Type = obj.TYPE_BRANCH
+				p = obj.Appendp(bne, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_CONTROL
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R0
+				p = obj.Appendp(p, newprog)
+				p.As = ACPSID
+				p = obj.Appendp(p, newprog)
+				p.As = AHWORD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0xED2D
+				p = obj.Appendp(p, newprog)
+				p.As = AHWORD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0x8B10
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_R0
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_CONTROL
+				p = obj.Appendp(p, newprog)
+				p.As = ACPSIE
+				p = obj.Appendp(p, newprog)
+				p.As = obj.ANOP
+				bne.To.SetTarget(p)
+			}
+
+			if cursym.Func().Text.Mark&LEAF != 0 {
+				cursym.Set(obj.AttrLeaf, true)
+				if cursym.Func().Text.From.Sym.NoFrame() {
+					break
+				}
+			}
+
+			if !cursym.Func().Text.From.Sym.NoSplit() {
+				p = c.stacksplit(p, int32(autosize)) // emit split check
+			}
+
+			if autosize <= 255 {
+				// fram size fits into MOVW.W R14,-autosize(SP)
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.Scond |= C_WBIT
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REGLINK
+				p.To.Type = obj.TYPE_MEM
+				p.To.Offset = int64(-autosize)
+				p.To.Reg = REGSP
+				p.Spadj = int32(autosize)
+			} else {
+				// Frame size is too large for a MOVW.W instruction.
+				// Store link register before decrementing SP, so if a signal comes
+				// during the execution of the function prologue, the traceback
+				// code will not see a half-updated stack frame.
+				p = obj.Appendp(p, c.newprog)
+				p.Pos = p.Pos
+				p.As = ASUB
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = int64(autosize)
+				p.Reg = REGSP
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REGTMP
+
+				p = obj.Appendp(p, c.newprog)
+				p.Pos = p.Pos
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REGLINK
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REGTMP
+
+				p = obj.Appendp(p, c.newprog)
+				p.Pos = p.Pos
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REGTMP
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REGSP
+				p.Spadj = int32(autosize)
+			}
+
+			if cursym.Func().Text.From.Sym.ISR() {
+				p = obj.Appendp(p, newprog)
+				p.As = ABL
+				p.To.Type = obj.TYPE_BRANCH
+				p.To.Sym = c.ctxt.Lookup("runtime.identcurcpu")
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_R0
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REGG
+			}
+
+			if cursym.Func().Text.From.Sym.Wrapper() {
+				// if(g->panic != nil && g->panic->argp == FP) g->panic->argp = bottom-of-frame
+				//
+				//	MOVW g_panic(g), R1
+				//	CMP  $0, R1
+				//	BNE checkargp
+				// end:
+				//	NOP
+				// ... function ...
+				// checkargp:
+				//	MOVW panic_argp(R1), R2
+				//	ADD  $(autosize+4), R13, R3
+				//	CMP  R2, R3
+				//	BNE end
+				//	ADD  $4, R13, R4
+				//	MOVW R4, panic_argp(R1)
+				//	B    end
+				//
+				// The NOP is needed to give the jumps somewhere to land.
+				// It is a liblink NOP, not an ARM NOP: it encodes to 0 instruction bytes.
+
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REGG
+				p.From.Offset = 4 * int64(ctxt.Arch.PtrSize) // G.panic
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R1
+
+				p = obj.Appendp(p, newprog)
+				p.As = ACMP
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0
+				p.Reg = REG_R1
+
+				// B.NE checkargp
+				bne := obj.Appendp(p, newprog)
+				bne.As = ABNE
+				bne.To.Type = obj.TYPE_BRANCH
+
+				// end: NOP
+				end := obj.Appendp(bne, newprog)
+				end.As = obj.ANOP
+
+				// find end of function
+				var last *obj.Prog
+				for last = end; last.Link != nil; last = last.Link {
+				}
+
+				// MOVW panic_argp(R1), R2
+				mov := obj.Appendp(last, newprog)
+				mov.As = AMOVW
+				mov.From.Type = obj.TYPE_MEM
+				mov.From.Reg = REG_R1
+				mov.From.Offset = 0 // Panic.argp
+				mov.To.Type = obj.TYPE_REG
+				mov.To.Reg = REG_R2
+
+				// B.NE branch target is MOVW above
+				bne.To.SetTarget(mov)
+
+				// ADD $(autosize+4), R13, R3
+				p = obj.Appendp(mov, newprog)
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = int64(autosize) + 4
+				p.Reg = REG_R13
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R3
+
+				// CMP R2, R3
+				p = obj.Appendp(p, newprog)
+				p.As = ACMP
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_R2
+				p.Reg = REG_R3
+
+				// B.NE end
+				p = obj.Appendp(p, newprog)
+				p.As = ABNE
+				p.To.Type = obj.TYPE_BRANCH
+				p.To.SetTarget(end)
+
+				// ADD $4, R13, R4
+				p = obj.Appendp(p, newprog)
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 4
+				p.Reg = REG_R13
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R4
+
+				// MOVW R4, panic_argp(R1)
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_R4
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REG_R1
+				p.To.Offset = 0 // Panic.argp
+
+				// B end
+				p = obj.Appendp(p, newprog)
+				p.As = AB
+				p.To.Type = obj.TYPE_BRANCH
+				p.To.SetTarget(end)
+
+				// reset for subsequent passes
+				p = end
+			}
+
+		case obj.ARET:
+			nocache(p)
+			if cursym.Func().Text.Mark&LEAF != 0 && autosize == 0 {
+				p.As = AB
+				p.From = obj.Addr{}
+				if p.To.Sym != nil { // retjmp
+					p.To.Type = obj.TYPE_BRANCH
+				} else {
+					p.To.Type = obj.TYPE_MEM
+					p.To.Offset = 0
+					p.To.Reg = REGLINK
+				}
+				break
+			}
+
+			sym := p.To.Sym
+			p.To.Sym = nil
+
+			if autosize <= 255 {
+				// MOVW.P autosize(SP),PC
+				p.As = AMOVW
+				p.Scond |= C_PBIT
+				p.From.Type = obj.TYPE_MEM
+				p.From.Offset = int64(autosize)
+				p.From.Reg = REGSP
+				p.To.Type = obj.TYPE_REG
+				p.To.Offset = 0
+				p.To.Reg = REGPC
+			} else {
+				// MOVW (SP), REGLINK
+				// ADD  autosize, SP
+				// B    (REGLINK)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REGSP
+				p.To.Type = obj.TYPE_REG
+				p.To.Offset = 0
+				p.To.Reg = REGLINK
+				p = obj.Appendp(p, newprog)
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = int64(autosize)
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REGSP
+				p = obj.Appendp(p, newprog)
+				p.As = AB
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REGLINK
+			}
+
+			if cursym.Func().Text.From.Sym.ISR() {
+				if p.As == AMOVW {
+					// MOVW.P autosize(SP),PC -> MOVW.P autosize(SP),LR
+					p.To.Reg = REGLINK
+					p = obj.Appendp(p, newprog)
+				}
+				// emit interrupt handler epilogue:
+				//
+				//  TST  $0x10, LR
+				//  BNE  nofpuctx
+				//  MOVW CONTROL, R0
+				//  TST  $4, R0 // CONTROL.FPCA tells if FPU have been used
+				//  BEQ  nofpuctx
+				//  VPOP [D8-D15]
+				// nofpuctx:
+				//  POP  [R4-R11]
+				//
+				p.As = ATST
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0x10
+				p.Reg = REGLINK
+				bne := obj.Appendp(p, newprog)
+				bne.As = ABNE
+				bne.To.Type = obj.TYPE_BRANCH
+				p = obj.Appendp(bne, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_CONTROL
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R0
+				p = obj.Appendp(p, newprog)
+				p.As = ATST
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 4
+				p.Reg = REG_R0
+				beq := obj.Appendp(p, newprog)
+				beq.As = ABEQ
+				beq.To.Type = obj.TYPE_BRANCH
+				p = obj.Appendp(beq, newprog)
+				p.As = AHWORD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0xECBD
+				p = obj.Appendp(p, newprog)
+				p.As = AHWORD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0x8B10
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVM
+				p.Scond |= C_IA | C_WBIT
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REGSP
+				p.To.Type = obj.TYPE_REGLIST
+				p.To.Offset = 0xFF0
+				bne.To.SetTarget(p)
+				beq.To.SetTarget(p)
+				p = obj.Appendp(p, newprog)
+				p.As = AB
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REGLINK
+			}
+
+			// If there are instructions following this ARET, they come from a
+			// branch with the same stackframe, so no spadj.
+			if sym != nil { // retjmp
+				if p.As == AMOVW {
+					// MOVW.P autosize(SP),PC -> MOVW.P autosize(SP),LR
+					p.To.Reg = REGLINK
+					p = obj.Appendp(p, newprog)
+				}
+				p.As = AB
+				p.To.Type = obj.TYPE_BRANCH
+				p.To.Sym = sym
+			}
+
+		case AADD:
+			if p.From.Type == obj.TYPE_CONST && p.From.Reg == 0 && p.To.Type == obj.TYPE_REG && p.To.Reg == REGSP {
+				p.Spadj = int32(-p.From.Offset)
+			}
+
+		case ASUB:
+			if p.From.Type == obj.TYPE_CONST && p.From.Reg == 0 && p.To.Type == obj.TYPE_REG && p.To.Reg == REGSP {
+				p.Spadj = int32(p.From.Offset)
+			}
+
+		case AMOVW:
+			switch {
+			case p.From.Type == obj.TYPE_ADDR && (p.From.Name == obj.NAME_NONE || p.From.Name == obj.NAME_PARAM || p.From.Name == obj.NAME_AUTO):
+				offset := c.offset(&p.From)
+				switch p.From.Name {
+				case obj.NAME_PARAM, obj.NAME_AUTO:
+					p.Reg = REGSP
+				default:
+					p.Reg = p.From.Reg
+				}
+				p.From.Sym = nil
+				p.From.Name = obj.NAME_NONE
+				if offset == 0 {
+					p.From.Type = obj.TYPE_REG
+					p.From.Reg = p.Reg
+					p.From.Offset = 0
+					p.Reg = 0
+					break
+				}
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Reg = 0
+				p.From.Offset = offset
+				if p.Reg == REGSP && p.To.Reg == REGSP {
+					p.Spadj = int32(-offset)
+				}
+				p.Scond = C_PBIT // preserve flags
+			case (p.Scond&C_WBIT != 0) && p.To.Type == obj.TYPE_MEM && p.To.Reg == REGSP:
+				p.Spadj = int32(-p.To.Offset)
+				if p.To.Offset == -4 && (p.From.Reg <= REG_R7 || p.From.Reg == REG_R14) {
+					// T32 MOVW.W Rt, -4(R13) -> T16 MOVM.DB.W [Rt], (R13) == PUSH [Rt]
+					p.As = AMOVM
+					p.To.Offset = 0
+					p.From.Type = obj.TYPE_REGLIST
+					p.From.Offset = 1 << uint(p.From.Reg&15)
+					p.From.Reg = 0
+					p.Scond = C_DB | C_WBIT
+				}
+			case (p.Scond&C_PBIT != 0) && p.From.Type == obj.TYPE_MEM && p.From.Reg == REGSP:
+				p.Spadj = int32(-p.From.Offset)
+				if p.From.Offset == 4 && (p.To.Reg <= REG_R7 || p.To.Reg == REG_R15) {
+					// T32 MOVW.P 4(R13), Rt -> T16 MOVM.IA.W (R13), [Rt] == POP [Rt]
+					p.As = AMOVM
+					p.From.Offset = 0
+					p.To.Type = obj.TYPE_REGLIST
+					p.To.Offset = 1 << uint(p.To.Reg&15)
+					p.To.Reg = 0
+					p.Scond = C_IA | C_WBIT
+				}
+			}
+
+		case AMOVM:
+			// TODO: convert invalid MOVM with one register to MOVW
+
+		case obj.AGETCALLERPC:
+			if cursym.Leaf() {
+				/* MOVW LR, Rd */
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REGLINK
+				p.Scond = C_PBIT // preserve flags
+			} else {
+				/* MOVW (RSP), Rd */
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REGSP
+			}
+		}
+	}
+}
+
+func (c *Ctx) stacksplit(p *obj.Prog, framesize int32) *obj.Prog {
+	// MOVW g_stackguard(g), R1
+	p = obj.Appendp(p, c.newprog)
+
+	p.As = AMOVW
+	p.From.Type = obj.TYPE_MEM
+	p.From.Reg = REGG
+	p.From.Offset = 2 * int64(c.ctxt.Arch.PtrSize) // G.stackguard0
+	if c.cursym.CFunc() {
+		p.From.Offset = 3 * int64(c.ctxt.Arch.PtrSize) // G.stackguard1
+	}
+	p.To.Type = obj.TYPE_REG
+	p.To.Reg = REG_R1
+
+	// Mark the stack bound check and morestack call async nonpreemptible.
+	// If we get preempted here, when resumed the preemption request is
+	// cleared, but we'll still call morestack, which will double the stack
+	// unnecessarily. See issue #35470.
+	p = c.ctxt.StartUnsafePoint(p, c.newprog)
+
+	if framesize <= objabi.StackSmall {
+		// small stack: SP < stackguard
+		//	CMP	stackguard, SP
+		p = obj.Appendp(p, c.newprog)
+
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_R1
+		p.Reg = REGSP
+	} else if framesize <= objabi.StackBig {
+		// large stack: SP-framesize < stackguard-StackSmall
+		//	ADD $-(framesize-StackSmall), SP, R2
+		//	CMP stackguard, R2
+		p = obj.Appendp(p, c.newprog)
+
+		p.As = AADD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = -(int64(framesize) - objabi.StackSmall)
+		p.Reg = REGSP
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_R2
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_R1
+		p.Reg = REG_R2
+	} else {
+		// Such a large stack we need to protect against wraparound
+		// if SP is close to zero.
+		//	SP-stackguard+StackGuard < framesize + (StackGuard-StackSmall)
+		// The +StackGuard on both sides is required to keep the left side positive:
+		// SP is allowed to be slightly below stackguard. See stack.h.
+		//	CMP     $StackPreempt, R1
+		//	ADD.P.NE  $StackGuard, SP, R2
+		//	SUB.P.NE  R1, R2
+		//	MOVW.NE $(framesize+(StackGuard-StackSmall)), R3
+		//	CMP.NE  R3, R2
+		p = obj.Appendp(p, c.newprog)
+
+		p.As = ACMP
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(uint32(objabi.StackPreempt & (1<<32 - 1)))
+		p.Reg = REG_R1
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = AADD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(objabi.StackGuard)
+		p.Reg = REGSP
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_R2
+		p.Scond = C_PBIT | C_SCOND_NE
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = ASUB
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_R1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_R2
+		p.Scond = C_PBIT | C_SCOND_NE
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = AMOVW
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(framesize) + int64(objabi.StackGuard) - objabi.StackSmall
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_R3
+		p.Scond = C_SCOND_NE
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_R3
+		p.Reg = REG_R2
+		p.Scond = C_SCOND_NE
+	}
+
+	// BLS call-to-morestack
+	bls := obj.Appendp(p, c.newprog)
+	bls.As = ABLS
+	bls.To.Type = obj.TYPE_BRANCH
+
+	end := c.ctxt.EndUnsafePoint(bls, c.newprog, -1)
+
+	var last *obj.Prog
+	for last = c.cursym.Func().Text; last.Link != nil; last = last.Link {
+	}
+
+	// Now we are at the end of the function, but logically
+	// we are still in function prologue. We need to fix the
+	// SP data and PCDATA.
+	spfix := obj.Appendp(last, c.newprog)
+	spfix.As = obj.ANOP
+	spfix.Spadj = -framesize
+
+	pcdata := c.ctxt.EmitEntryStackMap(c.cursym, spfix, c.newprog)
+	pcdata = c.ctxt.StartUnsafePoint(pcdata, c.newprog)
+
+	// MOVW	LR, R3
+	movw := obj.Appendp(pcdata, c.newprog)
+	movw.As = AMOVW
+	movw.From.Type = obj.TYPE_REG
+	movw.From.Reg = REGLINK
+	movw.To.Type = obj.TYPE_REG
+	movw.To.Reg = REG_R3
+
+	bls.To.SetTarget(movw)
+
+	// BL runtime.morestack
+	call := obj.Appendp(movw, c.newprog)
+	call.As = obj.ACALL
+	call.To.Type = obj.TYPE_BRANCH
+	morestack := "runtime.morestack"
+	switch {
+	case c.cursym.CFunc():
+		morestack = "runtime.morestackc"
+	case !c.cursym.Func().Text.From.Sym.NeedCtxt():
+		morestack = "runtime.morestack_noctxt"
+	}
+	call.To.Sym = c.ctxt.Lookup(morestack)
+
+	pcdata = c.ctxt.EndUnsafePoint(call, c.newprog, -1)
+
+	// B start
+	b := obj.Appendp(pcdata, c.newprog)
+	b.As = obj.AJMP
+	b.To.Type = obj.TYPE_BRANCH
+	b.To.SetTarget(c.cursym.Func().Text.Link)
+	b.Spadj = +framesize
+
+	return end
+}
+
+func progedit(ctxt *obj.Link, p *obj.Prog, newprog obj.ProgAlloc) {
+	p.From.Class = 0
+	p.To.Class = 0
+
+	c := &Ctx{ctxt: ctxt, newprog: newprog}
+
+	// Rewrite RSB Rm, Rn, Rd to SUB Rn, Rm, Rd to allow T16 encoding
+	if p.As == ARSB && p.From.Type == obj.TYPE_REG {
+		p.As = ASUB
+		if p.Reg == 0 {
+			p.Reg = p.To.Reg
+		}
+		p.From.Reg, p.Reg = p.Reg, p.From.Reg
+	}
+	// Rewrite 3-arg to 2-arg
+	if p.Reg != 0 && p.To.Type == obj.TYPE_REG && p.To.Reg == p.Reg {
+		p.Reg = 0
+	}
+	// Rewrite TYPE_SHIFT R<<i(R) to TYPE_MEM
+	if p.From.Type == obj.TYPE_SHIFT && p.From.Reg != 0 {
+		if !shifttomem(&p.From) {
+			c.ctxt.Diag("bad src addr mode: %v", p)
+		}
+	} else if p.To.Type == obj.TYPE_SHIFT && p.To.Reg != 0 {
+		if !shifttomem(&p.To) {
+			c.ctxt.Diag("bad dst addr mode: %v", p)
+		}
+	}
+	if p.As == obj.ADUFFZERO || p.As == obj.ADUFFCOPY {
+		// as long we do not support dynamic linking they are identical to BL
+		p.As = ABL
+	}
+	// Rewrite B/BL to symbol as TYPE_BRANCH.
+	if p.As == AB || p.As == ABL || ABEQ <= p.As && p.As <= ACBNZ {
+		if p.To.Type == obj.TYPE_MEM && (p.To.Name == obj.NAME_EXTERN || p.To.Name == obj.NAME_STATIC) && p.To.Sym != nil {
+			p.To.Type = obj.TYPE_BRANCH
+		}
+		return
+	}
+	// Rewrite SLL/SRL/SRA/SRR/ to MOVW
+	if ASLL <= p.As && p.As <= ASRR {
+		Rn := int(p.To.Reg)
+		if p.Reg != 0 {
+			Rn = int(p.Reg)
+			p.Reg = 0
+		}
+		typ := int(p.As - ASLL)
+		p.As = AMOVW
+		if p.From.Type == obj.TYPE_REG {
+			p.From.Offset = int64(int(p.From.Reg)&15<<8 | typ<<5 | 1<<4 | Rn&15)
+		} else {
+			p.From.Offset = int64(int(p.From.Offset)<<7 | typ<<5 | Rn&15)
+		}
+		p.From.Type = obj.TYPE_SHIFT
+		p.From.Reg = 0
+		return
+	}
+	switch p.As {
+	// Rewrite float constants to values stored in memory.
+	case AMOVF:
+		if p.From.Type == obj.TYPE_FCONST && c.chipfloat(p.From.Val.(float64)) < 0 && (c.chipzero(p.From.Val.(float64)) < 0 || p.Scond&C_SCOND != C_SCOND_NONE) {
+			f32 := float32(p.From.Val.(float64))
+			p.From.Type = obj.TYPE_MEM
+			p.From.Sym = ctxt.Float32Sym(f32)
+			p.From.Name = obj.NAME_EXTERN
+			p.From.Offset = 0
+		}
+	case AMOVD:
+		if p.From.Type == obj.TYPE_FCONST && c.chipfloat(p.From.Val.(float64)) < 0 && (c.chipzero(p.From.Val.(float64)) < 0 || p.Scond&C_SCOND != C_SCOND_NONE) {
+			p.From.Type = obj.TYPE_MEM
+			p.From.Sym = ctxt.Float64Sym(p.From.Val.(float64))
+			p.From.Name = obj.NAME_EXTERN
+			p.From.Offset = 0
+		}
+	}
+}
+
+func nocache(p *obj.Prog) {
+	p.Optab = 0
+	p.From.Class = 0
+	if p.GetFrom3() != nil {
+		p.GetFrom3().Class = 0
+	}
+	p.To.Class = 0
+}
+
+func onesCount(u uint) int {
+	n := 0
+	for u != 0 {
+		n += int(u & 1)
+		u >>= 1
+	}
+	return n
+}
+
+func shifttomem(a *obj.Addr) bool {
+	if a.Offset>>4&3 != 0 {
+		return false // Thumb2 supports only LSL
+	}
+	shift := uint(a.Offset) >> 7 & 31
+	if shift > 3 {
+		return false // Thumb2 supports only 2-bit shift
+	}
+	a.Type = obj.TYPE_MEM
+	a.Index = REG_R0 + int16(a.Offset)&15
+	if shift != 0 {
+		a.Scale = 1 << shift
+	}
+	return true
+}
+
+var unaryDst = map[obj.As]bool{
+	AWORD:      true,
+	AB:         true,
+	ABL:        true,
+	ASWI:       true,
+	ABKPT:      true,
+	obj.AUNDEF: true,
+}
+
+var Link = obj.LinkArch{
+	Arch:           sys.ArchThumb,
+	Init:           buildop,
+	Preprocess:     preprocess,
+	Assemble:       span,
+	Progedit:       progedit,
+	UnaryDst:       unaryDst,
+	DWARFRegisters: ARMDWARFRegisters,
+}
diff --git a/src/cmd/internal/obj/thumb/optab.go b/src/cmd/internal/obj/thumb/optab.go
new file mode 100644
index 0000000000..12224bf0a0
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/optab.go
@@ -0,0 +1,1905 @@
+// Inferno utils/5l/span.c
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5l/span.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"cmd/internal/obj"
+	"cmd/internal/objabi"
+	"log"
+)
+
+type asmoutFunc func(c *Ctx, p *obj.Prog, out []uint16) int
+
+type Optab struct {
+	as     obj.As
+	a1     Aclass
+	a2     Aclass
+	a3     Aclass
+	size   uint8 // instr_num<<4 | code_size
+	flag   uint8
+	rscond uint8 // flags required by the instruction
+	oscond uint8 // optional flags accepted by the instruction
+	asmout asmoutFunc
+}
+
+const (
+	LFROM = 1 << iota
+	LTO
+	VALID
+	NOIT
+)
+
+const pcoff = 4 // in Thumb mode PC points 4 bytes forward
+
+// optab contains description of Thumb2 instruction set in slightly compressed form. buildop
+// decompress it to oprange keeping the order of instruction variants unchanged. oplook
+// returns the first matched description so the order matters. Current order ensures ISA
+// requirements and prefers 16-bit variants.
+//
+// optab is manual translation of instr_group.txt - please keep them in sync.
+// TODO: automatic translation of instr_group.txt.
+var optab = [...]Optab{
+	// as, a1, a2, a3, size, flag, rscond, oscond, asmout
+
+	{obj.ATEXT, C_LOREG, C_NONE, C_TEXTSIZE, 0, 0, 0, 0, nil},
+	{obj.AFUNCDATA, C_LCON, C_NONE, C_LOREG, 0, 0, 0, 0, nil},
+	{obj.APCDATA, C_LCON, C_NONE, C_LCON, 0, 0, 0, 0, nil},
+	{obj.ANOP, C_NONE, C_NONE, C_NONE, 0, VALID, 0, 0, nil},
+	{obj.ANOP, C_LCON, C_NONE, C_NONE, 0, VALID, 0, 0, nil},
+	{obj.ANOP, C_REG, C_NONE, C_NONE, 0, VALID, 0, 0, nil},
+	{obj.ANOP, C_FREG, C_NONE, C_NONE, 0, VALID, 0, 0, nil},
+
+	{AWORD, C_NONE, C_NONE, C_LCON, 0x14, 0, 0, 0, _WORD__u32},
+	{AWORD, C_NONE, C_NONE, C_LOREG, 0x14, 0, 0, 0, _WORD__u32},
+
+	{AHWORD, C_U16CON, C_NONE, C_NONE, 0x12, 0, 0, 0, _HWORD__u16},
+
+	{AADD, C_REG, C_NONE, C_REG, 0x12, 0, 0, C_PBIT, _ADD__Rm__Rdn},        // ADD Rm, Rdn
+	{AADD, C_U8CON2, C_SP, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u8_2__R13__Rd}, // ADD u8<<2, R13, Rd
+	{AADD, C_U8CON2, C_PC, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u8_2__R13__Rd}, // ADD u8<<2, R15, Rd
+	{as: obj.AXXX},
+
+	{AADD, C_RLO, C_RLO, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__Rm__Rn__Rd},                    // ADD   Rm, Rn, Rd
+	{AADD, C_RLO, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__Rm__Rn__Rd},                   // ADD   Rm, Rdn
+	{AADD, C_SHIFTI, C_REG, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd},  // ADD.s Rm<v>u5, Rn, Rd
+	{AADD, C_SHIFTI, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd}, // ADD.s Rm<v>u5, Rdn
+	{AADD, C_U3CON, C_RLO, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u3__Rn__Rd},                  // ADD   u3, Rn, Rd
+	{AADD, C_U8CON, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u8__Rdn},                    // ADD   u8, Rdn
+	{AADD, C_U7CON2, C_NONE, C_SP, 0x12, 0, C_SBIT, 0, _ADD__u7_2__SP},                   // ADD   u7<<2, R13
+	{AADD, C_U12CON, C_REG, C_REG, 0x14, 0, 0, C_PBIT, _ADD__u12__Rn__Rd},                // ADD   u12, Rn, Rd
+	{AADD, C_U12CON, C_NONE, C_REG, 0x14, 0, 0, C_PBIT, _ADD__u12__Rn__Rd},               // ADD   u12, Rdn
+	{AADD, C_E32CON, C_REG, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},      // ADD.s e32, Rn, Rd
+	{AADD, C_E32CON, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},     // ADD.s e32, Rdn
+	{as: ASUB},
+
+	{AADD, C_LCON, C_NONE, C_REG, 0x26, LFROM, 0, C_PBIT, _ADD__lit__Rdn}, // ADD   lit, Rdn
+	{as: obj.AXXX},
+
+	{AADD, C_LCON, C_RLO, C_RLO, 0x26, LFROM, C_SBIT, 0, _ADD__lit__Rn__Rd},           // ADD   lit, Rn, Rd
+	{AADD, C_LCON, C_NONE, C_RLO, 0x26, LFROM, C_SBIT, 0, _ADD__lit__Rn__Rd},          // ADD   lit, Rdn
+	{AADD, C_LCON, C_REG, C_REG, 0x28, LFROM, 0, C_SBIT | C_PBIT, _ADD__lit__Rn__Rd},  // ADD.s lit, Rn, Rd
+	{AADD, C_LCON, C_NONE, C_REG, 0x28, LFROM, 0, C_SBIT | C_PBIT, _ADD__lit__Rn__Rd}, // ADD.s lit, Rdn
+	{as: ASUB},
+
+	{AAND, C_RLO, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _AND__Rm__Rdn}, // AND Rm, Rdn
+	{as: AADC},
+	{as: ABIC},
+	{as: AEOR},
+	{as: AORR},
+	{as: ASBC},
+	{as: AMUL},
+	{as: AMVN},
+
+	{ARSB, C_ZCON, C_RLO, C_RLO, 0x12, 0, C_SBIT, 0, _AND__Rm__Rdn}, // RSB $0, Rn, Rd ; old NEG
+
+	{AAND, C_SHIFTI, C_REG, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd},  // AND.s Rm<v>u5, Rn, Rd
+	{AAND, C_SHIFTI, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd}, // AND.s Rm<v>u5, Rdn
+	{AAND, C_E32CON, C_REG, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},      // AND.s e32, Rn, Rd
+	{AAND, C_E32CON, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},     // AND.s e32, Rdn
+	{as: AADC},
+	{as: ABIC},
+	{as: AEOR},
+	{as: AORR},
+	{as: ARSB},
+	{as: ASBC},
+	{as: AORN},
+
+	{AAND, C_LCON, C_NONE, C_RLO, 0x26, LFROM, C_SBIT, 0, _ADD__lit__Rdn}, // AND lit, Rdn
+	{as: AADC},
+	{as: ABIC},
+	{as: AEOR},
+	{as: AORR},
+	{as: ASBC},
+
+	{AAND, C_LCON, C_REG, C_REG, 0x28, LFROM, 0, C_SBIT | C_PBIT, _ADD__lit__Rn__Rd},  // AND.s lit, Rn, Rd
+	{AAND, C_LCON, C_NONE, C_REG, 0x28, LFROM, 0, C_SBIT | C_PBIT, _ADD__lit__Rn__Rd}, // AND.s lit, Rdn
+	{as: AADC},
+	{as: ABIC},
+	{as: AEOR},
+	{as: AORR},
+	{as: ARSB},
+	{as: ASBC},
+	{as: AORN},
+
+	{AMOVW, C_REG, C_NONE, C_REG, 0x12, 0, 0, C_PBIT, _MOVW__Rm__Rd},                    // MOVW Rm, Rd
+	{AMOVW, C_RLO, C_NONE, C_RLO, 0x12, NOIT, C_SBIT, 0, _MOVW__Rm__Rd},                 // MOVW Rm, Rd
+	{AMOVW, C_SHIFTILO, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _MOVW__Rm_v_u5__Rd},          // MOVW Rm<v>u5, Rd ; vv != 11
+	{AMOVW, C_SHIFTRLO, C_NONE, C_NONE, 0x12, 0, C_SBIT, 0, _MOVW__Rdn_v_Rm__Rdn},       // MOVW Rdn<v>Rm, Rdn
+	{AMOVW, C_SHIFTR, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _MOVWs__Rn_v_Rm__Rd},  // MOVW.s Rn<v>Rm, Rd
+	{AMOVW, C_SHIFTR, C_NONE, C_NONE, 0x14, 0, 0, C_SBIT | C_PBIT, _MOVWs__Rn_v_Rm__Rd}, // MOVW.s Rn<v>Rm, Rd
+
+	{AMOVW, C_U8CON, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u8__Rdn},     // MOVW u8, Rd
+	{AMOVW, C_U16CON, C_NONE, C_REG, 0x14, 0, 0, C_PBIT, _MOVW__uyz16__Rd}, // MOVW uyz16, Rd
+	{AMOVT, C_U16CON, C_NONE, C_REG, 0x14, 0, 0, C_PBIT, _MOVW__uyz16__Rd}, // MOVT uyz16, Rd
+
+	{AMOVW, C_SHIFTI, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd}, // MOVW.s Rm<v>u5, Rd
+	{AMOVW, C_E32CON, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},     // MOVW.s e32, Rd
+	{AMOVW, C_LCON, C_NONE, C_REG, 0x14, LFROM, 0, C_PBIT, _MOVW__lit__Rd},                // MOVW lit, Rd
+	{as: AMVN}, // MVN lit, Rd is converted to MOVW -lit, Rd
+
+	{AMOVH, C_RLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVH__Rm__Rd},        // MOVH Rm, Rd
+	{AMOVH, C_SHIFTI, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVH__Rm_rot__Rd}, // MOVH Rm@>rot, Rd
+	{as: AMOVB},
+	{as: AMOVHU},
+	{as: AMOVBU},
+
+	{AMOVW, C_SPEC, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__SYSm__Rd}, // MOVW SYSm, Rd
+	{AMOVW, C_REG, C_NONE, C_SPEC, 0x14, 0, 0, 0, _MOVW__SYSm__Rd}, // MOVW Rn, SYSm
+
+	{AMOVW, C_FCR, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__FPSCR__Rt}, // MOVW FPSCR, Rt
+	{AMOVW, C_REG, C_NONE, C_FCR, 0x14, 0, 0, 0, _MOVW__FPSCR__Rt}, // MOVW Rt, FPSCR
+
+	{AMUL, C_REG, C_REG, C_REG, 0x14, 0, 0, C_PBIT, _MUL__Rm__Rn__Rd},  // MUL Rm, Rn, Rd
+	{AMUL, C_REG, C_NONE, C_REG, 0x14, 0, 0, C_PBIT, _MUL__Rm__Rn__Rd}, // MUL Rm, Rdn
+	{as: ADIV},
+	{as: ADIVU},
+
+	{AMULL, C_REG, C_REG, C_REGREG, 0x14, 0, 0, 0, _MULL__Rm__Rn__Rdh_Rdl}, // MULL Rm, Rn, (Rdh, Rdl)
+	{as: AMULLU},
+	{as: AMULAL},
+	{as: AMULALU},
+
+	{AMULA, C_REG, C_REG, C_REGREG2, 0x14, 0, 0, 0, _MULA__Rm__Rn__Ra__Rd}, // MULA Rm, Rn, Ra, Rd
+	{as: AMULS},
+	{as: AMULAWB},
+	{as: AMULAWT},
+
+	{AREV, C_RLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _REV__Rm__Rd}, // REV Rm, Rd
+	{as: AREV16},
+	{as: AREVSH},
+
+	{ACLZ, C_REG, C_NONE, C_REG, 0x14, 0, 0, 0, _CLZ__Rm__Rd}, // CLZ Rm, Rd
+	{as: AREV},
+	{as: AREV16},
+	{as: ARBIT},
+	{as: AREVSH},
+
+	{ASEL, C_REG, C_REG, C_REG, 0x14, 0, 0, 0, _CLZ__Rm__Rd},  // SEL Rm, Rn, Rd ; DSP extension
+	{ASEL, C_REG, C_NONE, C_REG, 0x14, 0, 0, 0, _CLZ__Rm__Rd}, // SEL Rm, Rd     ; DSP extension
+
+	{ABFX, C_LCON, C_REG, C_REG, 0x14, 0, 0, 0, _BFX__width__ulsb__Rn__Rd},  // BFX width, ulsb, Rn, Rd
+	{ABFX, C_LCON, C_NONE, C_REG, 0x14, 0, 0, 0, _BFX__width__ulsb__Rn__Rd}, // BFX width, ulsb, Rdn
+	{as: ABFXU},
+	{as: ABFI},
+
+	{ABFC, C_LCON, C_NONE, C_REG, 0x14, 0, 0, 0, _BFX__width__ulsb__Rn__Rd}, // BFC width, ulsb, Rd
+
+	{ATST, C_RLO, C_RLO, C_NONE, 0x12, 0, 0, 0, _TST__Rm__Rn}, // TST Rm, Rn
+	{as: ACMP},
+	{as: ACMN},
+
+	{ACMP, C_REG, C_REG, C_NONE, 0x12, 0, 0, 0, _CMP__Rm__Rn},   // CMP Rm, Rn
+	{ACMP, C_U8CON, C_RLO, C_NONE, 0x12, 0, 0, 0, _CMP__u8__Rn}, // CMP u8, Rn
+
+	{ATST, C_SHIFTI, C_REG, C_NONE, 0x14, 0, 0, 0, _TST__Rm_v_u5__Rn}, // TST Rm<v>u5, Rn
+	{ATST, C_E32CON, C_REG, C_NONE, 0x14, 0, 0, 0, _TST__e32__Rn},     // TST e32, Rn
+	{as: ATEQ},
+	{as: ACMN},
+	{as: ACMP},
+
+	{ACMN, C_LCON, C_RLO, C_NONE, 0x26, LFROM, 0, 0, _CMN__lit__Rn}, // CMN lit, Rn
+	{as: ATST},
+	{as: ACMP},
+
+	{ACMP, C_LCON, C_REG, C_NONE, 0x26, LFROM, 0, 0, _CMP__lit__Rn}, // CMP lit, Rn
+
+	{ATST, C_LCON, C_REG, C_NONE, 0x28, LFROM, 0, 0, _TST__lit__Rn}, // TST lit, Rn
+	{as: ATEQ},
+	{as: ACMN},
+
+	{AMOVW, C_WORLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt},  // MOVW  u5<<2(Rn), Rt
+	{AMOVW, C_WOSP, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u8_2_R13__Rt},  // MOVW  u8<<2(R13), Rt
+	{AMOVW, C_WOPC, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u8_2_R13__Rt},  // MOVW  u8<<2(R15), Rt
+	{AMOVHU, C_HORLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVHU u5<<1(Rn), Rt
+	{AMOVBU, C_BORLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVBU u5(Rn), Rt
+
+	{AMOVW, C_RLO, C_NONE, C_WORLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVW Rt, u5<<2(Rn)
+	{AMOVW, C_RLO, C_NONE, C_WOSP, 0x12, 0, 0, 0, _MOVW__u8_2_R13__Rt}, // MOVW Rt, u8<<2(R13)
+	{AMOVH, C_RLO, C_NONE, C_HORLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVH Rt, u5<<1(Rn)
+	{as: AMOVHU},
+
+	{AMOVB, C_RLO, C_NONE, C_BORLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVB Rt, u5(Rn)
+	{as: AMOVBU},
+
+	{AMOVW, C_RORLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__Rn_Rm__Rt},                 // MOVW (Rn)(Rm), Rt
+	{AMOVW, C_ROREG, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__Rn_Rm_1_u2__Rt},            // MOVW (Rn)(Rm*1<<u2), Rt
+	{AMOVW, C_SOPC, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__s12_Rn__Rt},                 // MOVW ±u12(R15), Rt
+	{AMOVW, C_UOREG, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__s12_Rn__Rt},                // MOVW u12(Rn), Rt
+	{AMOVW, C_SOREG, C_NONE, C_REG, 0x14, 0, 0, C_PBIT | C_WBIT, _MOVWpw__s8_Rn__Rt}, // MOVW.p.w ±u8(Rn), Rt
+	{as: AMOVH},
+	{as: AMOVHU},
+	{as: AMOVB},
+	{as: AMOVBU},
+
+	{AMOVW, C_LORLO, C_NONE, C_RLO, 0x26, LFROM, 0, 0, _MOVW__lit_Rn__Rt}, // MOVW lit(Rn), Rt
+	{as: AMOVHU},
+	{as: AMOVBU},
+
+	{AMOVW, C_LOREG, C_NONE, C_REG, 0x28, LFROM, 0, 0, _MOVW__lit_Rn__Rt}, // MOVW lit(Rn), Rt
+	{as: AMOVH},
+	{as: AMOVHU},
+	{as: AMOVB},
+	{as: AMOVBU},
+
+	{AMOVW, C_RLO, C_NONE, C_RORLO, 0x12, 0, 0, 0, _MOVW__Rn_Rm__Rt},                 // MOVW Rt, (Rn)(Rm)
+	{AMOVW, C_REG, C_NONE, C_ROREG, 0x14, 0, 0, 0, _MOVW__Rn_Rm_1_u2__Rt},            // MOVW Rt, (Rn)(Rm*1<<u2)
+	{AMOVW, C_REG, C_NONE, C_UOREG, 0x14, 0, 0, 0, _MOVW__s12_Rn__Rt},                // MOVW Rt, u12(Rn)
+	{AMOVW, C_REG, C_NONE, C_SOREG, 0x14, 0, 0, C_PBIT | C_WBIT, _MOVWpw__s8_Rn__Rt}, // MOVW.p.w Rt, ±u8(Rn)
+	{AMOVW, C_RLO, C_NONE, C_LORLO, 0x26, LTO, 0, 0, _MOVW__lit_Rn__Rt},              // MOVW Rt, lit(Rn)
+	{AMOVW, C_REG, C_NONE, C_LOREG, 0x28, LTO, 0, 0, _MOVW__lit_Rn__Rt},              // MOVW Rt, lit(Rn)
+	{as: AMOVH},
+	{as: AMOVHU},
+	{as: AMOVB},
+	{as: AMOVBU},
+
+	{AADDF, C_FREG, C_FREG, C_FREG, 0x14, 0, 0, 0, _ADDF__Fm__Fn__Fd}, // ADDF Fm, Fn, Fd
+	{AADDF, C_FREG, C_NONE, C_FREG, 0x14, 0, 0, 0, _ADDF__Fm__Fn__Fd}, // ADDF Fm, Fdn
+	{as: AADDD},
+	{as: ASUBF},
+	{as: ASUBD},
+	{as: AMULF},
+	{as: AMULD},
+	{as: ADIVF},
+	{as: ADIVD},
+	{as: AMULAF},
+	{as: AMULAD},
+	{as: AMULSF},
+	{as: AMULSD},
+	{as: ANMULF},
+	{as: ANMULD},
+
+	{ASQRTF, C_FREG, C_NONE, C_FREG, 0x14, 0, 0, 0, _SQRTF__Fm__Fd}, // SQRTF Fm, Fd
+	{as: ASQRTD},
+	{as: ANEGF},
+	{as: ANEGD},
+	{as: AMOVF},
+	{as: AMOVD},
+	{as: AABSF},
+	{as: AABSD},
+	{as: AMOVFD},
+	{as: AMOVDF},
+
+	{AMOVW, C_FREG, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__Fm__Rd}, // MOVW Fm, Rd},
+	{AMOVW, C_REG, C_NONE, C_FREG, 0x14, 0, 0, 0, _MOVW__Fm__Rd}, // MOVW Rt, Fd},
+
+	{AMOVFW, C_FREG, C_NONE, C_REG, 0x28, 0, 0, C_UBIT, _MOVFW__Fm__Rd}, // MOVFW Fm, Rd
+	{as: AMOVDW},
+	{AMOVWF, C_REG, C_NONE, C_FREG, 0x28, 0, 0, C_UBIT, _MOVFW__Fm__Rd}, // MOVWF Rm, Fd
+	{as: AMOVWD},
+
+	{ACMPF, C_FREG, C_FREG, C_NONE, 0x28, 0, 0, 0, _CMPF__Fm__Fd}, // CMPF Fm, Fd
+	{ACMPF, C_FREG, C_NONE, C_NONE, 0x28, 0, 0, 0, _CMPF__Fm__Fd}, // CMPF Fd
+	{as: ACMPD},
+
+	{AMOVF, C_FOREG, C_NONE, C_FREG, 0x14, 0, 0, 0, _MOVF__s8_2_Rn__Fd},    // MOVF ±u8<<2(Rn), Fd
+	{AMOVF, C_FREG, C_NONE, C_FOREG, 0x14, 0, 0, 0, _MOVF__s8_2_Rn__Fd},    // MOVF Fd, ±u8<<2(Rn)
+	{AMOVF, C_LOREG, C_NONE, C_FREG, 0x3A, LFROM, 0, 0, _MOVF__lit_Rn__Rt}, // MOVF lit(Rn), Rt
+	{AMOVF, C_FREG, C_NONE, C_LOREG, 0x3A, LTO, 0, 0, _MOVF__lit_Rn__Rt},   // MOVF Rt, lit(Rn)
+	{AMOVF, C_SFCON, C_NONE, C_FREG, 0x14, 0, 0, 0, _MOVF__f8__Fd},         // MOVF f8, Fd
+	{AMOVF, C_ZFCON, C_NONE, C_FREG, 0x28, 0, 0, 0, _MOVF__0__Fd},          // MOVF 0, Fd
+	{as: AMOVD},
+
+	{AIT, C_IT, C_NONE, C_NONE, 0x12, 0, 0, 0, _ITmask__firstcond}, // ITmask firstcond
+	{as: AITT},
+	{as: AITE},
+	{as: AITTT},
+	{as: AITET},
+	{as: AITTE},
+	{as: AITEE},
+	{as: AITTTT},
+	{as: AITETT},
+	{as: AITTET},
+	{as: AITEET},
+	{as: AITTTE},
+	{as: AITETE},
+	{as: AITTEE},
+	{as: AITEEE},
+
+	{AB, C_NONE, C_NONE, C_S11BRA, 0x12, 0, 0, 0, _B__i11_1}, // B i11<<1
+
+	{ACBZ, C_RLO, C_NONE, C_U6BRA, 0x12, 0, 0, 0, _CBZ__Rn__u6_1}, // CBZ Rn, u6<<1
+	{as: ACBNZ},
+
+	{AB, C_NONE, C_NONE, C_ZOREG, 0x12, 0, 0, 0, _B__Rm},      // B (Rm)  ; ARM: BX Rm
+	{AB, C_NONE, C_NONE, C_S24BRA, 0x14, 0, 0, 0, _B__ji24_1}, // B ji24<<1
+	{as: ABL},
+
+	{ABEQ, C_NONE, C_NONE, C_S8BRA, 0x12, 0, 0, 0, _Bcond__i8_1},    // Bcond i8<<1
+	{ABEQ, C_NONE, C_NONE, C_S20BRA, 0x14, 0, 0, 0, _Bcond__ji20_1}, // Bcond ji20<<1
+	{as: ABNE},
+	{as: ABCS},
+	{as: ABHS},
+	{as: ABCC},
+	{as: ABLO},
+	{as: ABMI},
+	{as: ABPL},
+	{as: ABVS},
+	{as: ABVC},
+	{as: ABHI},
+	{as: ABLS},
+	{as: ABGE},
+	{as: ABLT},
+	{as: ABGT},
+	{as: ABLE},
+
+	{ATBB, C_REG, C_REG, C_NONE, 0x14, 0, 0, 0, _TBB__Rm__Rn}, // TBB Rm, Rn
+	{as: ATBH},
+
+	{ASWI, C_NONE, C_NONE, C_U8CON, 0x12, 0, 0, 0, _SWI__u8}, // SWI u8
+	{ASWI, C_NONE, C_NONE, C_NONE, 0x12, 0, 0, 0, _SWI__u8},  // SWI
+	{as: ABKPT},
+	{as: obj.AUNDEF},
+
+	{AMOVM, C_LISTLOLR, C_NONE, C_ZOSP, 0x12, 0, C_DB | C_WBIT, 0, _PUSH__reglist}, // PUSH reglist (MOVM.DB.W)
+	{AMOVM, C_LISTLO, C_NONE, C_ZORLO, 0x12, 0, C_IA | C_WBIT, 0, _MOVM_IAW},       // MOVM.IA.W reglist, (Rn)
+	{AMOVM, C_LIST, C_NONE, C_LOREG, 0x14, 0, C_IA, C_WBIT, _MOVM_IAw},             // MOVM.IA.w reglist, (Rn)
+	{AMOVM, C_LIST, C_NONE, C_LOREG, 0x14, 0, C_DB, C_WBIT, _MOVM_IAw},             // MOVM.DB.w reglist, (Rn)
+
+	{AMOVM, C_ZOSP, C_NONE, C_LISTLOPC, 0x12, 0, C_IA | C_WBIT, 0, _PUSH__reglist}, // POP reglist (MOVM.IA.W)
+	{AMOVM, C_ZORLO, C_NONE, C_LISTLO, 0x12, 0, C_IA, C_WBIT, _MOVM_IAW},           // MOVM.IA.W (Rn), reglist
+	{AMOVM, C_LOREG, C_NONE, C_LIST, 0x14, 0, C_IA, C_WBIT, _MOVM_IAw},             // MOVM.IA.w (Rn), reglist
+	{AMOVM, C_LOREG, C_NONE, C_LIST, 0x14, 0, C_DB, C_WBIT, _MOVM_IAw},             // MOVM.DB.w (Rn), reglist
+
+	{ALDREX, C_LOREG, C_NONE, C_REG, 0x14, 0, 0, 0, _LDREX__u8_2_Rn__Rt}, // LDREX u8<<2(Rn), Rt
+	{ASTREX, C_LOREG, C_REG, C_REG, 0x14, 0, 0, 0, _LDREX__u8_2_Rn__Rt},  // STREX Rt, u8<<2(Rn), Rd
+
+	{ALDREXB, C_LOREG, C_NONE, C_REG, 0x14, 0, 0, 0, _LDREXB__Rn__Rt}, // LDREXB (Rn), Rt
+	{as: ALDREXH},
+
+	{ASTREXB, C_LOREG, C_REG, C_REG, 0x14, 0, 0, 0, _LDREXB__Rn__Rt}, // STREXB Rt, (Rn), Rd
+	{as: ASTREXH},
+
+	{ANOP2, C_NONE, C_NONE, C_NONE, 0x12, 0, 0, 0, _NOP2},
+	{as: AYIELD},
+	{as: AWFE},
+	{as: AWFI},
+	{as: ASEV},
+
+	{ACPSID, C_NONE, C_NONE, C_NONE, 0x12, 0, 0, 0, _CPSID},
+	{as: ACPSIE},
+
+	{ANOP4, C_NONE, C_NONE, C_NONE, 0x14, 0, 0, 0, _NOP4},
+	{as: ACLREX},
+
+	{ADSB, C_NONE, C_NONE, C_NONE, 0x14, 0, 0, 0, _DSB}, // DSB
+	{ADSB, C_MB, C_NONE, C_NONE, 0x14, 0, 0, 0, _DSB},   // DSB opt
+	{as: ADMB},
+	{as: AISB},
+}
+
+// 16-bit instructions
+
+// 0100 0100 dmmm mddd
+func _ADD__Rm__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	out[0] = uint16(0x44<<8 | Rd&8<<4 | Rm&15<<3 | Rd&7)
+	return 2
+}
+
+// 0001 10xm mmnn nddd
+func _ADD__Rm__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x1800
+	if p.As == ASUB {
+		o1 |= 0x0200
+	}
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg)
+	}
+	out[0] = uint16(o1 | Rm&7<<6 | Rn&7<<3 | Rd&7)
+	return 2
+}
+
+// 0001 11xu uunn nddd
+func _ADD__u3__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x1C00
+	if p.As == ASUB {
+		o1 |= 0x0200
+	}
+	u3 := int(p.From.Offset)
+	Rn := int(p.Reg)
+	Rd := int(p.To.Reg)
+	out[0] = uint16(o1 | u3<<6 | Rn&7<<3 | Rd&7)
+	return 2
+}
+
+// 1011 0000 xuuu uuuu
+func _ADD__u7_2__SP(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xB000
+	if p.As == ASUB {
+		o1 |= 0x0080
+	}
+	u7 := int(p.From.Offset) >> 2
+	out[0] = uint16(o1 | u7)
+	return 2
+}
+
+// 001x xddd uuuu uuuu
+func _ADD__u8__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x3000 // AADD
+	switch p.As {
+	case AMOVW:
+		o1 = 0x2000
+	case ASUB:
+		o1 = 0x3800
+	}
+	Rd := int(p.To.Reg)
+	u8 := int(p.From.Offset)
+	out[0] = uint16(o1 | Rd&7<<8 | u8)
+	return 2
+}
+
+// 1010 xddd uuuu uuuu
+func _ADD__u8_2__R13__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xA000
+	if p.Reg == REGSP {
+		o1 |= 0x0800
+	}
+	Rd := int(p.To.Reg)
+	u8 := int(p.From.Offset) >> 2
+	out[0] = uint16(o1 | Rd&7<<8 | u8)
+	return 2
+}
+
+// 0100 00xx xxmm mddd
+func _AND__Rm__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	var o1 int
+	switch p.As {
+	case AAND:
+		o1 = 0x4000
+	case AADC:
+		o1 = 0x4140
+	case ABIC:
+		o1 = 0x4380
+	case AEOR:
+		o1 = 0x4040
+	case AORR:
+		o1 = 0x4300
+	case ARSB:
+		o1 = 0x4240
+		Rm = int(p.Reg)
+	case ASBC:
+		o1 = 0x4180
+	case AMUL:
+		o1 = 0x4340
+	default: // AMVN
+		o1 = 0x43C0
+	}
+	out[0] = uint16(o1 | Rm&7<<3 | Rd&7)
+	return 2
+}
+
+// 1110 0iii iiii iiii
+func _B__i11_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	v := int(p.To.Target().Pc-p.Pc-pcoff) >> 1
+	out[0] = uint16(0xE000 | v&0x7FF)
+	return 2
+}
+
+// 0100 0111 xmmm m000
+func _B__Rm(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x4700 | int(p.To.Reg)&15<<3
+	if p.As == ABL {
+		o1 |= 0x0080
+		rel := obj.Addrel(c.cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 0
+		rel.Type = objabi.R_CALLIND
+	}
+	out[0] = uint16(o1)
+	return 2
+}
+
+// 1101 cccc iiii iiii
+func _Bcond__i8_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	v := int(p.To.Target().Pc-p.Pc-pcoff) >> 1
+	out[0] = uint16(0xD000 | obcond(p.As)<<8 | v&0xFF)
+	return 2
+}
+
+// 1011 x0u1 uuuu unnn
+func _CBZ__Rn__u6_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xB100
+	if p.As == ACBNZ {
+		o1 |= 0x0800
+	}
+	Rn := int(p.From.Reg & 7)
+	v := int(p.To.Target().Pc-p.Pc-4) >> 1
+	out[0] = uint16(o1 | v&0x20<<4 | v&0x1F<<3 | Rn)
+	return 2
+}
+
+// 0100 0101 nmmm mnnn
+func _CMP__Rm__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	out[0] = uint16(0x4500 | Rn&8<<4 | Rm&15<<3 | Rn&7)
+	return 2
+}
+
+// 0010 1nnn uuuu uuuu
+func _CMP__u8__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x2800
+	Rn := int(p.Reg)
+	u8 := int(p.From.Offset)
+	out[0] = uint16(o1 | Rn&7<<8 | u8)
+	return 2
+}
+
+// 1011 1111 cccc mmmm
+func _ITmask__firstcond(c *Ctx, p *obj.Prog, out []uint16) int {
+	out[0] = uint16(0xBF00 | int(p.Scond)<<4 | int(p.Mark))
+	return 2
+}
+
+// 1011 0010 xxmm mddd
+func _MOVH__Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xB200 // AMOVH
+	switch p.As {
+	case AMOVB:
+		o1 |= 0x0040
+	case AMOVHU:
+		o1 |= 0x0080
+	case AMOVBU:
+		o1 |= 0x00C0
+	}
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	out[0] = uint16(o1 | Rm&7<<3 | Rd&7)
+	return 2
+}
+
+// 1100 xnnn rrrr rrrr
+func _MOVM_IAW(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xC000
+	rlist, Rn := int(p.From.Offset), int(p.To.Reg)
+	if p.To.Type == obj.TYPE_REGLIST {
+		if !c.checkldm(p) {
+			return 0
+		}
+		rlist, Rn = int(p.To.Offset), int(p.From.Reg)
+		o1 |= 0x0800
+	}
+	out[0] = uint16(o1 | Rn&15<<8 | rlist)
+	return 2
+}
+
+// 0100 000x xxmm mddd (xxx => vv)
+func _MOVW__Rdn_v_Rm__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rdn, typ, Rm := shiftr(int(p.From.Offset))
+	var o1 int
+	switch typ {
+	case 0: // <<
+		o1 = 0x4080
+	case 1: // >>
+		o1 = 0x40C0
+	case 2: // ->
+		o1 = 0x4100
+	default: // @>
+		o1 = 0x41C0
+	}
+	out[0] = uint16(o1 | Rm<<3 | Rdn)
+	return 2
+}
+
+// 0100 0110 dmmm mddd
+// 0000 0000 00mm mddd
+func _MOVW__Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	if p.Scond&C_SBIT == 0 {
+		out[0] = uint16(0x4600 | Rd&8<<4 | Rm&15<<3 | Rd&7)
+	} else {
+		out[0] = uint16(0x0000 | Rm&7<<3 | Rd&7)
+	}
+	return 2
+}
+
+// 000v vuuu uumm mddd (vv != 11)
+func _MOVW__Rm_v_u5__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm, typ, count := shifti(int(p.From.Offset))
+	Rd := int(p.To.Reg) & 7
+	out[0] = uint16(typ<<11 | count<<6 | Rm<<3 | Rd)
+	return 2
+}
+
+// 0101 xxxm mmnn nttt
+func _MOVW__Rn_Rm__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1 int
+	mem, r := &p.From, &p.To
+	if r.Type == obj.TYPE_REG {
+		switch p.As {
+		case AMOVW: // LDR
+			o1 = 0x5800
+		case AMOVB: // LDRSB
+			o1 = 0x5600
+		case AMOVBU: // LDRB
+			o1 = 0x5C00
+		case AMOVH: // LDRSH
+			o1 = 0x5E00
+		default: // AMOVHU (LDRH)
+			o1 = 0x5A00
+		}
+	} else {
+		mem, r = r, mem
+		switch p.As {
+		case AMOVW: // STR
+			o1 = 0x5000
+		case AMOVB, AMOVBU: // STRB
+			o1 = 0x5400
+		default: // AMOVH, AMOVHU (STRH)
+			o1 = 0x5200
+		}
+	}
+	Rn := int(mem.Reg)
+	Rm := int(mem.Index)
+	Rt := int(r.Reg)
+	out[0] = uint16(o1 | Rm&7<<6 | Rn&7<<3 | Rt&7)
+	return 2
+}
+
+// xxxx xuuu uunn nttt
+func _MOVW__u5_2_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1 int
+	var shift uint
+	mem, r := &p.From, &p.To
+	if r.Type == obj.TYPE_REG {
+		switch p.As {
+		case AMOVW: // LDR
+			o1 = 0x6800
+			shift = 2
+		case AMOVBU: // LDRB
+			o1 = 0x7800
+		default: // AMOVHU (LDRH)
+			o1 = 0x8800
+			shift = 1
+		}
+	} else {
+		mem, r = r, mem
+		switch p.As {
+		case AMOVW: // STR
+			o1 = 0x6000
+			shift = 2
+		case AMOVB, AMOVBU: // STRB
+			o1 = 0x7000
+		default: // AMOVH, AMOVHU (STRH)
+			o1 = 0x8000
+			shift = 1
+		}
+	}
+	Rn := int(mem.Reg)
+	u5 := int(c.offset(mem)) >> shift
+	Rt := int(r.Reg)
+	out[0] = uint16(o1 | u5<<6 | Rn&7<<3 | Rt&7)
+	return 2
+}
+
+// xx0x xttt uuuu uuuu
+func _MOVW__u8_2_R13__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	var (
+		x uint16
+		r int16
+		a *obj.Addr
+	)
+	fr, to := &p.From, &p.To
+	switch {
+	case fr.Type == obj.TYPE_REG: // MOVW Rt, u8<<2(R13)
+		x = 0x9000
+		r = fr.Reg
+		a = to
+	case fr.Reg == REGSP: // MOVW u8<<2(R13), Rt
+		x = 0x9800
+		r = to.Reg
+		a = fr
+	default: // MOVW u8<<2(R15), Rt
+		x = 0x4800
+		r = to.Reg
+		a = fr
+	}
+	out[0] = x | uint16(r&7)<<8 | uint16(c.offset(a)>>2)
+	return 2
+}
+
+// 1011 1111 0xxx 0000
+func _NOP2(c *Ctx, p *obj.Prog, out []uint16) int {
+	out[0] = uint16(0xBF00 + int(p.As-ANOP2)<<4)
+	return 2
+}
+
+// 1011 0110 011x 0010
+func _CPSID(c *Ctx, p *obj.Prog, out []uint16) int {
+	o := 0xB662
+	if p.As == ACPSID {
+		o |= 0x10
+	}
+	out[0] = uint16(o)
+	return 2
+}
+
+// 1011 x10r rrrr rrrr
+func _PUSH__reglist(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xB400
+	rlist := int(p.From.Offset)
+	if p.To.Type == obj.TYPE_REGLIST {
+		rlist = int(p.To.Offset)
+		o1 |= 0x0800 | rlist>>7&0x100 // T16 POP supports PC
+	} else {
+		o1 |= rlist >> 6 & 0x100 // T16 PUSH supports LR
+	}
+	out[0] = uint16(o1 | rlist&0xFF)
+	return 2
+}
+
+// 1011 1010 xxmm mddd
+func _REV__Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xBA00 // AREV
+	switch p.As {
+	case AREV16:
+		o1 |= 0x0040
+	case AREVSH:
+		o1 |= 0x00C0
+	}
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	out[0] = uint16(o1 | Rm&7<<3 | Rd&7)
+	return 2
+}
+
+// 1xx1 111x uuuu uuuu
+func _SWI__u8(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xDF00 // ASWI
+	switch p.As {
+	case obj.AUNDEF:
+		o1 = 0xDE00
+	case ABKPT:
+		o1 = 0xBE00
+	}
+	u8 := int(p.To.Offset)
+	out[0] = uint16(o1 | u8)
+	return 2
+}
+
+// 0100 0010 xxmm mnnn
+func _TST__Rm__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1 int
+	switch p.As {
+	case ATST:
+		o1 = 0x4200
+	case ACMP:
+		o1 = 0x4280
+	default: // ACMN
+		o1 = 0x42C0
+	}
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	out[0] = uint16(o1 | Rm&7<<3 | Rn&7)
+	return 2
+}
+
+// 32-bit instructions
+
+// 1111 0exx xxxs nnnn  0eee dddd eeee eeee
+func _ADDs__e32__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := oadd32(p)
+	e1, e2 := encodeMIC(uint32(p.From.Offset))
+	out[0] = 0xF000 | o1 | e1
+	out[1] = o2 | e2
+	return 4
+}
+
+// 1110 101x xxxs nnnn  0uuu dddd uuvv mmmm
+func _ADDs__Rm_v_u5__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := oadd32(p)
+	out[0] = 0xEA00 | o1
+	out[1] = o2 | oshifti32(&p.From)
+	return 4
+}
+
+// 1111 0u10 x0x0 nnnn  0uuu dddd uuuu uuuu
+func _ADD__u12__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	imm := int(p.From.Offset)
+	Rd := int(p.To.Reg) & 15
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg) & 15
+	}
+	o1 := imm>>1&0x400 | Rn
+	o2 := imm&0x700<<4 | Rd<<8 | imm&0xFF
+	switch p.As {
+	case AADD:
+		o1 |= 0xF200
+	default: // ASUB
+		o1 |= 0xF2A0
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	return 4
+}
+
+// 1111 0jii iiii iiii  1xj1 jiii iiii iiii
+func _B__ji24_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xF000
+	o2 := 0x9000
+	if p.As == ABL {
+		o2 |= 0x4000
+	}
+	v := c.boffsetrel(p, o1, o2)
+	s := v >> 23 & 1
+	j1 := ^(v>>22 ^ s) & 1
+	j2 := ^(v>>21 ^ s) & 1
+	imm10 := v >> 11 & 0x3FF
+	imm11 := v & 0x7FF
+	out[0] = uint16(o1 | s<<10 | imm10)
+	out[1] = uint16(o2 | j1<<13 | j2<<11 | imm11)
+	return 4
+}
+
+// 1111 0jcc ccii iiii  10j0 jiii iiii iiii
+func _Bcond__ji20_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xF000 | obcond(p.As)<<6
+	o2 := 0x8000
+	v := c.boffsetrel(p, o1, o2)
+	out[0] = uint16(o1 | v>>9&0x400 | v>>11&0x3F)
+	out[1] = uint16(o2 | v>>4&0x2000 | v>>7&0x800 | v&0x7FF)
+	return 4
+}
+
+//1110 1000 1101 nnnn  1111 0000 000h mmmm
+func _TBB__Rm__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xE8D0, 0xF000
+	if p.As == ATBH {
+		o1 |= 0x0010
+	}
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Rm&15)
+	return 4
+}
+
+// 1111 0011 x1x0 nnnn  0uuu dddd uu0w wwww
+func _BFX__width__ulsb__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	width := int(p.From.Offset)
+	if len(p.RestArgs) == 0 {
+		c.ctxt.Diag("missing LSB: %v", p)
+		return 0
+	}
+	lsb := int(p.RestArgs[0].Offset)
+	if uint(lsb) > 31 || width <= 0 || lsb+width > 32 {
+		c.ctxt.Diag("wrong width or LSB: %v", p)
+		return 0
+	}
+	Rd := int(p.To.Reg)
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg)
+	}
+	var o1, o2 int
+	switch p.As {
+	case ABFX, ABFXU:
+		o1 = 0xF340
+		if p.As == ABFXU {
+			o1 |= 0x0080
+		}
+		o2 = width - 1
+	default: // ABFI, ABFC
+		o1 = 0xF360
+		if p.As == ABFC {
+			o1 |= 0x000F
+		}
+		o2 = lsb + width - 1
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | lsb&0x1C<<10 | Rd&15<<8 | lsb&3<<6)
+	return 4
+}
+
+// 1111 1010 10xx nnnn  1111 dddd 10xx mmmm
+func _CLZ__Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xFA90, 0xF080 // AREV
+	Rm := int(p.From.Reg)
+	Rn := Rm
+	Rd := int(p.To.Reg)
+	switch p.As {
+	case ACLZ:
+		o1 += 0x0020
+	case AREV16:
+		o2 |= 0x0010
+	case ARBIT:
+		o2 |= 0x0020
+	case AREVSH:
+		o2 |= 0x0030
+	case ASEL:
+		o1 += 0x0010
+		if p.Reg != 0 {
+			Rn = int(p.Reg)
+		} else {
+			Rn = Rd
+		}
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Rd&15<<8 | Rm&15)
+	return 4
+}
+
+func _DSB(c *Ctx, p *obj.Prog, out []uint16) int {
+	opt := int(REG_MB_SY)
+	if p.From.Reg != 0 {
+		opt = int(p.From.Reg)
+	}
+	out[0] = 0xF3BF
+	out[1] = uint16(0x8F40 + int(p.As-ADSB)<<4 + opt&15)
+	return 4
+}
+
+// 1110 1000 010x nnnn  tttt dddd uuuu uuuu
+func _LDREX__u8_2_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xE840
+	if p.As == ALDREX {
+		o1 |= 0x0010
+	}
+	offset := uint64(c.offset(&p.From))
+	if offset >= 1<<10 || offset&3 != 0 {
+		c.ctxt.Diag("bad offset: %v", p)
+		return 0
+	}
+	u8 := int(offset) >> 2
+	Rn := int(p.From.Reg)
+	Rt := int(p.To.Reg)
+	Rd := int(REGPC)
+	if p.Reg != 0 {
+		Rd = Rt
+		Rt = int(p.Reg)
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(Rt&15<<12 | Rd&15<<8 | u8)
+	return 4
+}
+
+// 1110 1000 110x nnnn  tttt 1111 010x dddd
+func _LDREXB__Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	if c.offset(&p.From) != 0 {
+		c.ctxt.Diag("offset not supported: %v", p)
+		return 0
+	}
+	o1, o2 := 0xE8C0, 0x0F40 // STREXB
+	switch p.As {
+	case ALDREXB:
+		o1 |= 0x0010
+	case ASTREXH:
+		o2 |= 0x0010
+	case ALDREXH:
+		o1 |= 0x0010
+		o2 |= 0x0010
+	}
+	Rn := int(p.From.Reg)
+	Rt := int(p.To.Reg)
+	Rd := int(REGPC)
+	if p.Reg != 0 {
+		Rd = Rt
+		Rt = int(p.Reg)
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Rt&15<<12 | Rd&15)
+	return 4
+}
+
+// 1111 1010 0x0x 1111  1111 dddd 10rr mmmm
+func _MOVH__Rm_rot__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xFA0F, 0xF080 // AMOVH
+	switch p.As {
+	case AMOVB:
+		o1 |= 0x0040
+	case AMOVHU:
+		o1 |= 0x0010
+	case AMOVBU:
+		o1 |= 0x0050
+	}
+	Rm := int(p.From.Reg) & 15
+	rot := 0
+	if p.From.Type == obj.TYPE_SHIFT {
+		var typ, count int
+		Rm, typ, count = shifti(int(p.From.Offset))
+		switch count {
+		case 0:
+			rot = 0
+			typ = 3
+		case 8:
+			rot = 1
+		case 16:
+			rot = 2
+		case 24:
+			rot = 3
+		default:
+			rot = -1
+		}
+		if typ != 3 || rot < 0 {
+			c.ctxt.Diag("only right rotation by 0,8,16,24 is supported: %v", p)
+			return 0
+		}
+	}
+	Rd := int(p.To.Reg) & 15
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | Rd<<8 | rot<<4 | Rm)
+	return 4
+}
+
+// 1110 100x x0wx nnnn  rr0r rrrr rrrr rrrr
+func _MOVM_IAw(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xE800
+	rlist, mem := uint(p.From.Offset), &p.To
+	if mem.Type == obj.TYPE_REGLIST {
+		if !c.checkldm(p) {
+			return 0
+		}
+		rlist, mem = uint(p.To.Offset), &p.From
+		if rlist&0xC000 == 0xC000 {
+			c.ctxt.Diag("both R14 and R15 in reglist: %v", p)
+			return 0
+		}
+		o1 |= 0x0010
+	} else if rlist&0x8000 != 0 {
+		c.ctxt.Diag("R15 in reglist: %v", p)
+		return 0
+	}
+	if rlist&0x2000 != 0 {
+		c.ctxt.Diag("R13 in reglist: %v", p)
+		return 0
+	}
+	if c.offset(mem) != 0 {
+		c.ctxt.Diag("offset not supported: %v", p)
+		return 0
+	}
+	if onesCount(rlist) < 2 {
+		c.ctxt.Diag("to few registers in reglist: %v", p)
+		return 0
+	}
+	if p.Scond&C_DB != 0 {
+		o1 |= 0x0100
+	} else {
+		o1 |= 0x0080
+	}
+	if p.Scond&C_WBIT != 0 {
+		o1 |= 0x0020
+	}
+	out[0] = uint16(o1 | int(mem.Reg)&15)
+	out[1] = uint16(rlist)
+	return 4
+}
+
+// 1111 100x 0xxx nnnn  tttt 0000 00uu mmmm
+func _MOVW__Rn_Rm_1_u2__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2, a := oldrstr32(p)
+	var u2 int
+	switch a.Scale {
+	case 0, 1:
+		break
+	case 2:
+		u2 = 1
+	case 4:
+		u2 = 2
+	case 8:
+		u2 = 3
+	default:
+		log.Fatal("bad scale")
+	}
+	Rm := int(a.Index)
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | u2<<4 | Rm&15)
+	return 4
+}
+
+// 1111 100x 1xxx nnnn  tttt uuuu uuuu uuuu
+// 1111 100x ±xxx 1111  tttt uuuu uuuu uuuu
+func _MOVW__s12_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2, a := oldrstr32(p)
+	imm12 := int(c.offset(a))
+	if imm12 < 0 {
+		imm12 = -imm12
+	} else {
+		o1 |= 0x0080
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | imm12&0xFFF)
+	return 4
+}
+
+// 1111 0011 1110 1111  1000 dddd mmmm mmmm
+// 1111 0011 1000 nnnn  1000 kk00 mmmm mmmm
+func _MOVW__SYSm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	if p.From.Reg >= REG_APSR && p.From.Reg <= REG_CONTROL {
+		SYSm := int(p.From.Reg)
+		Rd := int(p.To.Reg)
+		out[0] = 0xF3EF
+		out[1] = uint16(0x8000 | Rd&15<<8 | SYSm&31)
+	} else {
+		// TODO: support kk != 0b10 (nzcvq)
+		Rn := int(p.From.Reg)
+		SYSm := int(p.To.Reg)
+		out[0] = uint16(0xF380 | Rn&15)
+		out[1] = uint16(0x8800 | SYSm&31)
+	}
+	return 4
+}
+
+// 1110 1110 111x 0001  tttt 1010 0001 0000
+func _MOVW__FPSCR__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xEEE1, 0x0A10
+	var Rt int
+	if p.From.Reg == REG_FPSCR {
+		o1 |= 1 << 4
+		Rt = int(p.To.Reg)
+	} else {
+		Rt = int(p.From.Reg)
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | Rt&15<<12)
+	return 4
+}
+
+// 1111 0y10 x100 uuuu  0zzz dddd zzzz zzzz
+func _MOVW__uyz16__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := uint(0xF240)
+	if p.As == AMOVT {
+		o1 |= 0x80
+	}
+	imm := uint(p.From.Offset)
+	out[0] = uint16(o1 | imm>>12 | imm>>1&0x400)
+	out[1] = uint16(imm&0x700<<4 | uint(p.To.Reg)&15<<8 | imm&0xFF)
+	return 4
+}
+
+// 1111 100x 0xxx nnnn  tttt 1p±w uuuu uuuu
+func _MOVWpw__s8_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2, a := oldrstr32(p)
+	imm8 := int(c.offset(a))
+	if imm8 < 0 {
+		o2 |= 0x0800
+		imm8 = -imm8
+	} else {
+		o2 |= 0x0A00
+	}
+	switch p.Scond & (C_PBIT | C_WBIT) {
+	case 0:
+		o2 |= 1 << 10 // offset addressing (set p)
+	case C_PBIT:
+		o2 |= 1 << 8 // post-indexed addressing (set w)
+	case C_WBIT:
+		o2 |= 1<<10 | 1<<8 // pre-indexed addressing (set p,w)
+	default: // C_PBIT|C_WBIT
+		c.ctxt.Diag("invalid .P/.W suffix: %v", p)
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | imm8&0xFF)
+	return 4
+}
+
+// 1111 1010 0vvs nnnn  1111 dddd 0000 mmmm
+func _MOVWs__Rn_v_Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xFA00
+	if p.Scond&C_SBIT != 0 {
+		o1 |= 0x0010
+	}
+	Rn, typ, Rm := shiftr(int(p.From.Offset))
+	Rd := int(p.To.Reg) & 15
+	out[0] = uint16(o1 | typ<<5 | Rn)
+	out[1] = uint16(0xF000 | Rd<<8 | Rm)
+	return 4
+}
+
+// 1111 1011 xxxx nnnn  1111 dddd xxxx mmmm
+func _MUL__Rm__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xFB00, 0xF000 // MUL
+	switch p.As {
+	case ADIV:
+		o1 |= 0x0090
+		o2 |= 0x00F0
+	case ADIVU:
+		o1 |= 0x00B0
+		o2 |= 0x00F0
+	}
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg)
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Rd&15<<8 | Rm&15)
+	return 4
+}
+
+// 1111 1011 00xx nnnn  aaaa dddd 000x mmmm
+func _MULA__Rm__Rn__Ra__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xFB00, 0x0000 // AMULA
+	switch p.As {
+	case AMULS:
+		o2 |= 0x0010
+	case AMULAWB:
+		o1 |= 0x0030
+	case AMULAWT:
+		o1 |= 0x0030
+		o2 |= 0x0010
+	}
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	Rd := int(p.To.Reg)
+	Ra := int(p.To.Offset)
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Ra&15<<12 | Rd&15<<8 | Rm&15)
+	return 4
+}
+
+// 1111 1011 10x0 nnnn  llll hhhh 0000 mmmm
+func _MULL__Rm__Rn__Rdh_Rdl(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xFB00
+	switch p.As {
+	case AMULL:
+		o1 |= 0x0080
+	case AMULLU:
+		o1 |= 0x00A0
+	case AMULAL:
+		o1 |= 0x00C0
+	default: // AMULALU
+		o1 |= 0x00E0
+	}
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	Rdh := int(p.To.Reg)
+	Rdl := int(p.To.Offset)
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(Rdl&15<<12 | Rdh&15<<8 | Rm&15)
+	return 4
+}
+
+// 1111 0011 101x 1111  1000 xxxx 00x0 xxxx
+func _NOP4(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1, o2 uint16
+	switch p.As {
+	case ANOP4:
+		o1, o2 = 0xF3AF, 0x8000
+	default: // ACLREX
+		o1, o2 = 0xF3BF, 0x8F2F
+	}
+	out[0] = o1
+	out[1] = o2
+	return 4
+}
+
+// 1111 0e0x x0x1 nnnn  0eee 1111 eeee eeee
+func _TST__e32__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := encodeMIC(uint32(p.From.Offset))
+	switch p.As {
+	case ACMP:
+		o1 |= 0xF1B0
+	case ATST:
+		o1 |= 0xF010
+	case ACMN:
+		o1 |= 0xF110
+	default: // ATEQ
+		o1 |= 0xF090
+	}
+	out[0] = o1 | uint16(p.Reg&15)
+	out[1] = o2 | 15<<8
+	return 4
+}
+
+// 1110 101x x0x1 nnnn  0uuu 1111 uuvv mmmm
+func _TST__Rm_v_u5__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1 int
+	switch p.As {
+	case ACMP:
+		o1 = 0xEBB0
+	case ATST:
+		o1 = 0xEA10
+	case ACMN:
+		o1 = 0xEB10
+	default: // ATEQ
+		o1 = 0xEA90
+	}
+	out[0] = uint16(o1 | int(p.Reg&15))
+	out[1] = 0x0F00 | oshifti32(&p.From)
+	return 4
+}
+
+// Pseudoinstructions (they must not change the number of instructions declared in optab)
+
+func _ADD__lit__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	q := *p
+	q.As = AMOVW
+	q.To.Reg = REGTMP
+	n := _MOVW__lit__Rd(c, &q, out)
+	out = out[n/2:]
+	q.As = p.As
+	q.From = q.To
+	q.To = p.To
+	if p.As == AADD {
+		return n + _ADD__Rm__Rdn(c, &q, out)
+	}
+	return n + _AND__Rm__Rdn(c, &q, out)
+}
+
+func _ADD__lit__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	q := *p
+	q.As = AMOVW
+	q.To.Reg = REGTMP
+	n := _MOVW__lit__Rd(c, &q, out)
+	out = out[n/2:]
+	q.As = p.As
+	q.From = q.To
+	q.To = p.To
+	switch q.As {
+	case AADD, ASUB:
+		if q.Reg <= REG_R7 && q.To.Reg <= REG_R7 {
+			return n + _ADD__Rm__Rn__Rd(c, &q, out)
+		}
+	}
+	return n + _ADDs__Rm_v_u5__Rn__Rd(c, &q, out)
+}
+
+func _CMN__lit__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	return c.cmp__lit__Rn(_TST__Rm__Rn, p, out)
+}
+
+func _CMP__lit__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	return c.cmp__lit__Rn(_CMP__Rm__Rn, p, out)
+}
+
+func _MOVW__lit__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	v, short := c.litoffset(p, p.To.Reg <= REG_R7, 0)
+	var q obj.Prog
+	q.As = AMOVW
+	q.To = p.To
+	q.From.Type = obj.TYPE_MEM
+	q.From.Name = obj.NAME_NONE
+	q.From.Reg = REGPC
+	q.From.Offset = int64(v)
+	if short {
+		return _MOVW__u8_2_R13__Rt(c, &q, out)
+	}
+	return _MOVW__s12_Rn__Rt(c, &q, out)
+}
+
+func _MOVW__lit_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	q := *p
+	if q.To.Type == obj.TYPE_MEM {
+		q.From, q.To.Type = q.To, q.From.Type
+	}
+	q.As = AMOVW
+	q.To.Reg = REGTMP
+	n := _MOVW__lit__Rd(c, &q, out)
+	out = out[n/2:]
+	q = *p
+	mem, reg := &q.From, &q.To
+	if q.To.Type == obj.TYPE_MEM {
+		mem, reg = &q.To, &q.From
+	}
+	mem.Offset = 0
+	switch mem.Name {
+	case obj.NAME_STATIC, obj.NAME_EXTERN:
+		mem.Reg = REGTMP
+		if (q.To.Type == obj.TYPE_MEM || q.As != AMOVB && q.As != AMOVH) && reg.Reg <= REG_R7 {
+			return n + _MOVW__u5_2_Rn__Rt(c, &q, out)
+		}
+		return n + _MOVW__s12_Rn__Rt(c, &q, out)
+	default:
+		mem.Index = REGTMP
+		if mem.Reg <= REG_R7 && reg.Reg <= REG_R7 {
+			return n + _MOVW__Rn_Rm__Rt(c, &q, out)
+		}
+		return n + _MOVW__Rn_Rm_1_u2__Rt(c, &q, out)
+	}
+}
+
+func _TST__lit__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	return c.cmp__lit__Rn(_TST__Rm_v_u5__Rn, p, out)
+}
+
+func _WORD__u32(c *Ctx, p *obj.Prog, out []uint16) int {
+	if p.To.Sym != nil {
+		rel := obj.Addrel(c.cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 4
+		rel.Sym = p.To.Sym
+		rel.Add = p.To.Offset
+		rel.Type = objabi.R_ADDR
+		out[0] = 0
+		out[1] = 0
+	} else {
+		offset := int(p.To.Offset)
+		out[0] = uint16(offset)
+		out[1] = uint16(offset >> 16)
+	}
+	return 4
+}
+
+func _HWORD__u16(c *Ctx, p *obj.Prog, out []uint16) int {
+	out[0] = uint16(p.From.Offset)
+	return 2
+}
+
+// 1110 1101 ±d0x nnnn  dddd 101x uuuu uuuu
+func _MOVF__s8_2_Rn__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xED00
+	reg, mem := &p.From, &p.To
+	if mem.Type == obj.TYPE_REG {
+		reg, mem = mem, reg
+		o1 |= 0x0010
+	}
+	o2 := 0x0A00 | int(reg.Reg)&15<<12
+	if p.As == AMOVD {
+		o2 |= 0x100
+	}
+	o1 |= int(mem.Reg & 15)
+	offset := int(c.offset(mem)) >> 2
+	if offset >= 0 {
+		o1 |= 0x0080
+	} else {
+		offset = -offset
+	}
+	o2 |= offset
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	return 4
+}
+
+func _MOVF__lit_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	q := *p
+	if q.To.Type == obj.TYPE_MEM {
+		q.From, q.To.Type = q.To, q.From.Type
+	}
+	q.As = AMOVW
+	tr := q.To.Reg
+	q.To.Reg = REGTMP
+	n := _MOVW__lit__Rd(c, &q, out)
+	if q.From.Name != obj.NAME_STATIC && q.From.Name != obj.NAME_EXTERN {
+		q.As = AADD
+		q.From.Reg = tr
+		n += _ADD__Rm__Rdn(c, &q, out[n/2:])
+	}
+	q = *p
+	mem := &q.From
+	if q.To.Type == obj.TYPE_MEM {
+		mem = &q.To
+	}
+	mem.Name = obj.NAME_NONE
+	mem.Reg = REGTMP
+	mem.Offset = 0
+	return n + _MOVF__s8_2_Rn__Fd(c, &q, out[n/2:])
+}
+
+// 1110 1110 xdxx nnnn  dddd 101x nxm0 mmmm
+func _ADDF__Fm__Fn__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	Fm := int(p.From.Reg) & 15
+	Fd := int(p.To.Reg) & 15
+	Fn := Fd
+	if p.Reg != 0 {
+		Fn = int(p.Reg) & 15
+	}
+	o1 := 0xEE00 | Fn
+	o2 := 0x0A00 | Fd<<12 | Fm
+	switch p.As {
+	case AADDF:
+		o1 |= 0x030
+	case AADDD:
+		o1 |= 0x030
+		o2 |= 0x100
+	case ASUBF:
+		o1 |= 0x030
+		o2 |= 0x040
+	case ASUBD:
+		o1 |= 0x030
+		o2 |= 0x140
+	case AMULF:
+		o1 |= 0x020
+	case AMULD:
+		o1 |= 0x020
+		o2 |= 0x100
+	case ADIVF:
+		o1 |= 0x080
+	case ADIVD:
+		o1 |= 0x080
+		o2 |= 0x100
+	case AMULAF:
+		// nothing
+	case AMULAD:
+		o2 |= 0x100
+	case AMULSF:
+		o2 |= 0x040
+	case AMULSD:
+		o2 |= 0x140
+	case ANMULF:
+		o1 |= 0x020
+		o2 |= 0x040
+	default: // ANMULD
+		o1 |= 0x020
+		o2 |= 0x140
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	return 4
+}
+
+// 1110 1110 1d11 0xxx  dddd 101x x1m0 mmmm
+func _SQRTF__Fm__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	Fm := int(p.From.Reg) & 15
+	Fd := int(p.To.Reg) & 15
+	o1 := 0xEEB0
+	o2 := 0x0A40 | Fd<<12 | Fm
+	switch p.As {
+	case ASQRTF:
+		o1 |= 0x001
+		o2 |= 0x080
+	case ASQRTD:
+		o1 |= 0x001
+		o2 |= 0x180
+	case ANEGF:
+		o1 |= 0x001
+	case ANEGD:
+		o1 |= 0x001
+		o2 |= 0x100
+	case AMOVF:
+		// nothing
+	case AMOVD:
+		o2 |= 0x100
+	case AABSF:
+		o2 |= 0x080
+	case AABSD:
+		o2 |= 0x180
+	case AMOVFD:
+		o1 |= 0x007
+		o2 |= 0x080
+	default: // AMOVDF
+		o1 |= 0x007
+		o2 |= 0x180
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	return 4
+}
+
+// 1110 1110 00h1 nnnn  tttt 1011 n001 0000
+// 1110 1110 00h0 dddd  tttt 1011 d001 0000
+func _MOVW__Fm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	from := int(p.From.Reg) & 15
+	to := int(p.To.Reg) & 15
+	o1 := 0xEE00
+	o2 := 0x0B10
+	if Aclass(p.From.Class-1) == C_FREG {
+		o1 |= 0x10 | from
+		o2 |= to << 12
+	} else {
+		o1 |= to
+		o2 |= from << 12
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	return 4
+}
+
+// 1111 1110 1d11 1xxx  dddd 101x x1m0 mmmm
+func _MOVFW__Fm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	q := *p
+	Xm := int(p.From.Reg) & 15
+	Xd := int(p.To.Reg) & 15
+	Ftmp := int(FREGTMP) & 15
+	switch p.As {
+	case AMOVFW, AMOVDW:
+		o1 := 0xEEBC // from ARMv7-A RM (the ARMv7-M RM says 0xFEBC)
+		if p.Scond&C_UBIT == 0 {
+			o1 |= 1
+		}
+		o2 := 0x0AC0 | Ftmp<<12 | Xm
+		if p.As == AMOVDW {
+			o2 |= 0x100
+		}
+		out[0] = uint16(o1)
+		out[1] = uint16(o2)
+		q.From.Reg = FREGTMP
+		_MOVW__Fm__Rd(c, &q, out[2:4])
+	default: // AMOVWF, AMOVWD
+		q.To.Reg = FREGTMP
+		_MOVW__Fm__Rd(c, &q, out[0:2])
+		o1 := 0xEEB8 // from ARMv7-A RM (the ARMv7-M RM says 0xFEB8)
+		o2 := 0x0A40 | Xd<<12 | Ftmp
+		if p.As == AMOVWD {
+			o2 |= 0x100
+		}
+		if p.Scond&C_UBIT == 0 {
+			o2 |= 0x80
+		}
+		out[2] = uint16(o1)
+		out[3] = uint16(o2)
+	}
+	return 8
+}
+
+// 1110 1110 1d11 0100  dddd 101x e1m0 mmmm
+// 1110 1110 1d11 0101  dddd 101x e100 0000
+func _CMPF__Fm__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xEEB4
+	o2 := 0x0AC0 // e=1
+	if p.As == ACMPD {
+		o2 |= 0x100
+	}
+	if p.Reg != 0 {
+		o2 |= int(p.Reg&15)<<12 | int(p.From.Reg&15)
+	} else {
+		o1 |= 1
+		o2 |= int(p.From.Reg&15) << 12
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	// move flags from FPSCR to APSR (VMRS R15)
+	out[2] = 0xEEF1
+	out[3] = 0xFA10
+	return 8
+}
+
+// 1110 1110 1d11 ffff  dddd 101x 0000 ffff
+func _MOVF__f8__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	v := c.chipfloat(p.From.Val.(float64))
+	o1 := 0xEEB0 | v>>4&0xF
+	o2 := 0x0A00 | int(p.To.Reg)&15<<12 | v&0xF
+	if p.As == AMOVD {
+		o2 |= 0x100
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	return 4
+}
+
+// 1110 1110 1d11 ffff  dddd 101x 0000 ffff
+// 1110 1110 0d11 nnnn  dddd 101x n1m0 mmmm
+func _MOVF__0__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rd := int(p.To.Reg) & 15
+
+	// MOVF $1.0, Fd
+	o1 := 0xEEB7
+	o2 := 0x0A00 | Rd<<12
+	if p.As == AMOVD {
+		o2 |= 0x100
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+
+	// SUBF Fd, Fd, Fd
+	o1 = 0xEE30 | Rd
+	o2 = 0x0A40 | Rd<<12 | Rd
+	if p.As == AMOVD {
+		o2 |= 1 << 8
+	}
+	out[2] = uint16(o1)
+	out[3] = uint16(o2)
+
+	return 8
+}
+
+// ********
+
+func oldrstr32(p *obj.Prog) (o1, o2 int, mem *obj.Addr) {
+	var r *obj.Addr
+	mem, r = &p.From, &p.To
+	if r.Type == obj.TYPE_REG {
+		switch p.As {
+		case AMOVW: // LDR
+			o1 = 0xF850
+		case AMOVB: // LDRSB
+			o1 = 0xF910
+		case AMOVBU: // LDRB
+			o1 = 0xF810
+		case AMOVH: // LDRSH
+			o1 = 0xF930
+		default: // AMOVHU (LDRH)
+			o1 = 0xF830
+		}
+	} else {
+		mem, r = r, mem
+		switch p.As {
+		case AMOVW: // STR
+			o1 = 0xF840
+		case AMOVB, AMOVBU: // STRB
+			o1 = 0xF800
+		default: // AMOVH, AMOVHU (STRH)
+			o1 = 0xF820
+		}
+	}
+	o1 |= int(mem.Reg) & 15
+	o2 = int(r.Reg) & 15 << 12
+	return
+}
+
+// u must be representable as Modified Immediate Constant (checked by mic)
+func encodeMIC(u uint32) (o1, o2 uint16) {
+	if u>>8 == 0 {
+		// fast path for 00000000_00000000_00000000_abcdefgh
+		return 0, uint16(u)
+	}
+	for n := 31; n >= 8; n-- {
+		if v := rol(u, n); v&^0xFF == 0 {
+			o1 = uint16(n & 0x10 << 6)
+			o2 = uint16(n&0x0E<<11) | uint16(n&1<<7) | uint16(v&0x7F)
+			return
+		}
+	}
+	switch uint32(0) {
+	case u >> 24:
+		// 00000000_abcdefgh_00000000_abcdefgh
+		return 0, 0x1000 | uint16(u&0xFF)
+	case u << 24:
+		// abcdefgh_00000000_abcdefgh_00000000
+		return 0, 0x2000 | uint16(u>>8&0xFF)
+	}
+	// abcdefgh_abcdefgh_abcdefgh_abcdefgh
+	return 0, 0x3000 | uint16(u&0xFF)
+}
+
+func oadd32(p *obj.Prog) (o1, o2 uint16) {
+	switch p.As {
+	case AADD:
+		o1 = 0x100
+	case ASUB:
+		o1 = 0x1A0
+	case AAND:
+		o1 = 0x000
+	case AORR:
+		o1 = 0x040
+	case ABIC:
+		o1 = 0x020
+	case AORN:
+		o1 = 0x060
+	case AEOR:
+		o1 = 0x080
+	case AADC:
+		o1 = 0x140
+	case ARSB:
+		o1 = 0x1C0
+	case ASBC:
+		o1 = 0x160
+	case AMOVW:
+		o1 = 0x04F
+	case AMVN:
+		o1 = 0x06F
+	default:
+		log.Fatalf("not a data processing instruction")
+	}
+	if p.Scond&C_SBIT != 0 {
+		o1 |= 0x10
+	}
+	Rd := int(p.To.Reg) & 15
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg) & 15
+	}
+	return o1 | uint16(Rn), uint16(Rd) << 8
+}
+
+func shiftr(offset int) (Rn, typ, Rm int) {
+	Rn = offset & 15
+	typ = offset >> 5 & 3
+	Rm = offset >> 8 & 15
+	return
+}
+
+func shifti(offset int) (Rm, typ, count int) {
+	Rm = offset & 15
+	typ = offset >> 5 & 3
+	count = offset >> 7 & 31
+	return
+}
+
+func oshifti32(a *obj.Addr) uint16 {
+	Rm := int(a.Reg) & 15
+	var typ, count int
+	if a.Type == obj.TYPE_SHIFT {
+		Rm, typ, count = shifti(int(a.Offset))
+	}
+	return uint16(count&0x1C<<10 | count&3<<6 | typ<<4 | Rm)
+}
+
+func (c *Ctx) checkldm(p *obj.Prog) bool {
+	rinlist := 1<<uint(p.From.Reg&15)&p.To.Offset != 0 // true if Rn in reglist
+	w := p.Scond&C_WBIT != 0
+	if rinlist && w {
+		c.ctxt.Diag("Rn in reglist: %v", p)
+		return false
+	}
+	return true
+}
+
+func obcond(as obj.As) int {
+	switch {
+	case as <= ABCS:
+		return int(as - ABEQ)
+	case as == ABHS:
+		return 2
+	case as <= ABLO:
+		return 3
+	default: //  ABMI <= as:
+		return 4 + int(as-ABMI)
+	}
+}
+
+func (c *Ctx) boffsetrel(p *obj.Prog, o1, o2 int) int {
+	v := -pcoff
+	if p.To.Sym == nil {
+		if p.To.Target() != nil {
+			v += int(p.To.Target().Pc - p.Pc)
+		}
+		return v >> 1
+	}
+	rel := obj.Addrel(c.cursym)
+	rel.Off = int32(p.Pc)
+	rel.Siz = 4
+	rel.Sym = p.To.Sym
+	v += int(p.To.Offset)
+	rel.Add = int64(o2)<<48 | int64(o1)<<32 | int64(uint32(v))
+	rel.Type = objabi.R_CALLARM
+	return 0
+}
+
+func (c *Ctx) cmp__lit__Rn(cmp asmoutFunc, p *obj.Prog, out []uint16) int {
+	q := *p
+	q.As = AMOVW
+	q.To.Reg = REGTMP
+	q.To.Type = obj.TYPE_REG
+	n := _MOVW__lit__Rd(c, &q, out)
+	out = out[n/2:]
+	q.As = p.As
+	q.From.Type = obj.TYPE_REG
+	q.From.Reg = REGTMP
+	return n + cmp(c, &q, out)
+}
diff --git a/src/cmd/internal/obj/util.go b/src/cmd/internal/obj/util.go
index b9bacb7a22..6e23a2b88f 100644
--- a/src/cmd/internal/obj/util.go
+++ b/src/cmd/internal/obj/util.go
@@ -331,7 +331,7 @@ func writeDconv(w io.Writer, p *Prog, a *Addr, abiDetail bool) {
 		v := int(a.Offset)
 		ops := "<<>>->@>"
 		switch objabi.GOARCH {
-		case "arm":
+		case "arm", "thumb":
 			op := ops[((v>>5)&3)<<1:]
 			if v&(1<<4) != 0 {
 				fmt.Fprintf(w, "R%d%c%cR%d", v&15, op[0], op[1], (v>>8)&15)
@@ -507,6 +507,7 @@ const (
 	RBaseS390X = 14 * 1024 // range [14k, 15k)
 	RBaseRISCV = 15 * 1024 // range [15k, 16k)
 	RBaseWasm  = 16 * 1024
+	RBaseThumb = 17 * 1024
 )
 
 // RegisterRegister binds a pretty-printer (Rconv) for register
diff --git a/src/cmd/internal/objabi/head.go b/src/cmd/internal/objabi/head.go
index 48ff292307..5cbc1386ed 100644
--- a/src/cmd/internal/objabi/head.go
+++ b/src/cmd/internal/objabi/head.go
@@ -48,6 +48,7 @@ const (
 	Hsolaris
 	Hwindows
 	Haix
+	Hnoos
 )
 
 func (h *HeadType) Set(s string) error {
@@ -66,6 +67,8 @@ func (h *HeadType) Set(s string) error {
 		*h = Hlinux
 	case "netbsd":
 		*h = Hnetbsd
+	case "noos":
+		*h = Hnoos
 	case "openbsd":
 		*h = Hopenbsd
 	case "plan9":
@@ -96,6 +99,8 @@ func (h *HeadType) String() string {
 		return "linux"
 	case Hnetbsd:
 		return "netbsd"
+	case Hnoos:
+		return "noos"
 	case Hopenbsd:
 		return "openbsd"
 	case Hplan9:
diff --git a/src/cmd/internal/objabi/stack.go b/src/cmd/internal/objabi/stack.go
index 05a1d4a4b5..a960250c55 100644
--- a/src/cmd/internal/objabi/stack.go
+++ b/src/cmd/internal/objabi/stack.go
@@ -8,7 +8,6 @@ package objabi
 
 const (
 	STACKSYSTEM = 0
-	StackSystem = STACKSYSTEM
 	StackBig    = 4096
 	StackSmall  = 128
 )
@@ -17,9 +16,19 @@ const (
 	StackPreempt = -1314 // 0xfff...fade
 )
 
+var StackGuard, StackLimit, StackSystem int
+
 // Initialize StackGuard and StackLimit according to target system.
-var StackGuard = 928*stackGuardMultiplier() + StackSystem
-var StackLimit = StackGuard - StackSystem - StackSmall
+func init() {
+	if GOOS == "noos" && GOARCH == "thumb" {
+		StackSystem = 27 * 4
+		StackGuard = 464 + StackSystem
+	} else {
+		StackSystem = STACKSYSTEM
+		StackGuard = 928*stackGuardMultiplier() + StackSystem
+	}
+	StackLimit = StackGuard - StackSystem - StackSmall
+}
 
 // stackGuardMultiplier returns a multiplier to apply to the default
 // stack guard size. Larger multipliers are used for non-optimized
diff --git a/src/cmd/internal/objabi/util.go b/src/cmd/internal/objabi/util.go
index a73ab479a1..5a25a42d9e 100644
--- a/src/cmd/internal/objabi/util.go
+++ b/src/cmd/internal/objabi/util.go
@@ -52,6 +52,10 @@ func goarm() int {
 		return 6
 	case "7":
 		return 7
+	case "7f", "7F":
+		return 0x7F
+	case "7d", "7D":
+		return 0x7D
 	}
 	// Fail here, rather than validate at multiple call sites.
 	log.Fatalf("Invalid GOARM value. Must be 5, 6, or 7.")
diff --git a/src/cmd/internal/objfile/disasm.go b/src/cmd/internal/objfile/disasm.go
index b5f1cd1632..9e39a0a9fa 100644
--- a/src/cmd/internal/objfile/disasm.go
+++ b/src/cmd/internal/objfile/disasm.go
@@ -24,6 +24,7 @@ import (
 	"golang.org/x/arch/arm/armasm"
 	"golang.org/x/arch/arm64/arm64asm"
 	"golang.org/x/arch/ppc64/ppc64asm"
+	"golang.org/x/arch/thumb/thumbasm"
 	"golang.org/x/arch/x86/x86asm"
 )
 
@@ -37,6 +38,7 @@ type Disasm struct {
 	goarch    string           // GOARCH string
 	disasm    disasmFunc       // disassembler function for goarch
 	byteOrder binary.ByteOrder // byte order for goarch
+	gofile    bool
 }
 
 // Disasm returns a disassembler for the file f.
@@ -83,6 +85,7 @@ func (e *Entry) Disasm() (*Disasm, error) {
 		goarch:    goarch,
 		disasm:    disasm,
 		byteOrder: byteOrder,
+		gofile:    e.gofile,
 	}
 
 	return d, nil
@@ -202,7 +205,7 @@ func (d *Disasm) Print(w io.Writer, filter *regexp.Regexp, start, end uint64, pr
 		fc = NewFileCache(8)
 	}
 
-	tw := tabwriter.NewWriter(bw, 18, 8, 1, '\t', tabwriter.StripEscape)
+	tw := tabwriter.NewWriter(bw, 18, 8, 1, ' ', tabwriter.StripEscape)
 	for _, sym := range d.syms {
 		symStart := sym.Addr
 		symEnd := sym.Addr + uint64(sym.Size)
@@ -246,7 +249,12 @@ func (d *Disasm) Print(w io.Writer, filter *regexp.Regexp, start, end uint64, pr
 				fmt.Fprintf(tw, "  %s:%d\t%#x\t", base(file), line, pc)
 			}
 
-			if size%4 != 0 || d.goarch == "386" || d.goarch == "amd64" {
+			if d.goarch == "thumb" {
+				fmt.Fprintf(tw, "%04x", d.byteOrder.Uint16(code[i:i+2]))
+				if size == 4 {
+					fmt.Fprintf(tw, " %04x", d.byteOrder.Uint16(code[i+2:]))
+				}
+			} else if size%4 != 0 || d.goarch == "386" || d.goarch == "amd64" {
 				// Print instruction as bytes.
 				fmt.Fprintf(tw, "%x", code[i:i+size])
 			} else {
@@ -258,7 +266,7 @@ func (d *Disasm) Print(w io.Writer, filter *regexp.Regexp, start, end uint64, pr
 					fmt.Fprintf(tw, "%08x", d.byteOrder.Uint32(code[i+j:]))
 				}
 			}
-			fmt.Fprintf(tw, "\t%s\t\n", text)
+			fmt.Fprintf(tw, "\t%s\n", text)
 		})
 		tw.Flush()
 	}
@@ -273,19 +281,24 @@ func (d *Disasm) Decode(start, end uint64, relocs []Reloc, gnuAsm bool, f func(p
 	if end > d.textEnd {
 		end = d.textEnd
 	}
+	var pctoa uint64 // TODO(md) if more such cases pctoa can be a function
+	if !d.gofile && d.goarch == "thumb" {
+		pctoa = 1
+	}
 	code := d.text[:end-d.textStart]
 	lookup := d.lookup
 	for pc := start; pc < end; {
-		i := pc - d.textStart
-		text, size := d.disasm(code[i:], pc, lookup, d.byteOrder, gnuAsm)
+		addr := pc &^ pctoa
+		i := addr - d.textStart
+		text, size := d.disasm(code[i:], addr, lookup, d.byteOrder, gnuAsm)
 		file, line, _ := d.pcln.PCToLine(pc)
 		sep := "\t"
 		for len(relocs) > 0 && relocs[0].Addr < i+uint64(size) {
-			text += sep + relocs[0].Stringer.String(pc-start)
+			text += sep + relocs[0].Stringer.String(addr-start)
 			sep = " "
 			relocs = relocs[1:]
 		}
-		f(pc, uint64(size), file, line, text)
+		f(addr, uint64(size), file, line, text)
 		pc += uint64(size)
 	}
 }
@@ -383,6 +396,21 @@ func disasm_ppc64(code []byte, pc uint64, lookup lookupFunc, byteOrder binary.By
 	return text, size
 }
 
+func disasm_thumb(code []byte, pc uint64, lookup lookupFunc, byteOrder binary.ByteOrder, gnuAsm bool) (string, int) {
+	inst, err := thumbasm.Decode(code)
+	var text string
+	size := inst.Len
+	if err != nil || size == 0 || inst.Op == 0 {
+		size = 2
+		text = "?"
+	} else if gnuAsm {
+		text = fmt.Sprintf("%-36s // %s", thumbasm.GoSyntax(inst, pc, lookup, textReader{code, pc}), thumbasm.GNUSyntax(inst))
+	} else {
+		text = thumbasm.GoSyntax(inst, pc, lookup, textReader{code, pc})
+	}
+	return text, size
+}
+
 var disasms = map[string]disasmFunc{
 	"386":     disasm_386,
 	"amd64":   disasm_amd64,
@@ -390,6 +418,7 @@ var disasms = map[string]disasmFunc{
 	"arm64":   disasm_arm64,
 	"ppc64":   disasm_ppc64,
 	"ppc64le": disasm_ppc64,
+	"thumb":   disasm_thumb,
 }
 
 var byteOrders = map[string]binary.ByteOrder{
@@ -400,6 +429,7 @@ var byteOrders = map[string]binary.ByteOrder{
 	"ppc64":   binary.BigEndian,
 	"ppc64le": binary.LittleEndian,
 	"s390x":   binary.BigEndian,
+	"thumb":   binary.LittleEndian,
 }
 
 type Liner interface {
diff --git a/src/cmd/internal/objfile/elf.go b/src/cmd/internal/objfile/elf.go
index a48a9df5d6..9457d28502 100644
--- a/src/cmd/internal/objfile/elf.go
+++ b/src/cmd/internal/objfile/elf.go
@@ -98,6 +98,9 @@ func (f *elfFile) goarch() string {
 	case elf.EM_X86_64:
 		return "amd64"
 	case elf.EM_ARM:
+		if f.elf.Entry&1 != 0 {
+			return "thumb"
+		}
 		return "arm"
 	case elf.EM_AARCH64:
 		return "arm64"
diff --git a/src/cmd/internal/objfile/goobj.go b/src/cmd/internal/objfile/goobj.go
index f19bec5dcb..ea8b79eb4c 100644
--- a/src/cmd/internal/objfile/goobj.go
+++ b/src/cmd/internal/objfile/goobj.go
@@ -53,8 +53,9 @@ L:
 				}
 			}
 			entries = append(entries, &Entry{
-				name: e.Name,
-				raw:  &goobjFile{e.Obj, r, f, arch},
+				name:   e.Name,
+				raw:    &goobjFile{e.Obj, r, f, arch},
+				gofile: true,
 			})
 			continue
 		case archive.EntryNativeObj:
diff --git a/src/cmd/internal/objfile/objfile.go b/src/cmd/internal/objfile/objfile.go
index a58e0e159c..75330c7f42 100644
--- a/src/cmd/internal/objfile/objfile.go
+++ b/src/cmd/internal/objfile/objfile.go
@@ -30,8 +30,9 @@ type File struct {
 }
 
 type Entry struct {
-	name string
-	raw  rawFile
+	name   string
+	raw    rawFile
+	gofile bool
 }
 
 // A Sym is a symbol defined in an executable file.
diff --git a/src/cmd/internal/sys/arch.go b/src/cmd/internal/sys/arch.go
index e8687363de..8295644dff 100644
--- a/src/cmd/internal/sys/arch.go
+++ b/src/cmd/internal/sys/arch.go
@@ -21,6 +21,7 @@ const (
 	PPC64
 	RISCV64
 	S390X
+	Thumb
 	Wasm
 )
 
@@ -161,6 +162,16 @@ var ArchS390X = &Arch{
 	MinLC:     2,
 }
 
+var ArchThumb = &Arch{
+	Name:      "thumb",
+	Family:    Thumb,
+	ByteOrder: binary.LittleEndian,
+	PtrSize:   4,
+	RegSize:   4,
+	MinLC:     2,
+}
+
+
 var ArchWasm = &Arch{
 	Name:      "wasm",
 	Family:    Wasm,
@@ -183,5 +194,6 @@ var Archs = [...]*Arch{
 	ArchPPC64LE,
 	ArchRISCV64,
 	ArchS390X,
+	ArchThumb,
 	ArchWasm,
 }
diff --git a/src/cmd/link/internal/ld/asmb.go b/src/cmd/link/internal/ld/asmb.go
index fda0439455..b131674b0b 100644
--- a/src/cmd/link/internal/ld/asmb.go
+++ b/src/cmd/link/internal/ld/asmb.go
@@ -112,6 +112,7 @@ func asmb2(ctxt *Link) {
 		objabi.Hfreebsd,
 		objabi.Hlinux,
 		objabi.Hnetbsd,
+		objabi.Hnoos,
 		objabi.Hopenbsd,
 		objabi.Hsolaris:
 		asmbElf(ctxt)
diff --git a/src/cmd/link/internal/ld/data.go b/src/cmd/link/internal/ld/data.go
index 52035e9630..8d42dcc4ff 100644
--- a/src/cmd/link/internal/ld/data.go
+++ b/src/cmd/link/internal/ld/data.go
@@ -48,6 +48,7 @@ import (
 	"strings"
 	"sync"
 	"sync/atomic"
+	"unicode"
 )
 
 // isRuntimeDepPkg reports whether pkg is the runtime package or its dependency
@@ -148,7 +149,7 @@ func FoldSubSymbolOffset(ldr *loader.Loader, s loader.Sym) (loader.Sym, int64) {
 //
 // This is a performance-critical function for the linker; be careful
 // to avoid introducing unnecessary allocations in the main loop.
-func (st *relocSymState) relocsym(s loader.Sym, P []byte) {
+func (st *relocSymState) relocsym(s loader.Sym, P []byte, dwarf bool) {
 	ldr := st.ldr
 	relocs := ldr.Relocs(s)
 	if relocs.Count() == 0 {
@@ -353,6 +354,13 @@ func (st *relocSymState) relocsym(s loader.Sym, P []byte) {
 
 			o = ldr.SymValue(rs) + r.Add()
 
+			if !dwarf && target.IsThumb() && (ldr.SymType(r.Sym()) == sym.STEXT || ldr.SymType(r.Sym()) == sym.SABIALIAS) {
+				if o&1 != 0 {
+					panic("relocsym: thumb bit already set")
+				}
+				o |= 1
+			}
+
 			// On amd64, 4-byte offsets will be sign-extended, so it is impossible to
 			// access more than 2GB of static data; fail at link time is better than
 			// fail at runtime. See https://golang.org/issue/7980.
@@ -948,7 +956,7 @@ func writeBlock(ctxt *Link, out *OutBuf, ldr *loader.Loader, syms []loader.Sym,
 			addr = val
 		}
 		P := out.WriteSym(ldr, s)
-		st.relocsym(s, P)
+		st.relocsym(s, P, false)
 		if f, ok := ctxt.generatorSyms[s]; ok {
 			f(ctxt, s)
 		}
@@ -1416,6 +1424,64 @@ func (ctxt *Link) dodata(symGroupType []sym.SymKind) {
 	// Collect data symbols by type into data.
 	state := dodataState{ctxt: ctxt, symGroupType: symGroupType}
 	ldr := ctxt.loader
+
+	if ctxt.HeadType == objabi.Hnoos {
+		// leave some read-only variables in Flash (hack to save RAM)
+		for s := loader.Sym(1); s < loader.Sym(ldr.NSym()); s++ {
+			if t := state.symType(s); t != sym.SDATA && t != sym.SNOPTRDATA {
+				continue
+			}
+			name := ldr.SymName(s)
+			if len(name) > 8 && strings.HasPrefix(name, "unicode.") {
+				uname := name[8:]
+				if unicode.IsUpper(rune(uname[0])) || strings.HasPrefix(uname, ".stmp_") || strings.HasPrefix(uname, "fold") {
+					state.setSymType(s, sym.SRODATA)
+					continue
+				}
+			}
+			switch name {
+			case "embedded/rtos.errorsByNumber",
+				"math.mPi4", "math._tanP", "math._tanQ", "math._lgamA",
+				"math._lgamR", "math._lgamS", "math._lgamT", "math._lgamU",
+				"math._lgamV", "math._lgamW", "math._sin", "math._cos",
+				"math.pow10tab", "math.pow10postab32", "math.pow10negtab32",
+				"math.tanhP", "math.tanhQ", "math._gamP", "math._gamQ",
+				"math._gamS",
+				"math/big.pow5tab", "math/big._Accuracy_index",
+				"math/big._RoundingMode_index",
+				"math/rand.rngCooked", "math/rand.ke", "math/rand.we",
+				"math/rand.fe", "math/rand.kn", "math/rand.wn", "math/rand.fn",
+				"runtime.staticuint64s", "runtime.oneptrmask",
+				"runtime.fastlog2Table", "runtime.class_to_size",
+				"runtime.class_to_allocnpages", "runtime.class_to_divmagic",
+				"runtime.size_to_class8", "runtime.size_to_class128",
+				"runtime.waitReasonStrings", "runtime.boundsErrorFmts",
+				"runtime.boundsNegErrorFmts", "runtime.finalizer1",
+				"runtime.gcMarkWorkerModeStrings", "runtime.gStatusStrings",
+				"runtime.emptymspan", "runtime.levelBits", "runtime.levelShift",
+				"runtime.levelLogPages", "runtime.consec8tab",
+				"runtime/internal/sys.len8tab", "runtime/internal/sys.ntz8tab",
+				"runtime/internal/sys.deBruijn64tab",
+				"runtime/internal/sys.deBruijnIdx64ctz",
+				"runtime/internal/sys.deBruijnIdx32ctz",
+				"strconv.smallPowersOfTen", "strconv.powersOfTen",
+				"strconv.uint64pow10", "strconv.leftcheats",
+				"strconv.isPrint32", "strconv.isPrint16",
+				"strconv.isNotPrint32", "strconv.isNotPrint16",
+				"strconv.isGraphic", "strconv.float64info",
+				"strconv.float32info",
+				"syscall.errors",
+				"time.std0x", "time.longDayNames", "time.shortDayNames",
+				"time.shortMonthNames", "time.longMonthNames", "time.",
+				"time.daysBefore", "time.utcLoc",
+				"unicode.properties", "unicode.asciiFold", "unicode.caseOrbit",
+				"unicode/utf8.first", "unicode/utf8.acceptRanges":
+
+				state.setSymType(s, sym.SRODATA)
+			}
+		}
+	}
+
 	for s := loader.Sym(1); s < loader.Sym(ldr.NSym()); s++ {
 		if !ldr.AttrReachable(s) || ldr.AttrSpecial(s) || ldr.AttrSubSymbol(s) ||
 			!ldr.TopLevelSym(s) {
@@ -1810,6 +1876,13 @@ func (state *dodataState) allocateDataSections(ctxt *Link) {
 		ldr.SetSymSect(ldr.LookupOrCreateSym("runtime.types", 0), sect)
 		ldr.SetSymSect(ldr.LookupOrCreateSym("runtime.etypes", 0), sect)
 	}
+	if ctxt.HeadType == objabi.Hnoos {
+		ldr.SetSymSect(ldr.LookupOrCreateSym("runtime.ramstart", 0), sect)
+		ldr.SetSymSect(ldr.LookupOrCreateSym("runtime.ramend", 0), sect)
+		ldr.SetSymSect(ldr.LookupOrCreateSym("runtime.romdata", 0), sect)
+		ldr.SetSymSect(ldr.LookupOrCreateSym("runtime.nodmastart", 0), sect)
+		ldr.SetSymSect(ldr.LookupOrCreateSym("runtime.nodmaend", 0), sect)
+	}
 	for _, symn := range sym.ReadOnly {
 		symnStartValue := state.datsize
 		state.assignToSection(sect, symn, sym.SRODATA)
@@ -2339,9 +2412,11 @@ func (ctxt *Link) address() []*sym.Segment {
 	order = append(order, &Segtext)
 	Segtext.Rwx = 05
 	Segtext.Vaddr = va
+	Segtext.Laddr = va
 	for _, s := range Segtext.Sections {
 		va = uint64(Rnd(int64(va), int64(s.Align)))
 		s.Vaddr = va
+		s.Laddr = va
 		va += s.Length
 	}
 
@@ -2366,9 +2441,11 @@ func (ctxt *Link) address() []*sym.Segment {
 		order = append(order, &Segrodata)
 		Segrodata.Rwx = 04
 		Segrodata.Vaddr = va
+		Segrodata.Laddr = va
 		for _, s := range Segrodata.Sections {
 			va = uint64(Rnd(int64(va), int64(s.Align)))
 			s.Vaddr = va
+			s.Laddr = va
 			va += s.Length
 		}
 
@@ -2386,9 +2463,11 @@ func (ctxt *Link) address() []*sym.Segment {
 		order = append(order, &Segrelrodata)
 		Segrelrodata.Rwx = 06
 		Segrelrodata.Vaddr = va
+		Segrelrodata.Laddr = va
 		for _, s := range Segrelrodata.Sections {
 			va = uint64(Rnd(int64(va), int64(s.Align)))
 			s.Vaddr = va
+			s.Laddr = va
 			va += s.Length
 		}
 
@@ -2396,15 +2475,29 @@ func (ctxt *Link) address() []*sym.Segment {
 	}
 
 	va = uint64(Rnd(int64(va), int64(*FlagRound)))
-	if ctxt.HeadType == objabi.Haix && len(Segrelrodata.Sections) == 0 {
-		// Data sections are moved to an unreachable segment
-		// to ensure that they are position-independent.
-		// Already done if relro sections exist.
-		va += uint64(XCOFFDATABASE) - uint64(XCOFFTEXTBASE)
+	la := va
+	switch ctxt.HeadType {
+	case objabi.Haix:
+		if len(Segrelrodata.Sections) == 0 {
+			// Data sections are moved to an unreachable segment
+			// to ensure that they are position-independent.
+			// Already done if relro sections exist.
+			va += uint64(XCOFFDATABASE) - uint64(XCOFFTEXTBASE)
+			la = va
+		}
+	case objabi.Hnoos:
+		switch ctxt.Arch {
+		case sys.ArchThumb:
+			// Main stack on the lowest addresses so overflows can be detected
+			// even without MPU. Segdata.Laddr is set to main stack size (see
+			// ../thumb/asm.go:/Laddr = /) BUG: Assumes single-core system.
+			va = RAM.Base + Segdata.Laddr
+		}
 	}
 	order = append(order, &Segdata)
 	Segdata.Rwx = 06
 	Segdata.Vaddr = va
+	Segdata.Laddr = la
 	var data *sym.Section
 	var noptr *sym.Section
 	var bss *sym.Section
@@ -2418,13 +2511,16 @@ func (ctxt *Link) address() []*sym.Segment {
 			vlen = int64(Segdata.Sections[i+1].Vaddr - s.Vaddr)
 		}
 		s.Vaddr = va
+		s.Laddr = la
 		va += uint64(vlen)
 		Segdata.Length = va - Segdata.Vaddr
 		if s.Name == ".data" {
 			data = s
+			la += uint64(vlen)
 		}
 		if s.Name == ".noptrdata" {
 			noptr = s
+			la += uint64(vlen)
 		}
 		if s.Name == ".bss" {
 			bss = s
@@ -2439,18 +2535,23 @@ func (ctxt *Link) address() []*sym.Segment {
 	Segdata.Filelen = bss.Vaddr - Segdata.Vaddr
 
 	va = uint64(Rnd(int64(va), int64(*FlagRound)))
+	la = uint64(Rnd(int64(la), int64(*FlagRound)))
 	order = append(order, &Segdwarf)
 	Segdwarf.Rwx = 06
 	Segdwarf.Vaddr = va
+	Segdwarf.Vaddr = la
 	for i, s := range Segdwarf.Sections {
 		vlen := int64(s.Length)
 		if i+1 < len(Segdwarf.Sections) {
 			vlen = int64(Segdwarf.Sections[i+1].Vaddr - s.Vaddr)
 		}
 		s.Vaddr = va
+		s.Laddr = la
 		va += uint64(vlen)
+		la += uint64(vlen)
 		if ctxt.HeadType == objabi.Hwindows {
 			va = uint64(Rnd(int64(va), PEFILEALIGN))
+			la = uint64(Rnd(int64(la), PEFILEALIGN))
 		}
 		Segdwarf.Length = va - Segdwarf.Vaddr
 	}
@@ -2565,6 +2666,23 @@ func (ctxt *Link) address() []*sym.Segment {
 		ldr.SetSymSect(ldr.Lookup("_end", 0), ldr.SymSect(end))
 	}
 
+	if ctxt.HeadType == objabi.Hnoos {
+		ramstart := int64(RAM.Base)
+		ramend := int64(RAM.Base + RAM.Size)
+		ctxt.xdefine("runtime.ramstart", sym.SRODATA, ramstart)
+		ctxt.xdefine("runtime.ramend", sym.SRODATA, ramend)
+		ctxt.xdefine("runtime.romdata", sym.SRODATA, int64(Segdata.Laddr))
+		nodmastart := int64(NoDMA.Base)
+		nodmaend := int64(NoDMA.Base + NoDMA.Size)
+		if nodmastart == 0 && nodmaend == 0 {
+			// avoid "relocation does not fit in 32-bits" kind of errors
+			nodmastart = ramstart
+			nodmaend = ramstart
+		}
+		ctxt.xdefine("runtime.nodmastart", sym.SRODATA, nodmastart)
+		ctxt.xdefine("runtime.nodmaend", sym.SRODATA, nodmaend)
+	}
+
 	return order
 }
 
@@ -2647,7 +2765,7 @@ func compressSyms(ctxt *Link, syms []loader.Sym) []byte {
 		if relocs.Count() != 0 {
 			relocbuf = append(relocbuf[:0], P...)
 			P = relocbuf
-			st.relocsym(s, P)
+			st.relocsym(s, P, true)
 		}
 		if _, err := z.Write(P); err != nil {
 			log.Fatalf("compression failed: %s", err)
diff --git a/src/cmd/link/internal/ld/deadcode.go b/src/cmd/link/internal/ld/deadcode.go
index bfa8640ba9..16187f9de6 100644
--- a/src/cmd/link/internal/ld/deadcode.go
+++ b/src/cmd/link/internal/ld/deadcode.go
@@ -11,11 +11,36 @@ import (
 	"cmd/link/internal/loader"
 	"cmd/link/internal/sym"
 	"fmt"
+	"strconv"
 	"unicode"
 )
 
 var _ = fmt.Print
 
+var cortexmSystemHandlers = [...]string{
+	"runtime.nmiHandler",
+	"runtime.hardfaultHandler",
+	"runtime.memmanageHandler",
+	"runtime.busfaultHandler",
+	"runtime.usagefaultHandler",
+	"runtime.securefaultHandler",
+	"runtime.reservedHandler",
+	"runtime.reservedHandler",
+	"runtime.reservedHandler",
+	"runtime.svcallHandler",
+	"runtime.debugmonHandler",
+	"runtime.reservedHandler",
+	"runtime.pendsvHandler",
+	"SysTick_Handler",
+}
+
+func InterruptHandler(irqn int) string {
+	if irqn < 0 {
+		return cortexmSystemHandlers[irqn+14]
+	}
+	return "IRQ" + strconv.Itoa(irqn) + "_Handler"
+}
+
 type deadcodePass struct {
 	ctxt *Link
 	ldr  *loader.Loader
@@ -50,6 +75,20 @@ func (d *deadcodePass) init() {
 
 	var names []string
 
+	if d.ctxt.HeadType == objabi.Hnoos {
+		// mark interrupt handlers
+		var first, last int
+		switch d.ctxt.Arch.Family {
+		case sys.Thumb:
+			first, last = -14, 479
+		case sys.RISCV64:
+			first, last = 1, 1023
+		}
+		for i := first; i <= last; i++ {
+			names = append(names, InterruptHandler(i))
+		}
+	}
+
 	// In a normal binary, start at main.main and the init
 	// functions and mark what is reachable from there.
 	if d.ctxt.linkShared && (d.ctxt.BuildMode == BuildModeExe || d.ctxt.BuildMode == BuildModePIE) {
@@ -58,12 +97,12 @@ func (d *deadcodePass) init() {
 		// The external linker refers main symbol directly.
 		if d.ctxt.LinkMode == LinkExternal && (d.ctxt.BuildMode == BuildModeExe || d.ctxt.BuildMode == BuildModePIE) {
 			if d.ctxt.HeadType == objabi.Hwindows && d.ctxt.Arch.Family == sys.I386 {
-				*flagEntrySymbol = "_main"
+				*FlagEntrySymbol = "_main"
 			} else {
-				*flagEntrySymbol = "main"
+				*FlagEntrySymbol = "main"
 			}
 		}
-		names = append(names, *flagEntrySymbol)
+		names = append(names, *FlagEntrySymbol)
 		if !d.ctxt.linkShared && d.ctxt.BuildMode != BuildModePlugin {
 			// runtime.buildVersion and runtime.modinfo are referenced in .go.buildinfo section
 			// (see function buildinfo in data.go). They should normally be reachable from the
@@ -282,7 +321,7 @@ func (d *deadcodePass) markMethod(m methodref) {
 // deadcode marks all reachable symbols.
 //
 // The basis of the dead code elimination is a flood fill of symbols,
-// following their relocations, beginning at *flagEntrySymbol.
+// following their relocations, beginning at *FlagEntrySymbol.
 //
 // This flood fill is wrapped in logic for pruning unused methods.
 // All methods are mentioned by relocations on their receiver's *rtype.
diff --git a/src/cmd/link/internal/ld/dwarf.go b/src/cmd/link/internal/ld/dwarf.go
index 2ab9a55e96..47bda0ed4d 100644
--- a/src/cmd/link/internal/ld/dwarf.go
+++ b/src/cmd/link/internal/ld/dwarf.go
@@ -1171,7 +1171,17 @@ func expandFile(fname string) string {
 	if strings.HasPrefix(fname, src.FileSymPrefix) {
 		fname = fname[len(src.FileSymPrefix):]
 	}
-	return expandGoroot(fname)
+	fname = expandGoroot(fname)
+	if *stripFuncNames > 0 {
+		if *stripFuncNames == 1 {
+			if i := strings.LastIndex(fname, "/"); i >= 0 {
+				fname = fname[i+1:]
+			}
+		} else {
+			fname = ""
+		}
+	}
+	return fname
 }
 
 // writeDirFileTables emits the portion of the DWARF line table
diff --git a/src/cmd/link/internal/ld/elf.go b/src/cmd/link/internal/ld/elf.go
index f5823a8fbf..0c5c649db0 100644
--- a/src/cmd/link/internal/ld/elf.go
+++ b/src/cmd/link/internal/ld/elf.go
@@ -237,10 +237,10 @@ func Elfinit(ctxt *Link) {
 		ehdr.Shentsize = ELF64SHDRSIZE /* Must be ELF64SHDRSIZE */
 
 	// 32-bit architectures
-	case sys.ARM, sys.MIPS:
-		if ctxt.Arch.Family == sys.ARM {
+	case sys.ARM, sys.Thumb, sys.MIPS:
+		if ctxt.Arch.Family == sys.ARM || ctxt.Arch.Family == sys.Thumb {
 			// we use EABI on linux/arm, freebsd/arm, netbsd/arm.
-			if ctxt.HeadType == objabi.Hlinux || ctxt.HeadType == objabi.Hfreebsd || ctxt.HeadType == objabi.Hnetbsd {
+			if ctxt.HeadType == objabi.Hlinux || ctxt.HeadType == objabi.Hfreebsd || ctxt.HeadType == objabi.Hnetbsd || ctxt.HeadType == objabi.Hnoos {
 				// We set a value here that makes no indication of which
 				// float ABI the object uses, because this is information
 				// used by the dynamic linker to compare executables and
@@ -899,7 +899,7 @@ func elfphload(seg *sym.Segment) *ElfPhdr {
 		ph.Flags |= elf.PF_X
 	}
 	ph.Vaddr = seg.Vaddr
-	ph.Paddr = seg.Vaddr
+	ph.Paddr = seg.Laddr
 	ph.Memsz = seg.Length
 	ph.Off = seg.Fileoff
 	ph.Filesz = seg.Filelen
@@ -912,7 +912,7 @@ func elfphrelro(seg *sym.Segment) {
 	ph := newElfPhdr()
 	ph.Type = elf.PT_GNU_RELRO
 	ph.Vaddr = seg.Vaddr
-	ph.Paddr = seg.Vaddr
+	ph.Paddr = seg.Laddr
 	ph.Memsz = seg.Length
 	ph.Off = seg.Fileoff
 	ph.Filesz = seg.Filelen
@@ -1519,7 +1519,7 @@ func asmbElf(ctxt *Link) {
 		Exitf("unknown architecture in asmbelf: %v", ctxt.Arch.Family)
 	case sys.MIPS, sys.MIPS64:
 		eh.Machine = uint16(elf.EM_MIPS)
-	case sys.ARM:
+	case sys.ARM, sys.Thumb:
 		eh.Machine = uint16(elf.EM_ARM)
 	case sys.AMD64:
 		eh.Machine = uint16(elf.EM_X86_64)
@@ -1592,26 +1592,28 @@ func asmbElf(ctxt *Link) {
 	}
 
 	/* program header info */
-	pph = newElfPhdr()
-
-	pph.Type = elf.PT_PHDR
-	pph.Flags = elf.PF_R
-	pph.Off = uint64(eh.Ehsize)
-	pph.Vaddr = uint64(*FlagTextAddr) - uint64(HEADR) + pph.Off
-	pph.Paddr = uint64(*FlagTextAddr) - uint64(HEADR) + pph.Off
-	pph.Align = uint64(*FlagRound)
-
-	/*
-	 * PHDR must be in a loaded segment. Adjust the text
-	 * segment boundaries downwards to include it.
-	 */
-	{
-		o := int64(Segtext.Vaddr - pph.Vaddr)
-		Segtext.Vaddr -= uint64(o)
-		Segtext.Length += uint64(o)
-		o = int64(Segtext.Fileoff - pph.Off)
-		Segtext.Fileoff -= uint64(o)
-		Segtext.Filelen += uint64(o)
+	if ctxt.HeadType != objabi.Hnoos {
+		pph = newElfPhdr()
+
+		pph.Type = elf.PT_PHDR
+		pph.Flags = elf.PF_R
+		pph.Off = uint64(eh.Ehsize)
+		pph.Vaddr = uint64(*FlagTextAddr) - uint64(HEADR) + pph.Off
+		pph.Paddr = uint64(*FlagTextAddr) - uint64(HEADR) + pph.Off
+		pph.Align = uint64(*FlagRound)
+
+		/*
+		 * PHDR must be in a loaded segment. Adjust the text
+		 * segment boundaries downwards to include it.
+		 */
+		{
+			o := int64(Segtext.Vaddr - pph.Vaddr)
+			Segtext.Vaddr -= uint64(o)
+			Segtext.Length += uint64(o)
+			o = int64(Segtext.Fileoff - pph.Off)
+			Segtext.Fileoff -= uint64(o)
+			Segtext.Filelen += uint64(o)
+		}
 	}
 
 	if !*FlagD { /* -d suppresses dynamic loader format */
@@ -1695,7 +1697,7 @@ func asmbElf(ctxt *Link) {
 		phsh(pnote, sh)
 	}
 
-	if *flagBuildid != "" {
+	if *flagBuildid != "" && ctxt.HeadType != objabi.Hnoos {
 		sh := elfshname(".note.go.buildid")
 		resoff -= int64(elfgobuildid(sh, uint64(startva), uint64(resoff)))
 
@@ -2039,7 +2041,7 @@ elfobj:
 		if len(buildinfo) > 0 {
 			a += int64(elfwritebuildinfo(ctxt.Out))
 		}
-		if *flagBuildid != "" {
+		if *flagBuildid != "" && ctxt.HeadType != objabi.Hnoos {
 			a += int64(elfwritegobuildid(ctxt.Out))
 		}
 	}
diff --git a/src/cmd/link/internal/ld/lib.go b/src/cmd/link/internal/ld/lib.go
index 18db567041..edcc38de35 100644
--- a/src/cmd/link/internal/ld/lib.go
+++ b/src/cmd/link/internal/ld/lib.go
@@ -385,16 +385,16 @@ func libinit(ctxt *Link) {
 		Exitf("cannot create %s: %v", *flagOutfile, err)
 	}
 
-	if *flagEntrySymbol == "" {
+	if *FlagEntrySymbol == "" {
 		switch ctxt.BuildMode {
 		case BuildModeCShared, BuildModeCArchive:
-			*flagEntrySymbol = fmt.Sprintf("_rt0_%s_%s_lib", objabi.GOARCH, objabi.GOOS)
+			*FlagEntrySymbol = fmt.Sprintf("_rt0_%s_%s_lib", objabi.GOARCH, objabi.GOOS)
 		case BuildModeExe, BuildModePIE:
-			*flagEntrySymbol = fmt.Sprintf("_rt0_%s_%s", objabi.GOARCH, objabi.GOOS)
+			*FlagEntrySymbol = fmt.Sprintf("_rt0_%s_%s", objabi.GOARCH, objabi.GOOS)
 		case BuildModeShared, BuildModePlugin:
-			// No *flagEntrySymbol for -buildmode=shared and plugin
+			// No *FlagEntrySymbol for -buildmode=shared and plugin
 		default:
-			Errorf(nil, "unknown *flagEntrySymbol for buildmode %v", ctxt.BuildMode)
+			Errorf(nil, "unknown *FlagEntrySymbol for buildmode %v", ctxt.BuildMode)
 		}
 	}
 }
@@ -780,7 +780,7 @@ func (ctxt *Link) linksetup() {
 
 		// In addition, on ARM, the runtime depends on the linker
 		// recording the value of GOARM.
-		if ctxt.Arch.Family == sys.ARM {
+		if ctxt.Arch.Family == sys.ARM || ctxt.Arch.Family == sys.Thumb {
 			goarm := ctxt.loader.LookupOrCreateSym("runtime.goarm", 0)
 			sb := ctxt.loader.MakeSymbolUpdater(goarm)
 			sb.SetType(sym.SDATA)
@@ -2434,7 +2434,7 @@ func datoff(ldr *loader.Loader, s loader.Sym, addr int64) int64 {
 }
 
 func Entryvalue(ctxt *Link) int64 {
-	a := *flagEntrySymbol
+	a := *FlagEntrySymbol
 	if a[0] >= '0' && a[0] <= '9' {
 		return atolwhex(a)
 	}
@@ -2447,6 +2447,9 @@ func Entryvalue(ctxt *Link) int64 {
 	if !ctxt.IsAIX() && st != sym.STEXT {
 		ldr.Errorf(s, "entry not text")
 	}
+	if ctxt.Arch.Family == sys.Thumb {
+		return ldr.SymValue(s) + 1
+	}
 	return ldr.SymValue(s)
 }
 
diff --git a/src/cmd/link/internal/ld/main.go b/src/cmd/link/internal/ld/main.go
index 5a096f1b3b..0a8dfd9afc 100644
--- a/src/cmd/link/internal/ld/main.go
+++ b/src/cmd/link/internal/ld/main.go
@@ -41,6 +41,7 @@ import (
 	"os"
 	"runtime"
 	"runtime/pprof"
+	"strconv"
 	"strings"
 )
 
@@ -48,6 +49,8 @@ var (
 	pkglistfornote []byte
 	windowsgui     bool // writes a "GUI binary" instead of a "console binary"
 	ownTmpDir      bool // set to true if tmp dir created by linker (e.g. no -tmpdir)
+	RAM            MemBlock
+	NoDMA          MemBlock
 )
 
 func init() {
@@ -91,7 +94,7 @@ var (
 	FlagStrictDups    = flag.Int("strictdups", 0, "sanity check duplicate symbol contents during object file reading (1=warn 2=err).")
 	FlagRound         = flag.Int("R", -1, "set address rounding `quantum`")
 	FlagTextAddr      = flag.Int64("T", -1, "set text segment `address`")
-	flagEntrySymbol   = flag.String("E", "", "set `entry` symbol name")
+	FlagEntrySymbol   = flag.String("E", "", "set `entry` symbol name")
 
 	cpuprofile     = flag.String("cpuprofile", "", "write cpu profile to `file`")
 	memprofile     = flag.String("memprofile", "", "write memory profile to `file`")
@@ -99,8 +102,44 @@ var (
 
 	benchmarkFlag     = flag.String("benchmark", "", "set to 'mem' or 'cpu' to enable phase benchmarking")
 	benchmarkFileFlag = flag.String("benchmarkprofile", "", "emit phase profiles to `base`_phase.{cpu,mem}prof")
+
+	stripFuncNames = flag.Int("stripfn", 0, "strip function names in pclntab, 1: remove package path, 2: blank names")
 )
 
+type MemBlock struct {
+	Base, Size uint64
+}
+
+func (mb *MemBlock) set(descr string) {
+	i := strings.IndexByte(descr, ':')
+	if i < 0 {
+		Exitf("memory layout (-M): no BASE:SIZE separator: %s", descr)
+	}
+	var err error
+	mb.Base, err = strconv.ParseUint(descr[:i], 0, 64)
+	if err != nil {
+		Exitf("memory layout (-M): bad BASE address: %v", err)
+	}
+	size := descr[i+1:]
+	scale := uint64(1)
+	switch size[len(size)-1] {
+	case 'K':
+		scale = 1024
+	case 'M':
+		scale = 1024 * 1024
+	case 'G':
+		scale = 1024 * 1024 * 1024
+	}
+	if scale != 1 {
+		size = size[:len(size)-1]
+	}
+	mb.Size, err = strconv.ParseUint(size, 0, 64)
+	if err != nil {
+		Exitf("memory layout (-M): bad SIZE: %v", err)
+	}
+	mb.Size *= scale
+}
+
 // Main is the main entry point for the linker code.
 func Main(arch *sys.Arch, theArch Arch) {
 	thearch = theArch
@@ -136,8 +175,29 @@ func Main(arch *sys.Arch, theArch Arch) {
 	objabi.Flagcount("v", "print link trace", &ctxt.Debugvlog)
 	objabi.Flagfn1("importcfg", "read import configuration from `file`", ctxt.readImportCfg)
 
+	var flagMemory string
+	if objabi.GOOS == "noos" {
+		flag.StringVar(&flagMemory, "M", "", "set memory layout: BASE1:SIZE1[,BASE2:SIZE2]")
+	}
+
 	objabi.Flagparse(usage)
 
+	if objabi.GOOS == "noos" {
+		descr := strings.Split(flagMemory, ",")
+		if len(descr) == 0 {
+			Exitf("memory layout (-M) not specified")
+		}
+		if len(descr) > 0 {
+			RAM.set(descr[0])
+		}
+		if len(descr) > 1 {
+			NoDMA.set(descr[1])
+		}
+		if len(descr) > 2 {
+			Exitf("-M describes more than two memory blocks")
+		}
+	}
+
 	if ctxt.Debugvlog > 0 {
 		// dump symbol info on crash
 		defer func() { ctxt.loader.Dump() }()
diff --git a/src/cmd/link/internal/ld/pcln.go b/src/cmd/link/internal/ld/pcln.go
index 72bf33e611..cb6f7433c3 100644
--- a/src/cmd/link/internal/ld/pcln.go
+++ b/src/cmd/link/internal/ld/pcln.go
@@ -147,7 +147,7 @@ func computeDeferReturn(ctxt *Link, deferReturnSym, s loader.Sym) uint32 {
 				switch target.Arch.Family {
 				case sys.AMD64, sys.I386:
 					deferreturn--
-				case sys.PPC64, sys.ARM, sys.ARM64, sys.MIPS, sys.MIPS64:
+				case sys.PPC64, sys.ARM, sys.ARM64, sys.MIPS, sys.MIPS64, sys.Thumb:
 					// no change
 				case sys.RISCV64:
 					// TODO(jsing): The JALR instruction is marked with
@@ -550,6 +550,12 @@ func (state *pclntab) generateFunctab(ctxt *Link, funcs []loader.Sym, inlSyms ma
 			// We need to write the offset.
 			setAddr = func(s *loader.SymbolBuilder, arch *sys.Arch, off int64, tgt loader.Sym, add int64) int64 {
 				if v := ldr.SymValue(tgt); v != 0 {
+					if arch.Family == sys.Thumb && (ldr.SymType(tgt) == sym.STEXT || ldr.SymType(tgt) == sym.SABIALIAS) {
+						if add&1 != 0 {
+							panic("generateFunctab: thumb bit already set")
+						}
+						add |= 1
+					}
 					s.SetUint(arch, off, uint64(v+add))
 				}
 				return 0
diff --git a/src/cmd/link/internal/ld/pe.go b/src/cmd/link/internal/ld/pe.go
index 5edaf54dd2..933aa3be17 100644
--- a/src/cmd/link/internal/ld/pe.go
+++ b/src/cmd/link/internal/ld/pe.go
@@ -472,7 +472,7 @@ func (f *peFile) addInitArray(ctxt *Link) *peSection {
 	ctxt.Out.SeekSet(int64(sect.pointerToRawData))
 	sect.checkOffset(ctxt.Out.Offset())
 
-	init_entry := ctxt.loader.Lookup(*flagEntrySymbol, 0)
+	init_entry := ctxt.loader.Lookup(*FlagEntrySymbol, 0)
 	addr := uint64(ctxt.loader.SymValue(init_entry)) - ctxt.loader.SymSect(init_entry).Vaddr
 	switch objabi.GOARCH {
 	case "386", "arm":
diff --git a/src/cmd/link/internal/ld/sym.go b/src/cmd/link/internal/ld/sym.go
index 75489720cc..4500e4211e 100644
--- a/src/cmd/link/internal/ld/sym.go
+++ b/src/cmd/link/internal/ld/sym.go
@@ -82,6 +82,7 @@ func (ctxt *Link) computeTLSOffset() {
 		objabi.Hnetbsd,
 		objabi.Hopenbsd,
 		objabi.Hdragonfly,
+		objabi.Hnoos,
 		objabi.Hsolaris:
 		/*
 		 * ELF uses TLS offset negative from FS.
diff --git a/src/cmd/link/internal/ld/symtab.go b/src/cmd/link/internal/ld/symtab.go
index f54cf9ea2f..926099fef8 100644
--- a/src/cmd/link/internal/ld/symtab.go
+++ b/src/cmd/link/internal/ld/symtab.go
@@ -126,6 +126,9 @@ func putelfsym(ctxt *Link, x loader.Sym, typ elf.SymType, curbind elf.SymBind) {
 	if ctxt.LinkMode == LinkExternal && elfshnum != elf.SHN_UNDEF {
 		addr -= int64(xosect.Vaddr)
 	}
+	if ctxt.IsThumb() && typ == elf.STT_FUNC {
+		addr += 1
+	}
 	other := int(elf.STV_DEFAULT)
 	if ldr.AttrVisibilityHidden(x) {
 		// TODO(mwhudson): We only set AttrVisibilityHidden in ldelf, i.e. when
@@ -430,7 +433,7 @@ func (ctxt *Link) symtab(pcln *pclntab) []sym.SymKind {
 	if !ctxt.IsAIX() {
 		switch ctxt.BuildMode {
 		case BuildModeCArchive, BuildModeCShared:
-			s := ldr.Lookup(*flagEntrySymbol, sym.SymVerABI0)
+			s := ldr.Lookup(*FlagEntrySymbol, sym.SymVerABI0)
 			if s != 0 {
 				addinitarrdata(ctxt, ldr, s)
 			}
diff --git a/src/cmd/link/internal/ld/target.go b/src/cmd/link/internal/ld/target.go
index f68de8fff1..e9bbf108eb 100644
--- a/src/cmd/link/internal/ld/target.go
+++ b/src/cmd/link/internal/ld/target.go
@@ -124,6 +124,10 @@ func (t *Target) IsS390X() bool {
 	return t.Arch.Family == sys.S390X
 }
 
+func (t *Target) IsThumb() bool {
+	return t.Arch.Family == sys.Thumb
+}
+
 func (t *Target) IsWasm() bool {
 	return t.Arch.Family == sys.Wasm
 }
diff --git a/src/cmd/link/internal/ld/xcoff.go b/src/cmd/link/internal/ld/xcoff.go
index ba818eaa96..4dbebb82ae 100644
--- a/src/cmd/link/internal/ld/xcoff.go
+++ b/src/cmd/link/internal/ld/xcoff.go
@@ -1288,7 +1288,7 @@ func (ctxt *Link) doxcoff() {
 	toc.SetVisibilityHidden(true)
 
 	// Add entry point to .loader symbols.
-	ep := ldr.Lookup(*flagEntrySymbol, 0)
+	ep := ldr.Lookup(*FlagEntrySymbol, 0)
 	if ep == 0 || !ldr.AttrReachable(ep) {
 		Exitf("wrong entry point")
 	}
@@ -1419,7 +1419,7 @@ func (f *xcoffFile) writeLdrScn(ctxt *Link, globalOff uint64) {
 		return r1.symndx < r2.symndx
 	})
 
-	ep := ldr.Lookup(*flagEntrySymbol, 0)
+	ep := ldr.Lookup(*FlagEntrySymbol, 0)
 	xldr := &XcoffLdRel64{
 		Lvaddr:  uint64(ldr.SymValue(ep)),
 		Lrtype:  0x3F00,
@@ -1547,7 +1547,7 @@ func (f *xcoffFile) writeFileHeader(ctxt *Link) {
 		f.xahdr.Ovstamp = 1 // based on dump -o
 		f.xahdr.Omagic = 0x10b
 		copy(f.xahdr.Omodtype[:], "1L")
-		entry := ldr.Lookup(*flagEntrySymbol, 0)
+		entry := ldr.Lookup(*FlagEntrySymbol, 0)
 		f.xahdr.Oentry = uint64(ldr.SymValue(entry))
 		f.xahdr.Osnentry = f.getXCOFFscnum(ldr.SymSect(entry))
 		toc := ldr.Lookup("TOC", 0)
diff --git a/src/cmd/link/internal/riscv64/asm.go b/src/cmd/link/internal/riscv64/asm.go
index c18e0540d8..1714f32077 100644
--- a/src/cmd/link/internal/riscv64/asm.go
+++ b/src/cmd/link/internal/riscv64/asm.go
@@ -20,7 +20,69 @@ import (
 // fakeLabelName matches the RISCV_FAKE_LABEL_NAME from binutils.
 const fakeLabelName = ".L0 "
 
+func lookupFuncSym(ldr *loader.Loader, name string) loader.Sym {
+	if s := ldr.Lookup(name, sym.SymVerABI0); s != 0 && ldr.SymType(s) == sym.STEXT {
+		return s
+	}
+	if s := ldr.Lookup(name, sym.SymVerABIInternal); s != 0 && ldr.SymType(s) == sym.STEXT {
+		return s
+	}
+	return 0
+}
+
 func gentext(ctxt *ld.Link, ldr *loader.Loader) {
+	if ctxt.HeadType != objabi.Hnoos {
+		return
+	}
+	vectors := ldr.CreateSymForUpdate("runtime.vectors", sym.SymVerABI0)
+	vectors.SetType(sym.STEXT)
+	vectors.SetReachable(true)
+	vectors.SetAlign(8)
+
+	unhandledInterrupt := ldr.Lookup("runtime.unhandledExternalInterrupt", sym.SymVerABI0)
+	if unhandledInterrupt == 0 {
+		ld.Errorf(nil, "runtime.unhandledExternalInterrupt not defined")
+	}
+
+	// search for user defined ISRs: //go:linkname functionName IRQ%d_Handler
+	var irqHandlers [1024]loader.Sym // BUG: 1024 is PLIC specific
+	irqNum := 1
+	for i := 1; i < len(irqHandlers); i++ {
+		s := lookupFuncSym(ldr, ld.InterruptHandler(i))
+		if s == 0 {
+			irqHandlers[i] = unhandledInterrupt
+		} else {
+			irqHandlers[i] = s
+			irqNum = i + 1
+		}
+	}
+
+	vectors.AddUint64(ctxt.Arch, uint64(irqNum))
+	relocs := vectors.AddRelocs(irqNum - 1)
+	for i, s := range irqHandlers[1:irqNum] {
+		ldr.MakeSymbolUpdater(s).SetReachable(true)
+		rel := relocs.At(i)
+		rel.SetSym(s)
+		rel.SetType(objabi.R_ADDR)
+		rel.SetSiz(8)
+		rel.SetOff(int32(vectors.AddUint64(ctxt.Arch, 0)))
+	}
+
+	ctxt.Textp = append(ctxt.Textp, vectors.Sym())
+
+	// move the entry symbol at the beggining of the text segment
+	entry := lookupFuncSym(ldr, *ld.FlagEntrySymbol)
+	if entry == 0 {
+		ld.Errorf(nil, "cannot find entry function: %s", *ld.FlagEntrySymbol)
+	}
+	for i, s := range ctxt.Textp {
+		if s == entry {
+			copy(ctxt.Textp[1:], ctxt.Textp[:i])
+			ctxt.Textp[0] = s
+			return
+		}
+	}
+	ldr.Errorf(entry, "cannot find symbol in ctxt.Textp")
 }
 
 func genSymsLate(ctxt *ld.Link, ldr *loader.Loader) {
diff --git a/src/cmd/link/internal/riscv64/obj.go b/src/cmd/link/internal/riscv64/obj.go
index 917324d922..c4d2624a41 100644
--- a/src/cmd/link/internal/riscv64/obj.go
+++ b/src/cmd/link/internal/riscv64/obj.go
@@ -54,6 +54,16 @@ func archinit(ctxt *ld.Link) {
 		if *ld.FlagRound == -1 {
 			*ld.FlagRound = 0x10000
 		}
+	case objabi.Hnoos:
+		*ld.FlagD = true
+		ld.Elfinit(ctxt)
+		ld.HEADR = ld.ELFRESERVE
+		if *ld.FlagTextAddr == -1 {
+			*ld.FlagTextAddr = int64(ld.RAM.Base)
+		}
+		if *ld.FlagRound == -1 {
+			*ld.FlagRound = 8
+		}
 	default:
 		ld.Exitf("unknown -H option: %v", ctxt.HeadType)
 	}
diff --git a/src/cmd/link/internal/sym/segment.go b/src/cmd/link/internal/sym/segment.go
index 97853b9355..ec1cbb4c5b 100644
--- a/src/cmd/link/internal/sym/segment.go
+++ b/src/cmd/link/internal/sym/segment.go
@@ -38,6 +38,7 @@ package sym
 type Segment struct {
 	Rwx      uint8  // permission as usual unix bits (5 = r-x etc)
 	Vaddr    uint64 // virtual address
+	Laddr    uint64 // load address
 	Length   uint64 // length in memory
 	Fileoff  uint64 // file offset
 	Filelen  uint64 // length on disk
@@ -50,6 +51,7 @@ type Section struct {
 	Align   int32
 	Name    string
 	Vaddr   uint64
+	Laddr   uint64 
 	Length  uint64
 	Seg     *Segment
 	Elfsect interface{} // an *ld.ElfShdr
diff --git a/src/cmd/link/internal/thumb/asm.go b/src/cmd/link/internal/thumb/asm.go
new file mode 100644
index 0000000000..9a65e1f24b
--- /dev/null
+++ b/src/cmd/link/internal/thumb/asm.go
@@ -0,0 +1,296 @@
+// Inferno utils/5l/asm.c
+// https://bitbucket.org/inferno-os/inferno-os/src/master/utils/5l/asm.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"cmd/internal/objabi"
+	"cmd/internal/sys"
+	"cmd/link/internal/ld"
+	"cmd/link/internal/loader"
+	"cmd/link/internal/sym"
+	"debug/elf"
+	"fmt"
+	"log"
+)
+
+const pcoff = 4 // in Thumb mode PC points 4 bytes forward
+
+func lookupFuncSym(ldr *loader.Loader, name string) loader.Sym {
+	if s := ldr.Lookup(name, sym.SymVerABI0); s != 0 && ldr.SymType(s) == sym.STEXT {
+		return s
+	}
+	if s := ldr.Lookup(name, sym.SymVerABIInternal); s != 0 && ldr.SymType(s) == sym.STEXT {
+		return s
+	}
+	return 0
+}
+
+func gentext(ctxt *ld.Link, ldr *loader.Loader) {
+	if ctxt.HeadType != objabi.Hnoos {
+		return
+	}
+	vectors := ldr.CreateSymForUpdate("runtime.vectors", sym.SymVerABI0)
+	vectors.SetType(sym.STEXT)
+	vectors.SetReachable(true)
+	vectors.SetAlign(4)
+
+	unhandledException := ldr.Lookup("runtime.unhandledException", sym.SymVerABI0)
+	if unhandledException == 0 {
+		ld.Errorf(nil, "runtime.unhandledException not defined")
+	}
+
+	// search for user defined ISRs: //go:linkname functionName IRQ%d_Handler
+	var irqHandlers [480]loader.Sym
+	irqNum := 0
+	for i := range irqHandlers {
+		s := lookupFuncSym(ldr, ld.InterruptHandler(i))
+		if s == 0 {
+			irqHandlers[i] = unhandledException
+		} else {
+			irqHandlers[i] = s
+			irqNum = i + 1 // BUG: saves memory but prevents detect all unhandled interrupts
+		}
+	}
+
+	ld.Segdata.Laddr = 2048 // communicate the main stack size to Link.address()
+	msp := uint32(ld.RAM.Base + ld.Segdata.Laddr)
+	vectors.AddUint32(ctxt.Arch, msp) // Main Stack Pointer after reset
+
+	relocs := vectors.AddRelocs(irqNum + 15)
+	addHandler := func(irqn int, fname string) {
+		s := lookupFuncSym(ldr, fname)
+		if s == 0 {
+			s = unhandledException
+		}
+		ldr.MakeSymbolUpdater(s).SetReachable(true)
+		rel := relocs.At(irqn + 15)
+		rel.SetSym(s)
+		rel.SetType(objabi.R_ADDR)
+		rel.SetSiz(4)
+		rel.SetOff(int32(vectors.AddUint32(ctxt.Arch, 0)))
+	}
+
+	// system exception handlers
+	addHandler(-15, *ld.FlagEntrySymbol) // reset handler
+	for irqn := -14; irqn < 0; irqn++ {
+		addHandler(irqn, ld.InterruptHandler(irqn)) // exception handler
+	}
+
+	// external interrupt handlers
+	for i, s := range irqHandlers[:irqNum] {
+		ldr.MakeSymbolUpdater(s).SetReachable(true)
+		rel := relocs.At(i + 15)
+		rel.SetSym(s)
+		rel.SetType(objabi.R_ADDR)
+		rel.SetSiz(4)
+		rel.SetOff(int32(vectors.AddUint32(ctxt.Arch, 0)))
+	}
+
+	// add the vectors symbol at the beggining of the text segment
+	ctxt.Textp = append(ctxt.Textp, 0)
+	copy(ctxt.Textp[1:], ctxt.Textp)
+	ctxt.Textp[0] = vectors.Sym()
+}
+
+func elfreloc1(ctxt *ld.Link, out *ld.OutBuf, ldr *loader.Loader, s loader.Sym, r loader.ExtReloc, ri int, sectoff int64) bool {
+	out.Write32(uint32(sectoff))
+
+	elfsym := ld.ElfSymForReloc(ctxt, r.Xsym)
+	siz := r.Size
+	switch r.Type {
+	default:
+		return false
+	case objabi.R_ADDR, objabi.R_DWARFSECREF:
+		if siz == 4 {
+			out.Write32(uint32(elf.R_ARM_ABS32) | uint32(elfsym)<<8)
+		} else {
+			return false
+		}
+	case objabi.R_PCREL:
+		if siz == 4 {
+			out.Write32(uint32(elf.R_ARM_REL32) | uint32(elfsym)<<8)
+		} else {
+			return false
+		}
+	case objabi.R_CALLARM:
+		if siz != 4 {
+			return false
+		}
+		relocs := ldr.Relocs(s)
+		r := relocs.At(ri)
+		// r.Add() contains branch opcode and initial addend
+		op := uint32(r.Add() >> 32)
+		switch op & 0xD000F800 {
+		case 0x9000F000: // B
+			out.Write32(uint32(elf.R_ARM_THM_JUMP24) | uint32(elfsym)<<8)
+		case 0xD000F000: // BL
+			out.Write32(uint32(elf.R_ARM_THM_PC22) | uint32(elfsym)<<8) // R_ARM_THM_CALL
+		case 0x8000F000: // Bcond
+			out.Write32(uint32(elf.R_ARM_THM_JUMP19) | uint32(elfsym)<<8)
+		default:
+			return false
+		}
+	}
+	return true
+}
+
+// Convert the direct jump relocation r to refer to a trampoline if the target is too far
+func trampoline(ctxt *ld.Link, ldr *loader.Loader, ri int, rs, s loader.Sym) {
+	relocs := ldr.Relocs(s)
+	r := relocs.At(ri)
+	switch r.Type() {
+	case objabi.R_CALLARM:
+		var maxoffset int64
+		switch uint32(r.Add()>>32) & 0x9000F800 {
+		case 0x9000F000: // B/BL imm24
+			maxoffset = 1 << 24
+		case 0x8000F000: // Bcond imm20
+			maxoffset = 1 << 20
+		default:
+			ldr.Errorf(s, "bad branch opcode")
+		}
+		t := (ldr.SymValue(rs) + int64(int32(r.Add())) - (ldr.SymValue(s) + int64(r.Off())))
+		if -maxoffset <= t && t < maxoffset {
+			return
+		}
+		// Direct call too far, need to insert trampoline.
+		// Look up existing trampolines first. If we found one within the range
+		// of direct call, we can reuse it. Otherwise create a new one.
+		offset := t + pcoff
+		var tramp loader.Sym
+		for i := 0; ; i++ {
+			oName := ldr.SymName(rs)
+			name := oName + fmt.Sprintf("%+d-tramp%d", offset, i)
+			tramp = ldr.LookupOrCreateSym(name, int(ldr.SymVersion(rs)))
+			ldr.SetAttrReachable(tramp, true)
+			if ldr.SymType(tramp) == sym.SDYNIMPORT {
+				// don't reuse trampoline defined in other module
+				continue
+			}
+			if oName == "runtime.deferreturn" {
+				ldr.SetIsDeferReturnTramp(tramp, true)
+			}
+			if ldr.SymValue(tramp) == 0 {
+				// either the trampoline does not exist -- we need to create one,
+				// or found one the address which is not assigned -- this will be
+				// laid down immediately after the current function. use this one.
+				break
+			}
+			t = (ldr.SymValue(tramp) - pcoff - (ldr.SymValue(s) + int64(r.Off())))
+			if -maxoffset <= t && t < maxoffset {
+				// found an existing trampoline that is not too far
+				// we can just use it
+				break
+			}
+		}
+		if ldr.SymType(tramp) == 0 {
+			// trampoline does not exist, create one
+			trampb := ldr.MakeSymbolUpdater(tramp)
+			ctxt.AddTramp(trampb)
+			gentramp(ctxt.Arch, ctxt.LinkMode, ldr, trampb, rs, int64(offset))
+		}
+		// modify reloc to point to tramp, which will be resolved later
+		sb := ldr.MakeSymbolUpdater(s)
+		relocs := sb.Relocs()
+		r := relocs.At(ri)
+		r.SetSym(tramp)
+		r.SetAdd(r.Add()&^0xFFFFFFFF | int64(-pcoff)&0xFFFFFFFF) // clear the offset embedded in the instruction
+	default:
+		ctxt.Errorf(s, "trampoline called with non-jump reloc: %d (%s)", r.Type(), sym.RelocName(ctxt.Arch, r.Type()))
+	}
+}
+
+// generate a trampoline to target+offset
+func gentramp(arch *sys.Arch, linkmode ld.LinkMode, ldr *loader.Loader, tramp *loader.SymbolBuilder, target loader.Sym, offset int64) {
+	tramp.SetSize(8) // 2+1 instructions
+	P := make([]byte, tramp.Size())
+	t := ldr.SymValue(target) + offset
+	o1 := uint16(0x4F00) // MOVW (R15), R7 // R15 is actual pc+4 (points to o3)
+	o2 := uint16(0x4738) // B  (R7)
+	o3 := uint32(t) | 1  // WORD $(target|1)
+	arch.ByteOrder.PutUint16(P, o1)
+	arch.ByteOrder.PutUint16(P[2:], o2)
+	arch.ByteOrder.PutUint32(P[4:], o3)
+	tramp.SetData(P)
+}
+
+func archreloc(target *ld.Target, ldr *loader.Loader, syms *ld.ArchSyms, r loader.Reloc, s loader.Sym, val int64) (o int64, nExtReloc int, ok bool) {
+	rs := r.Sym()
+	rs = ldr.ResolveABIAlias(rs)
+	if target.IsExternal() {
+		log.Fatalf("BUGL: external linking not supported")
+		return val, 0, false
+	}
+
+	const isOk = true
+	const noExtReloc = 0
+	switch r.Type() {
+	case objabi.R_PLT0, objabi.R_PLT1, objabi.R_PLT2:
+		log.Fatalf("BUGL: PLT not supported")
+	case objabi.R_CALLARM: // B, BL
+		// r.Add() returns branch opcode and initial addend
+		op := uint32(r.Add() >> 32)
+		t := (ldr.SymValue(rs) + int64(int32(r.Add())) - (ldr.SymValue(s) + int64(r.Off())))
+		switch op & 0x9000F800 {
+		case 0x9000F000: // B/BL imm24
+			if t < -1<<24 || 1<<24 <= t {
+				break
+			}
+			v := uint32(t >> 1)
+			s := v >> 23 & 1
+			j1 := ^(v>>22 ^ s) & 1
+			j2 := ^(v>>21 ^ s) & 1
+			imm10 := v >> 11 & 0x3FF
+			imm11 := v & 0x7FF
+			return int64(op | j1<<29 | j2<<27 | imm11<<16 | s<<10 | imm10), noExtReloc, isOk
+		case 0x8000F000: // Bcond imm20
+			if t < -1<<20 || 1<<20 <= t {
+				break
+			}
+			v := uint32(t >> 1)
+			s := v >> 19 & 1
+			j2 := v >> 18 & 1
+			j1 := v >> 17 & 1
+			imm6 := v >> 11 & 0x3F
+			imm11 := v & 0x7FF
+			return int64(op | j1<<29 | j2<<27 | imm11<<16 | s<<10 | imm6), noExtReloc, isOk
+		default:
+			ldr.Errorf(s, "bad branch opcode")
+		}
+		ldr.Errorf(s, "direct call too far: %s %x", ldr.SymName(rs), t)
+	}
+	return val, 0, false
+}
+
+func archrelocvariant(*ld.Target, *loader.Loader, loader.Reloc, sym.RelocVariant, loader.Sym, int64) int64 {
+	log.Fatalf("unexpected relocation variant")
+	return -1
+}
diff --git a/src/cmd/link/internal/thumb/l.go b/src/cmd/link/internal/thumb/l.go
new file mode 100644
index 0000000000..4e73b8d2f8
--- /dev/null
+++ b/src/cmd/link/internal/thumb/l.go
@@ -0,0 +1,75 @@
+// Inferno utils/5l/asm.c
+// https://bitbucket.org/inferno-os/inferno-os/src/master/utils/5l/asm.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+// Writing object files.
+
+// Inferno utils/5l/l.h
+// https://bitbucket.org/inferno-os/inferno-os/src/master/utils/5l/l.h
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+const (
+	maxAlign  = 8 // max data alignment
+	minAlign  = 1 // min data alignment
+	funcAlign = 4 // literal alignment
+)
+
+/* Used by ../internal/ld/dwarf.go */
+const (
+	dwarfRegSP = 13
+	dwarfRegLR = 14
+)
diff --git a/src/cmd/link/internal/thumb/obj.go b/src/cmd/link/internal/thumb/obj.go
new file mode 100644
index 0000000000..b3b23413f1
--- /dev/null
+++ b/src/cmd/link/internal/thumb/obj.go
@@ -0,0 +1,85 @@
+// Inferno utils/5l/obj.c
+// https://bitbucket.org/inferno-os/inferno-os/src/master/utils/5l/obj.c
+//
+//	Copyright © 1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright © 1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright © 1997-1999 Vita Nuova Limited
+//	Portions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright © 2004,2006 Bruce Ellis
+//	Portions Copyright © 2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright © 2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright © 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"cmd/internal/objabi"
+	"cmd/internal/sys"
+	"cmd/link/internal/ld"
+)
+
+func Init() (*sys.Arch, ld.Arch) {
+	arch := sys.ArchThumb
+	theArch := ld.Arch{
+		Funcalign:        funcAlign,
+		Maxalign:         maxAlign,
+		Minalign:         minAlign,
+		Dwarfregsp:       dwarfRegSP,
+		Dwarfreglr:       dwarfRegLR,
+		Archinit:         archinit,
+		Archreloc:        archreloc,
+		Archrelocvariant: archrelocvariant,
+		Trampoline:       trampoline,
+		Elfreloc1:        elfreloc1,
+		ElfrelocSize:     8,
+		Gentext:          gentext,
+	}
+	return arch, theArch
+}
+
+func archinit(ctxt *ld.Link) {
+	switch ctxt.HeadType {
+	default:
+		ld.Exitf("unknown -H option: %v", ctxt.HeadType)
+
+	case objabi.Hlinux:
+		*ld.FlagD = true // force static linking
+		ld.Elfinit(ctxt)
+		ld.HEADR = ld.ELFRESERVE
+		if *ld.FlagTextAddr == -1 {
+			*ld.FlagTextAddr = 0x10000 + int64(ld.HEADR)
+		}
+		if *ld.FlagRound == -1 {
+			*ld.FlagRound = 0x10000
+		}
+
+	case objabi.Hnoos:
+		*ld.FlagD = true
+		ld.Elfinit(ctxt)
+		ld.HEADR = ld.ELFRESERVE
+		if *ld.FlagTextAddr == -1 {
+			*ld.FlagTextAddr = 0
+		}
+		if *ld.FlagRound == -1 {
+			*ld.FlagRound = 8
+		}
+	}
+}
diff --git a/src/cmd/link/main.go b/src/cmd/link/main.go
index 6b4ca9706d..2908d4820e 100644
--- a/src/cmd/link/main.go
+++ b/src/cmd/link/main.go
@@ -16,6 +16,7 @@ import (
 	"cmd/link/internal/ppc64"
 	"cmd/link/internal/riscv64"
 	"cmd/link/internal/s390x"
+	"cmd/link/internal/thumb"
 	"cmd/link/internal/wasm"
 	"cmd/link/internal/x86"
 	"fmt"
@@ -62,6 +63,8 @@ func main() {
 		arch, theArch = riscv64.Init()
 	case "s390x":
 		arch, theArch = s390x.Init()
+	case "thumb":
+		arch, theArch = thumb.Init()
 	case "wasm":
 		arch, theArch = wasm.Init()
 	}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/decode.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/decode.go
new file mode 100644
index 0000000000..9dfa8dd5fa
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/decode.go
@@ -0,0 +1,558 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumbasm
+
+import (
+	"encoding/binary"
+	"fmt"
+)
+
+var (
+	errShort   = fmt.Errorf("truncated instruction")
+	errUnknown = fmt.Errorf("unknown instruction")
+	errBad     = fmt.Errorf("instruction")
+)
+
+// Decode decodes the leading bytes in src as a single instruction.
+func Decode(src []byte) (Inst, error) {
+	if len(src) < 2 {
+		return Inst{}, errShort
+	}
+	if src[1]>>3 > 0x1C {
+		if len(src) < 4 {
+			return Inst{}, errShort
+		}
+		enc := binary.LittleEndian.Uint32(src)
+		x := enc>>16 | enc<<16 // ARM documentation artifact
+		for i := range inst32formats {
+			f := &inst32formats[i]
+			if f.mask&x != f.value {
+				continue
+			}
+			var args Args
+			if f.args != nil {
+				args = f.args(x)
+				if args[0] == nil {
+					return Inst{}, errBad
+				}
+			}
+			return Inst{Op: f.op, Enc: enc, Len: 4, Args: args}, nil
+		}
+		return Inst{}, errUnknown
+	}
+	x := binary.LittleEndian.Uint16(src)
+	for i := range inst16formats {
+		f := &inst16formats[i]
+		if f.mask&x != f.value {
+			continue
+		}
+		var args Args
+		if f.args != nil {
+			args = f.args(x)
+			if args[0] == nil {
+				return Inst{}, errBad
+			}
+		}
+		return Inst{Op: f.op, Enc: uint32(x), Len: 2, Args: args}, nil
+	}
+	return Inst{}, errUnknown
+}
+
+type inst16format struct {
+	mask  uint16
+	value uint16
+	op    Op
+	args  func(enc uint16) Args
+}
+
+type inst32format struct {
+	mask  uint32
+	value uint32
+	op    Op
+	args  func(enc uint32) Args
+}
+
+// 16-bit instructions
+
+// 0100 01x0 dmmm mddd
+func _ADD__Rm__Rdn(enc uint16) Args {
+	return Args{Reg(enc>>4&8 | enc&7), Reg(enc >> 3 & 15)} // Rd, Rm
+}
+
+// 0001 10xm mmnn nddd
+func _ADD__Rm__Rn__Rd(enc uint16) Args {
+	return Args{Reg(enc & 7), Reg(enc >> 3 & 7), Reg(enc >> 6 & 7)}
+}
+
+// 0001 11xu uunn nddd
+func _ADD__u3__Rn__Rd(enc uint16) Args {
+	return Args{Reg(enc & 7), Reg(enc >> 3 & 7), Imm(enc >> 6 & 7)}
+}
+
+// 1011 0000 xuuu uuuu
+func _ADD__u7_2__R13(enc uint16) Args {
+	return Args{R13, Imm(enc & 0x7F << 2)} // R13, u7<<2
+}
+
+// 1010 xddd uuuu uuuu
+func _ADD__u8_2__R13__Rd(enc uint16) Args {
+	Rn := R15
+	if enc&0x800 != 0 {
+		Rn = R13
+	}
+	return Args{Reg(enc >> 8 & 7), Rn, Imm(enc & 0xFF << 2)}
+}
+
+// 0100 xxxx xxmm mddd
+func _AND__Rm__Rdn(enc uint16) Args {
+	args := Args{Reg(enc & 7), Reg(enc >> 3 & 7)} // Rdn, Rm
+	if enc&0xFFC0 == 0x4240 {
+		args[2] = Imm(0)
+	}
+	return args
+}
+
+// 1110 0iii iiii iiii
+func _B__i11_1(enc uint16) Args {
+	return Args{PCRel(enc) << 21 >> 20}
+}
+
+// 0100 0111 xmmm m000
+func _B__Rm(enc uint16) Args {
+	return Args{Reg(enc >> 3 & 15)} // Rm
+}
+
+// 1101 cccc iiii iiii
+func _Bcond__i8_1(enc uint16) Args {
+	return Args{PCRel(enc) << 24 >> 23}
+}
+
+// 1011 x0u1 uuuu unnn
+func _CBZ__Rn__u6_1(enc uint16) Args {
+	return Args{Reg(enc & 7), PCRel(enc>>4&0x20 | enc>>3&0x1F)}
+}
+
+// 1011 0110 011x 00if
+func _CPSIE(enc uint16) Args {
+	var s string
+	switch enc & 3 {
+	case 1:
+		s = "f"
+	case 2:
+		s = "i"
+	case 3:
+		s = "if"
+	}
+	return Args{Str(s)}
+}
+
+// 1011 1111 cccc mmmm
+func _ITmask__firstcond(enc uint16) Args {
+	return Args{Cond(enc >> 4 & 0xF)}
+}
+
+// 1100 xnnn rrrr rrrr
+func _MOVM_IAW(enc uint16) Args {
+	rlist := RegList(enc & 0xFF)
+	Rn := Reg(enc >> 8 & 7)
+	mode := AddrLDM_WB
+	if enc&0x800 != 0 && 1<<Rn&rlist != 0 {
+		mode = AddrLDM
+	}
+	return Args{Mem{Base: Rn, Mode: mode}, rlist}
+}
+
+// 0101 xxxm mmnn nttt
+func _MOVW__Rn_Rm__Rt(enc uint16) Args {
+	return Args{
+		Reg(enc & 7),
+		Mem{Base: Reg(enc >> 3 & 7), Mode: AddrOffset, Sign: 1, Index: Reg(enc >> 6 & 7)},
+	}
+}
+
+// 000v vuuu uumm mddd (vv != 11)
+func _MOVW__Rm_v_u5__Rd(enc uint16) Args {
+	return Args{Reg(enc & 7), Reg(enc >> 3 & 7), Imm(enc >> 6 & 0x1F)}
+}
+
+// xxxx xuuu uunn nttt
+func _MOVW__u5_2_Rn__Rt(enc uint16) Args {
+	Rt := Reg(enc & 7)
+	Rn := Reg(enc >> 3 & 7)
+	offset := uint(enc >> 6 & 0x1F)
+	switch enc >> 12 {
+	case 6:
+		offset <<= 2
+	case 8:
+		offset <<= 1
+	}
+	return Args{Rt, Mem{Base: Rn, Offset: int16(offset)}}
+}
+
+// 001x xddd uuuu uuuu
+func _MOVW__u8__Rd(enc uint16) Args {
+	return Args{Reg(enc >> 8 & 7), Imm(enc & 0xFF)}
+}
+
+// xx0x xttt uuuu uuuu
+func _MOVW__u8_2_R13__Rt(enc uint16) Args {
+	Rt := Reg(enc >> 8 & 7)
+	mem := Mem{Mode: AddrOffset, Offset: int16(enc & 0xFF << 2)}
+	switch enc >> 11 {
+	case 0x09: // Rt, [R15, u8<<2]
+		mem.Base = R15
+	case 0x12, 0x13: // Rt, [R13, u8<<2]
+		mem.Base = R13
+	}
+	return Args{Rt, mem}
+}
+
+// 1011 x10r rrrr rrrr
+func _PUSH__reglist(enc uint16) Args {
+	rlist := enc & 0xFF
+	lrpc := enc & 0x100
+	if enc&0x800 == 0 {
+		rlist |= lrpc << 6 // PUSH
+	} else {
+		rlist |= lrpc << 7 // POP
+	}
+	return Args{RegList(rlist)}
+}
+
+// 1xx1 111x uuuu uuuu
+func _UDF__u8(enc uint16) Args {
+	return Args{Imm(enc & 0xFF)}
+}
+
+// 32-bit instructions
+
+// 1111 0u10 x0x0 nnnn  0uuu dddd uuuu uuuu
+func _ADD__u12__Rn__Rd(enc uint32) Args {
+	return Args{
+		Reg(enc >> 8 & 15),
+		Reg(enc >> 16 & 15),
+		Imm(enc>>15&0x800 | enc>>4&0x700 | enc&0xFF),
+	}
+}
+
+// 1111 0e0x xxxs nnnn  0eee dddd eeee eeee   RSB.s     e32, Rn, Rd
+func _ANDs__e32__Rn__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), Reg(enc >> 16 & 15), decodeMIC(enc)}
+}
+
+// 1110 101x xxxs nnnn  0uuu dddd uuvv mmmm
+func _ANDs__Rm_v_u5__Rn__Rd(enc uint32) Args {
+	a := _TST__Rm_v_u5__Rn(enc)
+	return Args{Reg(enc >> 8 & 15), a[0], a[1]}
+}
+
+// 1111 0jii iiii iiii  1xj1 jiii iiii iiii
+func _B__ji24_1(enc uint32) Args {
+	s := enc >> 26 & 1
+	imm10 := enc >> 16 & 0x3FF
+	i1 := ^(enc>>13 ^ s) & 1
+	i2 := ^(enc>>11 ^ s) & 1
+	imm11 := enc & 0x7FF
+	enc = s<<23 | i1<<22 | i2<<21 | imm10<<11 | imm11
+	return Args{PCRel(enc) << 8 >> 7}
+}
+
+// 1111 0011 0110 nnnn  0uuu dddd uu0k kkkk
+func _BFC__width__ulsb__Rd(enc uint32) Args {
+	a := _BFX__width__ulsb__Rn__Rd(enc)
+	Rd := a[0]
+	Rn := a[1]
+	lsb := a[2]
+	width := a[3].(Imm) - lsb.(Imm)
+	if Rn.(Reg) == 15 {
+		return Args{Rd, lsb, width} // BFC
+	}
+	return Args{Rd, Rn, lsb, width} // BFI
+}
+
+// 1111 0011 x100 nnnn  0uuu dddd uu0w wwww
+func _BFX__width__ulsb__Rn__Rd(enc uint32) Args {
+	width := Imm(enc&0x1F) + 1
+	lsb := Imm(enc>>10&0x1C | enc>>6&0x3)
+	Rn := Reg(enc >> 16 & 15)
+	Rd := Reg(enc >> 8 & 15)
+	return Args{Rd, Rn, lsb, width}
+}
+
+// 1111 1010 10x1 mmmm  1111 dddd 10xx mmmm
+func _CLZ__Rm__Rd(enc uint32) Args {
+	Rn := Reg(enc >> 16 & 15)
+	Rd := Reg(enc >> 8 & 15)
+	Rm := Reg(enc & 15)
+	if Rn != Rm {
+		return Args{Rd, ArgErr(fmt.Sprintf("{!inconsistent Rm: %v/%v}", Rm, Rn))}
+	}
+	return Args{Rd, Rm}
+}
+
+// 1111 0011 1011 1111  1000 1111 0100 oooo
+func _DSB__opt(enc uint32) Args {
+	return Args{Imm(enc & 15)}
+}
+
+// 1110 1000 0101 nnnn  tttt 1111 uuuu uuuu
+func _LDREX__u8_2_Rn__Rt(enc uint32) Args {
+	return Args{
+		Reg(enc >> 12 & 15),
+		Mem{Base: Reg(enc >> 16 & 15), Mode: AddrOffset, Offset: int16(enc & 0xFF << 2)},
+	}
+}
+
+// 1110 1000 1101 nnnn  tttt 1111 010x 1111
+func _LDREXB__Rn__Rt(enc uint32) Args {
+	return Args{
+		Reg(enc >> 12 & 15),
+		Mem{Base: Reg(enc >> 16 & 15), Mode: AddrOffset},
+	}
+}
+
+// 1111 1010 0x0x 1111  1111 dddd 10rr mmmm
+func _MOVH__Rm_rot__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), RegShift{Reg(enc & 15), RotateRight, uint8(enc >> 1 & 0x18)}}
+}
+
+// 1110 100x x0wx nnnn  rr0r rrrr rrrr rrrr
+func _MOVM_IAw(enc uint32) Args {
+	mode := AddrLDM
+	if enc&0x200000 != 0 {
+		mode = AddrLDM_WB
+	}
+	Rn := Reg(enc >> 16 & 15)
+	return Args{Mem{Base: Rn, Mode: mode}, RegList(enc)}
+}
+
+// 1110 1110 111x 0001  tttt 1010 0001 0000
+func _MOVW__FPSCR__Rt(enc uint32) Args {
+	Rt := Reg(enc>>12) & 15
+	if Rt == 15 {
+		Rt = APSR
+	}
+	if enc>>20&1 != 0 {
+		return Args{Rt, FPSCR}
+	}
+	return Args{FPSCR, Rt}
+}
+
+// 1111 100x 0xxx nnnn  tttt 0000 00uu mmmm
+func _MOVW__Rn_Rm_1_u2__Rt(enc uint32) Args {
+	return Args{
+		Reg(enc >> 12 & 15),
+		Mem{
+			Base:  Reg(enc >> 16 & 15),
+			Mode:  AddrOffset,
+			Sign:  1,
+			Index: Reg(enc & 15),
+			Shift: ShiftLeft,
+			Count: uint8(enc >> 4 & 3),
+		},
+	}
+}
+
+// 1111 100x 1xxx nnnn  tttt uuuu uuuu uuuu
+// 1111 100x ±xxx 1111  tttt uuuu uuuu uuuu
+func _MOVW__s12_Rn__Rt(enc uint32) Args {
+	Rn := Reg(enc >> 16 & 15)
+	Rt := Reg(enc >> 12 & 15)
+	offset := int16(enc & 0xFFF)
+	if enc&0x800000 == 0 {
+		offset = -offset
+	}
+	return Args{Rt, Mem{Base: Rn, Offset: offset}}
+}
+
+// 1111 0y10 x100 uuuu  0zzz dddd zzzz zzzz
+func _MOVW__uyz16__Rd(enc uint32) Args {
+	Rd := Reg(enc >> 8 & 15)
+	imm := Imm(enc>>4&0xF000 | enc>>15&0x800 | enc>>4&0x700 | enc&0xFF)
+	return Args{Rd, imm}
+}
+
+// 1111 100x 0xxx nnnn  tttt 1p±w uuuu uuuu
+func _MOVWpw__s8_Rn__Rt(enc uint32) Args {
+	Rn := Reg(enc >> 16 & 15)
+	Rt := Reg(enc >> 12 & 15)
+	offset := int16(enc & 0xFF)
+	if enc&0x200 == 0 {
+		offset = -offset
+	}
+	p0w := enc >> 8 & 5
+	var mode AddrMode
+	switch p0w {
+	case 1:
+		mode = AddrPostIndex
+	case 4:
+		mode = AddrOffset
+	case 5:
+		mode = AddrPreIndex
+	default:
+		return Args{} // undefined
+	}
+	return Args{Rt, Mem{Base: Rn, Mode: mode, Offset: offset}}
+}
+
+// 1111 0e00 01xs 1111  0eee dddd eeee eeee
+func _MOVWs__e32__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), decodeMIC(enc)}
+}
+
+// 1111 1010 0vvs nnnn  1111 dddd 0000 mmmm
+func _MOVWs__Rn_v_Rm__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), Reg(enc >> 16 & 15), Reg(enc & 15)}
+}
+
+// 1111 0011 1110 1111  1000 dddd mmmm mmmm
+// 1111 0011 1000 nnnn  1000 kk00 mmmm mmmm
+func _MOVW__SYSm__Rd(enc uint32) Args {
+	SYSm := APSR + Reg(enc&0xFF)
+	if enc>>20 == 0xF3E {
+		// MRS
+		return Args{Reg(enc >> 8 & 15), SYSm}
+	} else {
+		// MSR
+		return Args{SYSm, Reg(enc >> 16 & 15)}
+	}
+}
+
+// 1111 1011 xxxx nnnn  1111 dddd xxxx mmmm
+func _MUL__Rm__Rn__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), Reg(enc >> 16 & 15), Reg(enc & 15)}
+}
+
+// 1111 1011 xxxx nnnn  llll hhhh 000x mmmm
+func _MULL__Rm__Rn__Rdh_Rdl(enc uint32) Args {
+	return Args{Reg(enc >> 12 & 15), Reg(enc>>8) & 15, Reg(enc >> 16 & 15), Reg(enc & 15)}
+}
+
+// 1111 1000 010x 1101 tttt 1xx1 0000 0100
+func _PUSH__Rt(enc uint32) Args {
+	return Args{RegList(1 << (enc >> 12 & 0xF))}
+}
+
+// 1110 1000 1101 nnnn  1111 0000 000h mmmm
+func _TBB__Rm__Rn(enc uint32) Args {
+	return Args{Reg(enc >> 16 & 15), Reg(enc & 15)}
+}
+
+// 1111 0e0x x0x1 nnnn  0eee 1111 eeee eeee
+func _TST__e32__Rn(enc uint32) Args {
+	return Args{Reg(enc >> 16 & 15), decodeMIC(enc)}
+}
+
+// 1110 101x x0x1 nnnn  0uuu 1111 uuvv mmmm
+func _TST__Rm_v_u5__Rn(enc uint32) Args {
+	return Args{Reg(enc >> 16 & 15), decodeShiftI(enc)}
+}
+
+// 1110 1010 01xs 1111  0uuu dddd uuvv mmmm
+func _MOVWs__Rm_v_u5__Rn(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), decodeShiftI(enc)}
+}
+
+// 1110 1000 0100 nnnn  tttt dddd uuuu uuuu
+func _STREX__Rt__u8_2_Rn__Rd(enc uint32) Args {
+	a := _LDREX__u8_2_Rn__Rt(enc)
+	return Args{Reg(enc >> 8 & 15), a[0], a[1]}
+}
+
+// 1110 1000 1100 nnnn  tttt 1111 010x dddd
+func _STREXB__Rn__Rt(enc uint32) Args {
+	a := _LDREXB__Rn__Rt(enc)
+	return Args{Reg(enc & 15), a[0], a[1]}
+}
+
+// 1110 1101 ±d0x nnnn  dddd 101x uuuu uuuu
+func _MOVF__s8_2_Rn__Fd(enc uint32) Args {
+	offset := int(enc) & 0xFF << 2
+	if (enc>>23)&1 == 0 {
+		offset = -offset
+	}
+	return Args{
+		_Fd(enc),
+		Mem{Mode: AddrOffset, Base: Reg(enc >> 16 & 15), Offset: int16(offset)},
+	}
+}
+
+// 1110 1110 1d11 0101  dddd 101x e100 0000
+func _CMPF__0__Fd(enc uint32) Args {
+	return Args{_Fd(enc), Imm(0)}
+}
+
+// 1110 1110 1d11 0100  dddd 101x e1m0 mmmm
+func _CMPF__Fm__Fd(enc uint32) Args {
+	return Args{_Fd(enc), _Fm(enc)}
+}
+
+// 1110 1110 1d11 0xxx  dddd 101x x1m0 mmmm
+func _SQRTF__Fm__Fd(enc uint32) Args {
+	return Args{_Fd(enc), _Fm(enc)}
+}
+
+// 1110 1110 00h0 dddd  tttt 1011 d001 0000
+// 1110 1110 00h1 nnnn  tttt 1011 n001 0000
+func _MOVW__Fm__Rd(enc uint32) Args {
+	F := S0 + Reg(enc>>16&15)
+	R := Reg(enc >> 12 & 15)
+	if enc>>20&1 == 0 {
+		return Args{F, R}
+	}
+	return Args{R, F}
+}
+
+// 1110 1110 1d11 ffff  dddd 101x 0000 ffff
+func _MOVF__f8__Fd(enc uint32) Args {
+	imm8 := Imm(enc>>12&0xF0 | enc&0xF)
+	return Args{_Fd(enc), imm8}
+}
+
+// 1110 1110 xdxx nnnn  dddd 101x nxm0 mmmm
+func _ADDF__Fm__Fn__Fd(enc uint32) Args {
+	return Args{_Fd(enc), _Fn(enc), _Fm(enc)}
+}
+
+func decodeMIC(enc uint32) ImmAlt {
+	rot := int(enc>>22&0x10 | enc>>11&0x0E | enc>>7&0x01)
+	if rot < 8 {
+		enc &= 0xFF
+	} else {
+		enc = enc&0x7F | 0x80
+	}
+	return ImmAlt{uint8(enc), uint8(rot)}
+}
+
+func decodeShiftI(enc uint32) RegShift {
+	Rm := Reg(enc & 15)
+	shift := Shift(enc >> 4 & 3)
+	count := uint8(enc>>10&0x1C | enc>>6&3)
+	if shift == RotateRight && count == 0 {
+		shift = RotateRightExt
+	}
+	return RegShift{Rm, shift, count}
+}
+
+func _Fm(enc uint32) Reg {
+	if (enc>>8)&1 != 0 {
+		return Reg(int(D0) + int((enc>>1)&0x10|enc&0x0F))
+	}
+	return Reg(int(S0) + int((enc<<1)&0x1E|(enc>>5)&0x01))
+}
+
+func _Fn(enc uint32) Reg {
+	if (enc>>8)&1 != 0 {
+		return Reg(int(D0) + int((enc>>3)&0x10|enc>>16&0x0F))
+	}
+	return Reg(int(S0) + int((enc>>15)&0x1E|(enc>>7)&0x01))
+}
+
+func _Fd(enc uint32) Reg {
+	if (enc>>8)&1 != 0 {
+		return Reg(int(D0) + int((enc>>18)&0x10|(enc>>12)&0x0F))
+	}
+	return Reg(int(S0) + int((enc>>11)&0x1E|(enc>>22)&0x01))
+}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/gnu.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/gnu.go
new file mode 100644
index 0000000000..fcb517a95b
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/gnu.go
@@ -0,0 +1,155 @@
+// Copyright 2014 The Go Authors.  All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumbasm
+
+import (
+	"bytes"
+	"fmt"
+	"strings"
+)
+
+var saveDot = strings.NewReplacer(
+	".F16", "_dot_F16",
+	".F32", "_dot_F32",
+	".F64", "_dot_F64",
+	".S32", "_dot_S32",
+	".U32", "_dot_U32",
+	".FXS", "_dot_S",
+	".FXU", "_dot_U",
+	".32", "_dot_32",
+)
+
+// GNUSyntax returns the GNU assembler syntax for the instruction, as defined by GNU binutils.
+// This form typically matches the syntax defined in the ARM Reference Manual.
+func GNUSyntax(inst Inst) string {
+	var buf bytes.Buffer
+	op := inst.Op.String()
+	op = saveDot.Replace(op)
+	op = strings.Replace(op, ".", "", -1)
+	op = strings.Replace(op, "_dot_", ".", -1)
+	op = strings.ToLower(op)
+	buf.WriteString(op)
+	sep := " "
+	for i, arg := range inst.Args {
+		if arg == nil {
+			break
+		}
+		text := gnuArg(&inst, i, arg)
+		if text == "" {
+			continue
+		}
+		buf.WriteString(sep)
+		sep = ", "
+		buf.WriteString(text)
+	}
+	return buf.String()
+}
+
+func gnuArg(inst *Inst, argIndex int, arg Arg) string {
+	switch inst.Op &^ 15 {
+	case LDRD, STRD:
+		if argIndex == 1 {
+			// second argument in consecutive pair not printed
+			return ""
+		}
+	}
+
+	switch arg := arg.(type) {
+	case Imm:
+		switch inst.Op &^ 15 {
+		case BKPT:
+			return fmt.Sprintf("%#04x", uint32(arg))
+		case SVC:
+			return fmt.Sprintf("%#08x", uint32(arg))
+		}
+		return fmt.Sprintf("#%d", int32(arg))
+
+	case ImmAlt:
+		return fmt.Sprintf("#%d, %d", arg.Val, arg.Rot)
+
+	case Mem:
+		R := gnuArg(inst, -1, arg.Base)
+		X := ""
+		if arg.Sign != 0 {
+			X = ""
+			if arg.Sign < 0 {
+				X = "-"
+			}
+			X += gnuArg(inst, -1, arg.Index)
+			if arg.Shift == ShiftLeft && arg.Count == 0 {
+				// nothing
+			} else if arg.Shift == RotateRightExt {
+				X += ", rrx"
+			} else {
+				X += fmt.Sprintf(", %s #%d", strings.ToLower(arg.Shift.String()), arg.Count)
+			}
+		} else {
+			X = fmt.Sprintf("#%d", arg.Offset)
+		}
+
+		switch arg.Mode {
+		case AddrOffset:
+			if X == "#0" {
+				return fmt.Sprintf("[%s]", R)
+			}
+			return fmt.Sprintf("[%s, %s]", R, X)
+		case AddrPreIndex:
+			return fmt.Sprintf("[%s, %s]!", R, X)
+		case AddrPostIndex:
+			return fmt.Sprintf("[%s], %s", R, X)
+		case AddrLDM:
+			if X == "#0" {
+				return R
+			}
+		case AddrLDM_WB:
+			if X == "#0" {
+				return R + "!"
+			}
+		}
+		return fmt.Sprintf("[%s Mode(%d) %s]", R, int(arg.Mode), X)
+
+	case PCRel:
+		return fmt.Sprintf(".%+#x", int32(arg)+4)
+
+	case Reg:
+		switch inst.Op &^ 15 {
+		case LDREX:
+			if argIndex == 0 {
+				return fmt.Sprintf("r%d", int32(arg))
+			}
+		}
+		switch arg {
+		case R10:
+			return "sl"
+		case R11:
+			return "fp"
+		case R12:
+			return "ip"
+		}
+
+	case RegList:
+		var buf bytes.Buffer
+		fmt.Fprintf(&buf, "{")
+		sep := ""
+		for i := 0; i < 16; i++ {
+			if arg&(1<<uint(i)) != 0 {
+				fmt.Fprintf(&buf, "%s%s", sep, gnuArg(inst, -1, Reg(i)))
+				sep = ", "
+			}
+		}
+		fmt.Fprintf(&buf, "}")
+		return buf.String()
+
+	case RegShift:
+		if arg.Shift == ShiftLeft && arg.Count == 0 {
+			return gnuArg(inst, -1, arg.Reg)
+		}
+		if arg.Shift == RotateRightExt {
+			return gnuArg(inst, -1, arg.Reg) + ", rrx"
+		}
+		return fmt.Sprintf("%s, %s #%d", gnuArg(inst, -1, arg.Reg), strings.ToLower(arg.Shift.String()), arg.Count)
+	}
+	return strings.ToLower(arg.String())
+}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/inst.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/inst.go
new file mode 100644
index 0000000000..1214bd09ff
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/inst.go
@@ -0,0 +1,493 @@
+// Copyright 2018 The Go Authors.  All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumbasm
+
+import (
+	"bytes"
+	"fmt"
+)
+
+type Op uint16
+
+func (op Op) String() string {
+	if op >= Op(len(opstr)) || opstr[op] == "" {
+		return fmt.Sprintf("Op(%d)", int(op))
+	}
+	return opstr[op]
+}
+
+// An Inst is a single instruction.
+type Inst struct {
+	Op   Op     // Opcode mnemonic
+	Enc  uint32 // Raw encoding bits.
+	Len  int    // Length of encoding in bytes.
+	Args Args   // Instruction arguments, in ARM manual order.
+}
+
+func (i Inst) String() string {
+	var buf bytes.Buffer
+	buf.WriteString(i.Op.String())
+	for j, arg := range i.Args {
+		if arg == nil {
+			break
+		}
+		if j == 0 {
+			buf.WriteString(" ")
+		} else {
+			buf.WriteString(", ")
+		}
+		buf.WriteString(arg.String())
+	}
+	return buf.String()
+}
+
+// An Args holds the instruction arguments.
+// If an instruction has fewer than 4 arguments,
+// the final elements in the array are nil.
+type Args [4]Arg
+
+// An Arg is a single instruction argument, one of these types:
+// Endian, Imm, Mem, PCRel, Reg, RegList, RegShift, RegShiftReg.
+type Arg interface {
+	IsArg()
+	String() string
+}
+
+type Float32Imm float32
+
+func (Float32Imm) IsArg() {}
+
+func (f Float32Imm) String() string {
+	return fmt.Sprintf("#%v", float32(f))
+}
+
+type Float64Imm float32
+
+func (Float64Imm) IsArg() {}
+
+func (f Float64Imm) String() string {
+	return fmt.Sprintf("#%v", float64(f))
+}
+
+// An Imm is an integer constant.
+type Imm uint32
+
+func (Imm) IsArg() {}
+
+func (i Imm) String() string {
+	return fmt.Sprintf("#%#x", uint32(i))
+}
+
+// An ImmAlt is an alternate encoding of an integer constant.
+type ImmAlt struct {
+	Val uint8
+	Rot uint8
+}
+
+func (ImmAlt) IsArg() {}
+
+func (i ImmAlt) Imm() Imm {
+	v := uint32(i.Val)
+	r := uint(i.Rot)
+	return Imm(v>>r | v<<(32-r))
+}
+
+func ror(x uint32, shift int) uint32 {
+	m := uint(shift) & 31
+	return x>>m | x<<(32-m)
+}
+
+func (i ImmAlt) String() string {
+	val, rot := uint32(i.Val), int(i.Rot)
+	var u uint32
+	var s string
+	if rot > 7 {
+		u = ror(val, rot)
+		s = "@>"
+	} else {
+		rot >>= 1
+		switch {
+		case rot == 0:
+			u = val
+		case val == 0:
+			return "UNPREDICTABLE!"
+		case rot == 1:
+			u = val<<16 | val
+		case rot == 2:
+			u = val<<24 | val<<8
+		default: // rot == 3
+			u = val<<24 | val<<16 | val<<8 | val
+		}
+		s = ":"
+	}
+	return fmt.Sprintf("#%#x{%#x%s%d}", u, val, s, rot)
+}
+
+// A Label is a text (code) address.
+type Label uint32
+
+func (Label) IsArg() {}
+
+func (i Label) String() string {
+	return fmt.Sprintf("%#x", uint32(i))
+}
+
+// A Reg is a single register.
+// The zero value denotes R0, not the absence of a register.
+type Reg uint8
+
+const (
+	R0 Reg = iota
+	R1
+	R2
+	R3
+	R4
+	R5
+	R6
+	R7
+	R8
+	R9
+	R10
+	R11
+	R12
+	R13
+	R14
+	R15
+
+	S0
+	S1
+	S2
+	S3
+	S4
+	S5
+	S6
+	S7
+	S8
+	S9
+	S10
+	S11
+	S12
+	S13
+	S14
+	S15
+	S16
+	S17
+	S18
+	S19
+	S20
+	S21
+	S22
+	S23
+	S24
+	S25
+	S26
+	S27
+	S28
+	S29
+	S30
+	S31
+
+	D0
+	D1
+	D2
+	D3
+	D4
+	D5
+	D6
+	D7
+	D8
+	D9
+	D10
+	D11
+	D12
+	D13
+	D14
+	D15
+	D16
+	D17
+	D18
+	D19
+	D20
+	D21
+	D22
+	D23
+	D24
+	D25
+	D26
+	D27
+	D28
+	D29
+	D30
+	D31
+
+	FPSCR
+
+	APSR        // 0
+	IAPSR       // 1
+	EAPSR       // 2
+	XPSR        // 3
+	_           // 4
+	IPSR        // 5
+	EPSR        // 6
+	IEPSR       // 7
+	MSP         // 8
+	PSP         // 9
+	_           // 10
+	_           // 11
+	_           // 12
+	_           // 13
+	_           // 14
+	_           // 15
+	PRIMASK     // 16
+	BASEPRI     // 17
+	BASEPRI_MAX // 18
+	FAULTMASK   // 19
+	CONTROL     // 20
+
+	SP = R13
+	LR = R14
+	PC = R15
+)
+
+func (Reg) IsArg() {}
+
+func (r Reg) String() string {
+	if R0 <= r && r <= R15 {
+		return fmt.Sprintf("R%d", int(r-R0))
+	}
+	if D0 <= r && r <= D15 {
+		return fmt.Sprintf("D%d", int(r-D0))
+	}
+	if S0 <= r && r <= S31 {
+		return fmt.Sprintf("S%d", int(r-S0))
+	}
+	switch r {
+	case APSR:
+		return "APSR"
+	case IAPSR:
+		return "IAPSR"
+	case EAPSR:
+		return "EAPSR"
+	case XPSR:
+		return "XPSR"
+	case IPSR:
+		return "IPSR"
+	case EPSR:
+		return "EPSR"
+	case IEPSR:
+		return "IEPSR"
+
+	case FPSCR:
+		return "FPSCR"
+
+	case MSP:
+		return "MSP"
+	case PSP:
+		return "PSP"
+
+	case PRIMASK:
+		return "PRIMASK"
+	case BASEPRI:
+		return "BASEPRI"
+	case BASEPRI_MAX:
+		return "BASEPRI_MAX"
+	case FAULTMASK:
+		return "FAULTMASK"
+	case CONTROL:
+		return "CONTROL"
+	}
+	return fmt.Sprintf("Reg(%d)", int(r))
+}
+
+// A RegX represents a fraction of a multi-value register.
+// The Index field specifies the index number,
+// but the size of the fraction is not specified.
+// It must be inferred from the instruction and the register type.
+// For example, in a VMOV instruction, RegX{D5, 1} represents
+// the top 32 bits of the 64-bit D5 register.
+type RegX struct {
+	Reg   Reg
+	Index int
+}
+
+func (RegX) IsArg() {}
+
+func (r RegX) String() string {
+	return fmt.Sprintf("%s[%d]", r.Reg, r.Index)
+}
+
+// A RegList is a register list.
+// Bits at indexes x = 0 through 15 indicate whether the corresponding Rx register is in the list.
+type RegList uint16
+
+func (RegList) IsArg() {}
+
+func (r RegList) String() string {
+	var buf bytes.Buffer
+	fmt.Fprintf(&buf, "{")
+	sep := ""
+	for i := 0; i < 16; i++ {
+		if r&(1<<uint(i)) != 0 {
+			fmt.Fprintf(&buf, "%s%s", sep, Reg(i).String())
+			sep = ","
+		}
+	}
+	fmt.Fprintf(&buf, "}")
+	return buf.String()
+}
+
+// An Endian is the argument to the SETEND instruction.
+type Endian uint8
+
+const (
+	LittleEndian Endian = 0
+	BigEndian    Endian = 1
+)
+
+func (Endian) IsArg() {}
+
+func (e Endian) String() string {
+	if e != 0 {
+		return "BE"
+	}
+	return "LE"
+}
+
+// A Shift describes an ARM shift operation.
+type Shift uint8
+
+const (
+	ShiftLeft        Shift = 0 // left shift
+	ShiftRight       Shift = 1 // logical (unsigned) right shift
+	ShiftRightSigned Shift = 2 // arithmetic (signed) right shift
+	RotateRight      Shift = 3 // right rotate
+	RotateRightExt   Shift = 4 // right rotate through carry (Count will always be 1)
+)
+
+var shiftName = [...]string{
+	"LSL", "LSR", "ASR", "ROR", "RRX",
+}
+
+func (s Shift) String() string {
+	if s < 5 {
+		return shiftName[s]
+	}
+	return fmt.Sprintf("Shift(%d)", int(s))
+}
+
+// A RegShift is a register shifted by a constant.
+type RegShift struct {
+	Reg   Reg
+	Shift Shift
+	Count uint8
+}
+
+func (RegShift) IsArg() {}
+
+func (r RegShift) String() string {
+	return fmt.Sprintf("%s %s #%d", r.Reg, r.Shift, r.Count)
+}
+
+// A PCRel describes a memory address (usually a code label)
+// as a distance relative to the PC+4.
+type PCRel int32
+
+func (PCRel) IsArg() {}
+
+func (r PCRel) String() string {
+	return fmt.Sprintf("PC%+#x", int32(r))
+}
+
+// An AddrMode is an ARM addressing mode.
+type AddrMode uint8
+
+const (
+	_             AddrMode = iota
+	AddrPostIndex          // [R], X – use address R, set R = R + X
+	AddrPreIndex           // [R, X]! – use address R + X, set R = R + X
+	AddrOffset             // [R, X] – use address R + X
+	AddrLDM                // R – [R] but formats as R, for LDM/STM only
+	AddrLDM_WB             // R! - [R], X where X is instruction-specific amount, for LDM/STM only
+)
+
+// A Mem is a memory reference made up of a base R and index expression X.
+// The effective memory address is R or R+X depending on AddrMode.
+// The index expression is X = Sign*(Index Shift Count) + Offset,
+// but in any instruction either Sign = 0 or Offset = 0.
+type Mem struct {
+	Base   Reg
+	Mode   AddrMode
+	Sign   int8
+	Index  Reg
+	Shift  Shift
+	Count  uint8
+	Offset int16
+}
+
+func (Mem) IsArg() {}
+
+func (m Mem) String() string {
+	R := m.Base.String()
+	X := ""
+	if m.Sign != 0 {
+		X = "+"
+		if m.Sign < 0 {
+			X = "-"
+		}
+		X += m.Index.String()
+		if m.Shift != ShiftLeft || m.Count != 0 {
+			X += fmt.Sprintf(", %s #%d", m.Shift, m.Count)
+		}
+	} else {
+		X = fmt.Sprintf("#%d", m.Offset)
+	}
+
+	switch m.Mode {
+	case AddrOffset:
+		if X == "#0" {
+			return fmt.Sprintf("[%s]", R)
+		}
+		return fmt.Sprintf("[%s, %s]", R, X)
+	case AddrPreIndex:
+		return fmt.Sprintf("[%s, %s]!", R, X)
+	case AddrPostIndex:
+		return fmt.Sprintf("[%s], %s", R, X)
+	case AddrLDM:
+		if X == "#0" {
+			return R
+		}
+	case AddrLDM_WB:
+		if X == "#0" {
+			return R + "!"
+		}
+	}
+	return fmt.Sprintf("[%s Mode(%d) %s]", R, int(m.Mode), X)
+}
+
+// A Cond is an argument of the IT instruction.
+type Cond uint8
+
+func (c Cond) IsArg() {}
+
+func (c Cond) String() string {
+	if c > 14 {
+		return "??"
+	}
+	i := int(c) * 2
+	return "EQNEHSLOMIPLVSVCHILSGELTGTLEAL"[i : i+2]
+}
+
+type Str string
+
+func (s Str) IsArg()         {}
+func (s Str) String() string { return string(s) }
+
+type ArgErr string
+
+func (e ArgErr) IsArg() {}
+
+func (e ArgErr) String() string {
+	return string(e)
+}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/plan9x.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/plan9x.go
new file mode 100644
index 0000000000..f7f614ccbc
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/plan9x.go
@@ -0,0 +1,412 @@
+// Copyright 2014 The Go Authors.  All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumbasm
+
+import (
+	"bytes"
+	"encoding/binary"
+	"fmt"
+	"io"
+	"math"
+	"strings"
+)
+
+// GoSyntax returns the Go assembler syntax for the instruction.
+// The syntax was originally defined by Plan 9.
+// The pc is the program counter of the instruction, used for expanding
+// PC-relative addresses into absolute ones.
+// The symname function queries the symbol table for the program
+// being disassembled. Given a target address it returns the name and base
+// address of the symbol containing the target, if any; otherwise it returns "", 0.
+// The reader r should read from the text segment using text addresses
+// as offsets; it is used to display pc-relative loads as constant loads.
+func GoSyntax(inst Inst, pc uint64, symname func(uint64) (string, uint64), text io.ReaderAt) string {
+	if symname == nil {
+		symname = func(uint64) (string, uint64) { return "", 0 }
+	}
+
+	//switch inst.Op {
+	//case BX, BLX:
+	//	if inst.Op == BX {
+	//		inst.Op = B
+	//	} else {
+	//		inst.Op = BL
+	//	}
+	//	inst.Args[0] = Mem{Base: inst.Args[0].(Reg), Mode: AddrOffset}
+	//}
+
+	var args []string
+	for _, a := range inst.Args {
+		if a == nil {
+			break
+		}
+		args = append(args, plan9Arg(&inst, pc, symname, a))
+	}
+
+	op := inst.Op.String()
+
+	switch inst.Op {
+	case LDR, LDRB, LDRH, LDRSB, LDRSH, VLDR:
+		// Check for RET
+		reg, _ := inst.Args[0].(Reg)
+		mem, _ := inst.Args[1].(Mem)
+		if inst.Op == LDR && reg == R15 && mem.Base == SP && mem.Sign == 0 && mem.Mode == AddrPostIndex {
+			return fmt.Sprintf("RET%s #%d", op[3:], mem.Offset)
+		}
+
+		// Check for PC-relative load.
+		if mem.Base == PC && mem.Sign == 0 && mem.Mode == AddrOffset && text != nil {
+			addr := uint32(pc) + 4 + uint32(mem.Offset)
+			buf := make([]byte, 8)
+			switch inst.Op {
+			case LDRB, LDRSB:
+				if _, err := text.ReadAt(buf[:1], int64(addr)); err != nil {
+					break
+				}
+				args[1] = fmt.Sprintf("$%#x", buf[0])
+
+			case LDRH, LDRSH:
+				if _, err := text.ReadAt(buf[:2], int64(addr)); err != nil {
+					break
+				}
+				args[1] = fmt.Sprintf("$%#x", binary.LittleEndian.Uint16(buf))
+
+			case LDR:
+				if _, err := text.ReadAt(buf[:4], int64(addr)); err != nil {
+					break
+				}
+				x := binary.LittleEndian.Uint32(buf)
+				if s, base := symname(uint64(x)); s != "" && uint64(x) == base {
+					args[1] = fmt.Sprintf("$%s(SB)", s)
+				} else {
+					args[1] = fmt.Sprintf("$%#x", x)
+				}
+
+			case VLDR:
+				switch {
+				case strings.HasPrefix(args[0], "D"): // VLDR.F64
+					if _, err := text.ReadAt(buf, int64(addr)); err != nil {
+						break
+					}
+					args[1] = fmt.Sprintf("$%f", math.Float64frombits(binary.LittleEndian.Uint64(buf)))
+				case strings.HasPrefix(args[0], "S"): // VLDR.F32
+					if _, err := text.ReadAt(buf[:4], int64(addr)); err != nil {
+						break
+					}
+					args[1] = fmt.Sprintf("$%f", math.Float32frombits(binary.LittleEndian.Uint32(buf)))
+				default:
+					panic(fmt.Sprintf("wrong FP register: %v", inst))
+				}
+			}
+		}
+	}
+
+	// Move addressing mode into opcode suffix.
+	suffix := ""
+	switch inst.Op {
+	case PLD, PLI:
+		if mem, ok := inst.Args[0].(Mem); ok {
+			args[0], suffix = memOpTrans(mem)
+		} else {
+			panic(fmt.Sprintf("illegal instruction: %v", inst))
+		}
+	case LDR, LDRB, LDRSB, LDRH, LDRSH, STR, STRB, STRH, VLDR, VSTR, LDREX, LDREXH, LDREXB:
+		if mem, ok := inst.Args[1].(Mem); ok {
+			args[1], suffix = memOpTrans(mem)
+		} else {
+			panic(fmt.Sprintf("illegal instruction: %v", inst))
+		}
+	case STREX, STREXB, STREXH:
+		if mem, ok := inst.Args[2].(Mem); ok {
+			args[2], suffix = memOpTrans(mem)
+		} else {
+			panic(fmt.Sprintf("illegal instruction: %v", inst))
+		}
+	}
+
+	switch inst.Op {
+	case STR, STRB, STRH, CBZ, CBNZ:
+		break
+	default:
+		// Reverse args, placing dest last.
+		for i, j := 0, len(args)-1; i < j; i, j = i+1, j-1 {
+			args[i], args[j] = args[j], args[i]
+		}
+	}
+
+	// For MLA-like instructions, the addend is the third operand.
+	switch inst.Op {
+	case SMLAWT, SMLAWB, MLA, MLS, SMMLA, SMMLS, SMLABB, SMLATB, SMLABT, SMLATT, SMLAD, SMLADX, SMLSD, SMLSDX:
+		args = []string{args[1], args[2], args[0], args[3]}
+	}
+	// For STREX like instructions, the memory operands comes first.
+	switch inst.Op {
+	case STREX, STREXB, STREXH:
+		args = []string{args[1], args[0], args[2]}
+	}
+
+	// special process for FP instructions
+	op, args = fpTrans(&inst, op, args)
+
+	// LDR/STR like instructions -> MOV like
+	switch inst.Op {
+	case MOV:
+		op = "MOVW" + op[3:]
+	case LDR, MSR, MRS:
+		op = "MOVW" + op[3:] + suffix
+	case VMRS, VMSR:
+		op = "MOVW" + op[4:] + suffix
+	case LDRB, UXTB:
+		op = "MOVBU" + op[4:] + suffix
+	case LDRSB:
+		op = "MOVBS" + op[5:] + suffix
+	case SXTB:
+		op = "MOVBS" + op[4:] + suffix
+	case LDRH, UXTH:
+		op = "MOVHU" + op[4:] + suffix
+	case LDRSH:
+		op = "MOVHS" + op[5:] + suffix
+	case SXTH:
+		op = "MOVHS" + op[4:] + suffix
+	case STR:
+		op = "MOVW" + op[3:] + suffix
+	case STRB:
+		op = "MOVB" + op[4:] + suffix
+	case STRH:
+		op = "MOVH" + op[4:] + suffix
+	case VSTR:
+		args[0], args[1] = args[1], args[0]
+	default:
+		op = op + suffix
+	}
+
+	if args != nil {
+		op += " " + strings.Join(args, ", ")
+	}
+
+	return op
+}
+
+// assembler syntax for the various shifts.
+// @x> is a lie; the assembler uses @> 0
+// instead of @x> 1, but i wanted to be clear that it
+// was a different operation (rotate right extended, not rotate right).
+var plan9Shift = []string{"<<", ">>", "->", "@>", "@x>"}
+
+func plan9Arg(inst *Inst, pc uint64, symname func(uint64) (string, uint64), arg Arg) string {
+	switch a := arg.(type) {
+	case Endian:
+
+	case Imm:
+		return fmt.Sprintf("$%d", uint32(a))
+
+	case Mem:
+		if a.Mode == AddrOffset && a.Sign == 0 && a.Offset == 0 {
+			return fmt.Sprintf("(R%d)", a.Base)
+		}
+
+	case PCRel:
+		addr := uint64(int64(pc) + 4 + int64(a))
+		if s, base := symname(addr); s != "" && addr == base {
+			return fmt.Sprintf("%s(SB)", s)
+		}
+		return fmt.Sprintf("%#x", addr)
+
+	case Reg:
+		if a < 16 {
+			return fmt.Sprintf("R%d", int(a))
+		}
+
+	case RegList:
+		var buf bytes.Buffer
+		start := -2
+		end := -2
+		fmt.Fprintf(&buf, "[")
+		flush := func() {
+			if start >= 0 {
+				if buf.Len() > 1 {
+					fmt.Fprintf(&buf, ",")
+				}
+				if start == end {
+					fmt.Fprintf(&buf, "R%d", start)
+				} else {
+					fmt.Fprintf(&buf, "R%d-R%d", start, end)
+				}
+				start = -2
+				end = -2
+			}
+		}
+		for i := 0; i < 16; i++ {
+			if a&(1<<uint(i)) != 0 {
+				if i == end+1 {
+					end++
+					continue
+				}
+				start = i
+				end = i
+			} else {
+				flush()
+			}
+		}
+		flush()
+		fmt.Fprintf(&buf, "]")
+		return buf.String()
+
+	case RegShift:
+		return fmt.Sprintf("R%d%s$%d", int(a.Reg), plan9Shift[a.Shift], int(a.Count))
+
+	}
+	return arg.String()
+}
+
+// convert memory operand from GNU syntax to Plan 9 syntax, for example,
+// [r5] -> (R5)
+// [r6, #4080] -> 0xff0(R6)
+// [r2, r0, ror #1] -> (R2)(R0@>1)
+// inst [r2, -r0, ror #1] -> INST.U (R2)(R0@>1)
+// input:
+//   a memory operand
+// return values:
+//   corresponding memory operand in Plan 9 syntax
+//   .W/.P/.U suffix
+func memOpTrans(mem Mem) (string, string) {
+	suffix := ""
+	switch mem.Mode {
+	case AddrOffset, AddrLDM:
+		// no suffix
+	case AddrPreIndex, AddrLDM_WB:
+		suffix = ".W"
+	case AddrPostIndex:
+		suffix = ".P"
+	}
+	off := ""
+	if mem.Offset != 0 {
+		off = fmt.Sprintf("%#x", mem.Offset)
+	}
+	base := fmt.Sprintf("(R%d)", int(mem.Base))
+	index := ""
+	if mem.Sign != 0 {
+		sign := ""
+		if mem.Sign < 0 {
+			suffix += ".U"
+		}
+		shift := ""
+		if mem.Count != 0 {
+			shift = fmt.Sprintf("%s%d", plan9Shift[mem.Shift], mem.Count)
+		}
+		index = fmt.Sprintf("(%sR%d%s)", sign, int(mem.Index), shift)
+	}
+	return off + base + index, suffix
+}
+
+type goFPInfo struct {
+	op        Op
+	transArgs []int  // indexes of arguments which need transformation
+	gnuName   string // instruction name in GNU syntax
+	goName    string // instruction name in Plan 9 syntax
+}
+
+var fpInst []goFPInfo = []goFPInfo{
+	{VADD_F32, []int{2, 1, 0}, "VADD", "ADDF"},
+	{VADD_F64, []int{2, 1, 0}, "VADD", "ADDD"},
+	{VSUB_F32, []int{2, 1, 0}, "VSUB", "SUBF"},
+	{VSUB_F64, []int{2, 1, 0}, "VSUB", "SUBD"},
+	{VMUL_F32, []int{2, 1, 0}, "VMUL", "MULF"},
+	{VMUL_F64, []int{2, 1, 0}, "VMUL", "MULD"},
+	{VNMUL_F32, []int{2, 1, 0}, "VNMUL", "NMULF"},
+	{VNMUL_F64, []int{2, 1, 0}, "VNMUL", "NMULD"},
+	{VMLA_F32, []int{2, 1, 0}, "VMLA", "MULAF"},
+	{VMLA_F64, []int{2, 1, 0}, "VMLA", "MULAD"},
+	{VMLS_F32, []int{2, 1, 0}, "VMLS", "MULSF"},
+	{VMLS_F64, []int{2, 1, 0}, "VMLS", "MULSD"},
+	{VNMLA_F32, []int{2, 1, 0}, "VNMLA", "NMULAF"},
+	{VNMLA_F64, []int{2, 1, 0}, "VNMLA", "NMULAD"},
+	{VNMLS_F32, []int{2, 1, 0}, "VNMLS", "NMULSF"},
+	{VNMLS_F64, []int{2, 1, 0}, "VNMLS", "NMULSD"},
+	{VDIV_F32, []int{2, 1, 0}, "VDIV", "DIVF"},
+	{VDIV_F64, []int{2, 1, 0}, "VDIV", "DIVD"},
+	{VNEG_F32, []int{1, 0}, "VNEG", "NEGF"},
+	{VNEG_F64, []int{1, 0}, "VNEG", "NEGD"},
+	{VABS_F32, []int{1, 0}, "VABS", "ABSF"},
+	{VABS_F64, []int{1, 0}, "VABS", "ABSD"},
+	{VSQRT_F32, []int{1, 0}, "VSQRT", "SQRTF"},
+	{VSQRT_F64, []int{1, 0}, "VSQRT", "SQRTD"},
+	{VCMP_F32, []int{1, 0}, "VCMP", "CMPF"},
+	{VCMP_F64, []int{1, 0}, "VCMP", "CMPD"},
+	{VCMPE_F32, []int{1, 0}, "VCMPE", "CMPF"},
+	{VCMPE_F64, []int{1, 0}, "VCMPE", "CMPD"},
+	{VLDR, []int{1}, "VLDR", "MOV"},
+	{VSTR, []int{1}, "VSTR", "MOV"},
+	{VMOV_F32, []int{1, 0}, "VMOV", "MOVF"},
+	{VMOV_F64, []int{1, 0}, "VMOV", "MOVD"},
+	{VMOV_32, []int{1, 0}, "VMOV", "MOVW"},
+	{VMOV, []int{1, 0}, "VMOV", "MOVW"},
+	{VCVT_F64_F32, []int{1, 0}, "VCVT", "MOVFD"},
+	{VCVT_F32_F64, []int{1, 0}, "VCVT", "MOVDF"},
+	{VCVT_F32_U32, []int{1, 0}, "VCVT", "MOVWF.U"},
+	{VCVT_F32_S32, []int{1, 0}, "VCVT", "MOVWF"},
+	{VCVT_S32_F32, []int{1, 0}, "VCVT", "MOVFW"},
+	{VCVT_U32_F32, []int{1, 0}, "VCVT", "MOVFW.U"},
+	{VCVT_F64_U32, []int{1, 0}, "VCVT", "MOVWD.U"},
+	{VCVT_F64_S32, []int{1, 0}, "VCVT", "MOVWD"},
+	{VCVT_S32_F64, []int{1, 0}, "VCVT", "MOVDW"},
+	{VCVT_U32_F64, []int{1, 0}, "VCVT", "MOVDW.U"},
+}
+
+// convert FP instructions from GNU syntax to Plan 9 syntax, for example,
+// vadd.f32 s0, s3, s4 -> ADDF F0, S3, F2
+// vsub.f64 d0, d2, d4 -> SUBD F0, F2, F4
+// vldr s2, [r11] -> MOVF (R11), F1
+// inputs: instruction name and arguments in GNU syntax
+// return values: corresponding instruction name and arguments in Plan 9 syntax
+func fpTrans(inst *Inst, op string, args []string) (string, []string) {
+	for _, fp := range fpInst {
+		if inst.Op == fp.op {
+			// remove gnu syntax suffixes
+			op = strings.Replace(op, ".F32", "", -1)
+			op = strings.Replace(op, ".F64", "", -1)
+			op = strings.Replace(op, ".S32", "", -1)
+			op = strings.Replace(op, ".U32", "", -1)
+			op = strings.Replace(op, ".32", "", -1)
+			// compose op name
+			if fp.op == VLDR || fp.op == VSTR {
+				switch {
+				case strings.HasPrefix(args[fp.transArgs[0]], "D"):
+					op = "MOVD" + op[len(fp.gnuName):]
+				case strings.HasPrefix(args[fp.transArgs[0]], "S"):
+					op = "MOVF" + op[len(fp.gnuName):]
+				default:
+					panic(fmt.Sprintf("wrong FP register: %v", inst))
+				}
+			} else {
+				op = fp.goName + op[len(fp.gnuName):]
+			}
+			// transform registers
+			for ix, ri := range fp.transArgs {
+				switch {
+				case strings.HasSuffix(args[ri], "[1]"): // MOVW Rx, Dy[1]
+					break
+				case strings.HasSuffix(args[ri], "[0]"): // Dx[0] -> Fx
+					args[ri] = strings.Replace(args[ri], "[0]", "", -1)
+					fallthrough
+				case strings.HasPrefix(args[ri], "D"): // Dx -> Fx
+					args[ri] = "F" + args[ri][1:]
+				case strings.HasPrefix(args[ri], "S"):
+					if inst.Args[ix].(Reg)&1 == 0 { // Sx -> Fy, y = x/2, if x is even
+						args[ri] = fmt.Sprintf("F%d", (inst.Args[ix].(Reg)-S0)/2)
+					}
+				case strings.HasPrefix(args[ri], "$"): // CMPF/CMPD $0, Fx
+					break
+				case strings.HasPrefix(args[ri], "R"): // MOVW Rx, Dy[1]
+					break
+				default:
+					panic(fmt.Sprintf("wrong FP register: %v", inst))
+				}
+			}
+			break
+		}
+	}
+	return op, args
+}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/tables.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/tables.go
new file mode 100644
index 0000000000..0ac56ab759
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/tables.go
@@ -0,0 +1,838 @@
+package thumbasm
+
+const (
+	_ Op = iota
+	ADD
+	ADD_S
+	ADC
+	ADC_S
+	AND
+	AND_S
+	ASR
+	ASR_S
+	B
+	BCC
+	BCS
+	BEQ
+	BFC
+	BFI
+	BGE
+	BGT
+	BHI
+	BIC
+	BIC_S
+	BKPT
+	BL
+	BLE
+	BLS
+	BLT
+	BLX
+	BMI
+	BNE
+	BPL
+	BVC
+	BVS
+	BX
+	CBNZ
+	CBZ
+	CLREX
+	CLZ
+	CMN
+	CMP
+	CPSID
+	CPSIE
+	DMB
+	DSB
+	EOR
+	EOR_S
+	ISB
+	IT
+	ITE
+	ITEE
+	ITEEE
+	ITEET
+	ITET
+	ITETE
+	ITETT
+	ITT
+	ITTE
+	ITTEE
+	ITTET
+	ITTT
+	ITTTE
+	ITTTT
+	LDMIA
+	LDMDB
+	LDR
+	LDRB
+	LDRD
+	LDREX
+	LDREXB
+	LDREXH
+	LDRH
+	LDRSB
+	LDRSH
+	LSL
+	LSL_S
+	LSR
+	LSR_S
+	MLA
+	MLS
+	MOV
+	MOV_S
+	MOVT
+	MOVW
+	MRS
+	MSR
+	MVN
+	MVN_S
+	MUL
+	NOP
+	ORN
+	ORN_S
+	ORR
+	ORR_S
+	PLD
+	PLI
+	POP
+	PUSH
+	RBIT
+	REV
+	REV16
+	REVSH
+	ROR
+	ROR_S
+	RSB
+	RSB_S
+	SBC
+	SBC_S
+	SBFX
+	SDIV
+	SEL
+	SEV
+	SMLABB
+	SMLABT
+	SMLAD
+	SMLADX
+	SMLAL
+	SMLATB
+	SMLATT
+	SMLAWB
+	SMLAWT
+	SMLSD
+	SMLSDX
+	SMMLA
+	SMMLS
+	SMULL
+	STMIA
+	STMDB
+	STR
+	STRB
+	STRD
+	STREX
+	STREXB
+	STREXH
+	STRH
+	SUB
+	SUB_S
+	SVC
+	SXTB
+	SXTH
+	TBB
+	TBH
+	TST
+	TEQ
+	UBFX
+	UDF
+	UDIV
+	UMLAL
+	UMULL
+	UXTB
+	UXTH
+
+	VABS_F32
+	VABS_F64
+	VADD_F32
+	VADD_F64
+	VCMP_F32
+	VCMP_F64
+	VCMPE_F32
+	VCMPE_F64
+	VCVT_F32_FXS16
+	VCVT_F32_FXS32
+	VCVT_F32_FXU16
+	VCVT_F32_FXU32
+	VCVT_F64_FXS16
+	VCVT_F64_FXS32
+	VCVT_F64_FXU16
+	VCVT_F64_FXU32
+	VCVT_F32_U32
+	VCVT_F32_S32
+	VCVT_F64_U32
+	VCVT_F64_S32
+	VCVT_F64_F32
+	VCVT_F32_F64
+	VCVT_FXS16_F32
+	VCVT_FXS16_F64
+	VCVT_FXS32_F32
+	VCVT_FXS32_F64
+	VCVT_FXU16_F32
+	VCVT_FXU16_F64
+	VCVT_FXU32_F32
+	VCVT_FXU32_F64
+	VCVTB_F32_F16
+	VCVTB_F16_F32
+	VCVTT_F32_F16
+	VCVTT_F16_F32
+	VCVTR_U32_F32
+	VCVTR_U32_F64
+	VCVTR_S32_F32
+	VCVTR_S32_F64
+	VCVT_U32_F32
+	VCVT_U32_F64
+	VCVT_S32_F32
+	VCVT_S32_F64
+	VDIV_F32
+	VDIV_F64
+	VLDR
+	VMLA_F32
+	VMLA_F64
+	VMLS_F32
+	VMLS_F64
+	VMOV
+	VMOV_32
+	VMOV_F32
+	VMOV_F64
+	VMRS
+	VMSR
+	VMUL_F32
+	VMUL_F64
+	VNEG_F32
+	VNEG_F64
+	VNMLS_F32
+	VNMLS_F64
+	VNMLA_F32
+	VNMLA_F64
+	VNMUL_F32
+	VNMUL_F64
+	VSQRT_F32
+	VSQRT_F64
+	VSTR
+	VSUB_F32
+	VSUB_F64
+
+	WFE
+	WFI
+	YIELD
+)
+
+var opstr = [...]string{
+	ADD:            "ADD",
+	ADD_S:          "ADD.S",
+	ADC:            "ADC",
+	ADC_S:          "ADC.S",
+	AND:            "AND",
+	AND_S:          "AND.S",
+	ASR:            "ASR",
+	ASR_S:          "ASR.S",
+	B:              "B",
+	BCC:            "BCC",
+	BCS:            "BCS",
+	BEQ:            "BEQ",
+	BFC:            "BFC",
+	BFI:            "BFI",
+	BGE:            "BGE",
+	BGT:            "BGT",
+	BHI:            "BHI",
+	BIC:            "BIC",
+	BIC_S:          "BIC.S",
+	BKPT:           "BKPT",
+	BL:             "BL",
+	BLE:            "BLE",
+	BLS:            "BLS",
+	BLT:            "BLT",
+	BLX:            "BLX",
+	BMI:            "BMI",
+	BNE:            "BNE",
+	BPL:            "BPL",
+	BVC:            "BVC",
+	BVS:            "BVS",
+	BX:             "BX",
+	CBNZ:           "CBNZ",
+	CBZ:            "CBZ",
+	CLREX:          "CLREX",
+	CLZ:            "CLZ",
+	CMN:            "CMN",
+	CMP:            "CMP",
+	CPSID:          "CPSID",
+	CPSIE:          "CPSIE",
+	DMB:            "DMB",
+	DSB:            "DSB",
+	EOR:            "EOR",
+	EOR_S:          "EOR.S",
+	ISB:            "ISB",
+	IT:             "IT",
+	ITE:            "ITE",
+	ITEE:           "ITEE",
+	ITEEE:          "ITEEE",
+	ITEET:          "ITEET",
+	ITET:           "ITET",
+	ITETE:          "ITETE",
+	ITETT:          "ITETT",
+	ITT:            "ITT",
+	ITTE:           "ITTE",
+	ITTEE:          "ITTEE",
+	ITTET:          "ITTET",
+	ITTT:           "ITTT",
+	ITTTE:          "ITTTE",
+	ITTTT:          "ITTTT",
+	LDMIA:          "LDMIA",
+	LDMDB:          "LDMDB",
+	LDR:            "LDR",
+	LDRB:           "LDRB",
+	LDRD:           "LDRD",
+	LDREX:          "LDREX",
+	LDREXB:         "LDREXB",
+	LDREXH:         "LDREXH",
+	LDRH:           "LDRH",
+	LDRSB:          "LDRSB",
+	LDRSH:          "LDRSH",
+	LSL:            "LSL",
+	LSL_S:          "LSL.S",
+	LSR:            "LSR",
+	LSR_S:          "LSR.S",
+	MLA:            "MLA",
+	MLS:            "MLS",
+	MOV:            "MOV",
+	MOV_S:          "MOV.S",
+	MOVT:           "MOVT",
+	MOVW:           "MOVW",
+	MRS:            "MRS",
+	MSR:            "MSR",
+	MVN:            "MVN",
+	MVN_S:          "MVN.S",
+	MUL:            "MUL",
+	NOP:            "NOP",
+	ORN:            "ORN",
+	ORN_S:          "ORN.S",
+	ORR:            "ORR",
+	ORR_S:          "ORR.S",
+	PLD:            "PLD",
+	PLI:            "PLI",
+	POP:            "POP",
+	PUSH:           "PUSH",
+	RBIT:           "RBIT",
+	REV:            "REV",
+	REV16:          "REV16",
+	REVSH:          "REVSH",
+	ROR:            "ROR",
+	ROR_S:          "ROR.S",
+	RSB:            "RSB",
+	RSB_S:          "RSB.S",
+	SBC:            "SBC",
+	SBC_S:          "SBC.S",
+	SBFX:           "SBFX",
+	SDIV:           "SDIV",
+	SEL:            "SEL",
+	SEV:            "SEV",
+	SMLABB:         "SMLABB",
+	SMLABT:         "SMLABT",
+	SMLAD:          "SMLAD",
+	SMLADX:         "SMLADX",
+	SMLAL:          "SMLAL",
+	SMLATB:         "SMLATB",
+	SMLATT:         "SMLATT",
+	SMLAWB:         "SMLAWB",
+	SMLAWT:         "SMLAWT",
+	SMLSD:          "SMLSD",
+	SMLSDX:         "SMLSDX",
+	SMMLA:          "SMMLA",
+	SMMLS:          "SMMLS",
+	SMULL:          "SMULL",
+	STMIA:          "STMIA",
+	STMDB:          "STMDB",
+	STR:            "STR",
+	STRB:           "STRB",
+	STRD:           "STRD",
+	STREX:          "STREX",
+	STREXB:         "STREXB",
+	STREXH:         "STREXH",
+	STRH:           "STRH",
+	SUB:            "SUB",
+	SUB_S:          "SUB.S",
+	SVC:            "SVC",
+	SXTB:           "SXTB",
+	SXTH:           "SXTH",
+	TBB:            "TBB",
+	TBH:            "TBH",
+	TST:            "TST",
+	TEQ:            "TEQ",
+	UBFX:           "UBFX",
+	UDF:            "UDF",
+	UDIV:           "UDIV",
+	UMLAL:          "UMLAL",
+	UMULL:          "UMULL",
+	UXTB:           "UXTB",
+	UXTH:           "UXTH",
+	VABS_F32:       "VABS.F32",
+	VABS_F64:       "VABS.F64",
+	VADD_F32:       "VADD.F32",
+	VADD_F64:       "VADD.F64",
+	VCMP_F32:       "VCMP.F32",
+	VCMP_F64:       "VCMP.F64",
+	VCMPE_F32:      "VCMPE.F32",
+	VCMPE_F64:      "VCMPE.F64",
+	VCVT_F32_FXS16: "VCVT.F32.FXS16",
+	VCVT_F32_FXS32: "VCVT.F32.FXS32",
+	VCVT_F32_FXU16: "VCVT.F32.FXU16",
+	VCVT_F32_FXU32: "VCVT.F32.FXU32",
+	VCVT_F64_FXS16: "VCVT.F64.FXS16",
+	VCVT_F64_FXS32: "VCVT.F64.FXS32",
+	VCVT_F64_FXU16: "VCVT.F64.FXU16",
+	VCVT_F64_FXU32: "VCVT.F64.FXU32",
+	VCVT_F32_U32:   "VCVT.F32.U32",
+	VCVT_F32_S32:   "VCVT.F32.S32",
+	VCVT_F64_U32:   "VCVT.F64.U32",
+	VCVT_F64_S32:   "VCVT.F64.S32",
+	VCVT_F64_F32:   "VCVT.F64.F32",
+	VCVT_F32_F64:   "VCVT.F32.F64",
+	VCVT_FXS16_F32: "VCVT.FXS16.F32",
+	VCVT_FXS16_F64: "VCVT.FXS16.F64",
+	VCVT_FXS32_F32: "VCVT.FXS32.F32",
+	VCVT_FXS32_F64: "VCVT.FXS32.F64",
+	VCVT_FXU16_F32: "VCVT.FXU16.F32",
+	VCVT_FXU16_F64: "VCVT.FXU16.F64",
+	VCVT_FXU32_F32: "VCVT.FXU32.F32",
+	VCVT_FXU32_F64: "VCVT.FXU32.F64",
+	VCVTB_F32_F16:  "VCVTB.F32.F16",
+	VCVTB_F16_F32:  "VCVTB.F16.F32",
+	VCVTT_F32_F16:  "VCVTT.F32.F16",
+	VCVTT_F16_F32:  "VCVTT.F16.F32",
+	VCVTR_U32_F32:  "VCVTR.U32.F32",
+	VCVTR_U32_F64:  "VCVTR.U32.F64",
+	VCVTR_S32_F32:  "VCVTR.S32.F32",
+	VCVTR_S32_F64:  "VCVTR.S32.F64",
+	VCVT_U32_F32:   "VCVT.U32.F32",
+	VCVT_U32_F64:   "VCVT.U32.F64",
+	VCVT_S32_F32:   "VCVT.S32.F32",
+	VCVT_S32_F64:   "VCVT.S32.F64",
+	VDIV_F32:       "VDIV.F32",
+	VDIV_F64:       "VDIV.F64",
+	VLDR:           "VLDR",
+	VMLA_F32:       "VMLA.F32",
+	VMLA_F64:       "VMLA.F64",
+	VMLS_F32:       "VMLS.F32",
+	VMLS_F64:       "VMLS.F64",
+	VMOV:           "VMOV",
+	VMOV_32:        "VMOV.32",
+	VMOV_F32:       "VMOV.F32",
+	VMOV_F64:       "VMOV.F64",
+	VMRS:           "VMRS",
+	VMSR:           "VMSR",
+	VMUL_F32:       "VMUL.F32",
+	VMUL_F64:       "VMUL.F64",
+	VNEG_F32:       "VNEG.F32",
+	VNEG_F64:       "VNEG.F64",
+	VNMLS_F32:      "VNMLS.F32",
+	VNMLS_F64:      "VNMLS.F64",
+	VNMLA_F32:      "VNMLA.F32",
+	VNMLA_F64:      "VNMLA.F64",
+	VNMUL_F32:      "VNMUL.F32",
+	VNMUL_F64:      "VNMUL.F64",
+	VSQRT_F32:      "VSQRT.F32",
+	VSQRT_F64:      "VSQRT.F64",
+	VSTR:           "VSTR",
+	VSUB_F32:       "VSUB.F32",
+	VSUB_F64:       "VSUB.F64",
+	WFE:            "WFE",
+	WFI:            "WFI",
+	YIELD:          "YIELD",
+}
+
+var inst16formats = [...]inst16format{
+	{0xFFFF, 0xBF00, NOP, nil},
+	{0xFFFF, 0xBF10, YIELD, nil},
+	{0xFFFF, 0xBF20, WFE, nil},
+	{0xFFFF, 0xBF30, WFI, nil},
+	{0xFFFF, 0xBF40, SEV, nil},
+
+	{0xFFFC, 0xB660, CPSIE, _CPSIE},
+	{0xFFFC, 0xB670, CPSID, _CPSIE},
+
+	{0xFF0F, 0xBF00 | 8, IT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 4, ITT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 4, ITE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 0 | 2, ITTT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 0 | 2, ITET, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 4 | 2, ITTE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 4 | 2, ITEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 0 | 0 | 1, ITTTT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 0 | 0 | 1, ITETT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 4 | 0 | 1, ITTET, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 4 | 0 | 1, ITEET, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 0 | 2 | 1, ITTTE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 0 | 2 | 1, ITETE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 4 | 2 | 1, ITTEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 4 | 2 | 1, ITEEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 4, ITE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 4, ITT, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 0 | 2, ITEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 0 | 2, ITTE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 4 | 2, ITET, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 4 | 2, ITTT, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 0 | 0 | 1, ITEEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 0 | 0 | 1, ITTEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 4 | 0 | 1, ITETE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 4 | 0 | 1, ITTTE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 0 | 2 | 1, ITEET, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 0 | 2 | 1, ITTET, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 4 | 2 | 1, ITETT, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 4 | 2 | 1, ITTTT, _ITmask__firstcond},
+
+	{0xFF87, 0x4700, BX, _B__Rm},
+	{0xFF87, 0x4780, BLX, _B__Rm},
+
+	{0xFFC0, 0x0000, MOV_S, _AND__Rm__Rdn},
+	{0xFFC0, 0x4000, AND, _AND__Rm__Rdn},
+	{0xFFC0, 0x4040, EOR, _AND__Rm__Rdn},
+	{0xFFC0, 0x4140, ADC, _AND__Rm__Rdn},
+	{0xFFC0, 0x4180, SBC, _AND__Rm__Rdn},
+	{0xFFC0, 0x4200, TST, _AND__Rm__Rdn},
+	{0xFFC0, 0x4240, RSB, _AND__Rm__Rdn},
+	{0xFFC0, 0x4280, CMP, _AND__Rm__Rdn},
+	{0xFFC0, 0x42C0, CMN, _AND__Rm__Rdn},
+	{0xFFC0, 0x4300, ORR, _AND__Rm__Rdn},
+	{0xFFC0, 0x4340, MUL, _AND__Rm__Rdn},
+	{0xFFC0, 0x4380, BIC, _AND__Rm__Rdn},
+	{0xFFC0, 0x43C0, MVN, _AND__Rm__Rdn},
+	{0xFFC0, 0x4080, LSL, _AND__Rm__Rdn},
+	{0xFFC0, 0x40C0, LSR, _AND__Rm__Rdn},
+	{0xFFC0, 0x4100, ASR, _AND__Rm__Rdn},
+	{0xFFC0, 0x41C0, ROR, _AND__Rm__Rdn},
+	{0xFFC0, 0xBA00, REV, _AND__Rm__Rdn},
+	{0xFFC0, 0xBA40, REV16, _AND__Rm__Rdn},
+	{0xFFC0, 0xBAC0, REVSH, _AND__Rm__Rdn},
+	{0xFFC0, 0xB200, SXTH, _AND__Rm__Rdn},
+	{0xFFC0, 0xB240, SXTB, _AND__Rm__Rdn},
+	{0xFFC0, 0xB280, UXTH, _AND__Rm__Rdn},
+	{0xFFC0, 0xB2C0, UXTB, _AND__Rm__Rdn},
+
+	{0xFF80, 0xB000, ADD, _ADD__u7_2__R13},
+	{0xFF80, 0xB080, SUB, _ADD__u7_2__R13},
+
+	{0xFF00, 0x4400, ADD, _ADD__Rm__Rdn},
+	{0xFF00, 0x4600, MOV, _ADD__Rm__Rdn},
+	{0xFF00, 0x4500, CMP, _ADD__Rm__Rdn},
+
+	{0xFF00, 0xD000, BEQ, _Bcond__i8_1},
+	{0xFF00, 0xD100, BNE, _Bcond__i8_1},
+	{0xFF00, 0xD200, BCS, _Bcond__i8_1},
+	{0xFF00, 0xD300, BCC, _Bcond__i8_1},
+	{0xFF00, 0xD400, BMI, _Bcond__i8_1},
+	{0xFF00, 0xD500, BPL, _Bcond__i8_1},
+	{0xFF00, 0xD600, BVS, _Bcond__i8_1},
+	{0xFF00, 0xD700, BVC, _Bcond__i8_1},
+	{0xFF00, 0xD800, BHI, _Bcond__i8_1},
+	{0xFF00, 0xD900, BLS, _Bcond__i8_1},
+	{0xFF00, 0xDA00, BGE, _Bcond__i8_1},
+	{0xFF00, 0xDB00, BLT, _Bcond__i8_1},
+	{0xFF00, 0xDC00, BGT, _Bcond__i8_1},
+	{0xFF00, 0xDD00, BLE, _Bcond__i8_1},
+
+	{0xFF00, 0xDE00, UDF, _UDF__u8},
+	{0xFF00, 0xDF00, SVC, _UDF__u8},
+	{0xFF00, 0xBE00, BKPT, _UDF__u8},
+
+	{0xFE00, 0x1800, ADD, _ADD__Rm__Rn__Rd},
+	{0xFE00, 0x1A00, SUB, _ADD__Rm__Rn__Rd},
+
+	{0xFE00, 0xB400, PUSH, _PUSH__reglist},
+	{0xFE00, 0xBC00, POP, _PUSH__reglist},
+
+	{0xFE00, 0x1C00, ADD, _ADD__u3__Rn__Rd},
+	{0xFE00, 0x1E00, SUB, _ADD__u3__Rn__Rd},
+
+	{0xFD00, 0xB100, CBZ, _CBZ__Rn__u6_1},
+	{0xFD00, 0xB900, CBNZ, _CBZ__Rn__u6_1},
+
+	{0xF800, 0x0000, LSL, _MOVW__Rm_v_u5__Rd},
+	{0xF800, 0x0800, LSR, _MOVW__Rm_v_u5__Rd},
+	{0xF800, 0x1000, ASR, _MOVW__Rm_v_u5__Rd},
+
+	{0xF800, 0x2000, MOV, _MOVW__u8__Rd},
+	{0xF800, 0x2800, CMP, _MOVW__u8__Rd},
+	{0xF800, 0x3000, ADD, _MOVW__u8__Rd},
+	{0xF800, 0x3800, SUB, _MOVW__u8__Rd},
+
+	{0xF800, 0x4800, LDR, _MOVW__u8_2_R13__Rt},
+	{0xF800, 0x9000, STR, _MOVW__u8_2_R13__Rt},
+	{0xF800, 0x9800, LDR, _MOVW__u8_2_R13__Rt},
+
+	{0xF800, 0x6000, STR, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x7000, STRB, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x8000, STRH, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x6800, LDR, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x7800, LDRB, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x8800, LDRH, _MOVW__u5_2_Rn__Rt},
+
+	{0xF800, 0xE000, B, _B__i11_1},
+
+	{0xF800, 0xC000, STMIA, _MOVM_IAW},
+	{0xF800, 0xC800, LDMIA, _MOVM_IAW},
+
+	{0xF000, 0xA000, ADD, _ADD__u8_2__R13__Rd},
+
+	{0xFE00, 0x5800, LDR, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5E00, LDRSH, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5A00, LDRH, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5600, LDRSB, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5C00, LDRB, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5000, STR, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5200, STRH, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5400, STRB, _MOVW__Rn_Rm__Rt},
+}
+
+var inst32formats = [...]inst32format{
+	{0xFFFFFFFF, 0xF3AF8000, NOP, nil},
+	{0xFFFFFFFF, 0xF3BF8F2F, CLREX, nil},
+
+	{0xFFFF0FFF, 0xF84D0D04, PUSH, _PUSH__Rt},
+	{0xFFFF0FFF, 0xF85D0B04, POP, _PUSH__Rt},
+
+	{0xFFFFFFF0, 0xF3BF8F40, DSB, _DSB__opt},
+	{0xFFFFFFF0, 0xF3BF8F50, DMB, _DSB__opt},
+	{0xFFFFFFF0, 0xF3BF8F60, ISB, _DSB__opt},
+
+	{0xFFFF0FFF, 0xEEE10A10, VMSR, _MOVW__FPSCR__Rt},
+	{0xFFFF0FFF, 0xEEF10A10, VMRS, _MOVW__FPSCR__Rt},
+
+	{0xFFF0FFF0, 0xE8D0F000, TBB, _TBB__Rm__Rn},
+	{0xFFF0FFF0, 0xE8D0F010, TBH, _TBB__Rm__Rn},
+
+	{0xFFF0F0F0, 0xFB00F000, MUL, _MUL__Rm__Rn__Rd},
+	{0xFFF0F0F0, 0xFB90F0F0, SDIV, _MUL__Rm__Rn__Rd},
+	{0xFFF0F0F0, 0xFBB0F0F0, UDIV, _MUL__Rm__Rn__Rd},
+	{0xFFF0F0F0, 0xFAA0F080, SEL, _MUL__Rm__Rn__Rd},
+
+	{0xFFF0F0F0, 0xFAB0F080, CLZ, _CLZ__Rm__Rd},
+	{0xFFF0F0F0, 0xFA90F080, REV, _CLZ__Rm__Rd},
+	{0xFFF0F0F0, 0xFA90F090, REV16, _CLZ__Rm__Rd},
+	{0xFFF0F0F0, 0xFA90F0A0, RBIT, _CLZ__Rm__Rd},
+	{0xFFF0F0F0, 0xFA90F0B0, REVSH, _CLZ__Rm__Rd},
+
+	{0xFFF0F0F0, 0xFA00F000, LSL, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA10F000, LSL_S, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA20F000, LSR, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA30F000, LSR_S, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA40F000, ASR, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA50F000, ASR_S, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA60F000, ROR, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA70F000, ROR_S, _MOVWs__Rn_v_Rm__Rd},
+
+	{0xFFF00FFF, 0xE8D00F4F, LDREXB, _LDREXB__Rn__Rt},
+	{0xFFF00FFF, 0xE8D00F5F, LDREXB, _LDREXB__Rn__Rt},
+
+	{0xFFF00FF0, 0xE8C00F40, STREXB, _STREXB__Rn__Rt},
+	{0xFFF00FF0, 0xE8C00F50, STREXH, _STREXB__Rn__Rt},
+
+	{0xFFFFF0C0, 0xFA0FF080, SXTH, _MOVH__Rm_rot__Rd},
+	{0xFFFFF0C0, 0xFA4FF080, SXTB, _MOVH__Rm_rot__Rd},
+	{0xFFFFF0C0, 0xFA1FF080, UXTH, _MOVH__Rm_rot__Rd},
+	{0xFFFFF0C0, 0xFA5FF080, UXTB, _MOVH__Rm_rot__Rd},
+
+	{0xFFFFF000, 0xF3EF8000, MRS, _MOVW__SYSm__Rd},
+	{0xFFF0FF00, 0xF3808800, MSR, _MOVW__SYSm__Rd},
+
+	{0xFFF08F00, 0xEA100F00, TST, _TST__Rm_v_u5__Rn},
+	{0xFFF08F00, 0xEA900F00, TEQ, _TST__Rm_v_u5__Rn},
+	{0xFFF08F00, 0xEB100F00, CMN, _TST__Rm_v_u5__Rn},
+	{0xFFF08F00, 0xEBB00F00, CMP, _TST__Rm_v_u5__Rn},
+
+	{0xFFFF8000, 0xEA4F0000, MOV, _MOVWs__Rm_v_u5__Rn},
+	{0xFFFF8000, 0xEA5F0000, MOV_S, _MOVWs__Rm_v_u5__Rn},
+	{0xFFFF8000, 0xEA6F0000, MVN, _MOVWs__Rm_v_u5__Rn},
+	{0xFFFF8000, 0xEA7F0000, MVN_S, _MOVWs__Rm_v_u5__Rn},
+
+	{0xFFF000F0, 0xFB800000, SMULL, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFBA00000, UMULL, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFBC00000, SMLAL, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFBE00000, UMLAL, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFB000000, MLA, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFB000010, MLS, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFB300000, SMLAWB, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFB300010, SMLAWT, _MULL__Rm__Rn__Rdh_Rdl},
+
+	{0xFF7F0000, 0xF81F0000, LDRB, _MOVW__s12_Rn__Rt},
+	{0xFF7F0000, 0xF83F0000, LDRH, _MOVW__s12_Rn__Rt},
+	{0xFF7F0000, 0xF85F0000, LDR, _MOVW__s12_Rn__Rt},
+	{0xFF7F0000, 0xF91F0000, LDRSB, _MOVW__s12_Rn__Rt},
+	{0xFF7F0000, 0xF93F0000, LDRSH, _MOVW__s12_Rn__Rt},
+
+	{0xFFF00FC0, 0xF8500000, LDR, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF9300000, LDRSH, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8300000, LDRH, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF9100000, LDRSB, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8100000, LDRB, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8400000, STR, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8200000, STRH, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8000000, STRB, _MOVW__Rn_Rm_1_u2__Rt},
+
+	{0xFBFF8000, 0xF04F0000, MOV, _MOVWs__e32__Rd},
+	{0xFBFF8000, 0xF05F0000, MOV_S, _MOVWs__e32__Rd},
+	{0xFBFF8000, 0xF06F0000, MVN, _MOVWs__e32__Rd},
+	{0xFBFF8000, 0xF07F0000, MVN_S, _MOVWs__e32__Rd},
+
+	{0xFFF00800, 0xF8000800, STRB, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8100800, LDRB, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8200800, STRH, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8300800, LDRH, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8400800, STR, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8500800, LDR, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF9100800, LDRSB, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF9300800, LDRSH, _MOVWpw__s8_Rn__Rt},
+
+	{0xFFF08000, 0xEA000000, AND, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA100000, AND_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA200000, BIC, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA300000, BIC_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA400000, ORR, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA500000, ORR_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA600000, ORN, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA700000, ORN_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA800000, EOR, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA900000, EOR_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB000000, ADD, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB100000, ADD_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB400000, ADC, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB500000, ADC_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB600000, SBC, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB700000, SBC_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEBA00000, SUB, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEBB00000, SUB_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEBC00000, RSB, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEBD00000, RSB_S, _ANDs__Rm_v_u5__Rn__Rd},
+
+	{0xFFF00000, 0xF8900000, LDRB, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8800000, STRB, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8B00000, LDRH, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8A00000, STRH, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8D00000, LDR, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8C00000, STR, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF9900000, LDRSB, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF9B00000, LDRSH, _MOVW__s12_Rn__Rt},
+
+	{0xFFD02000, 0xE8900000, LDMIA, _MOVM_IAw},
+	{0xFFD02000, 0xE9100000, LDMDB, _MOVM_IAw},
+	{0xFFD02000, 0xE8800000, STMIA, _MOVM_IAw},
+	{0xFFD02000, 0xE9000000, STMDB, _MOVM_IAw},
+
+	{0xFBF08F00, 0xF0100F00, TST, _TST__e32__Rn},
+	{0xFBF08F00, 0xF0900F00, TEQ, _TST__e32__Rn},
+	{0xFBF08F00, 0xF1100F00, CMN, _TST__e32__Rn},
+	{0xFBF08F00, 0xF1B00F00, CMP, _TST__e32__Rn},
+
+	{0xFBF08000, 0xF0000000, AND, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0100000, AND_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0200000, BIC, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0300000, BIC_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0400000, ORR, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0500000, ORR_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0600000, ORN, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0700000, ORN_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0800000, EOR, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0900000, EOR_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1000000, ADD, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1100000, ADD_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1400000, ADC, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1500000, ADC_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1600000, SBC, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1700000, SBC_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1A00000, SUB, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1B00000, SUB_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1C00000, RSB, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1D00000, RSB_S, _ANDs__e32__Rn__Rd},
+
+	{0xFBF08000, 0xF2400000, MOVW, _MOVW__uyz16__Rd},
+	{0xFBF08000, 0xF2C00000, MOVT, _MOVW__uyz16__Rd},
+
+	{0xFBF08000, 0xF2000000, ADD, _ADD__u12__Rn__Rd},
+	{0xFBF08000, 0xF2A00000, SUB, _ADD__u12__Rn__Rd},
+
+	{0xF800D000, 0xF0009000, B, _B__ji24_1},
+	{0xF800D000, 0xF000D000, BL, _B__ji24_1},
+
+	{0xFFF00F00, 0xE8500F00, LDREX, _LDREX__u8_2_Rn__Rt},
+	{0xFFF00000, 0xE8400000, STREX, _STREX__Rt__u8_2_Rn__Rd},
+
+	{0xFFF08020, 0xF3400000, SBFX, _BFX__width__ulsb__Rn__Rd},
+	{0xFFF08020, 0xF3C00000, UBFX, _BFX__width__ulsb__Rn__Rd},
+
+	{0xFFFF8020, 0xF36F0000, BFC, _BFC__width__ulsb__Rd},
+	{0xFFF08020, 0xF3600000, BFI, _BFC__width__ulsb__Rd},
+
+	{0xFF300E00, 0xED100A00, VLDR, _MOVF__s8_2_Rn__Fd},
+	{0xFF300E00, 0xED000A00, VSTR, _MOVF__s8_2_Rn__Fd},
+
+	{0xFFBF0FFF, 0xEEB50A40, VCMP_F32, _CMPF__0__Fd},
+	{0xFFBF0FFF, 0xEEB50B40, VCMP_F64, _CMPF__0__Fd},
+	{0xFFBF0FFF, 0xEEB50AC0, VCMPE_F32, _CMPF__0__Fd},
+	{0xFFBF0FFF, 0xEEB50BC0, VCMPE_F64, _CMPF__0__Fd},
+
+	{0xFFBF0FD0, 0xEEB40A40, VCMP_F32, _CMPF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB40B40, VCMP_F64, _CMPF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB40AC0, VCMPE_F32, _CMPF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB40BC0, VCMPE_F64, _CMPF__Fm__Fd},
+
+	{0xFFBF0FD0, 0xEEB10AC0, VSQRT_F32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB10BC0, VSQRT_F64, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB10A40, VNEG_F32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB10B40, VNEG_F64, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB00A40, VMOV_F32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB00B40, VMOV_F64, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB00AC0, VABS_F32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB00BC0, VABS_F64, _SQRTF__Fm__Fd},
+
+	{0xFFBF0FD0, 0xEEB70AC0, VCVT_F64_F32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB70BC0, VCVT_F32_F64, _SQRTF__Fm__Fd},
+
+	// ARMv7-A
+	{0xFFBF0FD0, 0xEEBD0AC0, VCVT_S32_F32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEBC0AC0, VCVT_U32_F32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEBD0BC0, VCVT_S32_F64, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEBC0BC0, VCVT_U32_F64, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB80AC0, VCVT_F32_S32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB80A40, VCVT_F32_U32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB80BC0, VCVT_F64_S32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB80B40, VCVT_F64_U32, _SQRTF__Fm__Fd},
+
+	// ARMv7-M version of above (according to RM)
+	//{0xFFBF0FD0, 0xFEBD0AC0, VCVT_S32_F32, _SQRTF__Fm__Fd},
+	//{0xFFBF0FD0, 0xFEBC0AC0, VCVT_U32_F32, _SQRTF__Fm__Fd},
+	//{0xFFBF0FD0, 0xFEBD0BC0, VCVT_S32_F64, _SQRTF__Fm__Fd},
+	//{0xFFBF0FD0, 0xFEBC0BC0, VCVT_U32_F64, _SQRTF__Fm__Fd},
+	//{0xFFBF0FD0, 0xFEB80AC0, VCVT_F32_S32, _SQRTF__Fm__Fd},
+	//{0xFFBF0FD0, 0xFEB80A40, VCVT_F32_U32, _SQRTF__Fm__Fd},
+	//{0xFFBF0FD0, 0xFEB80BC0, VCVT_F64_S32, _SQRTF__Fm__Fd},
+	//{0xFFBF0FD0, 0xFEB80B40, VCVT_F64_U32, _SQRTF__Fm__Fd},
+
+	{0xFFF00F7F, 0xEE000B10, VMOV, _MOVW__Fm__Rd},
+	{0xFFF00F7F, 0xEE100B10, VMOV, _MOVW__Fm__Rd},
+
+	{0xFFB00FF0, 0xEEB00A00, VMOV_F32, _MOVF__f8__Fd},
+	{0xFFB00FF0, 0xEEB00B00, VMOV_F64, _MOVF__f8__Fd},
+
+	{0xFFB00F50, 0xEE300A00, VADD_F32, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE300B00, VADD_F64, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE300A40, VSUB_F32, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE300B40, VSUB_F64, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE200A00, VMUL_F32, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE200B00, VMUL_F64, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE800A00, VDIV_F32, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE800B00, VDIV_F64, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE000A00, VMLA_F32, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE000B00, VMLA_F64, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE000A40, VMLS_F32, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE000B40, VMLS_F64, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE200A40, VNMUL_F32, _ADDF__Fm__Fn__Fd},
+	{0xFFB00F50, 0xEE200B40, VNMUL_F64, _ADDF__Fm__Fn__Fd},
+}
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/asm_linux_thumb.s b/src/cmd/vendor/golang.org/x/sys/unix/asm_linux_thumb.s
new file mode 100644
index 0000000000..55b13c7ba4
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/asm_linux_thumb.s
@@ -0,0 +1,56 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build gc
+
+#include "textflag.h"
+
+//
+// System calls for arm, Linux
+//
+
+// Just jump to package syscall's implementation for all these functions.
+// The runtime may know about them.
+
+TEXT ·Syscall(SB),NOSPLIT,$0-28
+	B	syscall·Syscall(SB)
+
+TEXT ·Syscall6(SB),NOSPLIT,$0-40
+	B	syscall·Syscall6(SB)
+
+TEXT ·SyscallNoError(SB),NOSPLIT,$0-24
+	BL	runtime·entersyscall(SB)
+	MOVW	trap+0(FP), R7
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	MOVW	$0, R3
+	MOVW	$0, R4
+	MOVW	$0, R5
+	SWI	$0
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	BL	runtime·exitsyscall(SB)
+	RET
+
+TEXT ·RawSyscall(SB),NOSPLIT,$0-28
+	B	syscall·RawSyscall(SB)
+
+TEXT ·RawSyscall6(SB),NOSPLIT,$0-40
+	B	syscall·RawSyscall6(SB)
+
+TEXT ·RawSyscallNoError(SB),NOSPLIT,$0-24
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	SWI	$0
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	RET
+
+TEXT ·seek(SB),NOSPLIT,$0-28
+	B	syscall·seek(SB)
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/endian_little.go b/src/cmd/vendor/golang.org/x/sys/unix/endian_little.go
index 8822d8541f..18ea39f77c 100644
--- a/src/cmd/vendor/golang.org/x/sys/unix/endian_little.go
+++ b/src/cmd/vendor/golang.org/x/sys/unix/endian_little.go
@@ -2,7 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 //
-// +build 386 amd64 amd64p32 alpha arm arm64 mipsle mips64le mips64p32le nios2 ppc64le riscv riscv64 sh
+
+// +build 386 amd64 amd64p32 alpha arm arm64 mipsle mips64le mips64p32le nios2 ppc64le riscv riscv64 sh thumb
 
 package unix
 
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/fcntl_linux_32bit.go b/src/cmd/vendor/golang.org/x/sys/unix/fcntl_linux_32bit.go
index 8db48e5e06..b3d05c3e66 100644
--- a/src/cmd/vendor/golang.org/x/sys/unix/fcntl_linux_32bit.go
+++ b/src/cmd/vendor/golang.org/x/sys/unix/fcntl_linux_32bit.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build linux,386 linux,arm linux,mips linux,mipsle
+// +build linux,386 linux,arm linux,mips linux,mipsle linux,thumb
 
 package unix
 
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_arm.go b/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_armt.go
similarity index 99%
rename from src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_arm.go
rename to src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_armt.go
index 496837b1e3..4b1592501e 100644
--- a/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_arm.go
+++ b/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_armt.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build arm,linux
+// +build arm,linux thumb,linux
 
 package unix
 
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_gc_arm.go b/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_gc_armt.go
similarity index 90%
rename from src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_gc_arm.go
rename to src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_gc_armt.go
index 1a97baae73..a7f7ce51f9 100644
--- a/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_gc_arm.go
+++ b/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_gc_armt.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build arm,gc,linux
+// +build arm,gc,linux thumb,gc,linux
 
 package unix
 
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/zerrors_linux_thumb.go b/src/cmd/vendor/golang.org/x/sys/unix/zerrors_linux_thumb.go
new file mode 100644
index 0000000000..199f19f18b
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/zerrors_linux_thumb.go
@@ -0,0 +1,796 @@
+// mkerrors.sh -Wall -Werror -static -I/tmp/include
+// Code generated by the command above; see README.md. DO NOT EDIT.
+
+// +build thumb,linux
+
+// Code generated by cmd/cgo -godefs; DO NOT EDIT.
+// cgo -godefs -- -Wall -Werror -static -I/tmp/include _const.go
+
+package unix
+
+import "syscall"
+
+const (
+	B1000000                         = 0x1008
+	B115200                          = 0x1002
+	B1152000                         = 0x1009
+	B1500000                         = 0x100a
+	B2000000                         = 0x100b
+	B230400                          = 0x1003
+	B2500000                         = 0x100c
+	B3000000                         = 0x100d
+	B3500000                         = 0x100e
+	B4000000                         = 0x100f
+	B460800                          = 0x1004
+	B500000                          = 0x1005
+	B57600                           = 0x1001
+	B576000                          = 0x1006
+	B921600                          = 0x1007
+	BLKBSZGET                        = 0x80041270
+	BLKBSZSET                        = 0x40041271
+	BLKFLSBUF                        = 0x1261
+	BLKFRAGET                        = 0x1265
+	BLKFRASET                        = 0x1264
+	BLKGETSIZE                       = 0x1260
+	BLKGETSIZE64                     = 0x80041272
+	BLKPBSZGET                       = 0x127b
+	BLKRAGET                         = 0x1263
+	BLKRASET                         = 0x1262
+	BLKROGET                         = 0x125e
+	BLKROSET                         = 0x125d
+	BLKRRPART                        = 0x125f
+	BLKSECTGET                       = 0x1267
+	BLKSECTSET                       = 0x1266
+	BLKSSZGET                        = 0x1268
+	BOTHER                           = 0x1000
+	BS1                              = 0x2000
+	BSDLY                            = 0x2000
+	CBAUD                            = 0x100f
+	CBAUDEX                          = 0x1000
+	CIBAUD                           = 0x100f0000
+	CLOCAL                           = 0x800
+	CR1                              = 0x200
+	CR2                              = 0x400
+	CR3                              = 0x600
+	CRDLY                            = 0x600
+	CREAD                            = 0x80
+	CS6                              = 0x10
+	CS7                              = 0x20
+	CS8                              = 0x30
+	CSIZE                            = 0x30
+	CSTOPB                           = 0x40
+	ECHOCTL                          = 0x200
+	ECHOE                            = 0x10
+	ECHOK                            = 0x20
+	ECHOKE                           = 0x800
+	ECHONL                           = 0x40
+	ECHOPRT                          = 0x400
+	EFD_CLOEXEC                      = 0x80000
+	EFD_NONBLOCK                     = 0x800
+	EPOLL_CLOEXEC                    = 0x80000
+	EXTPROC                          = 0x10000
+	FF1                              = 0x8000
+	FFDLY                            = 0x8000
+	FICLONE                          = 0x40049409
+	FICLONERANGE                     = 0x4020940d
+	FLUSHO                           = 0x1000
+	FS_IOC_ENABLE_VERITY             = 0x40806685
+	FS_IOC_GETFLAGS                  = 0x80046601
+	FS_IOC_GET_ENCRYPTION_NONCE      = 0x8010661b
+	FS_IOC_GET_ENCRYPTION_POLICY     = 0x400c6615
+	FS_IOC_GET_ENCRYPTION_PWSALT     = 0x40106614
+	FS_IOC_SETFLAGS                  = 0x40046602
+	FS_IOC_SET_ENCRYPTION_POLICY     = 0x800c6613
+	F_GETLK                          = 0xc
+	F_GETLK64                        = 0xc
+	F_GETOWN                         = 0x9
+	F_RDLCK                          = 0x0
+	F_SETLK                          = 0xd
+	F_SETLK64                        = 0xd
+	F_SETLKW                         = 0xe
+	F_SETLKW64                       = 0xe
+	F_SETOWN                         = 0x8
+	F_UNLCK                          = 0x2
+	F_WRLCK                          = 0x1
+	HUPCL                            = 0x400
+	ICANON                           = 0x2
+	IEXTEN                           = 0x8000
+	IN_CLOEXEC                       = 0x80000
+	IN_NONBLOCK                      = 0x800
+	IOCTL_VM_SOCKETS_GET_LOCAL_CID   = 0x7b9
+	ISIG                             = 0x1
+	IUCLC                            = 0x200
+	IXOFF                            = 0x1000
+	IXON                             = 0x400
+	MAP_ANON                         = 0x20
+	MAP_ANONYMOUS                    = 0x20
+	MAP_DENYWRITE                    = 0x800
+	MAP_EXECUTABLE                   = 0x1000
+	MAP_GROWSDOWN                    = 0x100
+	MAP_HUGETLB                      = 0x40000
+	MAP_LOCKED                       = 0x2000
+	MAP_NONBLOCK                     = 0x10000
+	MAP_NORESERVE                    = 0x4000
+	MAP_POPULATE                     = 0x8000
+	MAP_STACK                        = 0x20000
+	MAP_SYNC                         = 0x80000
+	MCL_CURRENT                      = 0x1
+	MCL_FUTURE                       = 0x2
+	MCL_ONFAULT                      = 0x4
+	NFDBITS                          = 0x20
+	NLDLY                            = 0x100
+	NOFLSH                           = 0x80
+	NS_GET_NSTYPE                    = 0xb703
+	NS_GET_OWNER_UID                 = 0xb704
+	NS_GET_PARENT                    = 0xb702
+	NS_GET_USERNS                    = 0xb701
+	OLCUC                            = 0x2
+	ONLCR                            = 0x4
+	O_APPEND                         = 0x400
+	O_ASYNC                          = 0x2000
+	O_CLOEXEC                        = 0x80000
+	O_CREAT                          = 0x40
+	O_DIRECT                         = 0x10000
+	O_DIRECTORY                      = 0x4000
+	O_DSYNC                          = 0x1000
+	O_EXCL                           = 0x80
+	O_FSYNC                          = 0x101000
+	O_LARGEFILE                      = 0x20000
+	O_NDELAY                         = 0x800
+	O_NOATIME                        = 0x40000
+	O_NOCTTY                         = 0x100
+	O_NOFOLLOW                       = 0x8000
+	O_NONBLOCK                       = 0x800
+	O_PATH                           = 0x200000
+	O_RSYNC                          = 0x101000
+	O_SYNC                           = 0x101000
+	O_TMPFILE                        = 0x404000
+	O_TRUNC                          = 0x200
+	PARENB                           = 0x100
+	PARODD                           = 0x200
+	PENDIN                           = 0x4000
+	PERF_EVENT_IOC_DISABLE           = 0x2401
+	PERF_EVENT_IOC_ENABLE            = 0x2400
+	PERF_EVENT_IOC_ID                = 0x80042407
+	PERF_EVENT_IOC_MODIFY_ATTRIBUTES = 0x4004240b
+	PERF_EVENT_IOC_PAUSE_OUTPUT      = 0x40042409
+	PERF_EVENT_IOC_PERIOD            = 0x40082404
+	PERF_EVENT_IOC_QUERY_BPF         = 0xc004240a
+	PERF_EVENT_IOC_REFRESH           = 0x2402
+	PERF_EVENT_IOC_RESET             = 0x2403
+	PERF_EVENT_IOC_SET_BPF           = 0x40042408
+	PERF_EVENT_IOC_SET_FILTER        = 0x40042406
+	PERF_EVENT_IOC_SET_OUTPUT        = 0x2405
+	PPPIOCATTACH                     = 0x4004743d
+	PPPIOCATTCHAN                    = 0x40047438
+	PPPIOCCONNECT                    = 0x4004743a
+	PPPIOCDETACH                     = 0x4004743c
+	PPPIOCDISCONN                    = 0x7439
+	PPPIOCGASYNCMAP                  = 0x80047458
+	PPPIOCGCHAN                      = 0x80047437
+	PPPIOCGDEBUG                     = 0x80047441
+	PPPIOCGFLAGS                     = 0x8004745a
+	PPPIOCGIDLE                      = 0x8008743f
+	PPPIOCGIDLE32                    = 0x8008743f
+	PPPIOCGIDLE64                    = 0x8010743f
+	PPPIOCGL2TPSTATS                 = 0x80487436
+	PPPIOCGMRU                       = 0x80047453
+	PPPIOCGRASYNCMAP                 = 0x80047455
+	PPPIOCGUNIT                      = 0x80047456
+	PPPIOCGXASYNCMAP                 = 0x80207450
+	PPPIOCSACTIVE                    = 0x40087446
+	PPPIOCSASYNCMAP                  = 0x40047457
+	PPPIOCSCOMPRESS                  = 0x400c744d
+	PPPIOCSDEBUG                     = 0x40047440
+	PPPIOCSFLAGS                     = 0x40047459
+	PPPIOCSMAXCID                    = 0x40047451
+	PPPIOCSMRRU                      = 0x4004743b
+	PPPIOCSMRU                       = 0x40047452
+	PPPIOCSNPMODE                    = 0x4008744b
+	PPPIOCSPASS                      = 0x40087447
+	PPPIOCSRASYNCMAP                 = 0x40047454
+	PPPIOCSXASYNCMAP                 = 0x4020744f
+	PPPIOCXFERUNIT                   = 0x744e
+	PR_SET_PTRACER_ANY               = 0xffffffff
+	PTRACE_GETCRUNCHREGS             = 0x19
+	PTRACE_GETFDPIC                  = 0x1f
+	PTRACE_GETFDPIC_EXEC             = 0x0
+	PTRACE_GETFDPIC_INTERP           = 0x1
+	PTRACE_GETFPREGS                 = 0xe
+	PTRACE_GETHBPREGS                = 0x1d
+	PTRACE_GETVFPREGS                = 0x1b
+	PTRACE_GETWMMXREGS               = 0x12
+	PTRACE_GET_THREAD_AREA           = 0x16
+	PTRACE_OLDSETOPTIONS             = 0x15
+	PTRACE_SETCRUNCHREGS             = 0x1a
+	PTRACE_SETFPREGS                 = 0xf
+	PTRACE_SETHBPREGS                = 0x1e
+	PTRACE_SETVFPREGS                = 0x1c
+	PTRACE_SETWMMXREGS               = 0x13
+	PTRACE_SET_SYSCALL               = 0x17
+	PT_DATA_ADDR                     = 0x10004
+	PT_TEXT_ADDR                     = 0x10000
+	PT_TEXT_END_ADDR                 = 0x10008
+	RLIMIT_AS                        = 0x9
+	RLIMIT_MEMLOCK                   = 0x8
+	RLIMIT_NOFILE                    = 0x7
+	RLIMIT_NPROC                     = 0x6
+	RLIMIT_RSS                       = 0x5
+	RNDADDENTROPY                    = 0x40085203
+	RNDADDTOENTCNT                   = 0x40045201
+	RNDCLEARPOOL                     = 0x5206
+	RNDGETENTCNT                     = 0x80045200
+	RNDGETPOOL                       = 0x80085202
+	RNDRESEEDCRNG                    = 0x5207
+	RNDZAPENTCNT                     = 0x5204
+	RTC_AIE_OFF                      = 0x7002
+	RTC_AIE_ON                       = 0x7001
+	RTC_ALM_READ                     = 0x80247008
+	RTC_ALM_SET                      = 0x40247007
+	RTC_EPOCH_READ                   = 0x8004700d
+	RTC_EPOCH_SET                    = 0x4004700e
+	RTC_IRQP_READ                    = 0x8004700b
+	RTC_IRQP_SET                     = 0x4004700c
+	RTC_PIE_OFF                      = 0x7006
+	RTC_PIE_ON                       = 0x7005
+	RTC_PLL_GET                      = 0x801c7011
+	RTC_PLL_SET                      = 0x401c7012
+	RTC_RD_TIME                      = 0x80247009
+	RTC_SET_TIME                     = 0x4024700a
+	RTC_UIE_OFF                      = 0x7004
+	RTC_UIE_ON                       = 0x7003
+	RTC_VL_CLR                       = 0x7014
+	RTC_VL_READ                      = 0x80047013
+	RTC_WIE_OFF                      = 0x7010
+	RTC_WIE_ON                       = 0x700f
+	RTC_WKALM_RD                     = 0x80287010
+	RTC_WKALM_SET                    = 0x4028700f
+	SCM_TIMESTAMPING                 = 0x25
+	SCM_TIMESTAMPING_OPT_STATS       = 0x36
+	SCM_TIMESTAMPING_PKTINFO         = 0x3a
+	SCM_TIMESTAMPNS                  = 0x23
+	SCM_TXTIME                       = 0x3d
+	SCM_WIFI_STATUS                  = 0x29
+	SFD_CLOEXEC                      = 0x80000
+	SFD_NONBLOCK                     = 0x800
+	SIOCATMARK                       = 0x8905
+	SIOCGPGRP                        = 0x8904
+	SIOCGSTAMPNS_NEW                 = 0x80108907
+	SIOCGSTAMP_NEW                   = 0x80108906
+	SIOCINQ                          = 0x541b
+	SIOCOUTQ                         = 0x5411
+	SIOCSPGRP                        = 0x8902
+	SOCK_CLOEXEC                     = 0x80000
+	SOCK_DGRAM                       = 0x2
+	SOCK_NONBLOCK                    = 0x800
+	SOCK_STREAM                      = 0x1
+	SOL_SOCKET                       = 0x1
+	SO_ACCEPTCONN                    = 0x1e
+	SO_ATTACH_BPF                    = 0x32
+	SO_ATTACH_REUSEPORT_CBPF         = 0x33
+	SO_ATTACH_REUSEPORT_EBPF         = 0x34
+	SO_BINDTODEVICE                  = 0x19
+	SO_BINDTOIFINDEX                 = 0x3e
+	SO_BPF_EXTENSIONS                = 0x30
+	SO_BROADCAST                     = 0x6
+	SO_BSDCOMPAT                     = 0xe
+	SO_BUSY_POLL                     = 0x2e
+	SO_CNX_ADVICE                    = 0x35
+	SO_COOKIE                        = 0x39
+	SO_DETACH_REUSEPORT_BPF          = 0x44
+	SO_DOMAIN                        = 0x27
+	SO_DONTROUTE                     = 0x5
+	SO_ERROR                         = 0x4
+	SO_INCOMING_CPU                  = 0x31
+	SO_INCOMING_NAPI_ID              = 0x38
+	SO_KEEPALIVE                     = 0x9
+	SO_LINGER                        = 0xd
+	SO_LOCK_FILTER                   = 0x2c
+	SO_MARK                          = 0x24
+	SO_MAX_PACING_RATE               = 0x2f
+	SO_MEMINFO                       = 0x37
+	SO_NOFCS                         = 0x2b
+	SO_OOBINLINE                     = 0xa
+	SO_PASSCRED                      = 0x10
+	SO_PASSSEC                       = 0x22
+	SO_PEEK_OFF                      = 0x2a
+	SO_PEERCRED                      = 0x11
+	SO_PEERGROUPS                    = 0x3b
+	SO_PEERSEC                       = 0x1f
+	SO_PROTOCOL                      = 0x26
+	SO_RCVBUF                        = 0x8
+	SO_RCVBUFFORCE                   = 0x21
+	SO_RCVLOWAT                      = 0x12
+	SO_RCVTIMEO                      = 0x14
+	SO_RCVTIMEO_NEW                  = 0x42
+	SO_RCVTIMEO_OLD                  = 0x14
+	SO_REUSEADDR                     = 0x2
+	SO_REUSEPORT                     = 0xf
+	SO_RXQ_OVFL                      = 0x28
+	SO_SECURITY_AUTHENTICATION       = 0x16
+	SO_SECURITY_ENCRYPTION_NETWORK   = 0x18
+	SO_SECURITY_ENCRYPTION_TRANSPORT = 0x17
+	SO_SELECT_ERR_QUEUE              = 0x2d
+	SO_SNDBUF                        = 0x7
+	SO_SNDBUFFORCE                   = 0x20
+	SO_SNDLOWAT                      = 0x13
+	SO_SNDTIMEO                      = 0x15
+	SO_SNDTIMEO_NEW                  = 0x43
+	SO_SNDTIMEO_OLD                  = 0x15
+	SO_TIMESTAMPING                  = 0x25
+	SO_TIMESTAMPING_NEW              = 0x41
+	SO_TIMESTAMPING_OLD              = 0x25
+	SO_TIMESTAMPNS                   = 0x23
+	SO_TIMESTAMPNS_NEW               = 0x40
+	SO_TIMESTAMPNS_OLD               = 0x23
+	SO_TIMESTAMP_NEW                 = 0x3f
+	SO_TXTIME                        = 0x3d
+	SO_TYPE                          = 0x3
+	SO_WIFI_STATUS                   = 0x29
+	SO_ZEROCOPY                      = 0x3c
+	TAB1                             = 0x800
+	TAB2                             = 0x1000
+	TAB3                             = 0x1800
+	TABDLY                           = 0x1800
+	TCFLSH                           = 0x540b
+	TCGETA                           = 0x5405
+	TCGETS                           = 0x5401
+	TCGETS2                          = 0x802c542a
+	TCGETX                           = 0x5432
+	TCSAFLUSH                        = 0x2
+	TCSBRK                           = 0x5409
+	TCSBRKP                          = 0x5425
+	TCSETA                           = 0x5406
+	TCSETAF                          = 0x5408
+	TCSETAW                          = 0x5407
+	TCSETS                           = 0x5402
+	TCSETS2                          = 0x402c542b
+	TCSETSF                          = 0x5404
+	TCSETSF2                         = 0x402c542d
+	TCSETSW                          = 0x5403
+	TCSETSW2                         = 0x402c542c
+	TCSETX                           = 0x5433
+	TCSETXF                          = 0x5434
+	TCSETXW                          = 0x5435
+	TCXONC                           = 0x540a
+	TFD_CLOEXEC                      = 0x80000
+	TFD_NONBLOCK                     = 0x800
+	TIOCCBRK                         = 0x5428
+	TIOCCONS                         = 0x541d
+	TIOCEXCL                         = 0x540c
+	TIOCGDEV                         = 0x80045432
+	TIOCGETD                         = 0x5424
+	TIOCGEXCL                        = 0x80045440
+	TIOCGICOUNT                      = 0x545d
+	TIOCGISO7816                     = 0x80285442
+	TIOCGLCKTRMIOS                   = 0x5456
+	TIOCGPGRP                        = 0x540f
+	TIOCGPKT                         = 0x80045438
+	TIOCGPTLCK                       = 0x80045439
+	TIOCGPTN                         = 0x80045430
+	TIOCGPTPEER                      = 0x5441
+	TIOCGRS485                       = 0x542e
+	TIOCGSERIAL                      = 0x541e
+	TIOCGSID                         = 0x5429
+	TIOCGSOFTCAR                     = 0x5419
+	TIOCGWINSZ                       = 0x5413
+	TIOCINQ                          = 0x541b
+	TIOCLINUX                        = 0x541c
+	TIOCMBIC                         = 0x5417
+	TIOCMBIS                         = 0x5416
+	TIOCMGET                         = 0x5415
+	TIOCMIWAIT                       = 0x545c
+	TIOCMSET                         = 0x5418
+	TIOCM_CAR                        = 0x40
+	TIOCM_CD                         = 0x40
+	TIOCM_CTS                        = 0x20
+	TIOCM_DSR                        = 0x100
+	TIOCM_RI                         = 0x80
+	TIOCM_RNG                        = 0x80
+	TIOCM_SR                         = 0x10
+	TIOCM_ST                         = 0x8
+	TIOCNOTTY                        = 0x5422
+	TIOCNXCL                         = 0x540d
+	TIOCOUTQ                         = 0x5411
+	TIOCPKT                          = 0x5420
+	TIOCSBRK                         = 0x5427
+	TIOCSCTTY                        = 0x540e
+	TIOCSERCONFIG                    = 0x5453
+	TIOCSERGETLSR                    = 0x5459
+	TIOCSERGETMULTI                  = 0x545a
+	TIOCSERGSTRUCT                   = 0x5458
+	TIOCSERGWILD                     = 0x5454
+	TIOCSERSETMULTI                  = 0x545b
+	TIOCSERSWILD                     = 0x5455
+	TIOCSER_TEMT                     = 0x1
+	TIOCSETD                         = 0x5423
+	TIOCSIG                          = 0x40045436
+	TIOCSISO7816                     = 0xc0285443
+	TIOCSLCKTRMIOS                   = 0x5457
+	TIOCSPGRP                        = 0x5410
+	TIOCSPTLCK                       = 0x40045431
+	TIOCSRS485                       = 0x542f
+	TIOCSSERIAL                      = 0x541f
+	TIOCSSOFTCAR                     = 0x541a
+	TIOCSTI                          = 0x5412
+	TIOCSWINSZ                       = 0x5414
+	TIOCVHANGUP                      = 0x5437
+	TOSTOP                           = 0x100
+	TUNATTACHFILTER                  = 0x400854d5
+	TUNDETACHFILTER                  = 0x400854d6
+	TUNGETDEVNETNS                   = 0x54e3
+	TUNGETFEATURES                   = 0x800454cf
+	TUNGETFILTER                     = 0x800854db
+	TUNGETIFF                        = 0x800454d2
+	TUNGETSNDBUF                     = 0x800454d3
+	TUNGETVNETBE                     = 0x800454df
+	TUNGETVNETHDRSZ                  = 0x800454d7
+	TUNGETVNETLE                     = 0x800454dd
+	TUNSETCARRIER                    = 0x400454e2
+	TUNSETDEBUG                      = 0x400454c9
+	TUNSETFILTEREBPF                 = 0x800454e1
+	TUNSETGROUP                      = 0x400454ce
+	TUNSETIFF                        = 0x400454ca
+	TUNSETIFINDEX                    = 0x400454da
+	TUNSETLINK                       = 0x400454cd
+	TUNSETNOCSUM                     = 0x400454c8
+	TUNSETOFFLOAD                    = 0x400454d0
+	TUNSETOWNER                      = 0x400454cc
+	TUNSETPERSIST                    = 0x400454cb
+	TUNSETQUEUE                      = 0x400454d9
+	TUNSETSNDBUF                     = 0x400454d4
+	TUNSETSTEERINGEBPF               = 0x800454e0
+	TUNSETTXFILTER                   = 0x400454d1
+	TUNSETVNETBE                     = 0x400454de
+	TUNSETVNETHDRSZ                  = 0x400454d8
+	TUNSETVNETLE                     = 0x400454dc
+	UBI_IOCATT                       = 0x40186f40
+	UBI_IOCDET                       = 0x40046f41
+	UBI_IOCEBCH                      = 0x40044f02
+	UBI_IOCEBER                      = 0x40044f01
+	UBI_IOCEBISMAP                   = 0x80044f05
+	UBI_IOCEBMAP                     = 0x40084f03
+	UBI_IOCEBUNMAP                   = 0x40044f04
+	UBI_IOCMKVOL                     = 0x40986f00
+	UBI_IOCRMVOL                     = 0x40046f01
+	UBI_IOCRNVOL                     = 0x51106f03
+	UBI_IOCRPEB                      = 0x40046f04
+	UBI_IOCRSVOL                     = 0x400c6f02
+	UBI_IOCSETVOLPROP                = 0x40104f06
+	UBI_IOCSPEB                      = 0x40046f05
+	UBI_IOCVOLCRBLK                  = 0x40804f07
+	UBI_IOCVOLRMBLK                  = 0x4f08
+	UBI_IOCVOLUP                     = 0x40084f00
+	VDISCARD                         = 0xd
+	VEOF                             = 0x4
+	VEOL                             = 0xb
+	VEOL2                            = 0x10
+	VMIN                             = 0x6
+	VREPRINT                         = 0xc
+	VSTART                           = 0x8
+	VSTOP                            = 0x9
+	VSUSP                            = 0xa
+	VSWTC                            = 0x7
+	VT1                              = 0x4000
+	VTDLY                            = 0x4000
+	VTIME                            = 0x5
+	VWERASE                          = 0xe
+	WDIOC_GETBOOTSTATUS              = 0x80045702
+	WDIOC_GETPRETIMEOUT              = 0x80045709
+	WDIOC_GETSTATUS                  = 0x80045701
+	WDIOC_GETSUPPORT                 = 0x80285700
+	WDIOC_GETTEMP                    = 0x80045703
+	WDIOC_GETTIMELEFT                = 0x8004570a
+	WDIOC_GETTIMEOUT                 = 0x80045707
+	WDIOC_KEEPALIVE                  = 0x80045705
+	WDIOC_SETOPTIONS                 = 0x80045704
+	WORDSIZE                         = 0x20
+	XCASE                            = 0x4
+	XTABS                            = 0x1800
+)
+
+// Errors
+const (
+	EADDRINUSE      = syscall.Errno(0x62)
+	EADDRNOTAVAIL   = syscall.Errno(0x63)
+	EADV            = syscall.Errno(0x44)
+	EAFNOSUPPORT    = syscall.Errno(0x61)
+	EALREADY        = syscall.Errno(0x72)
+	EBADE           = syscall.Errno(0x34)
+	EBADFD          = syscall.Errno(0x4d)
+	EBADMSG         = syscall.Errno(0x4a)
+	EBADR           = syscall.Errno(0x35)
+	EBADRQC         = syscall.Errno(0x38)
+	EBADSLT         = syscall.Errno(0x39)
+	EBFONT          = syscall.Errno(0x3b)
+	ECANCELED       = syscall.Errno(0x7d)
+	ECHRNG          = syscall.Errno(0x2c)
+	ECOMM           = syscall.Errno(0x46)
+	ECONNABORTED    = syscall.Errno(0x67)
+	ECONNREFUSED    = syscall.Errno(0x6f)
+	ECONNRESET      = syscall.Errno(0x68)
+	EDEADLK         = syscall.Errno(0x23)
+	EDEADLOCK       = syscall.Errno(0x23)
+	EDESTADDRREQ    = syscall.Errno(0x59)
+	EDOTDOT         = syscall.Errno(0x49)
+	EDQUOT          = syscall.Errno(0x7a)
+	EHOSTDOWN       = syscall.Errno(0x70)
+	EHOSTUNREACH    = syscall.Errno(0x71)
+	EHWPOISON       = syscall.Errno(0x85)
+	EIDRM           = syscall.Errno(0x2b)
+	EILSEQ          = syscall.Errno(0x54)
+	EINPROGRESS     = syscall.Errno(0x73)
+	EISCONN         = syscall.Errno(0x6a)
+	EISNAM          = syscall.Errno(0x78)
+	EKEYEXPIRED     = syscall.Errno(0x7f)
+	EKEYREJECTED    = syscall.Errno(0x81)
+	EKEYREVOKED     = syscall.Errno(0x80)
+	EL2HLT          = syscall.Errno(0x33)
+	EL2NSYNC        = syscall.Errno(0x2d)
+	EL3HLT          = syscall.Errno(0x2e)
+	EL3RST          = syscall.Errno(0x2f)
+	ELIBACC         = syscall.Errno(0x4f)
+	ELIBBAD         = syscall.Errno(0x50)
+	ELIBEXEC        = syscall.Errno(0x53)
+	ELIBMAX         = syscall.Errno(0x52)
+	ELIBSCN         = syscall.Errno(0x51)
+	ELNRNG          = syscall.Errno(0x30)
+	ELOOP           = syscall.Errno(0x28)
+	EMEDIUMTYPE     = syscall.Errno(0x7c)
+	EMSGSIZE        = syscall.Errno(0x5a)
+	EMULTIHOP       = syscall.Errno(0x48)
+	ENAMETOOLONG    = syscall.Errno(0x24)
+	ENAVAIL         = syscall.Errno(0x77)
+	ENETDOWN        = syscall.Errno(0x64)
+	ENETRESET       = syscall.Errno(0x66)
+	ENETUNREACH     = syscall.Errno(0x65)
+	ENOANO          = syscall.Errno(0x37)
+	ENOBUFS         = syscall.Errno(0x69)
+	ENOCSI          = syscall.Errno(0x32)
+	ENODATA         = syscall.Errno(0x3d)
+	ENOKEY          = syscall.Errno(0x7e)
+	ENOLCK          = syscall.Errno(0x25)
+	ENOLINK         = syscall.Errno(0x43)
+	ENOMEDIUM       = syscall.Errno(0x7b)
+	ENOMSG          = syscall.Errno(0x2a)
+	ENONET          = syscall.Errno(0x40)
+	ENOPKG          = syscall.Errno(0x41)
+	ENOPROTOOPT     = syscall.Errno(0x5c)
+	ENOSR           = syscall.Errno(0x3f)
+	ENOSTR          = syscall.Errno(0x3c)
+	ENOSYS          = syscall.Errno(0x26)
+	ENOTCONN        = syscall.Errno(0x6b)
+	ENOTEMPTY       = syscall.Errno(0x27)
+	ENOTNAM         = syscall.Errno(0x76)
+	ENOTRECOVERABLE = syscall.Errno(0x83)
+	ENOTSOCK        = syscall.Errno(0x58)
+	ENOTSUP         = syscall.Errno(0x5f)
+	ENOTUNIQ        = syscall.Errno(0x4c)
+	EOPNOTSUPP      = syscall.Errno(0x5f)
+	EOVERFLOW       = syscall.Errno(0x4b)
+	EOWNERDEAD      = syscall.Errno(0x82)
+	EPFNOSUPPORT    = syscall.Errno(0x60)
+	EPROTO          = syscall.Errno(0x47)
+	EPROTONOSUPPORT = syscall.Errno(0x5d)
+	EPROTOTYPE      = syscall.Errno(0x5b)
+	EREMCHG         = syscall.Errno(0x4e)
+	EREMOTE         = syscall.Errno(0x42)
+	EREMOTEIO       = syscall.Errno(0x79)
+	ERESTART        = syscall.Errno(0x55)
+	ERFKILL         = syscall.Errno(0x84)
+	ESHUTDOWN       = syscall.Errno(0x6c)
+	ESOCKTNOSUPPORT = syscall.Errno(0x5e)
+	ESRMNT          = syscall.Errno(0x45)
+	ESTALE          = syscall.Errno(0x74)
+	ESTRPIPE        = syscall.Errno(0x56)
+	ETIME           = syscall.Errno(0x3e)
+	ETIMEDOUT       = syscall.Errno(0x6e)
+	ETOOMANYREFS    = syscall.Errno(0x6d)
+	EUCLEAN         = syscall.Errno(0x75)
+	EUNATCH         = syscall.Errno(0x31)
+	EUSERS          = syscall.Errno(0x57)
+	EXFULL          = syscall.Errno(0x36)
+)
+
+// Signals
+const (
+	SIGBUS    = syscall.Signal(0x7)
+	SIGCHLD   = syscall.Signal(0x11)
+	SIGCLD    = syscall.Signal(0x11)
+	SIGCONT   = syscall.Signal(0x12)
+	SIGIO     = syscall.Signal(0x1d)
+	SIGPOLL   = syscall.Signal(0x1d)
+	SIGPROF   = syscall.Signal(0x1b)
+	SIGPWR    = syscall.Signal(0x1e)
+	SIGSTKFLT = syscall.Signal(0x10)
+	SIGSTOP   = syscall.Signal(0x13)
+	SIGSYS    = syscall.Signal(0x1f)
+	SIGTSTP   = syscall.Signal(0x14)
+	SIGTTIN   = syscall.Signal(0x15)
+	SIGTTOU   = syscall.Signal(0x16)
+	SIGURG    = syscall.Signal(0x17)
+	SIGUSR1   = syscall.Signal(0xa)
+	SIGUSR2   = syscall.Signal(0xc)
+	SIGVTALRM = syscall.Signal(0x1a)
+	SIGWINCH  = syscall.Signal(0x1c)
+	SIGXCPU   = syscall.Signal(0x18)
+	SIGXFSZ   = syscall.Signal(0x19)
+)
+
+// Error table
+var errorList = [...]struct {
+	num  syscall.Errno
+	name string
+	desc string
+}{
+	{1, "EPERM", "operation not permitted"},
+	{2, "ENOENT", "no such file or directory"},
+	{3, "ESRCH", "no such process"},
+	{4, "EINTR", "interrupted system call"},
+	{5, "EIO", "input/output error"},
+	{6, "ENXIO", "no such device or address"},
+	{7, "E2BIG", "argument list too long"},
+	{8, "ENOEXEC", "exec format error"},
+	{9, "EBADF", "bad file descriptor"},
+	{10, "ECHILD", "no child processes"},
+	{11, "EAGAIN", "resource temporarily unavailable"},
+	{12, "ENOMEM", "cannot allocate memory"},
+	{13, "EACCES", "permission denied"},
+	{14, "EFAULT", "bad address"},
+	{15, "ENOTBLK", "block device required"},
+	{16, "EBUSY", "device or resource busy"},
+	{17, "EEXIST", "file exists"},
+	{18, "EXDEV", "invalid cross-device link"},
+	{19, "ENODEV", "no such device"},
+	{20, "ENOTDIR", "not a directory"},
+	{21, "EISDIR", "is a directory"},
+	{22, "EINVAL", "invalid argument"},
+	{23, "ENFILE", "too many open files in system"},
+	{24, "EMFILE", "too many open files"},
+	{25, "ENOTTY", "inappropriate ioctl for device"},
+	{26, "ETXTBSY", "text file busy"},
+	{27, "EFBIG", "file too large"},
+	{28, "ENOSPC", "no space left on device"},
+	{29, "ESPIPE", "illegal seek"},
+	{30, "EROFS", "read-only file system"},
+	{31, "EMLINK", "too many links"},
+	{32, "EPIPE", "broken pipe"},
+	{33, "EDOM", "numerical argument out of domain"},
+	{34, "ERANGE", "numerical result out of range"},
+	{35, "EDEADLK", "resource deadlock avoided"},
+	{36, "ENAMETOOLONG", "file name too long"},
+	{37, "ENOLCK", "no locks available"},
+	{38, "ENOSYS", "function not implemented"},
+	{39, "ENOTEMPTY", "directory not empty"},
+	{40, "ELOOP", "too many levels of symbolic links"},
+	{42, "ENOMSG", "no message of desired type"},
+	{43, "EIDRM", "identifier removed"},
+	{44, "ECHRNG", "channel number out of range"},
+	{45, "EL2NSYNC", "level 2 not synchronized"},
+	{46, "EL3HLT", "level 3 halted"},
+	{47, "EL3RST", "level 3 reset"},
+	{48, "ELNRNG", "link number out of range"},
+	{49, "EUNATCH", "protocol driver not attached"},
+	{50, "ENOCSI", "no CSI structure available"},
+	{51, "EL2HLT", "level 2 halted"},
+	{52, "EBADE", "invalid exchange"},
+	{53, "EBADR", "invalid request descriptor"},
+	{54, "EXFULL", "exchange full"},
+	{55, "ENOANO", "no anode"},
+	{56, "EBADRQC", "invalid request code"},
+	{57, "EBADSLT", "invalid slot"},
+	{59, "EBFONT", "bad font file format"},
+	{60, "ENOSTR", "device not a stream"},
+	{61, "ENODATA", "no data available"},
+	{62, "ETIME", "timer expired"},
+	{63, "ENOSR", "out of streams resources"},
+	{64, "ENONET", "machine is not on the network"},
+	{65, "ENOPKG", "package not installed"},
+	{66, "EREMOTE", "object is remote"},
+	{67, "ENOLINK", "link has been severed"},
+	{68, "EADV", "advertise error"},
+	{69, "ESRMNT", "srmount error"},
+	{70, "ECOMM", "communication error on send"},
+	{71, "EPROTO", "protocol error"},
+	{72, "EMULTIHOP", "multihop attempted"},
+	{73, "EDOTDOT", "RFS specific error"},
+	{74, "EBADMSG", "bad message"},
+	{75, "EOVERFLOW", "value too large for defined data type"},
+	{76, "ENOTUNIQ", "name not unique on network"},
+	{77, "EBADFD", "file descriptor in bad state"},
+	{78, "EREMCHG", "remote address changed"},
+	{79, "ELIBACC", "can not access a needed shared library"},
+	{80, "ELIBBAD", "accessing a corrupted shared library"},
+	{81, "ELIBSCN", ".lib section in a.out corrupted"},
+	{82, "ELIBMAX", "attempting to link in too many shared libraries"},
+	{83, "ELIBEXEC", "cannot exec a shared library directly"},
+	{84, "EILSEQ", "invalid or incomplete multibyte or wide character"},
+	{85, "ERESTART", "interrupted system call should be restarted"},
+	{86, "ESTRPIPE", "streams pipe error"},
+	{87, "EUSERS", "too many users"},
+	{88, "ENOTSOCK", "socket operation on non-socket"},
+	{89, "EDESTADDRREQ", "destination address required"},
+	{90, "EMSGSIZE", "message too long"},
+	{91, "EPROTOTYPE", "protocol wrong type for socket"},
+	{92, "ENOPROTOOPT", "protocol not available"},
+	{93, "EPROTONOSUPPORT", "protocol not supported"},
+	{94, "ESOCKTNOSUPPORT", "socket type not supported"},
+	{95, "ENOTSUP", "operation not supported"},
+	{96, "EPFNOSUPPORT", "protocol family not supported"},
+	{97, "EAFNOSUPPORT", "address family not supported by protocol"},
+	{98, "EADDRINUSE", "address already in use"},
+	{99, "EADDRNOTAVAIL", "cannot assign requested address"},
+	{100, "ENETDOWN", "network is down"},
+	{101, "ENETUNREACH", "network is unreachable"},
+	{102, "ENETRESET", "network dropped connection on reset"},
+	{103, "ECONNABORTED", "software caused connection abort"},
+	{104, "ECONNRESET", "connection reset by peer"},
+	{105, "ENOBUFS", "no buffer space available"},
+	{106, "EISCONN", "transport endpoint is already connected"},
+	{107, "ENOTCONN", "transport endpoint is not connected"},
+	{108, "ESHUTDOWN", "cannot send after transport endpoint shutdown"},
+	{109, "ETOOMANYREFS", "too many references: cannot splice"},
+	{110, "ETIMEDOUT", "connection timed out"},
+	{111, "ECONNREFUSED", "connection refused"},
+	{112, "EHOSTDOWN", "host is down"},
+	{113, "EHOSTUNREACH", "no route to host"},
+	{114, "EALREADY", "operation already in progress"},
+	{115, "EINPROGRESS", "operation now in progress"},
+	{116, "ESTALE", "stale file handle"},
+	{117, "EUCLEAN", "structure needs cleaning"},
+	{118, "ENOTNAM", "not a XENIX named type file"},
+	{119, "ENAVAIL", "no XENIX semaphores available"},
+	{120, "EISNAM", "is a named type file"},
+	{121, "EREMOTEIO", "remote I/O error"},
+	{122, "EDQUOT", "disk quota exceeded"},
+	{123, "ENOMEDIUM", "no medium found"},
+	{124, "EMEDIUMTYPE", "wrong medium type"},
+	{125, "ECANCELED", "operation canceled"},
+	{126, "ENOKEY", "required key not available"},
+	{127, "EKEYEXPIRED", "key has expired"},
+	{128, "EKEYREVOKED", "key has been revoked"},
+	{129, "EKEYREJECTED", "key was rejected by service"},
+	{130, "EOWNERDEAD", "owner died"},
+	{131, "ENOTRECOVERABLE", "state not recoverable"},
+	{132, "ERFKILL", "operation not possible due to RF-kill"},
+	{133, "EHWPOISON", "memory page has hardware error"},
+}
+
+// Signal table
+var signalList = [...]struct {
+	num  syscall.Signal
+	name string
+	desc string
+}{
+	{1, "SIGHUP", "hangup"},
+	{2, "SIGINT", "interrupt"},
+	{3, "SIGQUIT", "quit"},
+	{4, "SIGILL", "illegal instruction"},
+	{5, "SIGTRAP", "trace/breakpoint trap"},
+	{6, "SIGABRT", "aborted"},
+	{7, "SIGBUS", "bus error"},
+	{8, "SIGFPE", "floating point exception"},
+	{9, "SIGKILL", "killed"},
+	{10, "SIGUSR1", "user defined signal 1"},
+	{11, "SIGSEGV", "segmentation fault"},
+	{12, "SIGUSR2", "user defined signal 2"},
+	{13, "SIGPIPE", "broken pipe"},
+	{14, "SIGALRM", "alarm clock"},
+	{15, "SIGTERM", "terminated"},
+	{16, "SIGSTKFLT", "stack fault"},
+	{17, "SIGCHLD", "child exited"},
+	{18, "SIGCONT", "continued"},
+	{19, "SIGSTOP", "stopped (signal)"},
+	{20, "SIGTSTP", "stopped"},
+	{21, "SIGTTIN", "stopped (tty input)"},
+	{22, "SIGTTOU", "stopped (tty output)"},
+	{23, "SIGURG", "urgent I/O condition"},
+	{24, "SIGXCPU", "CPU time limit exceeded"},
+	{25, "SIGXFSZ", "file size limit exceeded"},
+	{26, "SIGVTALRM", "virtual timer expired"},
+	{27, "SIGPROF", "profiling timer expired"},
+	{28, "SIGWINCH", "window changed"},
+	{29, "SIGIO", "I/O possible"},
+	{30, "SIGPWR", "power failure"},
+	{31, "SIGSYS", "bad system call"},
+}
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/zsyscall_linux_thumb.go b/src/cmd/vendor/golang.org/x/sys/unix/zsyscall_linux_thumb.go
new file mode 100644
index 0000000000..88c0562306
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/zsyscall_linux_thumb.go
@@ -0,0 +1,715 @@
+// go run mksyscall.go -l32 -arm -tags linux,arm syscall_linux.go syscall_linux_arm.go
+// Code generated by the command above; see README.md. DO NOT EDIT.
+
+// +build linux,thumb
+
+package unix
+
+import (
+	"syscall"
+	"unsafe"
+)
+
+var _ syscall.Errno
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fanotifyMark(fd int, flags uint, mask uint64, dirFd int, pathname *byte) (err error) {
+	_, _, e1 := Syscall6(SYS_FANOTIFY_MARK, uintptr(fd), uintptr(flags), uintptr(mask), uintptr(mask>>32), uintptr(dirFd), uintptr(unsafe.Pointer(pathname)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fallocate(fd int, mode uint32, off int64, len int64) (err error) {
+	_, _, e1 := Syscall6(SYS_FALLOCATE, uintptr(fd), uintptr(mode), uintptr(off), uintptr(off>>32), uintptr(len), uintptr(len>>32))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Tee(rfd int, wfd int, len int, flags int) (n int64, err error) {
+	r0, r1, e1 := Syscall6(SYS_TEE, uintptr(rfd), uintptr(wfd), uintptr(len), uintptr(flags), 0, 0)
+	n = int64(int64(r1)<<32 | int64(r0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func pipe(p *[2]_C_int) (err error) {
+	_, _, e1 := RawSyscall(SYS_PIPE, uintptr(unsafe.Pointer(p)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func accept(s int, rsa *RawSockaddrAny, addrlen *_Socklen) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_ACCEPT, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func accept4(s int, rsa *RawSockaddrAny, addrlen *_Socklen, flags int) (fd int, err error) {
+	r0, _, e1 := Syscall6(SYS_ACCEPT4, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), uintptr(flags), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func bind(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := Syscall(SYS_BIND, uintptr(s), uintptr(addr), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func connect(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := Syscall(SYS_CONNECT, uintptr(s), uintptr(addr), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getgroups(n int, list *_Gid_t) (nn int, err error) {
+	r0, _, e1 := RawSyscall(SYS_GETGROUPS32, uintptr(n), uintptr(unsafe.Pointer(list)), 0)
+	nn = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setgroups(n int, list *_Gid_t) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETGROUPS32, uintptr(n), uintptr(unsafe.Pointer(list)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getsockopt(s int, level int, name int, val unsafe.Pointer, vallen *_Socklen) (err error) {
+	_, _, e1 := Syscall6(SYS_GETSOCKOPT, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(unsafe.Pointer(vallen)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setsockopt(s int, level int, name int, val unsafe.Pointer, vallen uintptr) (err error) {
+	_, _, e1 := Syscall6(SYS_SETSOCKOPT, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(vallen), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func socket(domain int, typ int, proto int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_SOCKET, uintptr(domain), uintptr(typ), uintptr(proto))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getpeername(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETPEERNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getsockname(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETSOCKNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func recvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_RECVFROM, uintptr(fd), uintptr(_p0), uintptr(len(p)), uintptr(flags), uintptr(unsafe.Pointer(from)), uintptr(unsafe.Pointer(fromlen)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_SENDTO, uintptr(s), uintptr(_p0), uintptr(len(buf)), uintptr(flags), uintptr(to), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func socketpair(domain int, typ int, flags int, fd *[2]int32) (err error) {
+	_, _, e1 := RawSyscall6(SYS_SOCKETPAIR, uintptr(domain), uintptr(typ), uintptr(flags), uintptr(unsafe.Pointer(fd)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func recvmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_RECVMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_SENDMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func dup2(oldfd int, newfd int) (err error) {
+	_, _, e1 := Syscall(SYS_DUP2, uintptr(oldfd), uintptr(newfd), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCreate(size int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_EPOLL_CREATE, uintptr(size), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(events) > 0 {
+		_p0 = unsafe.Pointer(&events[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_EPOLL_WAIT, uintptr(epfd), uintptr(_p0), uintptr(len(events)), uintptr(msec), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchown(fd int, uid int, gid int) (err error) {
+	_, _, e1 := Syscall(SYS_FCHOWN32, uintptr(fd), uintptr(uid), uintptr(gid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fstat(fd int, stat *Stat_t) (err error) {
+	_, _, e1 := Syscall(SYS_FSTAT64, uintptr(fd), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fstatat(dirfd int, path string, stat *Stat_t, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_FSTATAT64, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getegid() (egid int) {
+	r0, _ := RawSyscallNoError(SYS_GETEGID32, 0, 0, 0)
+	egid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Geteuid() (euid int) {
+	r0, _ := RawSyscallNoError(SYS_GETEUID32, 0, 0, 0)
+	euid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getgid() (gid int) {
+	r0, _ := RawSyscallNoError(SYS_GETGID32, 0, 0, 0)
+	gid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getuid() (uid int) {
+	r0, _ := RawSyscallNoError(SYS_GETUID32, 0, 0, 0)
+	uid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyInit() (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_INIT, 0, 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Lchown(path string, uid int, gid int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_LCHOWN32, uintptr(unsafe.Pointer(_p0)), uintptr(uid), uintptr(gid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Listen(s int, n int) (err error) {
+	_, _, e1 := Syscall(SYS_LISTEN, uintptr(s), uintptr(n), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Lstat(path string, stat *Stat_t) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_LSTAT64, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pause() (err error) {
+	_, _, e1 := Syscall(SYS_PAUSE, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Renameat(olddirfd int, oldpath string, newdirfd int, newpath string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_RENAMEAT, uintptr(olddirfd), uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendfile(outfd int, infd int, offset *int64, count int) (written int, err error) {
+	r0, _, e1 := Syscall6(SYS_SENDFILE64, uintptr(outfd), uintptr(infd), uintptr(unsafe.Pointer(offset)), uintptr(count), 0, 0)
+	written = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Select(nfd int, r *FdSet, w *FdSet, e *FdSet, timeout *Timeval) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS__NEWSELECT, uintptr(nfd), uintptr(unsafe.Pointer(r)), uintptr(unsafe.Pointer(w)), uintptr(unsafe.Pointer(e)), uintptr(unsafe.Pointer(timeout)), 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setfsgid(gid int) (prev int, err error) {
+	r0, _, e1 := Syscall(SYS_SETFSGID32, uintptr(gid), 0, 0)
+	prev = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setfsuid(uid int) (prev int, err error) {
+	r0, _, e1 := Syscall(SYS_SETFSUID32, uintptr(uid), 0, 0)
+	prev = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setregid(rgid int, egid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETREGID32, uintptr(rgid), uintptr(egid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setresgid(rgid int, egid int, sgid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRESGID32, uintptr(rgid), uintptr(egid), uintptr(sgid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setresuid(ruid int, euid int, suid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRESUID32, uintptr(ruid), uintptr(euid), uintptr(suid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setreuid(ruid int, euid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETREUID32, uintptr(ruid), uintptr(euid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Shutdown(fd int, how int) (err error) {
+	_, _, e1 := Syscall(SYS_SHUTDOWN, uintptr(fd), uintptr(how), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Splice(rfd int, roff *int64, wfd int, woff *int64, len int, flags int) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS_SPLICE, uintptr(rfd), uintptr(unsafe.Pointer(roff)), uintptr(wfd), uintptr(unsafe.Pointer(woff)), uintptr(len), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Stat(path string, stat *Stat_t) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_STAT64, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Ustat(dev int, ubuf *Ustat_t) (err error) {
+	_, _, e1 := Syscall(SYS_USTAT, uintptr(dev), uintptr(unsafe.Pointer(ubuf)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func futimesat(dirfd int, path string, times *[2]Timeval) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FUTIMESAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Gettimeofday(tv *Timeval) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETTIMEOFDAY, uintptr(unsafe.Pointer(tv)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func utimes(path string, times *[2]Timeval) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UTIMES, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pread(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_PREAD64, uintptr(fd), uintptr(_p0), uintptr(len(p)), 0, uintptr(offset), uintptr(offset>>32))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pwrite(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_PWRITE64, uintptr(fd), uintptr(_p0), uintptr(len(p)), 0, uintptr(offset), uintptr(offset>>32))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Truncate(path string, length int64) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_TRUNCATE64, uintptr(unsafe.Pointer(_p0)), 0, uintptr(length), uintptr(length>>32), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Ftruncate(fd int, length int64) (err error) {
+	_, _, e1 := Syscall6(SYS_FTRUNCATE64, uintptr(fd), 0, uintptr(length), uintptr(length>>32), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func mmap2(addr uintptr, length uintptr, prot int, flags int, fd int, pageOffset uintptr) (xaddr uintptr, err error) {
+	r0, _, e1 := Syscall6(SYS_MMAP2, uintptr(addr), uintptr(length), uintptr(prot), uintptr(flags), uintptr(fd), uintptr(pageOffset))
+	xaddr = uintptr(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getrlimit(resource int, rlim *rlimit32) (err error) {
+	_, _, e1 := RawSyscall(SYS_UGETRLIMIT, uintptr(resource), uintptr(unsafe.Pointer(rlim)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setrlimit(resource int, rlim *rlimit32) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRLIMIT, uintptr(resource), uintptr(unsafe.Pointer(rlim)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func poll(fds *PollFd, nfds int, timeout int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_POLL, uintptr(unsafe.Pointer(fds)), uintptr(nfds), uintptr(timeout))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func armSyncFileRange(fd int, flags int, off int64, n int64) (err error) {
+	_, _, e1 := Syscall6(SYS_ARM_SYNC_FILE_RANGE, uintptr(fd), uintptr(flags), uintptr(off), uintptr(off>>32), uintptr(n), uintptr(n>>32))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func kexecFileLoad(kernelFd int, initrdFd int, cmdlineLen int, cmdline string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(cmdline)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_KEXEC_FILE_LOAD, uintptr(kernelFd), uintptr(initrdFd), uintptr(cmdlineLen), uintptr(unsafe.Pointer(_p0)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/zsysnum_linux_thumb.go b/src/cmd/vendor/golang.org/x/sys/unix/zsysnum_linux_thumb.go
new file mode 100644
index 0000000000..b2832cc6fb
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/zsysnum_linux_thumb.go
@@ -0,0 +1,402 @@
+// go run linux/mksysnum.go -Wall -Werror -static -I/tmp/include /tmp/include/asm/unistd.h
+// Code generated by the command above; see README.md. DO NOT EDIT.
+
+// +build thumb,linux
+
+package unix
+
+const (
+	SYS_RESTART_SYSCALL              = 0
+	SYS_EXIT                         = 1
+	SYS_FORK                         = 2
+	SYS_READ                         = 3
+	SYS_WRITE                        = 4
+	SYS_OPEN                         = 5
+	SYS_CLOSE                        = 6
+	SYS_CREAT                        = 8
+	SYS_LINK                         = 9
+	SYS_UNLINK                       = 10
+	SYS_EXECVE                       = 11
+	SYS_CHDIR                        = 12
+	SYS_MKNOD                        = 14
+	SYS_CHMOD                        = 15
+	SYS_LCHOWN                       = 16
+	SYS_LSEEK                        = 19
+	SYS_GETPID                       = 20
+	SYS_MOUNT                        = 21
+	SYS_SETUID                       = 23
+	SYS_GETUID                       = 24
+	SYS_PTRACE                       = 26
+	SYS_PAUSE                        = 29
+	SYS_ACCESS                       = 33
+	SYS_NICE                         = 34
+	SYS_SYNC                         = 36
+	SYS_KILL                         = 37
+	SYS_RENAME                       = 38
+	SYS_MKDIR                        = 39
+	SYS_RMDIR                        = 40
+	SYS_DUP                          = 41
+	SYS_PIPE                         = 42
+	SYS_TIMES                        = 43
+	SYS_BRK                          = 45
+	SYS_SETGID                       = 46
+	SYS_GETGID                       = 47
+	SYS_GETEUID                      = 49
+	SYS_GETEGID                      = 50
+	SYS_ACCT                         = 51
+	SYS_UMOUNT2                      = 52
+	SYS_IOCTL                        = 54
+	SYS_FCNTL                        = 55
+	SYS_SETPGID                      = 57
+	SYS_UMASK                        = 60
+	SYS_CHROOT                       = 61
+	SYS_USTAT                        = 62
+	SYS_DUP2                         = 63
+	SYS_GETPPID                      = 64
+	SYS_GETPGRP                      = 65
+	SYS_SETSID                       = 66
+	SYS_SIGACTION                    = 67
+	SYS_SETREUID                     = 70
+	SYS_SETREGID                     = 71
+	SYS_SIGSUSPEND                   = 72
+	SYS_SIGPENDING                   = 73
+	SYS_SETHOSTNAME                  = 74
+	SYS_SETRLIMIT                    = 75
+	SYS_GETRUSAGE                    = 77
+	SYS_GETTIMEOFDAY                 = 78
+	SYS_SETTIMEOFDAY                 = 79
+	SYS_GETGROUPS                    = 80
+	SYS_SETGROUPS                    = 81
+	SYS_SYMLINK                      = 83
+	SYS_READLINK                     = 85
+	SYS_USELIB                       = 86
+	SYS_SWAPON                       = 87
+	SYS_REBOOT                       = 88
+	SYS_MUNMAP                       = 91
+	SYS_TRUNCATE                     = 92
+	SYS_FTRUNCATE                    = 93
+	SYS_FCHMOD                       = 94
+	SYS_FCHOWN                       = 95
+	SYS_GETPRIORITY                  = 96
+	SYS_SETPRIORITY                  = 97
+	SYS_STATFS                       = 99
+	SYS_FSTATFS                      = 100
+	SYS_SYSLOG                       = 103
+	SYS_SETITIMER                    = 104
+	SYS_GETITIMER                    = 105
+	SYS_STAT                         = 106
+	SYS_LSTAT                        = 107
+	SYS_FSTAT                        = 108
+	SYS_VHANGUP                      = 111
+	SYS_WAIT4                        = 114
+	SYS_SWAPOFF                      = 115
+	SYS_SYSINFO                      = 116
+	SYS_FSYNC                        = 118
+	SYS_SIGRETURN                    = 119
+	SYS_CLONE                        = 120
+	SYS_SETDOMAINNAME                = 121
+	SYS_UNAME                        = 122
+	SYS_ADJTIMEX                     = 124
+	SYS_MPROTECT                     = 125
+	SYS_SIGPROCMASK                  = 126
+	SYS_INIT_MODULE                  = 128
+	SYS_DELETE_MODULE                = 129
+	SYS_QUOTACTL                     = 131
+	SYS_GETPGID                      = 132
+	SYS_FCHDIR                       = 133
+	SYS_BDFLUSH                      = 134
+	SYS_SYSFS                        = 135
+	SYS_PERSONALITY                  = 136
+	SYS_SETFSUID                     = 138
+	SYS_SETFSGID                     = 139
+	SYS__LLSEEK                      = 140
+	SYS_GETDENTS                     = 141
+	SYS__NEWSELECT                   = 142
+	SYS_FLOCK                        = 143
+	SYS_MSYNC                        = 144
+	SYS_READV                        = 145
+	SYS_WRITEV                       = 146
+	SYS_GETSID                       = 147
+	SYS_FDATASYNC                    = 148
+	SYS__SYSCTL                      = 149
+	SYS_MLOCK                        = 150
+	SYS_MUNLOCK                      = 151
+	SYS_MLOCKALL                     = 152
+	SYS_MUNLOCKALL                   = 153
+	SYS_SCHED_SETPARAM               = 154
+	SYS_SCHED_GETPARAM               = 155
+	SYS_SCHED_SETSCHEDULER           = 156
+	SYS_SCHED_GETSCHEDULER           = 157
+	SYS_SCHED_YIELD                  = 158
+	SYS_SCHED_GET_PRIORITY_MAX       = 159
+	SYS_SCHED_GET_PRIORITY_MIN       = 160
+	SYS_SCHED_RR_GET_INTERVAL        = 161
+	SYS_NANOSLEEP                    = 162
+	SYS_MREMAP                       = 163
+	SYS_SETRESUID                    = 164
+	SYS_GETRESUID                    = 165
+	SYS_POLL                         = 168
+	SYS_NFSSERVCTL                   = 169
+	SYS_SETRESGID                    = 170
+	SYS_GETRESGID                    = 171
+	SYS_PRCTL                        = 172
+	SYS_RT_SIGRETURN                 = 173
+	SYS_RT_SIGACTION                 = 174
+	SYS_RT_SIGPROCMASK               = 175
+	SYS_RT_SIGPENDING                = 176
+	SYS_RT_SIGTIMEDWAIT              = 177
+	SYS_RT_SIGQUEUEINFO              = 178
+	SYS_RT_SIGSUSPEND                = 179
+	SYS_PREAD64                      = 180
+	SYS_PWRITE64                     = 181
+	SYS_CHOWN                        = 182
+	SYS_GETCWD                       = 183
+	SYS_CAPGET                       = 184
+	SYS_CAPSET                       = 185
+	SYS_SIGALTSTACK                  = 186
+	SYS_SENDFILE                     = 187
+	SYS_VFORK                        = 190
+	SYS_UGETRLIMIT                   = 191
+	SYS_MMAP2                        = 192
+	SYS_TRUNCATE64                   = 193
+	SYS_FTRUNCATE64                  = 194
+	SYS_STAT64                       = 195
+	SYS_LSTAT64                      = 196
+	SYS_FSTAT64                      = 197
+	SYS_LCHOWN32                     = 198
+	SYS_GETUID32                     = 199
+	SYS_GETGID32                     = 200
+	SYS_GETEUID32                    = 201
+	SYS_GETEGID32                    = 202
+	SYS_SETREUID32                   = 203
+	SYS_SETREGID32                   = 204
+	SYS_GETGROUPS32                  = 205
+	SYS_SETGROUPS32                  = 206
+	SYS_FCHOWN32                     = 207
+	SYS_SETRESUID32                  = 208
+	SYS_GETRESUID32                  = 209
+	SYS_SETRESGID32                  = 210
+	SYS_GETRESGID32                  = 211
+	SYS_CHOWN32                      = 212
+	SYS_SETUID32                     = 213
+	SYS_SETGID32                     = 214
+	SYS_SETFSUID32                   = 215
+	SYS_SETFSGID32                   = 216
+	SYS_GETDENTS64                   = 217
+	SYS_PIVOT_ROOT                   = 218
+	SYS_MINCORE                      = 219
+	SYS_MADVISE                      = 220
+	SYS_FCNTL64                      = 221
+	SYS_GETTID                       = 224
+	SYS_READAHEAD                    = 225
+	SYS_SETXATTR                     = 226
+	SYS_LSETXATTR                    = 227
+	SYS_FSETXATTR                    = 228
+	SYS_GETXATTR                     = 229
+	SYS_LGETXATTR                    = 230
+	SYS_FGETXATTR                    = 231
+	SYS_LISTXATTR                    = 232
+	SYS_LLISTXATTR                   = 233
+	SYS_FLISTXATTR                   = 234
+	SYS_REMOVEXATTR                  = 235
+	SYS_LREMOVEXATTR                 = 236
+	SYS_FREMOVEXATTR                 = 237
+	SYS_TKILL                        = 238
+	SYS_SENDFILE64                   = 239
+	SYS_FUTEX                        = 240
+	SYS_SCHED_SETAFFINITY            = 241
+	SYS_SCHED_GETAFFINITY            = 242
+	SYS_IO_SETUP                     = 243
+	SYS_IO_DESTROY                   = 244
+	SYS_IO_GETEVENTS                 = 245
+	SYS_IO_SUBMIT                    = 246
+	SYS_IO_CANCEL                    = 247
+	SYS_EXIT_GROUP                   = 248
+	SYS_LOOKUP_DCOOKIE               = 249
+	SYS_EPOLL_CREATE                 = 250
+	SYS_EPOLL_CTL                    = 251
+	SYS_EPOLL_WAIT                   = 252
+	SYS_REMAP_FILE_PAGES             = 253
+	SYS_SET_TID_ADDRESS              = 256
+	SYS_TIMER_CREATE                 = 257
+	SYS_TIMER_SETTIME                = 258
+	SYS_TIMER_GETTIME                = 259
+	SYS_TIMER_GETOVERRUN             = 260
+	SYS_TIMER_DELETE                 = 261
+	SYS_CLOCK_SETTIME                = 262
+	SYS_CLOCK_GETTIME                = 263
+	SYS_CLOCK_GETRES                 = 264
+	SYS_CLOCK_NANOSLEEP              = 265
+	SYS_STATFS64                     = 266
+	SYS_FSTATFS64                    = 267
+	SYS_TGKILL                       = 268
+	SYS_UTIMES                       = 269
+	SYS_ARM_FADVISE64_64             = 270
+	SYS_PCICONFIG_IOBASE             = 271
+	SYS_PCICONFIG_READ               = 272
+	SYS_PCICONFIG_WRITE              = 273
+	SYS_MQ_OPEN                      = 274
+	SYS_MQ_UNLINK                    = 275
+	SYS_MQ_TIMEDSEND                 = 276
+	SYS_MQ_TIMEDRECEIVE              = 277
+	SYS_MQ_NOTIFY                    = 278
+	SYS_MQ_GETSETATTR                = 279
+	SYS_WAITID                       = 280
+	SYS_SOCKET                       = 281
+	SYS_BIND                         = 282
+	SYS_CONNECT                      = 283
+	SYS_LISTEN                       = 284
+	SYS_ACCEPT                       = 285
+	SYS_GETSOCKNAME                  = 286
+	SYS_GETPEERNAME                  = 287
+	SYS_SOCKETPAIR                   = 288
+	SYS_SEND                         = 289
+	SYS_SENDTO                       = 290
+	SYS_RECV                         = 291
+	SYS_RECVFROM                     = 292
+	SYS_SHUTDOWN                     = 293
+	SYS_SETSOCKOPT                   = 294
+	SYS_GETSOCKOPT                   = 295
+	SYS_SENDMSG                      = 296
+	SYS_RECVMSG                      = 297
+	SYS_SEMOP                        = 298
+	SYS_SEMGET                       = 299
+	SYS_SEMCTL                       = 300
+	SYS_MSGSND                       = 301
+	SYS_MSGRCV                       = 302
+	SYS_MSGGET                       = 303
+	SYS_MSGCTL                       = 304
+	SYS_SHMAT                        = 305
+	SYS_SHMDT                        = 306
+	SYS_SHMGET                       = 307
+	SYS_SHMCTL                       = 308
+	SYS_ADD_KEY                      = 309
+	SYS_REQUEST_KEY                  = 310
+	SYS_KEYCTL                       = 311
+	SYS_SEMTIMEDOP                   = 312
+	SYS_VSERVER                      = 313
+	SYS_IOPRIO_SET                   = 314
+	SYS_IOPRIO_GET                   = 315
+	SYS_INOTIFY_INIT                 = 316
+	SYS_INOTIFY_ADD_WATCH            = 317
+	SYS_INOTIFY_RM_WATCH             = 318
+	SYS_MBIND                        = 319
+	SYS_GET_MEMPOLICY                = 320
+	SYS_SET_MEMPOLICY                = 321
+	SYS_OPENAT                       = 322
+	SYS_MKDIRAT                      = 323
+	SYS_MKNODAT                      = 324
+	SYS_FCHOWNAT                     = 325
+	SYS_FUTIMESAT                    = 326
+	SYS_FSTATAT64                    = 327
+	SYS_UNLINKAT                     = 328
+	SYS_RENAMEAT                     = 329
+	SYS_LINKAT                       = 330
+	SYS_SYMLINKAT                    = 331
+	SYS_READLINKAT                   = 332
+	SYS_FCHMODAT                     = 333
+	SYS_FACCESSAT                    = 334
+	SYS_PSELECT6                     = 335
+	SYS_PPOLL                        = 336
+	SYS_UNSHARE                      = 337
+	SYS_SET_ROBUST_LIST              = 338
+	SYS_GET_ROBUST_LIST              = 339
+	SYS_SPLICE                       = 340
+	SYS_ARM_SYNC_FILE_RANGE          = 341
+	SYS_TEE                          = 342
+	SYS_VMSPLICE                     = 343
+	SYS_MOVE_PAGES                   = 344
+	SYS_GETCPU                       = 345
+	SYS_EPOLL_PWAIT                  = 346
+	SYS_KEXEC_LOAD                   = 347
+	SYS_UTIMENSAT                    = 348
+	SYS_SIGNALFD                     = 349
+	SYS_TIMERFD_CREATE               = 350
+	SYS_EVENTFD                      = 351
+	SYS_FALLOCATE                    = 352
+	SYS_TIMERFD_SETTIME              = 353
+	SYS_TIMERFD_GETTIME              = 354
+	SYS_SIGNALFD4                    = 355
+	SYS_EVENTFD2                     = 356
+	SYS_EPOLL_CREATE1                = 357
+	SYS_DUP3                         = 358
+	SYS_PIPE2                        = 359
+	SYS_INOTIFY_INIT1                = 360
+	SYS_PREADV                       = 361
+	SYS_PWRITEV                      = 362
+	SYS_RT_TGSIGQUEUEINFO            = 363
+	SYS_PERF_EVENT_OPEN              = 364
+	SYS_RECVMMSG                     = 365
+	SYS_ACCEPT4                      = 366
+	SYS_FANOTIFY_INIT                = 367
+	SYS_FANOTIFY_MARK                = 368
+	SYS_PRLIMIT64                    = 369
+	SYS_NAME_TO_HANDLE_AT            = 370
+	SYS_OPEN_BY_HANDLE_AT            = 371
+	SYS_CLOCK_ADJTIME                = 372
+	SYS_SYNCFS                       = 373
+	SYS_SENDMMSG                     = 374
+	SYS_SETNS                        = 375
+	SYS_PROCESS_VM_READV             = 376
+	SYS_PROCESS_VM_WRITEV            = 377
+	SYS_KCMP                         = 378
+	SYS_FINIT_MODULE                 = 379
+	SYS_SCHED_SETATTR                = 380
+	SYS_SCHED_GETATTR                = 381
+	SYS_RENAMEAT2                    = 382
+	SYS_SECCOMP                      = 383
+	SYS_GETRANDOM                    = 384
+	SYS_MEMFD_CREATE                 = 385
+	SYS_BPF                          = 386
+	SYS_EXECVEAT                     = 387
+	SYS_USERFAULTFD                  = 388
+	SYS_MEMBARRIER                   = 389
+	SYS_MLOCK2                       = 390
+	SYS_COPY_FILE_RANGE              = 391
+	SYS_PREADV2                      = 392
+	SYS_PWRITEV2                     = 393
+	SYS_PKEY_MPROTECT                = 394
+	SYS_PKEY_ALLOC                   = 395
+	SYS_PKEY_FREE                    = 396
+	SYS_STATX                        = 397
+	SYS_RSEQ                         = 398
+	SYS_IO_PGETEVENTS                = 399
+	SYS_MIGRATE_PAGES                = 400
+	SYS_KEXEC_FILE_LOAD              = 401
+	SYS_CLOCK_GETTIME64              = 403
+	SYS_CLOCK_SETTIME64              = 404
+	SYS_CLOCK_ADJTIME64              = 405
+	SYS_CLOCK_GETRES_TIME64          = 406
+	SYS_CLOCK_NANOSLEEP_TIME64       = 407
+	SYS_TIMER_GETTIME64              = 408
+	SYS_TIMER_SETTIME64              = 409
+	SYS_TIMERFD_GETTIME64            = 410
+	SYS_TIMERFD_SETTIME64            = 411
+	SYS_UTIMENSAT_TIME64             = 412
+	SYS_PSELECT6_TIME64              = 413
+	SYS_PPOLL_TIME64                 = 414
+	SYS_IO_PGETEVENTS_TIME64         = 416
+	SYS_RECVMMSG_TIME64              = 417
+	SYS_MQ_TIMEDSEND_TIME64          = 418
+	SYS_MQ_TIMEDRECEIVE_TIME64       = 419
+	SYS_SEMTIMEDOP_TIME64            = 420
+	SYS_RT_SIGTIMEDWAIT_TIME64       = 421
+	SYS_FUTEX_TIME64                 = 422
+	SYS_SCHED_RR_GET_INTERVAL_TIME64 = 423
+	SYS_PIDFD_SEND_SIGNAL            = 424
+	SYS_IO_URING_SETUP               = 425
+	SYS_IO_URING_ENTER               = 426
+	SYS_IO_URING_REGISTER            = 427
+	SYS_OPEN_TREE                    = 428
+	SYS_MOVE_MOUNT                   = 429
+	SYS_FSOPEN                       = 430
+	SYS_FSCONFIG                     = 431
+	SYS_FSMOUNT                      = 432
+	SYS_FSPICK                       = 433
+	SYS_PIDFD_OPEN                   = 434
+	SYS_CLONE3                       = 435
+	SYS_CLOSE_RANGE                  = 436
+	SYS_OPENAT2                      = 437
+	SYS_PIDFD_GETFD                  = 438
+	SYS_FACCESSAT2                   = 439
+)
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/ztypes_linux_thumb.go b/src/cmd/vendor/golang.org/x/sys/unix/ztypes_linux_thumb.go
new file mode 100644
index 0000000000..af17dc28e4
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/ztypes_linux_thumb.go
@@ -0,0 +1,614 @@
+// cgo -godefs -- -Wall -Werror -static -I/tmp/include linux/types.go | go run mkpost.go
+// Code generated by the command above; see README.md. DO NOT EDIT.
+
+// +build thumb,linux
+
+package unix
+
+const (
+	SizeofPtr  = 0x4
+	SizeofLong = 0x4
+)
+
+type (
+	_C_long int32
+)
+
+type Timespec struct {
+	Sec  int32
+	Nsec int32
+}
+
+type Timeval struct {
+	Sec  int32
+	Usec int32
+}
+
+type Timex struct {
+	Modes     uint32
+	Offset    int32
+	Freq      int32
+	Maxerror  int32
+	Esterror  int32
+	Status    int32
+	Constant  int32
+	Precision int32
+	Tolerance int32
+	Time      Timeval
+	Tick      int32
+	Ppsfreq   int32
+	Jitter    int32
+	Shift     int32
+	Stabil    int32
+	Jitcnt    int32
+	Calcnt    int32
+	Errcnt    int32
+	Stbcnt    int32
+	Tai       int32
+	_         [44]byte
+}
+
+type Time_t int32
+
+type Tms struct {
+	Utime  int32
+	Stime  int32
+	Cutime int32
+	Cstime int32
+}
+
+type Utimbuf struct {
+	Actime  int32
+	Modtime int32
+}
+
+type Rusage struct {
+	Utime    Timeval
+	Stime    Timeval
+	Maxrss   int32
+	Ixrss    int32
+	Idrss    int32
+	Isrss    int32
+	Minflt   int32
+	Majflt   int32
+	Nswap    int32
+	Inblock  int32
+	Oublock  int32
+	Msgsnd   int32
+	Msgrcv   int32
+	Nsignals int32
+	Nvcsw    int32
+	Nivcsw   int32
+}
+
+type Stat_t struct {
+	Dev     uint64
+	_       uint16
+	_       uint32
+	Mode    uint32
+	Nlink   uint32
+	Uid     uint32
+	Gid     uint32
+	Rdev    uint64
+	_       uint16
+	_       [4]byte
+	Size    int64
+	Blksize int32
+	_       [4]byte
+	Blocks  int64
+	Atim    Timespec
+	Mtim    Timespec
+	Ctim    Timespec
+	Ino     uint64
+}
+
+type Dirent struct {
+	Ino    uint64
+	Off    int64
+	Reclen uint16
+	Type   uint8
+	Name   [256]uint8
+	_      [5]byte
+}
+
+type Flock_t struct {
+	Type   int16
+	Whence int16
+	_      [4]byte
+	Start  int64
+	Len    int64
+	Pid    int32
+	_      [4]byte
+}
+
+type DmNameList struct {
+	Dev  uint64
+	Next uint32
+	Name [0]byte
+	_    [4]byte
+}
+
+const (
+	FADV_DONTNEED = 0x4
+	FADV_NOREUSE  = 0x5
+)
+
+type RawSockaddr struct {
+	Family uint16
+	Data   [14]uint8
+}
+
+type RawSockaddrAny struct {
+	Addr RawSockaddr
+	Pad  [96]uint8
+}
+
+type Iovec struct {
+	Base *byte
+	Len  uint32
+}
+
+type Msghdr struct {
+	Name       *byte
+	Namelen    uint32
+	Iov        *Iovec
+	Iovlen     uint32
+	Control    *byte
+	Controllen uint32
+	Flags      int32
+}
+
+type Cmsghdr struct {
+	Len   uint32
+	Level int32
+	Type  int32
+}
+
+const (
+	SizeofIovec   = 0x8
+	SizeofMsghdr  = 0x1c
+	SizeofCmsghdr = 0xc
+)
+
+const (
+	SizeofSockFprog = 0x8
+)
+
+type PtraceRegs struct {
+	Uregs [18]uint32
+}
+
+type FdSet struct {
+	Bits [32]int32
+}
+
+type Sysinfo_t struct {
+	Uptime    int32
+	Loads     [3]uint32
+	Totalram  uint32
+	Freeram   uint32
+	Sharedram uint32
+	Bufferram uint32
+	Totalswap uint32
+	Freeswap  uint32
+	Procs     uint16
+	Pad       uint16
+	Totalhigh uint32
+	Freehigh  uint32
+	Unit      uint32
+	_         [8]uint8
+}
+
+type Ustat_t struct {
+	Tfree  int32
+	Tinode uint32
+	Fname  [6]uint8
+	Fpack  [6]uint8
+}
+
+type EpollEvent struct {
+	Events uint32
+	PadFd  int32
+	Fd     int32
+	Pad    int32
+}
+
+const (
+	POLLRDHUP = 0x2000
+)
+
+type Sigset_t struct {
+	Val [32]uint32
+}
+
+const _C__NSIG = 0x41
+
+type Termios struct {
+	Iflag  uint32
+	Oflag  uint32
+	Cflag  uint32
+	Lflag  uint32
+	Line   uint8
+	Cc     [19]uint8
+	Ispeed uint32
+	Ospeed uint32
+}
+
+type Taskstats struct {
+	Version                   uint16
+	Ac_exitcode               uint32
+	Ac_flag                   uint8
+	Ac_nice                   uint8
+	_                         [4]byte
+	Cpu_count                 uint64
+	Cpu_delay_total           uint64
+	Blkio_count               uint64
+	Blkio_delay_total         uint64
+	Swapin_count              uint64
+	Swapin_delay_total        uint64
+	Cpu_run_real_total        uint64
+	Cpu_run_virtual_total     uint64
+	Ac_comm                   [32]uint8
+	Ac_sched                  uint8
+	Ac_pad                    [3]uint8
+	_                         [4]byte
+	Ac_uid                    uint32
+	Ac_gid                    uint32
+	Ac_pid                    uint32
+	Ac_ppid                   uint32
+	Ac_btime                  uint32
+	_                         [4]byte
+	Ac_etime                  uint64
+	Ac_utime                  uint64
+	Ac_stime                  uint64
+	Ac_minflt                 uint64
+	Ac_majflt                 uint64
+	Coremem                   uint64
+	Virtmem                   uint64
+	Hiwater_rss               uint64
+	Hiwater_vm                uint64
+	Read_char                 uint64
+	Write_char                uint64
+	Read_syscalls             uint64
+	Write_syscalls            uint64
+	Read_bytes                uint64
+	Write_bytes               uint64
+	Cancelled_write_bytes     uint64
+	Nvcsw                     uint64
+	Nivcsw                    uint64
+	Ac_utimescaled            uint64
+	Ac_stimescaled            uint64
+	Cpu_scaled_run_real_total uint64
+	Freepages_count           uint64
+	Freepages_delay_total     uint64
+	Thrashing_count           uint64
+	Thrashing_delay_total     uint64
+	Ac_btime64                uint64
+}
+
+type cpuMask uint32
+
+const (
+	_NCPUBITS = 0x20
+)
+
+const (
+	CBitFieldMaskBit0  = 0x1
+	CBitFieldMaskBit1  = 0x2
+	CBitFieldMaskBit2  = 0x4
+	CBitFieldMaskBit3  = 0x8
+	CBitFieldMaskBit4  = 0x10
+	CBitFieldMaskBit5  = 0x20
+	CBitFieldMaskBit6  = 0x40
+	CBitFieldMaskBit7  = 0x80
+	CBitFieldMaskBit8  = 0x100
+	CBitFieldMaskBit9  = 0x200
+	CBitFieldMaskBit10 = 0x400
+	CBitFieldMaskBit11 = 0x800
+	CBitFieldMaskBit12 = 0x1000
+	CBitFieldMaskBit13 = 0x2000
+	CBitFieldMaskBit14 = 0x4000
+	CBitFieldMaskBit15 = 0x8000
+	CBitFieldMaskBit16 = 0x10000
+	CBitFieldMaskBit17 = 0x20000
+	CBitFieldMaskBit18 = 0x40000
+	CBitFieldMaskBit19 = 0x80000
+	CBitFieldMaskBit20 = 0x100000
+	CBitFieldMaskBit21 = 0x200000
+	CBitFieldMaskBit22 = 0x400000
+	CBitFieldMaskBit23 = 0x800000
+	CBitFieldMaskBit24 = 0x1000000
+	CBitFieldMaskBit25 = 0x2000000
+	CBitFieldMaskBit26 = 0x4000000
+	CBitFieldMaskBit27 = 0x8000000
+	CBitFieldMaskBit28 = 0x10000000
+	CBitFieldMaskBit29 = 0x20000000
+	CBitFieldMaskBit30 = 0x40000000
+	CBitFieldMaskBit31 = 0x80000000
+	CBitFieldMaskBit32 = 0x100000000
+	CBitFieldMaskBit33 = 0x200000000
+	CBitFieldMaskBit34 = 0x400000000
+	CBitFieldMaskBit35 = 0x800000000
+	CBitFieldMaskBit36 = 0x1000000000
+	CBitFieldMaskBit37 = 0x2000000000
+	CBitFieldMaskBit38 = 0x4000000000
+	CBitFieldMaskBit39 = 0x8000000000
+	CBitFieldMaskBit40 = 0x10000000000
+	CBitFieldMaskBit41 = 0x20000000000
+	CBitFieldMaskBit42 = 0x40000000000
+	CBitFieldMaskBit43 = 0x80000000000
+	CBitFieldMaskBit44 = 0x100000000000
+	CBitFieldMaskBit45 = 0x200000000000
+	CBitFieldMaskBit46 = 0x400000000000
+	CBitFieldMaskBit47 = 0x800000000000
+	CBitFieldMaskBit48 = 0x1000000000000
+	CBitFieldMaskBit49 = 0x2000000000000
+	CBitFieldMaskBit50 = 0x4000000000000
+	CBitFieldMaskBit51 = 0x8000000000000
+	CBitFieldMaskBit52 = 0x10000000000000
+	CBitFieldMaskBit53 = 0x20000000000000
+	CBitFieldMaskBit54 = 0x40000000000000
+	CBitFieldMaskBit55 = 0x80000000000000
+	CBitFieldMaskBit56 = 0x100000000000000
+	CBitFieldMaskBit57 = 0x200000000000000
+	CBitFieldMaskBit58 = 0x400000000000000
+	CBitFieldMaskBit59 = 0x800000000000000
+	CBitFieldMaskBit60 = 0x1000000000000000
+	CBitFieldMaskBit61 = 0x2000000000000000
+	CBitFieldMaskBit62 = 0x4000000000000000
+	CBitFieldMaskBit63 = 0x8000000000000000
+)
+
+type SockaddrStorage struct {
+	Family uint16
+	_      [122]uint8
+	_      uint32
+}
+
+type HDGeometry struct {
+	Heads     uint8
+	Sectors   uint8
+	Cylinders uint16
+	Start     uint32
+}
+
+type Statfs_t struct {
+	Type    int32
+	Bsize   int32
+	Blocks  uint64
+	Bfree   uint64
+	Bavail  uint64
+	Files   uint64
+	Ffree   uint64
+	Fsid    Fsid
+	Namelen int32
+	Frsize  int32
+	Flags   int32
+	Spare   [4]int32
+	_       [4]byte
+}
+
+type TpacketHdr struct {
+	Status  uint32
+	Len     uint32
+	Snaplen uint32
+	Mac     uint16
+	Net     uint16
+	Sec     uint32
+	Usec    uint32
+}
+
+const (
+	SizeofTpacketHdr = 0x18
+)
+
+type RTCPLLInfo struct {
+	Ctrl    int32
+	Value   int32
+	Max     int32
+	Min     int32
+	Posmult int32
+	Negmult int32
+	Clock   int32
+}
+
+type BlkpgPartition struct {
+	Start   int64
+	Length  int64
+	Pno     int32
+	Devname [64]uint8
+	Volname [64]uint8
+	_       [4]byte
+}
+
+const (
+	BLKPG = 0x1269
+)
+
+type XDPUmemReg struct {
+	Addr     uint64
+	Len      uint64
+	Size     uint32
+	Headroom uint32
+	Flags    uint32
+	_        [4]byte
+}
+
+type CryptoUserAlg struct {
+	Name        [64]uint8
+	Driver_name [64]uint8
+	Module_name [64]uint8
+	Type        uint32
+	Mask        uint32
+	Refcnt      uint32
+	Flags       uint32
+}
+
+type CryptoStatAEAD struct {
+	Type         [64]uint8
+	Encrypt_cnt  uint64
+	Encrypt_tlen uint64
+	Decrypt_cnt  uint64
+	Decrypt_tlen uint64
+	Err_cnt      uint64
+}
+
+type CryptoStatAKCipher struct {
+	Type         [64]uint8
+	Encrypt_cnt  uint64
+	Encrypt_tlen uint64
+	Decrypt_cnt  uint64
+	Decrypt_tlen uint64
+	Verify_cnt   uint64
+	Sign_cnt     uint64
+	Err_cnt      uint64
+}
+
+type CryptoStatCipher struct {
+	Type         [64]uint8
+	Encrypt_cnt  uint64
+	Encrypt_tlen uint64
+	Decrypt_cnt  uint64
+	Decrypt_tlen uint64
+	Err_cnt      uint64
+}
+
+type CryptoStatCompress struct {
+	Type            [64]uint8
+	Compress_cnt    uint64
+	Compress_tlen   uint64
+	Decompress_cnt  uint64
+	Decompress_tlen uint64
+	Err_cnt         uint64
+}
+
+type CryptoStatHash struct {
+	Type      [64]uint8
+	Hash_cnt  uint64
+	Hash_tlen uint64
+	Err_cnt   uint64
+}
+
+type CryptoStatKPP struct {
+	Type                      [64]uint8
+	Setsecret_cnt             uint64
+	Generate_public_key_cnt   uint64
+	Compute_shared_secret_cnt uint64
+	Err_cnt                   uint64
+}
+
+type CryptoStatRNG struct {
+	Type          [64]uint8
+	Generate_cnt  uint64
+	Generate_tlen uint64
+	Seed_cnt      uint64
+	Err_cnt       uint64
+}
+
+type CryptoStatLarval struct {
+	Type [64]uint8
+}
+
+type CryptoReportLarval struct {
+	Type [64]uint8
+}
+
+type CryptoReportHash struct {
+	Type       [64]uint8
+	Blocksize  uint32
+	Digestsize uint32
+}
+
+type CryptoReportCipher struct {
+	Type        [64]uint8
+	Blocksize   uint32
+	Min_keysize uint32
+	Max_keysize uint32
+}
+
+type CryptoReportBlkCipher struct {
+	Type        [64]uint8
+	Geniv       [64]uint8
+	Blocksize   uint32
+	Min_keysize uint32
+	Max_keysize uint32
+	Ivsize      uint32
+}
+
+type CryptoReportAEAD struct {
+	Type        [64]uint8
+	Geniv       [64]uint8
+	Blocksize   uint32
+	Maxauthsize uint32
+	Ivsize      uint32
+}
+
+type CryptoReportComp struct {
+	Type [64]uint8
+}
+
+type CryptoReportRNG struct {
+	Type     [64]uint8
+	Seedsize uint32
+}
+
+type CryptoReportAKCipher struct {
+	Type [64]uint8
+}
+
+type CryptoReportKPP struct {
+	Type [64]uint8
+}
+
+type CryptoReportAcomp struct {
+	Type [64]uint8
+}
+
+type LoopInfo struct {
+	Number           int32
+	Device           uint16
+	Inode            uint32
+	Rdevice          uint16
+	Offset           int32
+	Encrypt_type     int32
+	Encrypt_key_size int32
+	Flags            int32
+	Name             [64]uint8
+	Encrypt_key      [32]uint8
+	Init             [2]uint32
+	Reserved         [4]uint8
+}
+
+type TIPCSubscr struct {
+	Seq     TIPCServiceRange
+	Timeout uint32
+	Filter  uint32
+	Handle  [8]uint8
+}
+
+type TIPCSIOCLNReq struct {
+	Peer     uint32
+	Id       uint32
+	Linkname [68]uint8
+}
+
+type TIPCSIOCNodeIDReq struct {
+	Peer uint32
+	Id   [16]uint8
+}
+
+type PPSKInfo struct {
+	Assert_sequence uint32
+	Clear_sequence  uint32
+	Assert_tu       PPSKTime
+	Clear_tu        PPSKTime
+	Current_mode    int32
+	_               [4]byte
+}
+
+const (
+	PPS_GETPARAMS = 0x800470a1
+	PPS_SETPARAMS = 0x400470a2
+	PPS_GETCAP    = 0x800470a3
+	PPS_FETCH     = 0xc00470a4
+)
diff --git a/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/asmdecl/asmdecl.go b/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/asmdecl/asmdecl.go
index eb0016b18f..32bbdd5b74 100644
--- a/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/asmdecl/asmdecl.go
+++ b/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/asmdecl/asmdecl.go
@@ -89,6 +89,7 @@ var (
 	asmArchPpc64LE  = asmArch{name: "ppc64le", bigEndian: false, stack: "R1", lr: true}
 	asmArchRISCV64  = asmArch{name: "riscv64", bigEndian: false, stack: "SP", lr: true}
 	asmArchS390X    = asmArch{name: "s390x", bigEndian: true, stack: "R15", lr: true}
+	asmArchThumb    = asmArch{name: "thumb", bigEndian: false, stack: "R13", lr: true}
 	asmArchWasm     = asmArch{name: "wasm", bigEndian: false, stack: "SP", lr: false}
 
 	arches = []*asmArch{
@@ -104,6 +105,7 @@ var (
 		&asmArchPpc64LE,
 		&asmArchRISCV64,
 		&asmArchS390X,
+		&asmArchThumb,
 		&asmArchWasm,
 	}
 )
diff --git a/src/crypto/md5/md5block_decl.go b/src/crypto/md5/md5block_decl.go
index f251e03d7f..922240ce02 100644
--- a/src/crypto/md5/md5block_decl.go
+++ b/src/crypto/md5/md5block_decl.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build amd64 386 arm ppc64le ppc64 s390x arm64
+// +build amd64 386 arm ppc64le ppc64 s390x arm64 thumb
 
 package md5
 
diff --git a/src/crypto/md5/md5block_generic.go b/src/crypto/md5/md5block_generic.go
index 0b46e70b60..55722ce881 100644
--- a/src/crypto/md5/md5block_generic.go
+++ b/src/crypto/md5/md5block_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !amd64,!386,!arm,!ppc64le,!ppc64,!s390x,!arm64
+// +build !amd64,!386,!arm,!ppc64le,!ppc64,!s390x,!arm64,!thumb
 
 package md5
 
diff --git a/src/crypto/md5/md5block_thumb.s b/src/crypto/md5/md5block_thumb.s
new file mode 100644
index 0000000000..9d9a625283
--- /dev/null
+++ b/src/crypto/md5/md5block_thumb.s
@@ -0,0 +1,300 @@
+// Copyright 2013 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+//
+// Thumb version of md5block.go
+
+#include "textflag.h"
+
+// Register definitions
+#define Rtable	R0	// Pointer to MD5 constants table
+#define Rdata	R1	// Pointer to data to hash
+#define Ra	R2	// MD5 accumulator
+#define Rb	R3	// MD5 accumulator
+#define Rc	R4	// MD5 accumulator
+#define Rd	R5	// MD5 accumulator
+#define Rc0	R6	// MD5 constant
+// r7 is OK provided you check the assembler that no synthetic instructions use it
+#define Rc1	R7	// MD5 constant
+#define Rc2	R8	// MD5 constant
+// r9, r10 are forbidden
+#define Rc3	R11	// MD5 constant
+#define Rt0	R12	// temporary
+#define Rt1	R14	// temporary
+
+// func block(dig *digest, p []byte)
+// 0(FP) is *digest
+// 4(FP) is p.array (struct Slice)
+// 8(FP) is p.len
+//12(FP) is p.cap
+//
+// Stack frame
+#define p_end	end-4(SP)	// pointer to the end of data
+#define p_data	data-8(SP)	// current data pointer
+#define buf	buffer-(8+4*16)(SP)	//16 words temporary buffer
+		// 3 words at 4..12(R13) for called routine parameters
+
+TEXT	·block(SB), NOSPLIT, $84-16
+	MOVW	p+4(FP), Rdata	// pointer to the data
+	MOVW	p_len+8(FP), Rt0	// number of bytes
+	ADD	Rdata, Rt0
+	MOVW	Rt0, p_end	// pointer to end of data
+
+loop:
+	MOVW	Rdata, p_data	// Save Rdata
+	AND.S	$3, Rdata, Rt0	// TST $3, Rdata not working see issue 5921
+	BEQ	aligned			// aligned detected - skip copy
+
+	// Copy the unaligned source data into the aligned temporary buffer
+	// memmove(to=4(R13), from=8(R13), n=12(R13)) - Corrupts all registers
+	MOVW	$buf, Rtable	// to
+	MOVW	$64, Rc0		// n
+	ADD     $4, R13, Ra
+	MOVM.IA	[Rtable,Rdata,Rc0], (Ra)
+	BL	runtime·memmove(SB)
+
+	// Point to the local aligned copy of the data
+	MOVW	$buf, Rdata
+
+aligned:
+	// Point to the table of constants
+	// A PC relative add would be cheaper than this
+	MOVW	$·table(SB), Rtable
+
+	// Load up initial MD5 accumulator
+	MOVW	dig+0(FP), Rc0
+	MOVM.IA (Rc0), [Ra,Rb,Rc,Rd]
+
+// a += (((c^d)&b)^d) + X[index] + const
+// a = a<<shift | a>>(32-shift) + b
+#define ROUND1(Ra, Rb, Rc, Rd, index, shift, Rconst) \
+	EOR	Rc, Rd, Rt0		; \
+	AND	Rb, Rt0			; \
+	EOR	Rd, Rt0			; \
+	MOVW	(index<<2)(Rdata), Rt1	; \
+	ADD	Rt1, Rt0			; \
+	ADD	Rconst, Rt0			; \
+	ADD	Rt0, Ra			; \
+	ADD	Ra@>(32-shift), Rb, Ra	;
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND1(Ra, Rb, Rc, Rd,  0,	7, Rc0)
+	ROUND1(Rd, Ra, Rb, Rc,  1, 12, Rc1)
+	ROUND1(Rc, Rd, Ra, Rb,  2, 17, Rc2)
+	ROUND1(Rb, Rc, Rd, Ra,  3, 22, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND1(Ra, Rb, Rc, Rd,  4,	7, Rc0)
+	ROUND1(Rd, Ra, Rb, Rc,  5, 12, Rc1)
+	ROUND1(Rc, Rd, Ra, Rb,  6, 17, Rc2)
+	ROUND1(Rb, Rc, Rd, Ra,  7, 22, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND1(Ra, Rb, Rc, Rd,  8,	7, Rc0)
+	ROUND1(Rd, Ra, Rb, Rc,  9, 12, Rc1)
+	ROUND1(Rc, Rd, Ra, Rb, 10, 17, Rc2)
+	ROUND1(Rb, Rc, Rd, Ra, 11, 22, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND1(Ra, Rb, Rc, Rd, 12,	7, Rc0)
+	ROUND1(Rd, Ra, Rb, Rc, 13, 12, Rc1)
+	ROUND1(Rc, Rd, Ra, Rb, 14, 17, Rc2)
+	ROUND1(Rb, Rc, Rd, Ra, 15, 22, Rc3)
+
+// a += (((b^c)&d)^c) + X[index] + const
+// a = a<<shift | a>>(32-shift) + b
+#define ROUND2(Ra, Rb, Rc, Rd, index, shift, Rconst) \
+	EOR	Rb, Rc, Rt0		; \
+	AND	Rd, Rt0			; \
+	EOR	Rc, Rt0			; \
+	MOVW	(index<<2)(Rdata), Rt1	; \
+	ADD	Rt1, Rt0			; \
+	ADD	Rconst, Rt0			; \
+	ADD	Rt0, Ra			; \
+	ADD	Ra@>(32-shift), Rb, Ra	;
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND2(Ra, Rb, Rc, Rd,  1,	5, Rc0)
+	ROUND2(Rd, Ra, Rb, Rc,  6,	9, Rc1)
+	ROUND2(Rc, Rd, Ra, Rb, 11, 14, Rc2)
+	ROUND2(Rb, Rc, Rd, Ra,  0, 20, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND2(Ra, Rb, Rc, Rd,  5,	5, Rc0)
+	ROUND2(Rd, Ra, Rb, Rc, 10,	9, Rc1)
+	ROUND2(Rc, Rd, Ra, Rb, 15, 14, Rc2)
+	ROUND2(Rb, Rc, Rd, Ra,  4, 20, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND2(Ra, Rb, Rc, Rd,  9,	5, Rc0)
+	ROUND2(Rd, Ra, Rb, Rc, 14,	9, Rc1)
+	ROUND2(Rc, Rd, Ra, Rb,  3, 14, Rc2)
+	ROUND2(Rb, Rc, Rd, Ra,  8, 20, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND2(Ra, Rb, Rc, Rd, 13,	5, Rc0)
+	ROUND2(Rd, Ra, Rb, Rc,  2,	9, Rc1)
+	ROUND2(Rc, Rd, Ra, Rb,  7, 14, Rc2)
+	ROUND2(Rb, Rc, Rd, Ra, 12, 20, Rc3)
+
+// a += (b^c^d) + X[index] + const
+// a = a<<shift | a>>(32-shift) + b
+#define ROUND3(Ra, Rb, Rc, Rd, index, shift, Rconst) \
+	EOR	Rb, Rc, Rt0		; \
+	EOR	Rd, Rt0			; \
+	MOVW	(index<<2)(Rdata), Rt1	; \
+	ADD	Rt1, Rt0			; \
+	ADD	Rconst, Rt0			; \
+	ADD	Rt0, Ra			; \
+	ADD	Ra@>(32-shift), Rb, Ra	;
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND3(Ra, Rb, Rc, Rd,  5,	4, Rc0)
+	ROUND3(Rd, Ra, Rb, Rc,  8, 11, Rc1)
+	ROUND3(Rc, Rd, Ra, Rb, 11, 16, Rc2)
+	ROUND3(Rb, Rc, Rd, Ra, 14, 23, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND3(Ra, Rb, Rc, Rd,  1,	4, Rc0)
+	ROUND3(Rd, Ra, Rb, Rc,  4, 11, Rc1)
+	ROUND3(Rc, Rd, Ra, Rb,  7, 16, Rc2)
+	ROUND3(Rb, Rc, Rd, Ra, 10, 23, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND3(Ra, Rb, Rc, Rd, 13,	4, Rc0)
+	ROUND3(Rd, Ra, Rb, Rc,  0, 11, Rc1)
+	ROUND3(Rc, Rd, Ra, Rb,  3, 16, Rc2)
+	ROUND3(Rb, Rc, Rd, Ra,  6, 23, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND3(Ra, Rb, Rc, Rd,  9,	4, Rc0)
+	ROUND3(Rd, Ra, Rb, Rc, 12, 11, Rc1)
+	ROUND3(Rc, Rd, Ra, Rb, 15, 16, Rc2)
+	ROUND3(Rb, Rc, Rd, Ra,  2, 23, Rc3)
+
+// a += (c^(b|^d)) + X[index] + const
+// a = a<<shift | a>>(32-shift) + b
+#define ROUND4(Ra, Rb, Rc, Rd, index, shift, Rconst) \
+	MVN	Rd, Rt0			; \
+	ORR	Rb, Rt0			; \
+	EOR	Rc, Rt0			; \
+	MOVW	(index<<2)(Rdata), Rt1	; \
+	ADD	Rt1, Rt0			; \
+	ADD	Rconst, Rt0			; \
+	ADD	Rt0, Ra			; \
+	ADD	Ra@>(32-shift), Rb, Ra	;
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND4(Ra, Rb, Rc, Rd,  0,	6, Rc0)
+	ROUND4(Rd, Ra, Rb, Rc,  7, 10, Rc1)
+	ROUND4(Rc, Rd, Ra, Rb, 14, 15, Rc2)
+	ROUND4(Rb, Rc, Rd, Ra,  5, 21, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND4(Ra, Rb, Rc, Rd, 12,	6, Rc0)
+	ROUND4(Rd, Ra, Rb, Rc,  3, 10, Rc1)
+	ROUND4(Rc, Rd, Ra, Rb, 10, 15, Rc2)
+	ROUND4(Rb, Rc, Rd, Ra,  1, 21, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND4(Ra, Rb, Rc, Rd,  8,	6, Rc0)
+	ROUND4(Rd, Ra, Rb, Rc, 15, 10, Rc1)
+	ROUND4(Rc, Rd, Ra, Rb,  6, 15, Rc2)
+	ROUND4(Rb, Rc, Rd, Ra, 13, 21, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND4(Ra, Rb, Rc, Rd,  4,	6, Rc0)
+	ROUND4(Rd, Ra, Rb, Rc, 11, 10, Rc1)
+	ROUND4(Rc, Rd, Ra, Rb,  2, 15, Rc2)
+	ROUND4(Rb, Rc, Rd, Ra,  9, 21, Rc3)
+
+	MOVW	dig+0(FP), Rt0
+	MOVM.IA (Rt0), [Rc0,Rc1,Rc2,Rc3]
+
+	ADD	Rc0, Ra
+	ADD	Rc1, Rb
+	ADD	Rc2, Rc
+	ADD	Rc3, Rd
+
+	MOVM.IA [Ra,Rb,Rc,Rd], (Rt0)
+
+	MOVW	p_data, Rdata
+	MOVW	p_end, Rt0
+	ADD	$64, Rdata
+	CMP	Rt0, Rdata
+	BLO	loop
+
+	RET
+
+// MD5 constants table
+
+	// Round 1
+	DATA	·table+0x00(SB)/4, $0xd76aa478
+	DATA	·table+0x04(SB)/4, $0xe8c7b756
+	DATA	·table+0x08(SB)/4, $0x242070db
+	DATA	·table+0x0c(SB)/4, $0xc1bdceee
+	DATA	·table+0x10(SB)/4, $0xf57c0faf
+	DATA	·table+0x14(SB)/4, $0x4787c62a
+	DATA	·table+0x18(SB)/4, $0xa8304613
+	DATA	·table+0x1c(SB)/4, $0xfd469501
+	DATA	·table+0x20(SB)/4, $0x698098d8
+	DATA	·table+0x24(SB)/4, $0x8b44f7af
+	DATA	·table+0x28(SB)/4, $0xffff5bb1
+	DATA	·table+0x2c(SB)/4, $0x895cd7be
+	DATA	·table+0x30(SB)/4, $0x6b901122
+	DATA	·table+0x34(SB)/4, $0xfd987193
+	DATA	·table+0x38(SB)/4, $0xa679438e
+	DATA	·table+0x3c(SB)/4, $0x49b40821
+	// Round 2
+	DATA	·table+0x40(SB)/4, $0xf61e2562
+	DATA	·table+0x44(SB)/4, $0xc040b340
+	DATA	·table+0x48(SB)/4, $0x265e5a51
+	DATA	·table+0x4c(SB)/4, $0xe9b6c7aa
+	DATA	·table+0x50(SB)/4, $0xd62f105d
+	DATA	·table+0x54(SB)/4, $0x02441453
+	DATA	·table+0x58(SB)/4, $0xd8a1e681
+	DATA	·table+0x5c(SB)/4, $0xe7d3fbc8
+	DATA	·table+0x60(SB)/4, $0x21e1cde6
+	DATA	·table+0x64(SB)/4, $0xc33707d6
+	DATA	·table+0x68(SB)/4, $0xf4d50d87
+	DATA	·table+0x6c(SB)/4, $0x455a14ed
+	DATA	·table+0x70(SB)/4, $0xa9e3e905
+	DATA	·table+0x74(SB)/4, $0xfcefa3f8
+	DATA	·table+0x78(SB)/4, $0x676f02d9
+	DATA	·table+0x7c(SB)/4, $0x8d2a4c8a
+	// Round 3
+	DATA	·table+0x80(SB)/4, $0xfffa3942
+	DATA	·table+0x84(SB)/4, $0x8771f681
+	DATA	·table+0x88(SB)/4, $0x6d9d6122
+	DATA	·table+0x8c(SB)/4, $0xfde5380c
+	DATA	·table+0x90(SB)/4, $0xa4beea44
+	DATA	·table+0x94(SB)/4, $0x4bdecfa9
+	DATA	·table+0x98(SB)/4, $0xf6bb4b60
+	DATA	·table+0x9c(SB)/4, $0xbebfbc70
+	DATA	·table+0xa0(SB)/4, $0x289b7ec6
+	DATA	·table+0xa4(SB)/4, $0xeaa127fa
+	DATA	·table+0xa8(SB)/4, $0xd4ef3085
+	DATA	·table+0xac(SB)/4, $0x04881d05
+	DATA	·table+0xb0(SB)/4, $0xd9d4d039
+	DATA	·table+0xb4(SB)/4, $0xe6db99e5
+	DATA	·table+0xb8(SB)/4, $0x1fa27cf8
+	DATA	·table+0xbc(SB)/4, $0xc4ac5665
+	// Round 4
+	DATA	·table+0xc0(SB)/4, $0xf4292244
+	DATA	·table+0xc4(SB)/4, $0x432aff97
+	DATA	·table+0xc8(SB)/4, $0xab9423a7
+	DATA	·table+0xcc(SB)/4, $0xfc93a039
+	DATA	·table+0xd0(SB)/4, $0x655b59c3
+	DATA	·table+0xd4(SB)/4, $0x8f0ccc92
+	DATA	·table+0xd8(SB)/4, $0xffeff47d
+	DATA	·table+0xdc(SB)/4, $0x85845dd1
+	DATA	·table+0xe0(SB)/4, $0x6fa87e4f
+	DATA	·table+0xe4(SB)/4, $0xfe2ce6e0
+	DATA	·table+0xe8(SB)/4, $0xa3014314
+	DATA	·table+0xec(SB)/4, $0x4e0811a1
+	DATA	·table+0xf0(SB)/4, $0xf7537e82
+	DATA	·table+0xf4(SB)/4, $0xbd3af235
+	DATA	·table+0xf8(SB)/4, $0x2ad7d2bb
+	DATA	·table+0xfc(SB)/4, $0xeb86d391
+	// Global definition
+	GLOBL	·table(SB),8,$256
diff --git a/src/crypto/sha1/sha1block_decl.go b/src/crypto/sha1/sha1block_decl.go
index 9c7df4e40a..e533191f6f 100644
--- a/src/crypto/sha1/sha1block_decl.go
+++ b/src/crypto/sha1/sha1block_decl.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build arm 386 s390x
+// +build arm 386 s390x thumb
 
 package sha1
 
diff --git a/src/crypto/sha1/sha1block_generic.go b/src/crypto/sha1/sha1block_generic.go
index f95ea0eee4..79f055eda3 100644
--- a/src/crypto/sha1/sha1block_generic.go
+++ b/src/crypto/sha1/sha1block_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !amd64,!386,!arm,!s390x,!arm64
+// +build !amd64,!386,!arm,!s390x,!arm64,!thumb
 
 package sha1
 
diff --git a/src/crypto/sha1/sha1block_thumb.s b/src/crypto/sha1/sha1block_thumb.s
new file mode 100644
index 0000000000..1ac915398e
--- /dev/null
+++ b/src/crypto/sha1/sha1block_thumb.s
@@ -0,0 +1,220 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+//
+// Thumb version of md5block.go
+
+#include "textflag.h"
+
+// SHA-1 block routine. See sha1block.go for Go equivalent.
+//
+// There are 80 rounds of 4 types:
+//   - rounds 0-15 are type 1 and load data (ROUND1 macro).
+//   - rounds 16-19 are type 1 and do not load data (ROUND1x macro).
+//   - rounds 20-39 are type 2 and do not load data (ROUND2 macro).
+//   - rounds 40-59 are type 3 and do not load data (ROUND3 macro).
+//   - rounds 60-79 are type 4 and do not load data (ROUND4 macro).
+//
+// Each round loads or shuffles the data, then computes a per-round
+// function of b, c, d, and then mixes the result into and rotates the
+// five registers a, b, c, d, e holding the intermediate results.
+//
+// The register rotation is implemented by rotating the arguments to
+// the round macros instead of by explicit move instructions.
+
+// Register definitions
+#define Rdata	R0	// Pointer to incoming data
+#define Rconst	R1	// Current constant for SHA round
+#define Ra	R2		// SHA-1 accumulator
+#define Rb	R3		// SHA-1 accumulator
+#define Rc	R4		// SHA-1 accumulator
+#define Rd	R5		// SHA-1 accumulator
+#define Re	R6		// SHA-1 accumulator
+// R7 is OK provided you check the assembler that no synthetic instructions use it
+#define Rt0	R7		// Temporary
+#define Rt1	R8		// Temporary
+// r9, r10 are forbidden
+#define Rt2	R11		// Temporary
+#define Rctr	R12	// loop counter
+#define Rw	R14		// point to w buffer
+
+// func block(dig *digest, p []byte)
+// 0(FP) is *digest
+// 4(FP) is p.array (struct Slice)
+// 8(FP) is p.len
+//12(FP) is p.cap
+//
+// Stack frame
+#define p_end	end-4(SP)		// pointer to the end of data
+#define p_data	data-8(SP)	// current data pointer (unused?)
+#define w_buf	buf-(8+4*80)(SP)	//80 words temporary buffer w uint32[80]
+#define saved	abcde-(8+4*80+4*5)(SP)	// saved sha1 registers a,b,c,d,e - these must be last (unused?)
+// Total size +4 for saved LR is 352
+
+	// w[i] = p[j]<<24 | p[j+1]<<16 | p[j+2]<<8 | p[j+3]
+	// e += w[i]
+#define LOAD(Re) \
+	MOVBU	2(Rdata), Rt0 ; \
+	MOVBU	3(Rdata), Rt1 ; \
+	MOVBU	1(Rdata), Rt2 ; \
+	ORR	Rt0<<8, Rt1, Rt0	    ; \
+	MOVBU.P	4(Rdata), Rt1 ; \
+	ORR	Rt2<<16, Rt0, Rt0	    ; \
+	ORR	Rt1<<24, Rt0, Rt0	    ; \
+	MOVW.P	Rt0, 4(Rw)		    ; \
+	ADD	Rt0, Re, Re
+
+	// tmp := w[(i-3)&0xf] ^ w[(i-8)&0xf] ^ w[(i-14)&0xf] ^ w[(i)&0xf]
+	// w[i&0xf] = tmp<<1 | tmp>>(32-1)
+	// e += w[i&0xf]
+#define SHUFFLE(Re) \
+	MOVW	(-16*4)(Rw), Rt0 ; \
+	MOVW	(-14*4)(Rw), Rt1 ; \
+	MOVW	(-8*4)(Rw), Rt2  ; \
+	EOR	Rt0, Rt1, Rt0  ; \
+	MOVW	(-3*4)(Rw), Rt1  ; \
+	EOR	Rt2, Rt0, Rt0  ; \
+	EOR	Rt0, Rt1, Rt0  ; \
+	MOVW	Rt0@>(32-1), Rt0  ; \
+	MOVW.P	Rt0, 4(Rw)	  ; \
+	ADD	Rt0, Re, Re
+
+	// t1 = (b & c) | ((~b) & d)
+#define FUNC1(Ra, Rb, Rc, Rd, Re) \
+	MVN	Rb, Rt1	   ; \
+	AND	Rb, Rc, Rt0  ; \
+	AND	Rd, Rt1, Rt1 ; \
+	ORR	Rt0, Rt1, Rt1
+
+	// t1 = b ^ c ^ d
+#define FUNC2(Ra, Rb, Rc, Rd, Re) \
+	EOR	Rb, Rc, Rt1 ; \
+	EOR	Rd, Rt1, Rt1
+
+	// t1 = (b & c) | (b & d) | (c & d) =
+	// t1 = (b & c) | ((b | c) & d)
+#define FUNC3(Ra, Rb, Rc, Rd, Re) \
+	ORR	Rb, Rc, Rt0  ; \
+	AND	Rb, Rc, Rt1  ; \
+	AND	Rd, Rt0, Rt0 ; \
+	ORR	Rt0, Rt1, Rt1
+
+#define FUNC4 FUNC2
+
+	// a5 := a<<5 | a>>(32-5)
+	// b = b<<30 | b>>(32-30)
+	// e = a5 + t1 + e + const
+#define MIX(Ra, Rb, Rc, Rd, Re) \
+	ADD	Rt1, Re, Re	 ; \
+	MOVW	Rb@>(32-30), Rb	 ; \
+	ADD	Ra@>(32-5), Re, Re ; \
+	ADD	Rconst, Re, Re
+
+#define ROUND1(Ra, Rb, Rc, Rd, Re) \
+	LOAD(Re)		; \
+	FUNC1(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+#define ROUND1x(Ra, Rb, Rc, Rd, Re) \
+	SHUFFLE(Re)	; \
+	FUNC1(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+#define ROUND2(Ra, Rb, Rc, Rd, Re) \
+	SHUFFLE(Re)	; \
+	FUNC2(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+#define ROUND3(Ra, Rb, Rc, Rd, Re) \
+	SHUFFLE(Re)	; \
+	FUNC3(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+#define ROUND4(Ra, Rb, Rc, Rd, Re) \
+	SHUFFLE(Re)	; \
+	FUNC4(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+
+// func block(dig *digest, p []byte)
+TEXT	·block(SB), 0, $352-16
+	MOVW	p+4(FP), Rdata	// pointer to the data
+	MOVW	p_len+8(FP), Rt0	// number of bytes
+	ADD	Rdata, Rt0
+	MOVW	Rt0, p_end	// pointer to end of data
+
+	// Load up initial SHA-1 accumulator
+	MOVW	dig+0(FP), Rt0
+	MOVM.IA (Rt0), [Ra,Rb,Rc,Rd,Re]
+
+loop:
+	// Save registers at SP+4 onwards
+	
+	ADD     $4, R13, Rt0
+	MOVM.IA [Ra,Rb,Rc,Rd,Re], (Rt0)
+
+	MOVW	$w_buf, Rw
+	MOVW	$0x5A827999, Rconst
+	MOVW	$3, Rctr
+loop1:	ROUND1(Ra, Rb, Rc, Rd, Re)
+	ROUND1(Re, Ra, Rb, Rc, Rd)
+	ROUND1(Rd, Re, Ra, Rb, Rc)
+	ROUND1(Rc, Rd, Re, Ra, Rb)
+	ROUND1(Rb, Rc, Rd, Re, Ra)
+	SUB.S	$1, Rctr
+	BNE	loop1
+
+	ROUND1(Ra, Rb, Rc, Rd, Re)
+	ROUND1x(Re, Ra, Rb, Rc, Rd)
+	ROUND1x(Rd, Re, Ra, Rb, Rc)
+	ROUND1x(Rc, Rd, Re, Ra, Rb)
+	ROUND1x(Rb, Rc, Rd, Re, Ra)
+
+	MOVW	$0x6ED9EBA1, Rconst
+	MOVW	$4, Rctr
+loop2:	ROUND2(Ra, Rb, Rc, Rd, Re)
+	ROUND2(Re, Ra, Rb, Rc, Rd)
+	ROUND2(Rd, Re, Ra, Rb, Rc)
+	ROUND2(Rc, Rd, Re, Ra, Rb)
+	ROUND2(Rb, Rc, Rd, Re, Ra)
+	SUB.S	$1, Rctr
+	BNE	loop2
+
+	MOVW	$0x8F1BBCDC, Rconst
+	MOVW	$4, Rctr
+loop3:	ROUND3(Ra, Rb, Rc, Rd, Re)
+	ROUND3(Re, Ra, Rb, Rc, Rd)
+	ROUND3(Rd, Re, Ra, Rb, Rc)
+	ROUND3(Rc, Rd, Re, Ra, Rb)
+	ROUND3(Rb, Rc, Rd, Re, Ra)
+	SUB.S	$1, Rctr
+	BNE	loop3
+
+	MOVW	$0xCA62C1D6, Rconst
+	MOVW	$4, Rctr
+loop4:	ROUND4(Ra, Rb, Rc, Rd, Re)
+	ROUND4(Re, Ra, Rb, Rc, Rd)
+	ROUND4(Rd, Re, Ra, Rb, Rc)
+	ROUND4(Rc, Rd, Re, Ra, Rb)
+	ROUND4(Rb, Rc, Rd, Re, Ra)
+	SUB.S	$1, Rctr
+	BNE	loop4
+
+	// Accumulate - restoring registers from SP+4
+	ADD     $4, R13, Rt0
+	MOVM.IA (Rt0), [Rt0,Rt1,Rt2,Rctr,Rw]
+	ADD	Rt0, Ra
+	ADD	Rt1, Rb
+	ADD	Rt2, Rc
+	ADD	Rctr, Rd
+	ADD	Rw, Re
+
+	MOVW	p_end, Rt0
+	CMP	Rt0, Rdata
+	BLO	loop
+
+	// Save final SHA-1 accumulator
+	MOVW	dig+0(FP), Rt0
+	MOVM.IA [Ra,Rb,Rc,Rd,Re], (Rt0)
+
+	RET
diff --git a/src/embedded/arch/cortexm/systim/asm_thumb.s b/src/embedded/arch/cortexm/systim/asm_thumb.s
new file mode 100644
index 0000000000..8a8cecf7bd
--- /dev/null
+++ b/src/embedded/arch/cortexm/systim/asm_thumb.s
@@ -0,0 +1,26 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+#define ICSR_ADDR 0xE000ED04
+#define ICSR_PENDSVSET (1<<28)
+
+TEXT SysTick_Handler(SB),NOSPLIT|NOFRAME,$0-0
+	// set PendSV bit first to avoid DSB but ensure exception tail-chaining
+	MOVW  $ICSR_ADDR, R0
+	MOVW  $ICSR_PENDSVSET, R1
+	MOVW  R1, (R0)
+	SEV   // see ARM Errata 563915
+
+	// increment systim.reloadcnt (64-bit counter)
+	MOVW   $·systim(SB), R0
+	MOVW   (R0), R1
+	ADD.S  $1, R1
+	MOVW   R1, (R0)
+	RET.CC
+	MOVW  4(R0), R1
+	ADD   $1, R1
+	MOVW  R1, 4(R0)
+	RET
diff --git a/src/embedded/arch/cortexm/systim/systim_thumb.go b/src/embedded/arch/cortexm/systim/systim_thumb.go
new file mode 100644
index 0000000000..8cdbf2e6a5
--- /dev/null
+++ b/src/embedded/arch/cortexm/systim/systim_thumb.go
@@ -0,0 +1,75 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package systim iplements ticking system timer using the ARMv7-M SysTick
+// peripheral. The SysTick limitations do not allow to implement precise
+// tickless system timer.
+//
+// This implementation uses non-atomic loads of 64-bit counter and for this
+// reason it can be used only in uniprocessor system and only if the Nanotime
+// function is called with priority lower or same as the SysTick exception.
+package systim
+
+import (
+	"embedded/rtos"
+	"internal/cpu/cortexm/scb"
+	"internal/cpu/cortexm/systick"
+	_ "unsafe"
+)
+
+var systim struct {
+	reloadcnt int64 // must be the first field
+	periodns  int64
+}
+
+// Nanotime returns
+//go:nosplit
+func Nanotime() int64 {
+	st := systick.SYSTICK()
+	var downtick uint32
+	reloadcnt := systim.reloadcnt // non-atomic!
+	for {
+		downtick = uint32(st.CVR.LoadBits(systick.CURRENT))
+		reloadcnt1 := systim.reloadcnt // non-atomic!
+		if reloadcnt1 == reloadcnt {
+			break
+		}
+		reloadcnt = reloadcnt1
+	}
+	// the SysTick asserts an exception when the CURRENT changes from 1 to 0
+	periodtick := uint32(st.RVR.LoadBits(systick.RELOAD) + 1)
+	tick := uint32(0)
+	if downtick != 0 {
+		tick = periodtick - downtick
+	}
+	return systim.periodns*reloadcnt +
+		systim.periodns*int64(tick)/int64(periodtick)
+}
+
+// Setup setups Cortex-M SysTick timer to work as sytem timer.
+//  periodns - number of nanoseconds between SysTick interrupts,
+//  clkhz    - frequency of SysTick clock source,
+//  external - clock source (true: external clock, false: CPU clock).
+// Setup must be run in privileged mode.
+func Setup(periodns, clkhz int64, external bool) {
+	st := systick.SYSTICK()
+	if periodns <= 0 || clkhz <= 0 {
+		st.CSR.ClearBits(systick.ENABLE | systick.TICKINT)
+		systim.periodns = 0
+		return
+	}
+	// Set SysTick exception priority accortding to rtos package.
+	scb.SCB().SHPR3.StoreBits(scb.PRI_SysTick, 255-rtos.IntPrioSysTimer)
+	systim.periodns = periodns
+	systim.reloadcnt = 1 // ensure that nanotime never return zero
+	periodtick := uint32((periodns*clkhz + 5e8) / 1e9)
+	st.RVR.StoreBits(systick.RELOAD, systick.RVR(periodtick-1))
+	st.CVR.StoreBits(systick.CURRENT, 0)
+
+	cfg := systick.ENABLE | systick.TICKINT
+	if !external {
+		cfg |= systick.CLKSOURCE
+	}
+	st.CSR.Store(systick.ENABLE | systick.TICKINT)
+}
diff --git a/src/embedded/arch/riscv/systim/systim_riscv64.go b/src/embedded/arch/riscv/systim/systim_riscv64.go
new file mode 100644
index 0000000000..fd71960056
--- /dev/null
+++ b/src/embedded/arch/riscv/systim/systim_riscv64.go
@@ -0,0 +1,57 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package systim iplements tickless system timer using the RISC-V CLINT timer.
+package systim
+
+import (
+	"embedded/rtos"
+	"internal/cpu/riscv/clint"
+	_ "unsafe" // for linkname
+)
+
+var timerHz uint64
+
+// Setup setups CLINT timer to work as sytem timer.
+//  clkhz - frequency of CLINT timer clock source
+func Setup(clkhz int64) {
+	timerHz = uint64(clkhz)
+	rtos.SetSystemTimer(nanotime, setAlarm)
+}
+
+//go:nosplit
+func nanotime() int64 {
+	return int64(mulDiv(clint.CLINT().MTIME.U64.Load(), 1e9, timerHz))
+}
+
+//go:nosplit
+func setAlarm(ns int64) {
+	timecmp := uint64(1<<64 - 1)
+	if ns >= 0 {
+		timecmp = mulDivUp(uint64(ns), timerHz, 1e9)
+	}
+	clint.CLINT().MTIMECMP[cpuid()].U64.Store(timecmp)
+}
+
+//go:nosplit
+func mulDiv(x, m, d uint64) uint64 {
+	divx := x / d
+	modx := x - divx*d
+	divm := m / d
+	modm := m - divm*d
+	return divx*m + modx*divm + modx*modm/d
+}
+
+//go:nosplit
+func mulDivUp(x, m, d uint64) uint64 {
+	o := d - 1
+	divx := (x + o) / d
+	modx := x - divx*d
+	divm := (m + o) / d
+	modm := m - divm*d
+	return divx*m + modx*divm + (modx*modm+o)/d
+}
+
+//go:linkname cpuid runtime.cpuid
+func cpuid() int
diff --git a/src/embedded/doc.go b/src/embedded/doc.go
new file mode 100644
index 0000000000..cb634ecc6a
--- /dev/null
+++ b/src/embedded/doc.go
@@ -0,0 +1,3 @@
+// Embedded programming with Go
+
+package embedded
diff --git a/src/embedded/mmio/asm_386.s b/src/embedded/mmio/asm_386.s
new file mode 100644
index 0000000000..2af0c0590d
--- /dev/null
+++ b/src/embedded/mmio/asm_386.s
@@ -0,0 +1,41 @@
+#include "textflag.h"
+
+TEXT ·load32(SB),NOSPLIT,$0-12
+	MOVL  addr+0(FP), AX
+	MOVL  (AX), BX
+	MOVL  BX, ret+4(FP)
+	RET
+
+TEXT ·load16(SB),NOSPLIT,$0-10
+	MOVL  addr+0(FP), AX
+	MOVW  (AX), BX
+	MOVW  BX, ret+4(FP)
+	RET
+
+TEXT ·load8(SB),NOSPLIT,$0-9
+	MOVL  addr+0(FP), AX
+	MOVB  (AX), BX
+	MOVB  BX, ret+4(FP)
+	RET
+
+TEXT ·store32(SB),NOSPLIT,$0-12
+	MOVL  addr+0(FP), AX
+	MOVL  v+4(FP), BX
+	MOVL  BX, (AX)
+	RET
+
+TEXT ·store16(SB),NOSPLIT,$0-10
+	MOVL  addr+0(FP), AX
+	MOVW  v+4(FP), BX
+	MOVW  BX, (AX)
+	RET
+
+TEXT ·store8(SB),NOSPLIT,$0-9
+	MOVL  addr+0(FP), AX
+	MOVB  v+4(FP), BX
+	MOVB  BX, (AX)
+	RET
+
+TEXT ·MB(SB),NOSPLIT,$0
+	// BUG: memory barrier instruction
+	RET
diff --git a/src/embedded/mmio/asm_amd64.s b/src/embedded/mmio/asm_amd64.s
new file mode 100644
index 0000000000..56b1702be1
--- /dev/null
+++ b/src/embedded/mmio/asm_amd64.s
@@ -0,0 +1,41 @@
+#include "textflag.h"
+
+TEXT ·load32(SB),NOSPLIT,$0-12
+	MOVQ  addr+0(FP), AX
+	MOVL  (AX), BX
+	MOVL  BX, ret+4(FP)
+	RET
+
+TEXT ·load16(SB),NOSPLIT,$0-10
+	MOVQ  addr+0(FP), AX
+	MOVW  (AX), BX
+	MOVW  BX, ret+4(FP)
+	RET
+
+TEXT ·load8(SB),NOSPLIT,$0-9
+	MOVQ  addr+0(FP), AX
+	MOVB  (AX), BX
+	MOVB  BX, ret+4(FP)
+	RET
+
+TEXT ·store32(SB),NOSPLIT,$0-12
+	MOVQ  addr+0(FP), AX
+	MOVL  v+4(FP), BX
+	MOVL  BX, (AX)
+	RET
+
+TEXT ·store16(SB),NOSPLIT,$0-10
+	MOVQ  addr+0(FP), AX
+	MOVW  v+4(FP), BX
+	MOVW  BX, (AX)
+	RET
+
+TEXT ·store8(SB),NOSPLIT,$0-9
+	MOVQ  addr+0(FP), AX
+	MOVB  v+4(FP), BX
+	MOVB  BX, (AX)
+	RET
+
+TEXT ·MB(SB),NOSPLIT,$0
+	// BUG: memory barrier instruction
+	RET
diff --git a/src/embedded/mmio/asm_arm.s b/src/embedded/mmio/asm_arm.s
new file mode 100644
index 0000000000..01f6d6c06e
--- /dev/null
+++ b/src/embedded/mmio/asm_arm.s
@@ -0,0 +1,43 @@
+#include "textflag.h"
+
+TEXT ·load32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  (R0), R1
+	MOVW  R1, ret+4(FP)
+	RET
+
+TEXT ·load16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  (R0), R1
+	MOVH   R1, ret+4(FP)
+	RET
+
+TEXT ·load8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  (R0), R1
+	MOVB   R1, ret+4(FP)
+	RET
+
+TEXT ·store32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  v+4(FP), R1
+	MOVW  R1, (R0)
+	RET
+
+TEXT ·store16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  v+4(FP), R1
+	MOVH   R1, (R0)
+	RET
+
+TEXT ·store8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  v+4(FP), R1
+	MOVB   R1, (R0)
+	RET
+
+TEXT ·MB(SB),NOSPLIT,$0
+	// use DSB instead of DMB because an IO access can affect CPU directly (eg:
+	// generate interrupt, change CPU behavior via memory-mapped control reg.)
+	DMB // BUG: DSB not supported
+	RET
diff --git a/src/embedded/mmio/asm_arm64.s b/src/embedded/mmio/asm_arm64.s
new file mode 100644
index 0000000000..deda63517c
--- /dev/null
+++ b/src/embedded/mmio/asm_arm64.s
@@ -0,0 +1,43 @@
+#include "textflag.h"
+
+TEXT ·load32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  (R0), R1
+	MOVW  R1, ret+4(FP)
+	RET
+
+TEXT ·load16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  (R0), R1
+	MOVH   R1, ret+4(FP)
+	RET
+
+TEXT ·load8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  (R0), R1
+	MOVB   R1, ret+4(FP)
+	RET
+
+TEXT ·store32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  v+4(FP), R1
+	MOVW  R1, (R0)
+	RET
+
+TEXT ·store16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  v+4(FP), R1
+	MOVH   R1, (R0)
+	RET
+
+TEXT ·store8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  v+4(FP), R1
+	MOVB   R1, (R0)
+	RET
+
+TEXT ·MB(SB),NOSPLIT,$0
+	// use DSB instead of DMB because an IO access can affect CPU directly (eg:
+	// generate interrupt, change CPU behavior via memory-mapped control reg.)
+	DSB $0xF
+	RET
diff --git a/src/embedded/mmio/asm_riscv64.s b/src/embedded/mmio/asm_riscv64.s
new file mode 100644
index 0000000000..c972d078ec
--- /dev/null
+++ b/src/embedded/mmio/asm_riscv64.s
@@ -0,0 +1,56 @@
+#include "textflag.h"
+
+
+TEXT ·load64(SB),NOSPLIT,$0-8
+	MOV  addr+0(FP), A0
+	MOV  (A0), A1
+	MOV  A1, ret+8(FP)
+	RET
+
+TEXT ·load32(SB),NOSPLIT,$0-8
+	MOV    addr+0(FP), A0
+	MOVWU  (A0), A1
+	MOVW   A1, ret+8(FP)
+	RET
+
+TEXT ·load16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), A0
+	MOVHU  (A0), A1
+	MOVH   A1, ret+8(FP)
+	RET
+
+TEXT ·load8(SB),NOSPLIT,$0-5
+	MOV    addr+0(FP), A0
+	MOVBU  (A0), A1
+	MOVB   A1, ret+8(FP)
+	RET
+
+TEXT ·store64(SB),NOSPLIT,$0-8
+	MOV  addr+0(FP), A0
+	MOV  v+8(FP), A1
+	MOV  A1, (A0)
+	RET
+
+TEXT ·store32(SB),NOSPLIT,$0-8
+	MOV    addr+0(FP), A0
+	MOVWU  v+8(FP), A1
+	MOVW   A1, (A0)
+	RET
+
+TEXT ·store16(SB),NOSPLIT,$0-6
+	MOV    addr+0(FP), A0
+	MOVHU  v+8(FP), A1
+	MOVH   A1, (A0)
+	RET
+
+TEXT ·store8(SB),NOSPLIT,$0-5
+	MOV    addr+0(FP), A0
+	MOVBU  v+8(FP), A1
+	MOVB   A1, (A0)
+	RET
+
+#define FENCE WORD $0x0ff0000f
+
+TEXT ·MB(SB),NOSPLIT,$0
+	FENCE
+	RET
diff --git a/src/embedded/mmio/asm_thumb.s b/src/embedded/mmio/asm_thumb.s
new file mode 100644
index 0000000000..e81563f556
--- /dev/null
+++ b/src/embedded/mmio/asm_thumb.s
@@ -0,0 +1,43 @@
+#include "textflag.h"
+
+TEXT ·load32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  (R0), R1
+	MOVW  R1, ret+4(FP)
+	RET
+
+TEXT ·load16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  (R0), R1
+	MOVH   R1, ret+4(FP)
+	RET
+
+TEXT ·load8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  (R0), R1
+	MOVB   R1, ret+4(FP)
+	RET
+
+TEXT ·store32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  v+4(FP), R1
+	MOVW  R1, (R0)
+	RET
+
+TEXT ·store16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  v+4(FP), R1
+	MOVH   R1, (R0)
+	RET
+
+TEXT ·store8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  v+4(FP), R1
+	MOVB   R1, (R0)
+	RET
+
+TEXT ·MB(SB),NOSPLIT,$0
+	 // use DSB instead of DMB because an IO access can affect CPU directly (eg:
+	 // generate interrupt, change CPU behavior via memory-mapped control reg.)
+	DSB 
+	RET
diff --git a/src/embedded/mmio/doc.go b/src/embedded/mmio/doc.go
new file mode 100644
index 0000000000..7d5839ff8c
--- /dev/null
+++ b/src/embedded/mmio/doc.go
@@ -0,0 +1,12 @@
+// Package mmio provides data types and methods that can be used to define and
+// access memory mapped registers of peripherals in embedded systems.
+//
+// You can not use ordinary load/store operations to access memory mapped
+// peripheral registers because these operations can be reordered or optimized
+// out by the compiler. This is unacceptable because in case of MMIO the order
+// of accessing registers matters and even reading from register can have side
+// effects.
+//
+// This package has some support from compiler to ensure order and speed of
+// I/O operations.
+package mmio
\ No newline at end of file
diff --git a/src/embedded/mmio/gen.sh b/src/embedded/mmio/gen.sh
new file mode 100755
index 0000000000..68602fe837
--- /dev/null
+++ b/src/embedded/mmio/gen.sh
@@ -0,0 +1,5 @@
+#!/bin/sh
+
+sed 's/32/16/g' mmio32.go >mmio16.go
+sed 's/32/8/g' mmio32.go >mmio8.go
+sed 's/32/64/g' mmio32.go >mmio64.go
\ No newline at end of file
diff --git a/src/embedded/mmio/mb.go b/src/embedded/mmio/mb.go
new file mode 100644
index 0000000000..d9cb0cdcb9
--- /dev/null
+++ b/src/embedded/mmio/mb.go
@@ -0,0 +1,16 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+// MB is a full memory barrier. It ensures that any memory access (normal or
+// I/O) after it, in program order, do not execute until all explicit memory
+// accesses before it complete.
+//
+// Many embedded systems ensure the access order within their I/O address space
+// but the order between load/store in normal RAM and I/O memory is not
+// guaranteed mainly because of different buses for I/O and RAM. Some systems
+// only ensure the access order to registers of the same peripheral. Use MB to
+// properly order operations on different peripherals and RAM.
+func MB()
diff --git a/src/embedded/mmio/mmio16.go b/src/embedded/mmio/mmio16.go
new file mode 100644
index 0000000000..1a0f1b2685
--- /dev/null
+++ b/src/embedded/mmio/mmio16.go
@@ -0,0 +1,106 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+import "unsafe"
+
+//go:noescape
+func load16(addr *uint16) uint16
+
+//go:noescape
+func store16(addr *uint16, v uint16)
+
+// An U16 represents 16-bit memory mapped register.
+//go:notinheap
+type U16 struct {
+	r uint16
+}
+
+// Addr returns the address of r as uintptr.
+func (r *U16) Addr() uintptr {
+	return uintptr(unsafe.Pointer(r))
+}
+
+// SetBit sets n-th bit in r. This is not an atomic operation.
+func (r *U16) SetBit(n int) {
+	store16(&r.r, load16(&r.r)|uint16(1)<<uint(n))
+}
+
+// ClearBit clears n-th bit in r. This is not an atomic operation.
+func (r *U16) ClearBit(n int) {
+	store16(&r.r, load16(&r.r)&^uint16(1)<<uint(n))
+}
+
+// Bit returns the value of n-th bit in r (0 or 1).
+func (r *U16) LoadBit(n int) int {
+	return int(load16(&r.r)>>uint(n)) & 1
+}
+
+// StoreBit sets the value of n-th bit in r to least significant bit of v. This
+// is not an atomic operation.
+func (r *U16) StoreBit(n, v int) {
+	mask := uint16(1) << uint(n)
+	store16(&r.r, load16(&r.r)&^mask|uint16(v<<uint(n))&mask)
+}
+
+// Bits returns the value od r logicaly anded with mask. It is a convenient
+// replacement for r.Load()&mask.
+func (r *U16) LoadBits(mask uint16) uint16 {
+	return load16(&r.r) & mask
+}
+
+// StoreBits stores bits in r selected by mask. It is convenient replacement for
+// r.Store(r.Load()&^mask | bits&mask). This is not an atomic operation.
+func (r *U16) StoreBits(mask, bits uint16) {
+	store16(&r.r, load16(&r.r)&^mask|bits&mask)
+}
+
+// SetBits sets bits in r selected by mask. This is not an atomic operation.
+func (r *U16) SetBits(mask uint16) {
+	store16(&r.r, load16(&r.r)|mask)
+}
+
+// ClearBits clears bits in r selected by mask. This is not an atomic operation.
+func (r *U16) ClearBits(mask uint16) {
+	store16(&r.r, load16(&r.r)&^mask)
+}
+
+// Load returns the value of r.
+func (r *U16) Load() uint16 {
+	return load16(&r.r)
+}
+
+// Store stores v in r.
+func (r *U16) Store(v uint16) {
+	store16(&r.r, v)
+}
+
+//func (r *U16) Field(mask uint16) int {
+//	return bits.Field16(r.r, mask)
+//}
+//func (r *U16) SetField(mask uint16, v int) {
+//	r.StoreBits(mask, bits.MakeField16(v, mask))
+//}
+
+// An UM16 represents a set of bits in R selected by Mask.
+type UM16 struct {
+	R    *U16
+	Mask uint16
+}
+
+// Set sets all bits in b. This is not an atomic operation.
+func (b UM16) Set() { b.R.SetBits(b.Mask) }
+
+// Clear clears all bits in b. This is not an atomic operation.
+func (b UM16) Clear() { b.R.ClearBits(b.Mask) }
+
+// Load returns the value of b.
+func (b UM16) Load() uint16 { return b.R.LoadBits(b.Mask) }
+
+// Store stores bits in b. This is not an atomic operation.
+func (b UM16) Store(bits uint16) { b.R.StoreBits(b.Mask, bits) }
+
+//func (b UM16) LoadVal() int   { return b.R.Field(uint16(b.Mask)) }
+//func (b UM16) StoreVal(v int) { b.R.SetField(b.Mask, v) }
diff --git a/src/embedded/mmio/mmio32.go b/src/embedded/mmio/mmio32.go
new file mode 100644
index 0000000000..6a821a7cc4
--- /dev/null
+++ b/src/embedded/mmio/mmio32.go
@@ -0,0 +1,106 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+import "unsafe"
+
+//go:noescape
+func load32(addr *uint32) uint32
+
+//go:noescape
+func store32(addr *uint32, v uint32)
+
+// An U32 represents 32-bit memory mapped register.
+//go:notinheap
+type U32 struct {
+	r uint32
+}
+
+// Addr returns the address of r as uintptr.
+func (r *U32) Addr() uintptr {
+	return uintptr(unsafe.Pointer(r))
+}
+
+// SetBit sets n-th bit in r. This is not an atomic operation.
+func (r *U32) SetBit(n int) {
+	store32(&r.r, load32(&r.r)|uint32(1)<<uint(n))
+}
+
+// ClearBit clears n-th bit in r. This is not an atomic operation.
+func (r *U32) ClearBit(n int) {
+	store32(&r.r, load32(&r.r)&^uint32(1)<<uint(n))
+}
+
+// Bit returns the value of n-th bit in r (0 or 1).
+func (r *U32) LoadBit(n int) int {
+	return int(load32(&r.r)>>uint(n)) & 1
+}
+
+// StoreBit sets the value of n-th bit in r to least significant bit of v. This
+// is not an atomic operation.
+func (r *U32) StoreBit(n, v int) {
+	mask := uint32(1) << uint(n)
+	store32(&r.r, load32(&r.r)&^mask|uint32(v<<uint(n))&mask)
+}
+
+// Bits returns the value od r logicaly anded with mask. It is a convenient
+// replacement for r.Load()&mask.
+func (r *U32) LoadBits(mask uint32) uint32 {
+	return load32(&r.r) & mask
+}
+
+// StoreBits stores bits in r selected by mask. It is convenient replacement for
+// r.Store(r.Load()&^mask | bits&mask). This is not an atomic operation.
+func (r *U32) StoreBits(mask, bits uint32) {
+	store32(&r.r, load32(&r.r)&^mask|bits&mask)
+}
+
+// SetBits sets bits in r selected by mask. This is not an atomic operation.
+func (r *U32) SetBits(mask uint32) {
+	store32(&r.r, load32(&r.r)|mask)
+}
+
+// ClearBits clears bits in r selected by mask. This is not an atomic operation.
+func (r *U32) ClearBits(mask uint32) {
+	store32(&r.r, load32(&r.r)&^mask)
+}
+
+// Load returns the value of r.
+func (r *U32) Load() uint32 {
+	return load32(&r.r)
+}
+
+// Store stores v in r.
+func (r *U32) Store(v uint32) {
+	store32(&r.r, v)
+}
+
+//func (r *U32) Field(mask uint32) int {
+//	return bits.Field32(r.r, mask)
+//}
+//func (r *U32) SetField(mask uint32, v int) {
+//	r.StoreBits(mask, bits.MakeField32(v, mask))
+//}
+
+// An UM32 represents a set of bits in R selected by Mask.
+type UM32 struct {
+	R    *U32
+	Mask uint32
+}
+
+// Set sets all bits in b. This is not an atomic operation.
+func (b UM32) Set() { b.R.SetBits(b.Mask) }
+
+// Clear clears all bits in b. This is not an atomic operation.
+func (b UM32) Clear() { b.R.ClearBits(b.Mask) }
+
+// Load returns the value of b.
+func (b UM32) Load() uint32 { return b.R.LoadBits(b.Mask) }
+
+// Store stores bits in b. This is not an atomic operation.
+func (b UM32) Store(bits uint32) { b.R.StoreBits(b.Mask, bits) }
+
+//func (b UM32) LoadVal() int   { return b.R.Field(uint32(b.Mask)) }
+//func (b UM32) StoreVal(v int) { b.R.SetField(b.Mask, v) }
diff --git a/src/embedded/mmio/mmio64.go b/src/embedded/mmio/mmio64.go
new file mode 100644
index 0000000000..fde5054459
--- /dev/null
+++ b/src/embedded/mmio/mmio64.go
@@ -0,0 +1,106 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+import "unsafe"
+
+//go:noescape
+func load64(addr *uint64) uint64
+
+//go:noescape
+func store64(addr *uint64, v uint64)
+
+// An U64 represents 64-bit memory mapped register.
+//go:notinheap
+type U64 struct {
+	r uint64
+}
+
+// Addr returns the address of r as uintptr.
+func (r *U64) Addr() uintptr {
+	return uintptr(unsafe.Pointer(r))
+}
+
+// SetBit sets n-th bit in r. This is not an atomic operation.
+func (r *U64) SetBit(n int) {
+	store64(&r.r, load64(&r.r)|uint64(1)<<uint(n))
+}
+
+// ClearBit clears n-th bit in r. This is not an atomic operation.
+func (r *U64) ClearBit(n int) {
+	store64(&r.r, load64(&r.r)&^uint64(1)<<uint(n))
+}
+
+// Bit returns the value of n-th bit in r (0 or 1).
+func (r *U64) LoadBit(n int) int {
+	return int(load64(&r.r)>>uint(n)) & 1
+}
+
+// StoreBit sets the value of n-th bit in r to least significant bit of v. This
+// is not an atomic operation.
+func (r *U64) StoreBit(n, v int) {
+	mask := uint64(1) << uint(n)
+	store64(&r.r, load64(&r.r)&^mask|uint64(v<<uint(n))&mask)
+}
+
+// Bits returns the value od r logicaly anded with mask. It is a convenient
+// replacement for r.Load()&mask.
+func (r *U64) LoadBits(mask uint64) uint64 {
+	return load64(&r.r) & mask
+}
+
+// StoreBits stores bits in r selected by mask. It is convenient replacement for
+// r.Store(r.Load()&^mask | bits&mask). This is not an atomic operation.
+func (r *U64) StoreBits(mask, bits uint64) {
+	store64(&r.r, load64(&r.r)&^mask|bits&mask)
+}
+
+// SetBits sets bits in r selected by mask. This is not an atomic operation.
+func (r *U64) SetBits(mask uint64) {
+	store64(&r.r, load64(&r.r)|mask)
+}
+
+// ClearBits clears bits in r selected by mask. This is not an atomic operation.
+func (r *U64) ClearBits(mask uint64) {
+	store64(&r.r, load64(&r.r)&^mask)
+}
+
+// Load returns the value of r.
+func (r *U64) Load() uint64 {
+	return load64(&r.r)
+}
+
+// Store stores v in r.
+func (r *U64) Store(v uint64) {
+	store64(&r.r, v)
+}
+
+//func (r *U64) Field(mask uint64) int {
+//	return bits.Field64(r.r, mask)
+//}
+//func (r *U64) SetField(mask uint64, v int) {
+//	r.StoreBits(mask, bits.MakeField64(v, mask))
+//}
+
+// An UM64 represents a set of bits in R selected by Mask.
+type UM64 struct {
+	R    *U64
+	Mask uint64
+}
+
+// Set sets all bits in b. This is not an atomic operation.
+func (b UM64) Set() { b.R.SetBits(b.Mask) }
+
+// Clear clears all bits in b. This is not an atomic operation.
+func (b UM64) Clear() { b.R.ClearBits(b.Mask) }
+
+// Load returns the value of b.
+func (b UM64) Load() uint64 { return b.R.LoadBits(b.Mask) }
+
+// Store stores bits in b. This is not an atomic operation.
+func (b UM64) Store(bits uint64) { b.R.StoreBits(b.Mask, bits) }
+
+//func (b UM64) LoadVal() int   { return b.R.Field(uint64(b.Mask)) }
+//func (b UM64) StoreVal(v int) { b.R.SetField(b.Mask, v) }
diff --git a/src/embedded/mmio/mmio8.go b/src/embedded/mmio/mmio8.go
new file mode 100644
index 0000000000..29bba9b979
--- /dev/null
+++ b/src/embedded/mmio/mmio8.go
@@ -0,0 +1,106 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+import "unsafe"
+
+//go:noescape
+func load8(addr *uint8) uint8
+
+//go:noescape
+func store8(addr *uint8, v uint8)
+
+// An U8 represents 8-bit memory mapped register.
+//go:notinheap
+type U8 struct {
+	r uint8
+}
+
+// Addr returns the address of r as uintptr.
+func (r *U8) Addr() uintptr {
+	return uintptr(unsafe.Pointer(r))
+}
+
+// SetBit sets n-th bit in r. This is not an atomic operation.
+func (r *U8) SetBit(n int) {
+	store8(&r.r, load8(&r.r)|uint8(1)<<uint(n))
+}
+
+// ClearBit clears n-th bit in r. This is not an atomic operation.
+func (r *U8) ClearBit(n int) {
+	store8(&r.r, load8(&r.r)&^uint8(1)<<uint(n))
+}
+
+// Bit returns the value of n-th bit in r (0 or 1).
+func (r *U8) LoadBit(n int) int {
+	return int(load8(&r.r)>>uint(n)) & 1
+}
+
+// StoreBit sets the value of n-th bit in r to least significant bit of v. This
+// is not an atomic operation.
+func (r *U8) StoreBit(n, v int) {
+	mask := uint8(1) << uint(n)
+	store8(&r.r, load8(&r.r)&^mask|uint8(v<<uint(n))&mask)
+}
+
+// Bits returns the value od r logicaly anded with mask. It is a convenient
+// replacement for r.Load()&mask.
+func (r *U8) LoadBits(mask uint8) uint8 {
+	return load8(&r.r) & mask
+}
+
+// StoreBits stores bits in r selected by mask. It is convenient replacement for
+// r.Store(r.Load()&^mask | bits&mask). This is not an atomic operation.
+func (r *U8) StoreBits(mask, bits uint8) {
+	store8(&r.r, load8(&r.r)&^mask|bits&mask)
+}
+
+// SetBits sets bits in r selected by mask. This is not an atomic operation.
+func (r *U8) SetBits(mask uint8) {
+	store8(&r.r, load8(&r.r)|mask)
+}
+
+// ClearBits clears bits in r selected by mask. This is not an atomic operation.
+func (r *U8) ClearBits(mask uint8) {
+	store8(&r.r, load8(&r.r)&^mask)
+}
+
+// Load returns the value of r.
+func (r *U8) Load() uint8 {
+	return load8(&r.r)
+}
+
+// Store stores v in r.
+func (r *U8) Store(v uint8) {
+	store8(&r.r, v)
+}
+
+//func (r *U8) Field(mask uint8) int {
+//	return bits.Field8(r.r, mask)
+//}
+//func (r *U8) SetField(mask uint8, v int) {
+//	r.StoreBits(mask, bits.MakeField8(v, mask))
+//}
+
+// An UM8 represents a set of bits in R selected by Mask.
+type UM8 struct {
+	R    *U8
+	Mask uint8
+}
+
+// Set sets all bits in b. This is not an atomic operation.
+func (b UM8) Set() { b.R.SetBits(b.Mask) }
+
+// Clear clears all bits in b. This is not an atomic operation.
+func (b UM8) Clear() { b.R.ClearBits(b.Mask) }
+
+// Load returns the value of b.
+func (b UM8) Load() uint8 { return b.R.LoadBits(b.Mask) }
+
+// Store stores bits in b. This is not an atomic operation.
+func (b UM8) Store(bits uint8) { b.R.StoreBits(b.Mask, bits) }
+
+//func (b UM8) LoadVal() int   { return b.R.Field(uint8(b.Mask)) }
+//func (b UM8) StoreVal(v int) { b.R.SetField(b.Mask, v) }
diff --git a/src/embedded/rtos/asm_riscv64.s b/src/embedded/rtos/asm_riscv64.s
new file mode 100644
index 0000000000..b313945c7b
--- /dev/null
+++ b/src/embedded/rtos/asm_riscv64.s
@@ -0,0 +1,6 @@
+#include "textflag.h"
+
+// func publicationBarrier()
+TEXT ·publicationBarrier(SB),NOSPLIT|NOFRAME,$0-0
+	WORD  $0x0ff0000f  // FENCE
+	RET
diff --git a/src/embedded/rtos/asm_thumb.s b/src/embedded/rtos/asm_thumb.s
new file mode 100644
index 0000000000..ed9af99c25
--- /dev/null
+++ b/src/embedded/rtos/asm_thumb.s
@@ -0,0 +1,10 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+// func publicationBarrier()
+TEXT ·publicationBarrier(SB),NOSPLIT|NOFRAME,$0-0
+	DMB  MB_ST // must be system wide
+	RET
diff --git a/src/embedded/rtos/errors.go b/src/embedded/rtos/errors.go
new file mode 100644
index 0000000000..da45f6899a
--- /dev/null
+++ b/src/embedded/rtos/errors.go
@@ -0,0 +1,19 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+type Error struct{ s string }
+
+func (e *Error) Error() string { return e.s }
+
+var (
+	ErrUknown         = &Error{"rtos: unknown error"}
+	ErrNotSuppoted    = &Error{"rtos: operation not supported"}
+	ErrBadPrivLevel   = &Error{"rtos: bad privilege level"}
+	ErrInsufPrivLevel = &Error{"rtos: insufficient privilege level"}
+	ErrBadIntNumber   = &Error{"rtos: bad interrupt number"}
+	ErrBadIntPrio     = &Error{"rtos: bad interrupt priority"}
+	ErrBadIntCtx      = &Error{"rtos: bad interrupt context"}
+)
diff --git a/src/embedded/rtos/errors_noos.go b/src/embedded/rtos/errors_noos.go
new file mode 100644
index 0000000000..45b39532be
--- /dev/null
+++ b/src/embedded/rtos/errors_noos.go
@@ -0,0 +1,25 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+var errorsByNumber = [...]*Error{
+	0: ErrUknown,
+	1: ErrNotSuppoted,
+	2: ErrBadPrivLevel,
+	3: ErrInsufPrivLevel,
+	4: ErrBadIntNumber,
+	5: ErrBadIntPrio,
+	6: ErrBadIntCtx,
+}
+
+func errnoError(errno int) error {
+	if errno == 0 {
+		return nil
+	}
+	if uint(errno) > uint(len(errorsByNumber)) {
+		errno = 0
+	}
+	return errorsByNumber[errno]
+}
diff --git a/src/embedded/rtos/irq.go b/src/embedded/rtos/irq.go
new file mode 100644
index 0000000000..1cd23ebc65
--- /dev/null
+++ b/src/embedded/rtos/irq.go
@@ -0,0 +1,71 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package rtos defines seven well known interrupt priority levels among which
+// five are ordered by increasing urgency as follows: IntPrioLowest, IntPrioLow,
+// IntPrioMid, IntPrioHigh, IntPrioHighest. There can be additional priority
+// levels between the defined ones and some or all of them can have the same
+// effecive level.
+//
+// You can use ordinary equality operators to compare interrupt priority levels.
+// The defined priority levels satisfy the following inequality:
+//
+// Lowest <= Low <= IntPrioMid <= High <= Highest
+//
+// Architecture that supports interrupt nesting must ensure the incoming
+// interrupt request can not preempt the interrupt handler that runs with the
+// same or higher priority.
+//
+// The IntPrioSysCall and IntPrioSysTimer are special priority levels. Do not
+// use them to set or calclulate an interrupt priority if their values are
+// outside of [IntPrioLowest, IntPrioHighest] range.
+package rtos
+
+const (
+	IntPrioHighest = intPrioHighest
+	IntPrioHigh    = intPrioHigh
+	IntPrioMid     = intPrioMid
+	IntPrioLow     = intPrioLow
+	IntPrioLowest  = intPrioLowest
+
+	IntPrioSysCall  = intPrioSysCall
+	IntPrioSysTimer = intPrioSysTimer
+
+	IntPrioCurrent = intPrioCurrent
+)
+
+// IRQ represents a source of interrupts. It provides interface to basic
+// operations such as enabling/disabling handling interrupts from this source
+// and setting its priority. There can be other (system) interrupt/exception
+// sources not exposed by this interface.
+type IRQ int
+
+// IntCtx represents an interrupt context. The context has implementation
+// specific meaning, can correspond to CPU, core, hardware thread, privilege
+// level or any combination thereof.
+type IntCtx intCtx
+
+// Enable sets the priority of the interrupt and enables interrupt requests in
+// the context specified by ctxid. The context has implementation specific
+// meaning, can correspond to CPU, core, hardware thread, privilege level or any
+// combination thereof.
+func (irq IRQ) Enable(prio int, ctx IntCtx) error {
+	return irqEnable(irq, prio, ctx)
+}
+
+// Disable disables interrupt requests int the context specified by ctxid.
+func (irq IRQ) Disable(ctx IntCtx) error {
+	return irqDisable(irq, ctx)
+}
+
+// Status reports whether the irq is enabled in context ctxid and returns its
+// priority.
+func (irq IRQ) Status(ctx IntCtx) (enabled bool, prio int, err error) {
+	return irqStatus(irq, ctx)
+}
+
+// HandlerMode reports whether the function is called in interupt handler mode.
+func HandlerMode() bool {
+	return handlerMode()
+}
diff --git a/src/embedded/rtos/irq_noos.go b/src/embedded/rtos/irq_noos.go
new file mode 100644
index 0000000000..a65378f14b
--- /dev/null
+++ b/src/embedded/rtos/irq_noos.go
@@ -0,0 +1,33 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import _ "unsafe"
+
+type intCtx int
+
+func irqEnable(irq IRQ, prio int, ctx IntCtx) error {
+	if uint(prio+1) > intPrioHighest+1 {
+		return ErrBadIntPrio
+	}
+	_, _, errno := runtime_irqctl(int(irq), prio, int(ctx))
+	return errnoError(errno)
+}
+
+func irqDisable(irq IRQ, ctx IntCtx) error {
+	_, _, errno := runtime_irqctl(int(irq), -2, int(ctx))
+	return errnoError(errno)
+}
+
+func irqStatus(irq IRQ, ctx IntCtx) (enabled bool, prio int, err error) {
+	en, prio, errno := runtime_irqctl(int(irq), -3, int(ctx))
+	return en != 0, prio, errnoError(errno)
+}
+
+//go:linkname runtime_irqctl runtime.irqctl
+func runtime_irqctl(irq, ctl, ctxid int) (enabled, prio, errno int)
+
+//go:linkname handlerMode runtime.isr
+func handlerMode() bool
diff --git a/src/embedded/rtos/irq_noos_riscv64.go b/src/embedded/rtos/irq_noos_riscv64.go
new file mode 100644
index 0000000000..75116c379f
--- /dev/null
+++ b/src/embedded/rtos/irq_noos_riscv64.go
@@ -0,0 +1,19 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+// Positive numbers are PLIC priority levels. It seems the lowest number of the
+// priority levels supported by real hardware is 7. CLIC based designs were not
+// considered.
+const (
+	intPrioHighest  = 7
+	intPrioHigh     = 6
+	intPrioMid      = 4
+	intPrioLow      = 2
+	intPrioLowest   = 1
+	intPrioCurrent  = 0
+	intPrioSysTimer = intPrioHighest + 1
+	intPrioSysCall  = intPrioHighest + 1
+)
diff --git a/src/embedded/rtos/irq_noos_thumb.go b/src/embedded/rtos/irq_noos_thumb.go
new file mode 100644
index 0000000000..7c56432c9b
--- /dev/null
+++ b/src/embedded/rtos/irq_noos_thumb.go
@@ -0,0 +1,18 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+const (
+	intPrioHighest  = 255 - 0<<5 // do not use with nRF52 SoftDevice
+	intPrioHigh     = 255 - 1<<5
+	intPrioSysTimer = 255 - 2<<5
+	intPrioMid      = 255 - 3<<5
+	intPrioSysCall  = 255 - 4<<5 // compatible with nRF52 SoftDevice
+	intPrioLow      = 255 - 5<<5
+	intPrioLowest   = 255 - 6<<5
+	intPrioPendSV   = 255 - 255 // unusable because can not wake the scheduler
+
+	intPrioCurrent = -1
+)
diff --git a/src/embedded/rtos/irq_x.go b/src/embedded/rtos/irq_x.go
new file mode 100644
index 0000000000..0668860a22
--- /dev/null
+++ b/src/embedded/rtos/irq_x.go
@@ -0,0 +1,38 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+import _ "unsafe"
+
+const (
+	intPrioHighest  = 0
+	intPrioHigh     = 0
+	intPrioSysTimer = 0
+	intPrioMid      = 0
+	intPrioSysCall  = 0
+	intPrioLow      = 0
+	intPrioLowest   = 0
+	intPrioCurrent  = -1
+)
+
+type intCtx int
+
+func irqEnable(irq IRQ, prio int, ctx IntCtx) error {
+	return ErrNotSuppoted
+}
+
+func irqDisable(irq IRQ, ctx IntCtx) error {
+	return ErrNotSuppoted
+}
+
+func irqStatus(irq IRQ, ctx IntCtx) (enabled bool, prio int, err error) {
+	return false, 0, ErrNotSuppoted
+}
+
+func handlerMode() bool {
+	return false
+}
diff --git a/src/embedded/rtos/nanotime.go b/src/embedded/rtos/nanotime.go
new file mode 100644
index 0000000000..aa6fce0d7d
--- /dev/null
+++ b/src/embedded/rtos/nanotime.go
@@ -0,0 +1,19 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"time"
+	_ "unsafe"
+)
+
+// Nanotime returns a time duration from some event in the past. Typically it's
+// roughly corresponds to the system runtime.
+func Nanotime() time.Duration {
+	return time.Duration(runtime_nanotime())
+}
+
+//go:linkname runtime_nanotime runtime.nanotime
+func runtime_nanotime() int64
diff --git a/src/embedded/rtos/note.go b/src/embedded/rtos/note.go
new file mode 100644
index 0000000000..1e2ba09611
--- /dev/null
+++ b/src/embedded/rtos/note.go
@@ -0,0 +1,48 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"time"
+	_ "unsafe"
+)
+
+// Note allows to communicate the occurrence of an event.
+//
+// Before any calls to Sleep or Wakeup, must call Clear to initialize the Note.
+//
+// Exactly one gorutine can call Sleep but there is allowed to multiple
+// gorutines or interrupt handlers to call Wakeup. Future Sleep will return
+// immediately.
+//
+// Subsequent Clear must be called only after previous Sleep has returned, e.g.
+// it is disallowed to call Clear straight after Wakeup.
+type Note struct {
+	// must be in sync with runtime.notel
+	key  uintptr
+	link uintptr
+}
+
+// Sleep sleeps on the cleared note until other goroutine or interrupt handler
+// call Wakeup or until the timeout.
+func (n *Note) Sleep(timeout time.Duration) bool {
+	return runtime_notetsleepg(n, int64(timeout))
+}
+
+// Wakeup wakeups the goroutine that sleeps on the note. The Wakeup remains in
+// effect until subequent Clear so future Sleep will return immediately.
+func (n *Note) Wakeup() { notewakeup(n) }
+
+// Clear clears the note. Any Wakeup that happened before Clear is forgotten.
+// Clear works as a publication barrier, that is, the Clear itself and any
+// memory writes preceding it in the program order happens before any memory
+// writes that follows it.
+func (n *Note) Clear() {
+	n.key = 0
+	publicationBarrier()
+}
+
+//go:linkname runtime_notetsleepg runtime.notetsleepg
+func runtime_notetsleepg(n *Note, ns int64) bool
diff --git a/src/embedded/rtos/note_noos.go b/src/embedded/rtos/note_noos.go
new file mode 100644
index 0000000000..d5eb336d7b
--- /dev/null
+++ b/src/embedded/rtos/note_noos.go
@@ -0,0 +1,12 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import _ "unsafe"
+
+//go:linkname notewakeup runtime.rtos_notewakeup
+func notewakeup(n *Note)
+
+func publicationBarrier()
\ No newline at end of file
diff --git a/src/embedded/rtos/note_os.go b/src/embedded/rtos/note_os.go
new file mode 100644
index 0000000000..51a42abc2c
--- /dev/null
+++ b/src/embedded/rtos/note_os.go
@@ -0,0 +1,15 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+import _ "unsafe"
+
+//go:linkname notewakeup runtime.notewakeup
+func notewakeup(n *Note)
+
+//go:linkname publicationBarrier runtime.publicationBarrier
+func publicationBarrier()
\ No newline at end of file
diff --git a/src/embedded/rtos/privlevel.go b/src/embedded/rtos/privlevel.go
new file mode 100644
index 0000000000..9ef54d1994
--- /dev/null
+++ b/src/embedded/rtos/privlevel.go
@@ -0,0 +1,15 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+// SetPrivLevel sets privilege level for current thread to newlevel. Level 0 is
+// the most privileged and allows access to all system resources. Any resource
+// available to the level n is also available to the level 0..n. If n < 0 the
+// privilege level is not changed. SetPrivLevel returns previous level number
+// and error. SetPrivLevel should be usually preceded by runtime.LockOSThread()
+// to ensure effect for the current goroutine.
+func SetPrivLevel(newlevel int) (oldlevel int, err error) {
+	return setPrivLevel(newlevel)
+}
diff --git a/src/embedded/rtos/privlevel_noos.go b/src/embedded/rtos/privlevel_noos.go
new file mode 100644
index 0000000000..08dd721f98
--- /dev/null
+++ b/src/embedded/rtos/privlevel_noos.go
@@ -0,0 +1,15 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import _ "unsafe"
+
+func setPrivLevel(newlevel int) (oldlevel int, err error) {
+	oldlevel, errno := runtime_setprivlevel(newlevel)
+	return oldlevel, errnoError(errno)
+}
+
+//go:linkname runtime_setprivlevel runtime.setprivlevel
+func runtime_setprivlevel(newlevel int) (oldlevel, errno int)
diff --git a/src/embedded/rtos/privlevel_x.go b/src/embedded/rtos/privlevel_x.go
new file mode 100644
index 0000000000..2b03a6b291
--- /dev/null
+++ b/src/embedded/rtos/privlevel_x.go
@@ -0,0 +1,11 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+func setPrivLevel(newlevel int) (oldlevel int, err error) {
+	return -1, ErrNotSuppoted
+}
diff --git a/src/embedded/rtos/systim.go b/src/embedded/rtos/systim.go
new file mode 100644
index 0000000000..4ba4e3910f
--- /dev/null
+++ b/src/embedded/rtos/systim.go
@@ -0,0 +1,36 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+// SetSysTimer registers two functions that the thread scheduler uses to
+// communicate with the system timer.
+//
+// Nanotime should return the monotonic time in nanoseconds.
+//
+// Setalarm is called by the scheduler with the ns >= 0 to ask the system timer
+// to wake the current CPU at the specified time. Sheduler can also pass ns < 0
+// if it do not want to be woken up by the system timer. A spurious or
+// inaccurate wakeups are acceptable. Setalarm reports whether the scheduler
+// can put itself to sleep.
+//
+// There are two main types of system timer implementations.
+//
+// The ticking implementation can simply wake up the CPU with a constant period.
+// In this case setalarm can be nil.
+//
+// The tickless implementation schould try to wake up the CPU only once, at ns
+// or just after ns.
+//
+// The System Timer interrupt handler, if used, must run with IntPrioSysTimer
+// priority.
+//
+// SetSystemTimer is intended to be called only once, at the very beginning of
+// the system initialization. If you want to change the system timer
+// implementation at a later stage (which is discouraged) you must ensure a
+// continuity of time and that all gorutines that sleeeps using old setalarm
+// function will be wakeup.
+func SetSystemTimer(nanotime func() int64, setalarm func(ns int64)) error {
+	return setSystemTimer(nanotime, setalarm)
+}
diff --git a/src/embedded/rtos/systim_noos.go b/src/embedded/rtos/systim_noos.go
new file mode 100644
index 0000000000..e45f8ba9d1
--- /dev/null
+++ b/src/embedded/rtos/systim_noos.go
@@ -0,0 +1,23 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"sync"
+	_ "unsafe"
+)
+
+var setsystimmx sync.Mutex
+
+func setSystemTimer(nanotime func() int64, setalarm func(ns int64)) error {
+	setsystimmx.Lock()
+	runtime_setsystim(nanotime, setalarm)
+	setsystimmx.Unlock()
+	return nil
+}
+
+//go:linkname runtime_setsystim runtime.setsystim
+func runtime_setsystim(nanotime func() int64, setalarm func(ns int64))
+
diff --git a/src/embedded/rtos/systim_x.go b/src/embedded/rtos/systim_x.go
new file mode 100644
index 0000000000..13abaa4584
--- /dev/null
+++ b/src/embedded/rtos/systim_x.go
@@ -0,0 +1,11 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+func setSystemTimer(nanotime func() int64, setalarm func(ns int64)) error {
+	return ErrNotSuppoted
+}
diff --git a/src/embedded/rtos/syswriter.go b/src/embedded/rtos/syswriter.go
new file mode 100644
index 0000000000..ebf660e624
--- /dev/null
+++ b/src/embedded/rtos/syswriter.go
@@ -0,0 +1,9 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+func SetSystemWriter(w func(fd int, p []byte) int) error {
+	return setSysWriter(w)
+}
diff --git a/src/embedded/rtos/syswriter_noos.go b/src/embedded/rtos/syswriter_noos.go
new file mode 100644
index 0000000000..1ddd5a921f
--- /dev/null
+++ b/src/embedded/rtos/syswriter_noos.go
@@ -0,0 +1,22 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"sync"
+	_ "unsafe"
+)
+
+var setsyswritermx sync.Mutex
+
+func setSysWriter(w func(fd int, p []byte) int) error {
+	setsyswritermx.Lock()
+	runtime_setsyswriter(w)
+	setsyswritermx.Unlock()
+	return nil
+}
+
+//go:linkname runtime_setsyswriter runtime.setsyswriter
+func runtime_setsyswriter(w func(fd int, p []byte) int)
diff --git a/src/embedded/rtos/syswriter_x.go b/src/embedded/rtos/syswriter_x.go
new file mode 100644
index 0000000000..7f6a6ff454
--- /dev/null
+++ b/src/embedded/rtos/syswriter_x.go
@@ -0,0 +1,11 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+func setSysWriter(w func(fd int, p []byte) int) error {
+	return ErrNotSuppoted
+}
diff --git a/src/embedded/rtos/utils.go b/src/embedded/rtos/utils.go
new file mode 100644
index 0000000000..e110659f27
--- /dev/null
+++ b/src/embedded/rtos/utils.go
@@ -0,0 +1,10 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import "unsafe"
+
+// copied from /home/michal/P/go/goroot/src/runtime/stubs.go
+func bool2int(x bool) int { return int(uint8(*(*uint8)(unsafe.Pointer(&x)))) }
diff --git a/src/embedded/rtos/vfs.go b/src/embedded/rtos/vfs.go
new file mode 100644
index 0000000000..f7aae7e502
--- /dev/null
+++ b/src/embedded/rtos/vfs.go
@@ -0,0 +1,358 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"internal/bytealg"
+	"io/fs"
+	"path"
+	"sync"
+	"sync/atomic"
+	"syscall"
+	"time"
+)
+
+// An FS interface is the minimum implementation of a hierarchical file system
+// required by the Virtual File System (VFS) provided for os package.
+//
+// Any FS implementation can also easily implement the fs.FS interface (and
+// vice versa) thanks to the use of the common fs.File interface to represent an
+// open file.
+//
+// The FS may implement the following optional methods recognized by VFS:
+//
+//	// Sync synchronizes cached writes to persistent storage. It can be called
+//	// at any time but it is specifically called by Unmount function after
+//	// unmounting the file system from VFS.
+//	Sync() error
+//
+//	Chmod(name string, mode fs.FileMode) error // see os.Chmod
+//	Mkdir(name, perm fs.FileMode) error        // see os.Mkdir
+//	Remove(name string) error                  // see os.Remove
+//	Rename(oldname, newname string) error      // see os.Rename
+//
+type FS interface {
+	// OpenWithFinalizer works like the os.OpenFile function but has an
+	// additional parameter. The returned fs.File implementation is obliged to
+	// call the closed function if the underlying file has been closed.
+	OpenWithFinalizer(name string, flag int, perm fs.FileMode, closed func()) (fs.File, error)
+
+	// Type returns the file system type, e.g. fat32, ext4, nfs, ramfs. The type
+	// name cannot be an empty string. It is recommended to use only lowercase
+	// letters and numbers in the type name.
+	Type() string
+
+	// Name returns the name (label) of a specific file system instance.
+	Name() string
+
+	// Usage returns the filesystem usage statistics. All four values are
+	// subject to change while the file system is used. Usage should return
+	// -1 for any unknown value.
+	Usage() (usedItems, maxItems int, usedBytes, maxBytes int64)
+}
+
+// A MountPoint represents a mounted file system.
+type MountPoint struct {
+	Prefix    string // path to FS
+	FS        FS     // mounted file system
+	OpenCount int32  // number of open files
+}
+
+func (mp *MountPoint) closed() {
+	if atomic.AddInt32(&mp.OpenCount, -1) < 0 {
+		panic("open count < 0")
+	}
+}
+
+type mountTable struct {
+	mu     sync.RWMutex
+	mounts []*MountPoint //  TODO: linked list?
+}
+
+var mtab mountTable
+
+// Mount mounts a filesystem with a provided prefix. Prefix can be any
+// slash-separated path and does not have to represent an existing directory
+// (in this respect it is similar to URL path). Mounted filesystem becomes
+// available to os package.
+func Mount(fsys FS, prefix string) error {
+	if prefix == "" || prefix[0] != '/' || fsys == nil {
+		return &fs.PathError{Op: "mount", Path: prefix, Err: syscall.EINVAL}
+	}
+	prefix = path.Clean(prefix)
+	mtab.mu.Lock()
+	mtab.mounts = append(mtab.mounts, &MountPoint{prefix, fsys, 0})
+	mtab.mu.Unlock()
+	return nil
+}
+
+// Unmount unmounts the last mounted filesystem that match fsys and prefix.
+// At least one parameter must be specified (not empty or nil).
+func Unmount(fsys FS, prefix string) error {
+	if prefix == "" && fsys == nil {
+		return &fs.PathError{Op: "unmount", Path: prefix, Err: syscall.ENOENT}
+	}
+	prefix = path.Clean(prefix)
+	mtab.mu.Lock()
+	remove := -1
+	for i := len(mtab.mounts) - 1; i >= 0; i-- {
+		mp := mtab.mounts[i]
+		if (prefix == "" || mp.Prefix == prefix) && (fsys == nil || mp.FS == fsys) {
+			remove = i
+			fsys = mp.FS
+			break
+		}
+	}
+	var err error
+	if remove < 0 {
+		err = syscall.ENOENT
+		goto skip
+	}
+	if atomic.LoadInt32(&mtab.mounts[remove].OpenCount) != 0 {
+		err = syscall.EBUSY
+		goto skip
+	}
+	copy(mtab.mounts[remove:], mtab.mounts[remove+1:])
+	mtab.mounts = mtab.mounts[:len(mtab.mounts)-1]
+skip:
+	mtab.mu.Unlock()
+	if err != nil {
+		return &fs.PathError{Op: "unmount", Path: prefix, Err: err}
+	}
+
+	// close the fsys if it has no another mount point
+
+	mtab.mu.RLock()
+	for _, mp := range mtab.mounts {
+		if mp.FS == fsys {
+			fsys = nil // fsys is still mounted with another prefix
+			break
+		}
+	}
+	mtab.mu.RUnlock()
+	if fsys == nil {
+		return nil
+	}
+	if fsys, ok := fsys.(interface{ Sync() error }); ok {
+		if err = fsys.Sync(); err != nil {
+			return &fs.PathError{Op: "unmount", Path: prefix, Err: err}
+		}
+	}
+	return nil
+}
+
+// Mounts returns the current list of mount points.
+func Mounts() []*MountPoint {
+	mtab.mu.RLock()
+	list := append([]*MountPoint{}, mtab.mounts...)
+	mtab.mu.RUnlock()
+	return list
+}
+
+func findMountPoint(name string) (mp *MountPoint, fsys FS, unrooted string) {
+	name = path.Clean(name)
+	nlen := len(name)
+	vdir := name == "/"
+	for i := len(mtab.mounts) - 1; i >= 0; i-- {
+		mp = mtab.mounts[i]
+		plen := len(mp.Prefix)
+		if nlen < plen {
+			if mp.Prefix[nlen] == '/' && mp.Prefix[:nlen] == name {
+				vdir = true
+			}
+			continue
+		}
+		if plen != nlen && name[plen] != '/' {
+			continue
+		}
+		if name[:plen] != mp.Prefix {
+			continue
+		}
+		if plen == nlen {
+			unrooted = "."
+		} else {
+			unrooted = name[plen+1:]
+		}
+		return mp, mp.FS, unrooted
+	}
+	if vdir {
+		unrooted = name
+	}
+	return nil, nil, unrooted
+}
+
+func chmod(name string, mode fs.FileMode) error {
+	mtab.mu.RLock()
+	_, fsys, unrooted := findMountPoint(name)
+	mtab.mu.RUnlock()
+	if fsys == nil {
+		return syscall.ENOENT
+	}
+	if fsys, ok := fsys.(interface {
+		Chmod(name string, mode fs.FileMode) error
+	}); ok {
+		return fsys.Chmod(unrooted, mode)
+	}
+	// try to use File.Chmod
+	f, err := fsys.OpenWithFinalizer(unrooted, syscall.O_RDONLY, 0, nil)
+	if err != nil {
+		return err
+	}
+	if f, ok := f.(interface {
+		Chmod(mode fs.FileMode) error
+	}); ok {
+		err = f.Chmod(mode)
+	} else {
+		err = syscall.ENOTSUP
+	}
+	cerr := f.Close()
+	if err != nil {
+		return err
+	}
+	return cerr
+}
+
+func rename(oldname, newname string) error {
+	mtab.mu.RLock()
+	_, oldfs, oldunrooted := findMountPoint(oldname)
+	_, newfs, newunrooted := findMountPoint(newname)
+	mtab.mu.RUnlock()
+	if oldfs == nil || newfs == nil {
+		return syscall.ENOENT
+	}
+	if oldfs == newfs {
+		if fsys, ok := oldfs.(interface {
+			Rename(oldname, newname string) error
+		}); ok {
+			return fsys.Rename(oldunrooted, newunrooted)
+		}
+	}
+	return syscall.ENOTSUP
+}
+
+func mkdir(name string, perm fs.FileMode) (err error) {
+	mtab.mu.RLock()
+	_, fsys, unrooted := findMountPoint(name)
+	mtab.mu.RUnlock()
+	if fsys == nil {
+		err = syscall.ENOENT
+		goto error
+	}
+	if fsys, ok := fsys.(interface {
+		Mkdir(name string, perm fs.FileMode) error
+	}); ok {
+		if err = fsys.Mkdir(unrooted, perm); err != nil {
+			goto error
+		}
+		return nil
+	}
+	err = syscall.ENOTSUP
+error:
+	return &fs.PathError{Op: "mkdir", Path: name, Err: err}
+}
+
+func remove(name string) (err error) {
+	mtab.mu.RLock()
+	_, fsys, unrooted := findMountPoint(name)
+	mtab.mu.RUnlock()
+	if fsys == nil {
+		err = syscall.ENOENT
+		goto error
+	}
+	if fsys, ok := fsys.(interface {
+		Remove(name string) error
+	}); ok {
+		if err = fsys.Remove(unrooted); err != nil {
+			goto error
+		}
+		return nil
+	}
+	err = syscall.ENOTSUP
+error:
+	return &fs.PathError{Op: "remove", Path: name, Err: err}
+}
+
+func openFile(name string, flag int, perm fs.FileMode) (f fs.File, err error) {
+	mtab.mu.RLock()
+	mp, fsys, unrooted := findMountPoint(name)
+	if mp != nil {
+		if atomic.AddInt32(&mp.OpenCount, 1) < 0 {
+			atomic.AddInt32(&mp.OpenCount, -1)
+			err = syscall.EMFILE
+		}
+	}
+	mtab.mu.RUnlock()
+	if err != nil {
+		return nil, err
+	}
+	if mp != nil {
+		return fsys.OpenWithFinalizer(unrooted, flag, perm, mp.closed)
+	}
+	if unrooted != "" {
+		if unrooted == "/" {
+			unrooted = ""
+		}
+		return &vdir{unrooted, &vfi_}, nil
+	}
+	return nil, syscall.ENOENT
+}
+
+// a vdir represents a fake directory at the level of a mount-point prefix
+type vdir struct {
+	name string
+	fs.FileInfo
+}
+
+func (d *vdir) Read(p []byte) (int, error) { return 0, syscall.EISDIR }
+func (d *vdir) Stat() (fs.FileInfo, error) { return d, nil }
+func (d *vdir) Close() error               { return nil }
+
+func (d *vdir) ReadDir(n int) (dl []fs.DirEntry, err error) {
+	nlen := len(d.name)
+	mtab.mu.RLock()
+	for _, mp := range mtab.mounts {
+		prefix := mp.Prefix
+		plen := len(prefix)
+		if plen <= nlen || prefix[nlen] != '/' || d.name != prefix[:nlen] {
+			continue
+		}
+		d := &vdir{name: prefix[nlen+1:]}
+		if i := bytealg.IndexByteString(d.name, '/'); i >= 0 {
+			d.name = d.name[:i]
+			d.FileInfo = &vfi_
+		} else {
+			f, err := openFile(prefix, syscall.O_RDONLY, 0)
+			if err != nil {
+				return nil, err
+			}
+			d.FileInfo, err = f.Stat()
+			if err != nil {
+				return nil, err
+			}
+			f.Close()
+		}
+		dl = append(dl, d)
+	}
+	mtab.mu.RUnlock()
+	return
+}
+
+// Additional methods to implement fs.DirEntry interface
+
+// overwrite d.FileInfo Name method
+func (d *vdir) Name() string               { return d.name }
+func (d *vdir) Type() fs.FileMode          { return d.Mode() }
+func (d *vdir) Info() (fs.FileInfo, error) { return d, nil }
+
+var vfi_ vfi
+
+type vfi struct{}
+
+func (_ *vfi) Name() string       { return "" }
+func (_ *vfi) Size() int64        { return 0 }
+func (_ *vfi) IsDir() bool        { return true }
+func (_ *vfi) Sys() interface{}   { return nil }
+func (_ *vfi) ModTime() time.Time { return time.Time{} }
+func (_ *vfi) Mode() fs.FileMode  { return fs.ModeDir | 0555 }
diff --git a/src/embedded/rtos/vfs_noos.go b/src/embedded/rtos/vfs_noos.go
new file mode 100644
index 0000000000..cf22674234
--- /dev/null
+++ b/src/embedded/rtos/vfs_noos.go
@@ -0,0 +1,14 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import _ "unsafe"
+
+//go:linkname openFile os.openFile
+
+//go:linkname chmod os.chmod
+//go:linkname mkdir syscall.Mkdir
+//go:linkname rename os.rename
+//go:linkname remove os.Remove
diff --git a/src/embedded/rtos/vfs_test.go b/src/embedded/rtos/vfs_test.go
new file mode 100644
index 0000000000..2f4368b549
--- /dev/null
+++ b/src/embedded/rtos/vfs_test.go
@@ -0,0 +1,117 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"errors"
+	"io"
+	"io/fs"
+	"path"
+	"testing"
+)
+
+type testfile struct {
+	closed func()
+}
+
+func (f *testfile) Stat() (fi fs.FileInfo, err error) { return }
+func (f *testfile) Read(p []byte) (int, error)        { return 0, io.EOF }
+
+func (f *testfile) Close() error {
+	if f.closed != nil {
+		f.closed()
+	}
+	f.closed = nil
+	return nil
+}
+
+type testfs struct {
+	name string
+}
+
+func (fs *testfs) OpenWithFinalizer(name string, flag int, perm fs.FileMode, closed func()) (fs.File, error) {
+	return &testfile{closed}, nil
+}
+
+func (fs *testfs) Type() string                    { return "testfs" }
+func (fs *testfs) Name() string                    { return fs.name }
+func (fs *testfs) Usage() (int, int, int64, int64) { return -1, -1, -1, -1 }
+
+type test struct {
+	prefix string
+	fs     FS
+}
+
+var (
+	fs1 = &testfs{name: "FS1"}
+	fs2 = &testfs{name: "FS2"}
+
+	tests = []test{
+		{"/fs1/prefix1/", fs1},
+		{"/fs1/prefix2", fs1},
+		{"/fs2//prefix1", fs2},
+		{"/../fs2/prefix2", fs2},
+		{"/common/prefix", fs1},
+		{"/common/prefix", fs2},
+	}
+)
+
+func checkMounts(t *testing.T) {
+	mounts := Mounts()
+	i := 0
+	for _, test := range tests {
+		if test.fs == nil {
+			continue
+		}
+		mp := mounts[i]
+		if mp.Prefix != path.Clean(test.prefix) {
+			t.Fatal(i, mp.Prefix, "!=", tests[i].prefix)
+		}
+		if mp.FS != test.fs {
+			t.Fatal(i, mp.FS.Name(), "!=", tests[i].fs.Name())
+		}
+		i++
+	}
+}
+
+func TestMountOpen(t *testing.T) {
+	// mount
+	for _, test := range tests {
+		if err := Mount(test.fs, test.prefix); err != nil {
+			t.Fatal(err)
+		}
+	}
+	checkMounts(t)
+	if err := Unmount(nil, "/"); !errors.Is(err, fs.ErrNotExist) {
+		t.Fatal("expected fs.ErrNotExist, got:", err)
+	}
+
+	// unmount
+	if err := Unmount(nil, "//fs2/prefix2"); err != nil {
+		t.Fatal(err)
+	}
+	tests[3].fs = nil
+	checkMounts(t)
+	if err := Unmount(nil, "/common/prefix/"); err != nil {
+		t.Fatal(err)
+	}
+	tests[5].fs = nil
+	checkMounts(t)
+
+	// open
+	f, err := openFile("/fs2/prefix1", 0, 0)
+	if err != nil {
+		t.Fatal(err)
+	}
+	if oc := mtab.mounts[2].OpenCount; oc != 1 {
+		t.Fatalf("OpenCount=%d, expexted 1:", oc)
+	}
+	if err := f.Close(); err != nil {
+		t.Fatal(err)
+	}
+	if oc := mtab.mounts[2].OpenCount; oc != 0 {
+		t.Fatalf("OpenCount=%d, expexted 0:", oc)
+	}
+}
diff --git a/src/go/build/deps_test.go b/src/go/build/deps_test.go
index c97c668cc4..6df84ef9c4 100644
--- a/src/go/build/deps_test.go
+++ b/src/go/build/deps_test.go
@@ -76,8 +76,27 @@ var depsRules = `
 	  unicode/utf8, unicode/utf16, unicode,
 	  unsafe;
 
+	# MMIO used by noos runtime
+	unsafe
+	< embedded/mmio
+	< internal/cpu/cortexm,
+	  internal/cpu/cortexm/acc,
+	  internal/cpu/cortexm/bitband,
+	  internal/cpu/cortexm/cmt,
+	  internal/cpu/cortexm/debug/itm,
+	  internal/cpu/cortexm/fpu,
+	  internal/cpu/cortexm/mpu,
+	  internal/cpu/cortexm/nvic,
+	  internal/cpu/cortexm/pft,
+	  internal/cpu/cortexm/scb,
+	  internal/cpu/cortexm/scid,
+	  internal/cpu/cortexm/systick,
+	  internal/cpu/riscv/clint,
+	  internal/cpu/riscv/plic
+	< MMIO;
+
 	# RUNTIME is the core runtime group of packages, all of them very light-weight.
-	internal/cpu, unsafe
+	internal/cpu, unsafe, MMIO
 	< internal/bytealg
 	< internal/unsafeheader
 	< runtime/internal/sys
@@ -159,7 +178,7 @@ var depsRules = `
 	# OS does not include reflection.
 	io/fs
 	< internal/testlog
-	< internal/poll
+	< internal/poll, embedded/rtos
 	< os
 	< os/signal;
 
@@ -514,6 +533,14 @@ var depsRules = `
 
 	FMT, container/heap, math/rand
 	< internal/trace;
+
+	# Embedded Go packages
+
+	embedded/rtos, internal/cpu/cortexm/scb, internal/cpu/cortexm/systick
+	< embedded/arch/cortexm/systim;
+
+	embedded/rtos, internal/cpu/riscv/clint
+	< embedded/arch/riscv/systim;
 `
 
 // listStdPkgs returns the same list of packages as "go list std".
diff --git a/src/go/build/syslist.go b/src/go/build/syslist.go
index 1275f7c986..4a2073394e 100644
--- a/src/go/build/syslist.go
+++ b/src/go/build/syslist.go
@@ -7,5 +7,5 @@ package build
 // List of past, present, and future known GOOS and GOARCH values.
 // Do not remove from this list, as these are used for go/build filename matching.
 
-const goosList = "aix android darwin dragonfly freebsd hurd illumos ios js linux nacl netbsd openbsd plan9 solaris windows zos "
-const goarchList = "386 amd64 amd64p32 arm armbe arm64 arm64be ppc64 ppc64le mips mipsle mips64 mips64le mips64p32 mips64p32le ppc riscv riscv64 s390 s390x sparc sparc64 wasm "
+const goosList = "aix android darwin dragonfly freebsd hurd illumos ios js linux nacl netbsd noos openbsd plan9 solaris windows zos "
+const goarchList = "386 amd64 amd64p32 arm armbe arm64 arm64be ppc64 ppc64le mips mipsle mips64 mips64le mips64p32 mips64p32le ppc riscv riscv64 s390 s390x sparc sparc64 thumb wasm "
diff --git a/src/go/types/example_test.go b/src/go/types/example_test.go
index 3747f3b15a..e3e719a870 100644
--- a/src/go/types/example_test.go
+++ b/src/go/types/example_test.go
@@ -5,7 +5,7 @@
 // Only run where builders (build.golang.org) have
 // access to compiled packages for import.
 //
-// +build !arm,!arm64
+// +build !arm,!arm64,!thumb
 
 package types_test
 
diff --git a/src/go/types/sizes.go b/src/go/types/sizes.go
index 6ab6157b82..f78c430352 100644
--- a/src/go/types/sizes.go
+++ b/src/go/types/sizes.go
@@ -169,6 +169,7 @@ var gcArchSizes = map[string]*StdSizes{
 	"ppc64le":  {8, 8},
 	"riscv64":  {8, 8},
 	"s390x":    {8, 8},
+	"thumb":    {4, 4},
 	"sparc64":  {8, 8},
 	"wasm":     {8, 8},
 	// When adding more architectures here,
@@ -180,7 +181,7 @@ var gcArchSizes = map[string]*StdSizes{
 //
 // Supported architectures for compiler "gc":
 // "386", "arm", "arm64", "amd64", "amd64p32", "mips", "mipsle",
-// "mips64", "mips64le", "ppc64", "ppc64le", "riscv64", "s390x", "sparc64", "wasm".
+// "mips64", "mips64le", "ppc64", "ppc64le", "riscv64", "s390x", "sparc64", "thumb", "wasm".
 func SizesFor(compiler, arch string) Sizes {
 	var m map[string]*StdSizes
 	switch compiler {
diff --git a/src/internal/bytealg/compare_generic.go b/src/internal/bytealg/compare_generic.go
index bd4489a6b9..e0a4bd9f59 100644
--- a/src/internal/bytealg/compare_generic.go
+++ b/src/internal/bytealg/compare_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !386,!amd64,!s390x,!arm,!arm64,!ppc64,!ppc64le,!mips,!mipsle,!wasm,!mips64,!mips64le
+// +build !386,!amd64,!s390x,!arm,!arm64,!ppc64,!ppc64le,!mips,!mipsle,!wasm,!mips64,!mips64le,!thumb
 
 package bytealg
 
diff --git a/src/internal/bytealg/compare_native.go b/src/internal/bytealg/compare_native.go
index b53ba97463..a3cf8ee988 100644
--- a/src/internal/bytealg/compare_native.go
+++ b/src/internal/bytealg/compare_native.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 amd64 s390x arm arm64 ppc64 ppc64le mips mipsle wasm mips64 mips64le
+// +build 386 amd64 s390x arm arm64 ppc64 ppc64le mips mipsle wasm mips64 mips64le thumb
 
 package bytealg
 
diff --git a/src/internal/bytealg/compare_thumb.s b/src/internal/bytealg/compare_thumb.s
new file mode 100644
index 0000000000..d9cfff1d73
--- /dev/null
+++ b/src/internal/bytealg/compare_thumb.s
@@ -0,0 +1,83 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+TEXT ·Compare(SB),NOSPLIT|NOFRAME,$0-28
+	MOVW  a_base+0(FP), R2
+	MOVW  a_len+4(FP), R0
+	MOVW  b_base+12(FP), R3
+	MOVW  b_len+16(FP), R1
+	ADD   $28, R13, R7
+	B     cmpbody<>(SB)
+
+TEXT runtime·cmpstring(SB),NOSPLIT|NOFRAME,$0-20
+	MOVW  a_base+0(FP), R2
+	MOVW  a_len+4(FP), R0
+	MOVW  b_base+8(FP), R3
+	MOVW  b_len+12(FP), R1
+	ADD   $20, R13, R7
+	B     cmpbody<>(SB)
+
+// On entry:
+// R0 is the length of a
+// R1 is the length of b
+// R2 points to the start of a
+// R3 points to the start of b
+// R7 points to return value (-1/0/1 will be written here); be carefull: R7 is REGTMP on thumb
+//
+// On exit:
+// R4, R5, R6 and R8 are clobbered
+TEXT cmpbody<>(SB),NOSPLIT|NOFRAME,$0-0
+	CMP      R2, R3
+	BEQ      samebytes
+	MOVW     R0, R6
+	CMP      R0, R1
+	MOVW.LT  R1, R6  // R6 is min(R0, R1)
+
+	CBZ   R6, samebytes
+	CMP   $4, R6
+	ADD   R2, R6     // R2 is current byte in a, R6 is the end of the range to compare
+	BLT   byte_loop  // length < 4
+	AND   $3, R2, R4
+	CBNZ  R4, byte_loop  // unaligned a, use byte-wise compare (TODO: try to align a)
+aligned_a:
+	AND   $3, R3, R4
+	CBNZ  R4, byte_loop  // unaligned b, use byte-wise compare
+	AND   $0xfffffffc, R6, R8
+	// length >= 4
+chunk4_loop:
+	MOVW.P  4(R2), R4
+	MOVW.P  4(R3), R5
+	CMP     R4, R5
+	BNE     cmp
+	CMP     R2, R8
+	BNE     chunk4_loop
+	CMP     R2, R6
+	BEQ     samebytes  // all compared bytes were the same; compare lengths
+byte_loop:
+	MOVBU.P  1(R2), R4
+	MOVBU.P  1(R3), R5
+	CMP      R4, R5
+	BNE      ret
+	CMP      R2, R6
+	BNE      byte_loop
+samebytes:
+	CMP      R0, R1
+	MOVW.LT  $1, R0
+	MOVW.GT  $-1, R0
+	MOVW.EQ  $0, R0
+	MOVW     R0, (R7)
+	RET
+ret:
+	// bytes differed
+	MOVW.LT  $1, R0
+	MOVW.GT  $-1, R0
+	MOVW     R0, (R7)
+	RET
+cmp:
+	SUB  $4, R2, R2
+	SUB  $4, R3, R3
+	B    byte_loop
diff --git a/src/internal/bytealg/count_generic.go b/src/internal/bytealg/count_generic.go
index 5575e81ab8..f6e6c07b8e 100644
--- a/src/internal/bytealg/count_generic.go
+++ b/src/internal/bytealg/count_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !amd64,!arm,!arm64,!ppc64le,!ppc64,!riscv64,!s390x
+// +build !amd64,!arm,!arm64,!ppc64le,!ppc64,!riscv64,!s390x,!thumb
 
 package bytealg
 
diff --git a/src/internal/bytealg/count_native.go b/src/internal/bytealg/count_native.go
index b1ff1d265a..98ab76a09e 100644
--- a/src/internal/bytealg/count_native.go
+++ b/src/internal/bytealg/count_native.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build amd64 arm arm64 ppc64le ppc64 riscv64 s390x
+// +build amd64 arm arm64 ppc64le ppc64 riscv64 s390x thumb
 
 package bytealg
 
diff --git a/src/internal/bytealg/count_thumb.s b/src/internal/bytealg/count_thumb.s
new file mode 100644
index 0000000000..1d7147f6b5
--- /dev/null
+++ b/src/internal/bytealg/count_thumb.s
@@ -0,0 +1,42 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+TEXT ·Count(SB),NOSPLIT,$0-20
+	MOVW   b_base+0(FP), R0
+	MOVW   b_len+4(FP), R1
+	MOVBU  c+12(FP), R2
+	MOVW   $ret+16(FP), R7
+	B      countbytebody<>(SB)
+
+TEXT ·CountString(SB),NOSPLIT,$0-16
+	MOVW   s_base+0(FP), R0
+	MOVW   s_len+4(FP), R1
+	MOVBU  c+8(FP), R2
+	MOVW   $ret+12(FP), R7
+	B      countbytebody<>(SB)
+
+// Input:
+// R0: data
+// R1: data length
+// R2: byte to find
+// R7: address to put result; be carefull: R7 is REGTMP on thumb
+//
+// On exit:
+// R4 and R8 are clobbered
+TEXT countbytebody<>(SB),NOSPLIT,$0
+	MOVW  $0, R8    // R8 = count of byte to search
+	CBZ   R1, done  // short path to handle 0-byte case
+	ADD   R0, R1    // R1 is the end of the range
+byte_loop:
+	MOVBU.P  1(R0), R4
+	CMP      R4, R2
+	ADD.EQ   $1, R8
+	CMP      R0, R1
+	BNE      byte_loop
+done:
+	MOVW  R8, (R7)
+	RET
diff --git a/src/internal/bytealg/equal_thumb.s b/src/internal/bytealg/equal_thumb.s
new file mode 100644
index 0000000000..66f75e09a5
--- /dev/null
+++ b/src/internal/bytealg/equal_thumb.s
@@ -0,0 +1,89 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+#define REGCTXT R11
+
+// memequal(a, b unsafe.Pointer, size uintptr) bool
+TEXT runtime·memequal(SB),NOSPLIT|NOFRAME,$0-13
+	MOVW  a+0(FP), R0
+	MOVW  b+4(FP), R2
+	CMP   R0, R2
+	BEQ   eq
+	MOVW  size+8(FP), R1
+	CBZ   R1, eq  // short path to handle 0-byte case
+	MOVW  $ret+12(FP), R7
+	B     memeqbody<>(SB)
+eq:
+	MOVW  $1, R0
+	MOVB  R0, ret+12(FP)
+	RET
+
+// memequal_varlen(a, b unsafe.Pointer) bool
+TEXT runtime·memequal_varlen(SB),NOSPLIT|NOFRAME,$0-9
+	MOVW  a+0(FP), R0
+	MOVW  b+4(FP), R2
+	CMP   R0, R2
+	BEQ   eq
+	MOVW  4(REGCTXT), R1  // compiler stores size at offset 4 in the closure
+	CBZ   R1, eq          // short path to handle 0-byte case
+	MOVW  $ret+8(FP), R7
+	B     memeqbody<>(SB)
+eq:
+	MOVW  $1, R0
+	MOVB  R0, ret+8(FP)
+	RET
+
+// Input:
+// R0: data of a
+// R1: length
+// R2: data of b
+// R7: points to return value
+//
+// On exit:
+// R4, R5 and R6 are clobbered
+TEXT memeqbody<>(SB),NOSPLIT|NOFRAME,$0-0
+	CMP  $1, R1
+	BEQ  one  // 1-byte special case for better performance
+
+	CMP   $4, R1
+	ADD   R0, R1     // R1 is the end of the range to compare
+	BLT   byte_loop  // length < 4
+	AND   $3, R0, R6
+	CBNZ  R6, byte_loop  // unaligned a, use byte-wise compare (TODO: try to align a)
+	AND   $3, R2, R6
+	CBNZ  R6, byte_loop  // unaligned b, use byte-wise compare
+	AND   $0xfffffffc, R1, R6
+	// length >= 4
+chunk4_loop:
+	MOVW.P  4(R0), R4
+	MOVW.P  4(R2), R5
+	CMP     R4, R5
+	BNE     notequal
+	CMP     R0, R6
+	BNE     chunk4_loop
+	CMP     R0, R1
+	BEQ     equal  // reached the end
+byte_loop:
+	MOVBU.P  1(R0), R4
+	MOVBU.P  1(R2), R5
+	CMP      R4, R5
+	BNE      notequal
+	CMP      R0, R1
+	BNE      byte_loop
+equal:
+	MOVW  $1, R0
+	MOVB  R0, (R7)
+	RET
+one:
+	MOVBU  (R0), R4
+	MOVBU  (R2), R5
+	CMP    R4, R5
+	BEQ    equal
+notequal:
+	MOVW  $0, R0
+	MOVB  R0, (R7)
+	RET
diff --git a/src/internal/bytealg/indexbyte_generic.go b/src/internal/bytealg/indexbyte_generic.go
index 0b012a8850..f820675833 100644
--- a/src/internal/bytealg/indexbyte_generic.go
+++ b/src/internal/bytealg/indexbyte_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !386,!amd64,!s390x,!arm,!arm64,!ppc64,!ppc64le,!mips,!mipsle,!mips64,!mips64le,!riscv64,!wasm
+// +build !386,!amd64,!s390x,!arm,!arm64,!ppc64,!ppc64le,!mips,!mipsle,!mips64,!mips64le,!riscv64,!thumb,!wasm
 
 package bytealg
 
diff --git a/src/internal/bytealg/indexbyte_native.go b/src/internal/bytealg/indexbyte_native.go
index f96c5be491..4db7cefd31 100644
--- a/src/internal/bytealg/indexbyte_native.go
+++ b/src/internal/bytealg/indexbyte_native.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 amd64 s390x arm arm64 ppc64 ppc64le mips mipsle mips64 mips64le riscv64 wasm
+// +build 386 amd64 s390x arm arm64 ppc64 ppc64le mips mipsle mips64 mips64le riscv64 thumb wasm
 
 package bytealg
 
diff --git a/src/internal/bytealg/indexbyte_thumb.s b/src/internal/bytealg/indexbyte_thumb.s
new file mode 100644
index 0000000000..ba48b98bc7
--- /dev/null
+++ b/src/internal/bytealg/indexbyte_thumb.s
@@ -0,0 +1,46 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+TEXT ·IndexByte(SB),NOSPLIT,$0-20
+	MOVW   b_base+0(FP), R0
+	MOVW   b_len+4(FP), R1
+	MOVBU  c+12(FP), R2  // byte to find
+	MOVW   $ret+16(FP), R5
+	B      indexbytebody<>(SB)
+
+TEXT ·IndexByteString(SB),NOSPLIT,$0-16
+	MOVW   s_base+0(FP), R0
+	MOVW   s_len+4(FP), R1
+	MOVBU  c+8(FP), R2  // byte to find
+	MOVW   $ret+12(FP), R5
+	B      indexbytebody<>(SB)
+
+// input:
+//  R0: data
+//  R1: data length
+//  R2: byte to find
+//  R5: address to put result
+TEXT indexbytebody<>(SB),NOSPLIT,$0-0
+	MOVW  R0, R4  // store base for later
+	ADD   R0, R1  // end
+
+loop:
+	CMP      R0, R1
+	BEQ      notfound
+	MOVBU.P  1(R0), R3
+	CMP      R2, R3
+	BNE      loop
+
+	SUB   $1, R0  // R0 will be one beyond the position we want
+	SUB   R4, R0  // remove base
+	MOVW  R0, (R5)
+	RET   
+
+notfound:
+	MOVW  $-1, R0
+	MOVW  R0, (R5)
+	RET   
diff --git a/src/internal/cpu/cortexm/acc/doc.go b/src/internal/cpu/cortexm/acc/doc.go
new file mode 100644
index 0000000000..6c7a4456ba
--- /dev/null
+++ b/src/internal/cpu/cortexm/acc/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package acc gives access to the Access Control registers.
+package acc
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/acc/periph_thumb.go b/src/internal/cpu/cortexm/acc/periph_thumb.go
new file mode 100644
index 0000000000..0b5e45fbfe
--- /dev/null
+++ b/src/internal/cpu/cortexm/acc/periph_thumb.go
@@ -0,0 +1,94 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  ACC  0xE000EF90  -  -  Access Control registers
+// Registers:
+//  0x00  32  ITCMCR  Instruction Tightly-Coupled Memory Control Register
+//  0x04  32  DTCMCR  Data Tightly-Coupled Memory Control Register
+//	0x08  32  AHBPCR  AHBP Control Register
+//  0x0C  32  CACR    L1 Cache Control Register
+//  0x10  32  AHBSCR  AHB Slave Control Register
+//  0x18  32  ABFSR   Auxiliary Bus Fault Status Register
+package acc
+
+const (
+	ITCMEN    ITCMCR = 1 << 0   //+ TCM enable.
+	ITCMRMW   ITCMCR = 1 << 1   //+ Read-Modify-Write (RMW) enable.
+	ITCMRETEN ITCMCR = 1 << 2   //+ Retry phase enable.
+	ITCMSZ    ITCMCR = 0xF << 3 //+ TCM size: 0:0K, 3:4K, 4:8K, ..., 16:16M.
+)
+
+const (
+	ITCMENn    = 0
+	ITCMRMWn   = 1
+	ITCMRETENn = 2
+	ITCMSZn    = 3
+)
+
+const (
+	DTCMEN    DTCMCR = 1 << 0   //+ TCM enable.
+	DTCMRMW   DTCMCR = 1 << 1   //+ Read-Modify-Write (RMW) enable.
+	DTCMRETEN DTCMCR = 1 << 2   //+ Retry phase enable.
+	DTCMSZ    DTCMCR = 0xF << 3 //+ TCM size. 0:0K, 3:4K, 4:8K, ..., 16:16M.
+)
+
+const (
+	DTCMENn    = 0
+	DTCMRMWn   = 1
+	DTCMRETENn = 2
+	DTCMSZn    = 3
+)
+
+const (
+	AHBPEN AHBPCR = 1 << 0   //+ AHBP enable.
+	AHBPSZ AHBPCR = 0x7 << 1 //+ AHBP size. 1:64M, 2:128M, 3:256M, 4:512M.
+)
+
+const (
+	AHBPENn = 0
+	AHBPSZn = 1
+)
+
+const (
+	SIWT    CACR = 1 << 0 //+ Shared cacheable-is-WT for data cache.
+	ECCDIS  CACR = 1 << 1 //+ ECC in the instruction and data cache.
+	FORCEWT CACR = 1 << 2 //+ Force Write-Through in the data cache.
+)
+
+const (
+	SIWTn    = 0
+	ECCDISn  = 1
+	FORCEWTn = 2
+)
+
+const (
+	CTL       AHBSCR = 0x3 << 0   //+ AHBS prioritization control.
+	TPRI      AHBSCR = 0x1FF << 2 //+ Thresh. exec. prio. for traffic demotion.
+	INITCOUNT AHBSCR = 0x1F << 11 //+ Fairness counter initialization value.
+)
+
+const (
+	CTLn       = 0
+	TPRIn      = 2
+	INITCOUNTn = 11
+)
+
+const (
+	ITCM     ABFSR = 1 << 0   //+ Asynchronous fault on ITCM interface
+	DTCM     ABFSR = 1 << 1   //+ Asynchronous fault on DTCM interface.
+	AHBP     ABFSR = 1 << 2   //+ Asynchronous fault on AHBP interface.
+	AXIM     ABFSR = 1 << 3   //+ Asynchronous fault on AXIM interface.
+	EPPB     ABFSR = 1 << 4   //+ Asynchronous fault on EPPB interface.
+	AXIMTYPE ABFSR = 0x3 << 8 //+ The type of fault on the AXIM interface.
+)
+
+const (
+	ITCMn     = 0
+	DTCMn     = 1
+	AHBPn     = 2
+	AXIMn     = 3
+	EPPBn     = 4
+	AXIMTYPEn = 8
+)
diff --git a/src/internal/cpu/cortexm/acc/xperiph_thumb.go b/src/internal/cpu/cortexm/acc/xperiph_thumb.go
new file mode 100644
index 0000000000..6e403ecefc
--- /dev/null
+++ b/src/internal/cpu/cortexm/acc/xperiph_thumb.go
@@ -0,0 +1,208 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package acc
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	ITCMCR RITCMCR
+	DTCMCR RDTCMCR
+	AHBPCR RAHBPCR
+	CACR   RCACR
+	AHBSCR RAHBSCR
+	_      uint32
+	ABFSR  RABFSR
+}
+
+func ACC() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000EF90))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type ITCMCR uint32
+
+type RITCMCR struct{ mmio.U32 }
+
+func (r *RITCMCR) LoadBits(mask ITCMCR) ITCMCR { return ITCMCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RITCMCR) StoreBits(mask, b ITCMCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RITCMCR) SetBits(mask ITCMCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RITCMCR) ClearBits(mask ITCMCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RITCMCR) Load() ITCMCR                { return ITCMCR(r.U32.Load()) }
+func (r *RITCMCR) Store(b ITCMCR)              { r.U32.Store(uint32(b)) }
+
+type RMITCMCR struct{ mmio.UM32 }
+
+func (rm RMITCMCR) Load() ITCMCR   { return ITCMCR(rm.UM32.Load()) }
+func (rm RMITCMCR) Store(b ITCMCR) { rm.UM32.Store(uint32(b)) }
+
+func ITCMEN_(p *Periph) RMITCMCR {
+	return RMITCMCR{mmio.UM32{&p.ITCMCR.U32, uint32(ITCMEN)}}
+}
+
+func ITCMRMW_(p *Periph) RMITCMCR {
+	return RMITCMCR{mmio.UM32{&p.ITCMCR.U32, uint32(ITCMRMW)}}
+}
+
+func ITCMRETEN_(p *Periph) RMITCMCR {
+	return RMITCMCR{mmio.UM32{&p.ITCMCR.U32, uint32(ITCMRETEN)}}
+}
+
+func ITCMSZ_(p *Periph) RMITCMCR {
+	return RMITCMCR{mmio.UM32{&p.ITCMCR.U32, uint32(ITCMSZ)}}
+}
+
+type DTCMCR uint32
+
+type RDTCMCR struct{ mmio.U32 }
+
+func (r *RDTCMCR) LoadBits(mask DTCMCR) DTCMCR { return DTCMCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RDTCMCR) StoreBits(mask, b DTCMCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDTCMCR) SetBits(mask DTCMCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RDTCMCR) ClearBits(mask DTCMCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RDTCMCR) Load() DTCMCR                { return DTCMCR(r.U32.Load()) }
+func (r *RDTCMCR) Store(b DTCMCR)              { r.U32.Store(uint32(b)) }
+
+type RMDTCMCR struct{ mmio.UM32 }
+
+func (rm RMDTCMCR) Load() DTCMCR   { return DTCMCR(rm.UM32.Load()) }
+func (rm RMDTCMCR) Store(b DTCMCR) { rm.UM32.Store(uint32(b)) }
+
+func DTCMEN_(p *Periph) RMDTCMCR {
+	return RMDTCMCR{mmio.UM32{&p.DTCMCR.U32, uint32(DTCMEN)}}
+}
+
+func DTCMRMW_(p *Periph) RMDTCMCR {
+	return RMDTCMCR{mmio.UM32{&p.DTCMCR.U32, uint32(DTCMRMW)}}
+}
+
+func DTCMRETEN_(p *Periph) RMDTCMCR {
+	return RMDTCMCR{mmio.UM32{&p.DTCMCR.U32, uint32(DTCMRETEN)}}
+}
+
+func DTCMSZ_(p *Periph) RMDTCMCR {
+	return RMDTCMCR{mmio.UM32{&p.DTCMCR.U32, uint32(DTCMSZ)}}
+}
+
+type AHBPCR uint32
+
+type RAHBPCR struct{ mmio.U32 }
+
+func (r *RAHBPCR) LoadBits(mask AHBPCR) AHBPCR { return AHBPCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RAHBPCR) StoreBits(mask, b AHBPCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RAHBPCR) SetBits(mask AHBPCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RAHBPCR) ClearBits(mask AHBPCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RAHBPCR) Load() AHBPCR                { return AHBPCR(r.U32.Load()) }
+func (r *RAHBPCR) Store(b AHBPCR)              { r.U32.Store(uint32(b)) }
+
+type RMAHBPCR struct{ mmio.UM32 }
+
+func (rm RMAHBPCR) Load() AHBPCR   { return AHBPCR(rm.UM32.Load()) }
+func (rm RMAHBPCR) Store(b AHBPCR) { rm.UM32.Store(uint32(b)) }
+
+func AHBPEN_(p *Periph) RMAHBPCR {
+	return RMAHBPCR{mmio.UM32{&p.AHBPCR.U32, uint32(AHBPEN)}}
+}
+
+func AHBPSZ_(p *Periph) RMAHBPCR {
+	return RMAHBPCR{mmio.UM32{&p.AHBPCR.U32, uint32(AHBPSZ)}}
+}
+
+type CACR uint32
+
+type RCACR struct{ mmio.U32 }
+
+func (r *RCACR) LoadBits(mask CACR) CACR { return CACR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCACR) StoreBits(mask, b CACR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCACR) SetBits(mask CACR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RCACR) ClearBits(mask CACR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RCACR) Load() CACR              { return CACR(r.U32.Load()) }
+func (r *RCACR) Store(b CACR)            { r.U32.Store(uint32(b)) }
+
+type RMCACR struct{ mmio.UM32 }
+
+func (rm RMCACR) Load() CACR   { return CACR(rm.UM32.Load()) }
+func (rm RMCACR) Store(b CACR) { rm.UM32.Store(uint32(b)) }
+
+func SIWT_(p *Periph) RMCACR {
+	return RMCACR{mmio.UM32{&p.CACR.U32, uint32(SIWT)}}
+}
+
+func ECCDIS_(p *Periph) RMCACR {
+	return RMCACR{mmio.UM32{&p.CACR.U32, uint32(ECCDIS)}}
+}
+
+func FORCEWT_(p *Periph) RMCACR {
+	return RMCACR{mmio.UM32{&p.CACR.U32, uint32(FORCEWT)}}
+}
+
+type AHBSCR uint32
+
+type RAHBSCR struct{ mmio.U32 }
+
+func (r *RAHBSCR) LoadBits(mask AHBSCR) AHBSCR { return AHBSCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RAHBSCR) StoreBits(mask, b AHBSCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RAHBSCR) SetBits(mask AHBSCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RAHBSCR) ClearBits(mask AHBSCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RAHBSCR) Load() AHBSCR                { return AHBSCR(r.U32.Load()) }
+func (r *RAHBSCR) Store(b AHBSCR)              { r.U32.Store(uint32(b)) }
+
+type RMAHBSCR struct{ mmio.UM32 }
+
+func (rm RMAHBSCR) Load() AHBSCR   { return AHBSCR(rm.UM32.Load()) }
+func (rm RMAHBSCR) Store(b AHBSCR) { rm.UM32.Store(uint32(b)) }
+
+func CTL_(p *Periph) RMAHBSCR {
+	return RMAHBSCR{mmio.UM32{&p.AHBSCR.U32, uint32(CTL)}}
+}
+
+func TPRI_(p *Periph) RMAHBSCR {
+	return RMAHBSCR{mmio.UM32{&p.AHBSCR.U32, uint32(TPRI)}}
+}
+
+func INITCOUNT_(p *Periph) RMAHBSCR {
+	return RMAHBSCR{mmio.UM32{&p.AHBSCR.U32, uint32(INITCOUNT)}}
+}
+
+type ABFSR uint32
+
+type RABFSR struct{ mmio.U32 }
+
+func (r *RABFSR) LoadBits(mask ABFSR) ABFSR { return ABFSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RABFSR) StoreBits(mask, b ABFSR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RABFSR) SetBits(mask ABFSR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RABFSR) ClearBits(mask ABFSR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RABFSR) Load() ABFSR               { return ABFSR(r.U32.Load()) }
+func (r *RABFSR) Store(b ABFSR)             { r.U32.Store(uint32(b)) }
+
+type RMABFSR struct{ mmio.UM32 }
+
+func (rm RMABFSR) Load() ABFSR   { return ABFSR(rm.UM32.Load()) }
+func (rm RMABFSR) Store(b ABFSR) { rm.UM32.Store(uint32(b)) }
+
+func ITCM_(p *Periph) RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(ITCM)}}
+}
+
+func DTCM_(p *Periph) RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(DTCM)}}
+}
+
+func AHBP_(p *Periph) RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(AHBP)}}
+}
+
+func AXIM_(p *Periph) RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(AXIM)}}
+}
+
+func EPPB_(p *Periph) RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(EPPB)}}
+}
+
+func AXIMTYPE_(p *Periph) RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(AXIMTYPE)}}
+}
diff --git a/src/internal/cpu/cortexm/bitband/bitband_thumb.go b/src/internal/cpu/cortexm/bitband/bitband_thumb.go
new file mode 100644
index 0000000000..f2bfda8dcf
--- /dev/null
+++ b/src/internal/cpu/cortexm/bitband/bitband_thumb.go
@@ -0,0 +1,82 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package bitband
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Bit struct {
+	a *mmio.U32
+}
+
+func (b Bit) Load() int {
+	return int(b.a.Load())
+}
+
+func (b Bit) Store(v int) {
+	b.a.Store(uint32(v))
+}
+
+func (b Bit) Set() {
+	b.Store(1)
+}
+
+func (b Bit) Clear() {
+	b.Store(0)
+}
+
+// 0x20000000 - 0x200FFFFF: SRAM bit-band region.
+// 0x22000000 - 0x23FFFFFF: SRAM bit-band alias.
+//
+// 0x40000000 - 0x400FFFFF: peripheral bit-band region.
+// 0x42000000 - 0x43FFFFFF: peripheral bit-band alias.
+func bitAlias(addr unsafe.Pointer) unsafe.Pointer {
+	a := uintptr(addr)
+	base := a &^ 0xfffff
+	if base != 0x40000000 && base != 0x20000000 {
+		panic("bitband: not in region")
+	}
+	base += 0x2000000
+	offset := a & 0xfffff
+	return unsafe.Pointer(base + offset*32)
+}
+
+type Bits8 struct {
+	a *[8]mmio.U32
+}
+
+func (b Bits8) Bit(n int) Bit {
+	return Bit{&b.a[n]}
+}
+
+func Alias8(r *mmio.U8) Bits8 {
+	return Bits8{(*[8]mmio.U32)(bitAlias(unsafe.Pointer(r)))}
+}
+
+type Bits16 struct {
+	a *[16]mmio.U32
+}
+
+func (b Bits16) Bit(n int) Bit {
+	return Bit{&b.a[n]}
+}
+
+func Alias16(r *mmio.U16) Bits16 {
+	return Bits16{(*[16]mmio.U32)(bitAlias(unsafe.Pointer(r)))}
+}
+
+type Bits32 struct {
+	a *[32]mmio.U32
+}
+
+func (b Bits32) Bit(n int) Bit {
+	return Bit{&b.a[n]}
+}
+
+func Alias32(r *mmio.U32) Bits32 {
+	return Bits32{(*[32]mmio.U32)(bitAlias(unsafe.Pointer(r)))}
+}
diff --git a/src/internal/cpu/cortexm/bitband/doc.go b/src/internal/cpu/cortexm/bitband/doc.go
new file mode 100644
index 0000000000..5363525408
--- /dev/null
+++ b/src/internal/cpu/cortexm/bitband/doc.go
@@ -0,0 +1,7 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package bitband allows to use bit-banding feature of some Cortex-M
+// microcontrollers.
+package bitband
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/cmt/doc.go b/src/internal/cpu/cortexm/cmt/doc.go
new file mode 100644
index 0000000000..2539221cbd
--- /dev/null
+++ b/src/internal/cpu/cortexm/cmt/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package cmt provides access to the Cache maintenance registers.
+package cmt
diff --git a/src/internal/cpu/cortexm/cmt/periph_thumb.go b/src/internal/cpu/cortexm/cmt/periph_thumb.go
new file mode 100644
index 0000000000..33e7077927
--- /dev/null
+++ b/src/internal/cpu/cortexm/cmt/periph_thumb.go
@@ -0,0 +1,17 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  CMT  0xE000EF50  -  -  Cache maintenance registers
+// Registers:
+//  0x00 32  ICIALLU  Instr. cache invalidate all to the Point of Unification
+//  0x08 32  ICIMVAU  Instr. cache invalidate by address to the PoU
+//  0x0C 32  DCIMVAC  Data cache invalidate by address to the Point of Coherency
+//  0x10 32  DCISW    Data cache invalidate by set/way
+//  0x14 32  DCCMVAU  Data cache clean by address to the PoU
+//  0x18 32  DCCMVAC  Data cache clean by address to the PoC
+//  0x1c 32  DCCSW    Data cache clean by set/way
+//  0x20 32  DCCIMVAC Data cache clean and invalidate by address to the PoC
+//  0x24 32  DCCISW   Data cache clean and invalidate by set/way
+package cmt
diff --git a/src/internal/cpu/cortexm/cmt/xperiph_thumb.go b/src/internal/cpu/cortexm/cmt/xperiph_thumb.go
new file mode 100644
index 0000000000..6b8f7ae593
--- /dev/null
+++ b/src/internal/cpu/cortexm/cmt/xperiph_thumb.go
@@ -0,0 +1,171 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package cmt
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	ICIALLU  RICIALLU
+	_        uint32
+	ICIMVAU  RICIMVAU
+	DCIMVAC  RDCIMVAC
+	DCISW    RDCISW
+	DCCMVAU  RDCCMVAU
+	DCCMVAC  RDCCMVAC
+	DCCSW    RDCCSW
+	DCCIMVAC RDCCIMVAC
+	DCCISW   RDCCISW
+}
+
+func CMT() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000EF50))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type ICIALLU uint32
+
+type RICIALLU struct{ mmio.U32 }
+
+func (r *RICIALLU) LoadBits(mask ICIALLU) ICIALLU { return ICIALLU(r.U32.LoadBits(uint32(mask))) }
+func (r *RICIALLU) StoreBits(mask, b ICIALLU)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICIALLU) SetBits(mask ICIALLU)          { r.U32.SetBits(uint32(mask)) }
+func (r *RICIALLU) ClearBits(mask ICIALLU)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RICIALLU) Load() ICIALLU                 { return ICIALLU(r.U32.Load()) }
+func (r *RICIALLU) Store(b ICIALLU)               { r.U32.Store(uint32(b)) }
+
+type RMICIALLU struct{ mmio.UM32 }
+
+func (rm RMICIALLU) Load() ICIALLU   { return ICIALLU(rm.UM32.Load()) }
+func (rm RMICIALLU) Store(b ICIALLU) { rm.UM32.Store(uint32(b)) }
+
+type ICIMVAU uint32
+
+type RICIMVAU struct{ mmio.U32 }
+
+func (r *RICIMVAU) LoadBits(mask ICIMVAU) ICIMVAU { return ICIMVAU(r.U32.LoadBits(uint32(mask))) }
+func (r *RICIMVAU) StoreBits(mask, b ICIMVAU)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICIMVAU) SetBits(mask ICIMVAU)          { r.U32.SetBits(uint32(mask)) }
+func (r *RICIMVAU) ClearBits(mask ICIMVAU)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RICIMVAU) Load() ICIMVAU                 { return ICIMVAU(r.U32.Load()) }
+func (r *RICIMVAU) Store(b ICIMVAU)               { r.U32.Store(uint32(b)) }
+
+type RMICIMVAU struct{ mmio.UM32 }
+
+func (rm RMICIMVAU) Load() ICIMVAU   { return ICIMVAU(rm.UM32.Load()) }
+func (rm RMICIMVAU) Store(b ICIMVAU) { rm.UM32.Store(uint32(b)) }
+
+type DCIMVAC uint32
+
+type RDCIMVAC struct{ mmio.U32 }
+
+func (r *RDCIMVAC) LoadBits(mask DCIMVAC) DCIMVAC { return DCIMVAC(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCIMVAC) StoreBits(mask, b DCIMVAC)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCIMVAC) SetBits(mask DCIMVAC)          { r.U32.SetBits(uint32(mask)) }
+func (r *RDCIMVAC) ClearBits(mask DCIMVAC)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCIMVAC) Load() DCIMVAC                 { return DCIMVAC(r.U32.Load()) }
+func (r *RDCIMVAC) Store(b DCIMVAC)               { r.U32.Store(uint32(b)) }
+
+type RMDCIMVAC struct{ mmio.UM32 }
+
+func (rm RMDCIMVAC) Load() DCIMVAC   { return DCIMVAC(rm.UM32.Load()) }
+func (rm RMDCIMVAC) Store(b DCIMVAC) { rm.UM32.Store(uint32(b)) }
+
+type DCISW uint32
+
+type RDCISW struct{ mmio.U32 }
+
+func (r *RDCISW) LoadBits(mask DCISW) DCISW { return DCISW(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCISW) StoreBits(mask, b DCISW)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCISW) SetBits(mask DCISW)        { r.U32.SetBits(uint32(mask)) }
+func (r *RDCISW) ClearBits(mask DCISW)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCISW) Load() DCISW               { return DCISW(r.U32.Load()) }
+func (r *RDCISW) Store(b DCISW)             { r.U32.Store(uint32(b)) }
+
+type RMDCISW struct{ mmio.UM32 }
+
+func (rm RMDCISW) Load() DCISW   { return DCISW(rm.UM32.Load()) }
+func (rm RMDCISW) Store(b DCISW) { rm.UM32.Store(uint32(b)) }
+
+type DCCMVAU uint32
+
+type RDCCMVAU struct{ mmio.U32 }
+
+func (r *RDCCMVAU) LoadBits(mask DCCMVAU) DCCMVAU { return DCCMVAU(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCMVAU) StoreBits(mask, b DCCMVAU)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCMVAU) SetBits(mask DCCMVAU)          { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCMVAU) ClearBits(mask DCCMVAU)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCMVAU) Load() DCCMVAU                 { return DCCMVAU(r.U32.Load()) }
+func (r *RDCCMVAU) Store(b DCCMVAU)               { r.U32.Store(uint32(b)) }
+
+type RMDCCMVAU struct{ mmio.UM32 }
+
+func (rm RMDCCMVAU) Load() DCCMVAU   { return DCCMVAU(rm.UM32.Load()) }
+func (rm RMDCCMVAU) Store(b DCCMVAU) { rm.UM32.Store(uint32(b)) }
+
+type DCCMVAC uint32
+
+type RDCCMVAC struct{ mmio.U32 }
+
+func (r *RDCCMVAC) LoadBits(mask DCCMVAC) DCCMVAC { return DCCMVAC(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCMVAC) StoreBits(mask, b DCCMVAC)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCMVAC) SetBits(mask DCCMVAC)          { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCMVAC) ClearBits(mask DCCMVAC)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCMVAC) Load() DCCMVAC                 { return DCCMVAC(r.U32.Load()) }
+func (r *RDCCMVAC) Store(b DCCMVAC)               { r.U32.Store(uint32(b)) }
+
+type RMDCCMVAC struct{ mmio.UM32 }
+
+func (rm RMDCCMVAC) Load() DCCMVAC   { return DCCMVAC(rm.UM32.Load()) }
+func (rm RMDCCMVAC) Store(b DCCMVAC) { rm.UM32.Store(uint32(b)) }
+
+type DCCSW uint32
+
+type RDCCSW struct{ mmio.U32 }
+
+func (r *RDCCSW) LoadBits(mask DCCSW) DCCSW { return DCCSW(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCSW) StoreBits(mask, b DCCSW)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCSW) SetBits(mask DCCSW)        { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCSW) ClearBits(mask DCCSW)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCSW) Load() DCCSW               { return DCCSW(r.U32.Load()) }
+func (r *RDCCSW) Store(b DCCSW)             { r.U32.Store(uint32(b)) }
+
+type RMDCCSW struct{ mmio.UM32 }
+
+func (rm RMDCCSW) Load() DCCSW   { return DCCSW(rm.UM32.Load()) }
+func (rm RMDCCSW) Store(b DCCSW) { rm.UM32.Store(uint32(b)) }
+
+type DCCIMVAC uint32
+
+type RDCCIMVAC struct{ mmio.U32 }
+
+func (r *RDCCIMVAC) LoadBits(mask DCCIMVAC) DCCIMVAC { return DCCIMVAC(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCIMVAC) StoreBits(mask, b DCCIMVAC)      { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCIMVAC) SetBits(mask DCCIMVAC)           { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCIMVAC) ClearBits(mask DCCIMVAC)         { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCIMVAC) Load() DCCIMVAC                  { return DCCIMVAC(r.U32.Load()) }
+func (r *RDCCIMVAC) Store(b DCCIMVAC)                { r.U32.Store(uint32(b)) }
+
+type RMDCCIMVAC struct{ mmio.UM32 }
+
+func (rm RMDCCIMVAC) Load() DCCIMVAC   { return DCCIMVAC(rm.UM32.Load()) }
+func (rm RMDCCIMVAC) Store(b DCCIMVAC) { rm.UM32.Store(uint32(b)) }
+
+type DCCISW uint32
+
+type RDCCISW struct{ mmio.U32 }
+
+func (r *RDCCISW) LoadBits(mask DCCISW) DCCISW { return DCCISW(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCISW) StoreBits(mask, b DCCISW)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCISW) SetBits(mask DCCISW)         { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCISW) ClearBits(mask DCCISW)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCISW) Load() DCCISW                { return DCCISW(r.U32.Load()) }
+func (r *RDCCISW) Store(b DCCISW)              { r.U32.Store(uint32(b)) }
+
+type RMDCCISW struct{ mmio.UM32 }
+
+func (rm RMDCCISW) Load() DCCISW   { return DCCISW(rm.UM32.Load()) }
+func (rm RMDCCISW) Store(b DCCISW) { rm.UM32.Store(uint32(b)) }
diff --git a/src/internal/cpu/cortexm/debug/itm/doc.go b/src/internal/cpu/cortexm/debug/itm/doc.go
new file mode 100644
index 0000000000..b9833cab7a
--- /dev/null
+++ b/src/internal/cpu/cortexm/debug/itm/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package itm gives access to the Instrumentation Trace Macrocell registers.
+package itm
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/debug/itm/periph_thumb.go b/src/internal/cpu/cortexm/debug/itm/periph_thumb.go
new file mode 100644
index 0000000000..df77223e51
--- /dev/null
+++ b/src/internal/cpu/cortexm/debug/itm/periph_thumb.go
@@ -0,0 +1,31 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Peripheral: ITM_Periph
+// Instances:
+//  ITM  0xE0000000  -  -  Instrumentation Trace Macrocell registers
+// Registers:
+//  0x000  32  STIM[256]  Stimulus Port Registers
+//  0xE00  32  TER[8]     Trace Enable Register
+//  0xE40  32  TPR        Trace Privilege Register
+//  0xE80  32  TCR        Trace Control Register
+//  0xFD0  32  PID[8]     Peripheral Identification registers
+//  0xFF0  32  CID[4]     Component Identification registers
+package itm
+
+const (
+	ITMENA     TCR = 0x01 << 0  //+ enable ITM
+	TSENA      TCR = 0x01 << 1  //+ enable local timestamp generation
+	SYNCENA    TCR = 0x01 << 2  //+ enables packet transmission for a sync. TPIU
+	TXENA      TCR = 0x01 << 3  //+ enable forwarding packets from DWT to ITM
+	SWOENA     TCR = 0x01 << 4  //+ enable async. clocking of the timestamp cntr
+	TSPrescale TCR = 0x03 << 8  //+ local timestamp prescaler
+	TSP0       TCR = 0x00 << 8  //  no prescaling
+	TSP4       TCR = 0x01 << 8  //  divide by 4
+	TSP16      TCR = 0x02 << 8  //  divide by 16
+	TSP64      TCR = 0x03 << 8  //  divide by 64
+	GTSFREQ    TCR = 0x03 << 10 //+ global timestamp frequency
+	TraceBusID TCR = 0x7F << 16 //+ identifier for multi-source trace stream
+	BUSY       TCR = 0x01 << 23 //+ set if ITM is currently processing events
+)
diff --git a/src/internal/cpu/cortexm/debug/itm/xperiph_thumb.go b/src/internal/cpu/cortexm/debug/itm/xperiph_thumb.go
new file mode 100644
index 0000000000..e0a3a0f48d
--- /dev/null
+++ b/src/internal/cpu/cortexm/debug/itm/xperiph_thumb.go
@@ -0,0 +1,159 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package itm
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type ITM_Periph struct {
+	STIM [256]RSTIM
+	_    [640]uint32
+	TER  [8]RTER
+	_    [8]uint32
+	TPR  RTPR
+	_    [15]uint32
+	TCR  RTCR
+	_    [83]uint32
+	PID  [8]RPID
+	CID  [4]RCID
+}
+
+func ITM() *ITM_Periph { return (*ITM_Periph)(unsafe.Pointer(uintptr(0xE0000000))) }
+
+func (p *ITM_Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type STIM uint32
+
+type RSTIM struct{ mmio.U32 }
+
+func (r *RSTIM) LoadBits(mask STIM) STIM { return STIM(r.U32.LoadBits(uint32(mask))) }
+func (r *RSTIM) StoreBits(mask, b STIM)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSTIM) SetBits(mask STIM)       { r.U32.SetBits(uint32(mask)) }
+func (r *RSTIM) ClearBits(mask STIM)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RSTIM) Load() STIM              { return STIM(r.U32.Load()) }
+func (r *RSTIM) Store(b STIM)            { r.U32.Store(uint32(b)) }
+
+type RMSTIM struct{ mmio.UM32 }
+
+func (rm RMSTIM) Load() STIM   { return STIM(rm.UM32.Load()) }
+func (rm RMSTIM) Store(b STIM) { rm.UM32.Store(uint32(b)) }
+
+type TER uint32
+
+type RTER struct{ mmio.U32 }
+
+func (r *RTER) LoadBits(mask TER) TER { return TER(r.U32.LoadBits(uint32(mask))) }
+func (r *RTER) StoreBits(mask, b TER) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RTER) SetBits(mask TER)      { r.U32.SetBits(uint32(mask)) }
+func (r *RTER) ClearBits(mask TER)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RTER) Load() TER             { return TER(r.U32.Load()) }
+func (r *RTER) Store(b TER)           { r.U32.Store(uint32(b)) }
+
+type RMTER struct{ mmio.UM32 }
+
+func (rm RMTER) Load() TER   { return TER(rm.UM32.Load()) }
+func (rm RMTER) Store(b TER) { rm.UM32.Store(uint32(b)) }
+
+type TPR uint32
+
+type RTPR struct{ mmio.U32 }
+
+func (r *RTPR) LoadBits(mask TPR) TPR { return TPR(r.U32.LoadBits(uint32(mask))) }
+func (r *RTPR) StoreBits(mask, b TPR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RTPR) SetBits(mask TPR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RTPR) ClearBits(mask TPR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RTPR) Load() TPR             { return TPR(r.U32.Load()) }
+func (r *RTPR) Store(b TPR)           { r.U32.Store(uint32(b)) }
+
+type RMTPR struct{ mmio.UM32 }
+
+func (rm RMTPR) Load() TPR   { return TPR(rm.UM32.Load()) }
+func (rm RMTPR) Store(b TPR) { rm.UM32.Store(uint32(b)) }
+
+type TCR uint32
+
+type RTCR struct{ mmio.U32 }
+
+func (r *RTCR) LoadBits(mask TCR) TCR { return TCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RTCR) StoreBits(mask, b TCR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RTCR) SetBits(mask TCR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RTCR) ClearBits(mask TCR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RTCR) Load() TCR             { return TCR(r.U32.Load()) }
+func (r *RTCR) Store(b TCR)           { r.U32.Store(uint32(b)) }
+
+type RMTCR struct{ mmio.UM32 }
+
+func (rm RMTCR) Load() TCR   { return TCR(rm.UM32.Load()) }
+func (rm RMTCR) Store(b TCR) { rm.UM32.Store(uint32(b)) }
+
+func ITMENA_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(ITMENA)}}
+}
+
+func TSENA_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(TSENA)}}
+}
+
+func SYNCENA_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(SYNCENA)}}
+}
+
+func TXENA_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(TXENA)}}
+}
+
+func SWOENA_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(SWOENA)}}
+}
+
+func TSPrescale_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(TSPrescale)}}
+}
+
+func GTSFREQ_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(GTSFREQ)}}
+}
+
+func TraceBusID_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(TraceBusID)}}
+}
+
+func BUSY_(p *ITM_Periph) RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(BUSY)}}
+}
+
+type PID uint32
+
+type RPID struct{ mmio.U32 }
+
+func (r *RPID) LoadBits(mask PID) PID { return PID(r.U32.LoadBits(uint32(mask))) }
+func (r *RPID) StoreBits(mask, b PID) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RPID) SetBits(mask PID)      { r.U32.SetBits(uint32(mask)) }
+func (r *RPID) ClearBits(mask PID)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RPID) Load() PID             { return PID(r.U32.Load()) }
+func (r *RPID) Store(b PID)           { r.U32.Store(uint32(b)) }
+
+type RMPID struct{ mmio.UM32 }
+
+func (rm RMPID) Load() PID   { return PID(rm.UM32.Load()) }
+func (rm RMPID) Store(b PID) { rm.UM32.Store(uint32(b)) }
+
+type CID uint32
+
+type RCID struct{ mmio.U32 }
+
+func (r *RCID) LoadBits(mask CID) CID { return CID(r.U32.LoadBits(uint32(mask))) }
+func (r *RCID) StoreBits(mask, b CID) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCID) SetBits(mask CID)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCID) ClearBits(mask CID)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCID) Load() CID             { return CID(r.U32.Load()) }
+func (r *RCID) Store(b CID)           { r.U32.Store(uint32(b)) }
+
+type RMCID struct{ mmio.UM32 }
+
+func (rm RMCID) Load() CID   { return CID(rm.UM32.Load()) }
+func (rm RMCID) Store(b CID) { rm.UM32.Store(uint32(b)) }
diff --git a/src/internal/cpu/cortexm/doc.go b/src/internal/cpu/cortexm/doc.go
new file mode 100644
index 0000000000..a8525c9f3b
--- /dev/null
+++ b/src/internal/cpu/cortexm/doc.go
@@ -0,0 +1,7 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package cortexm defines types and constants specific to the ARMv7-M and
+// ARMv8-M ISA.
+package cortexm
diff --git a/src/internal/cpu/cortexm/exce_thumb.go b/src/internal/cpu/cortexm/exce_thumb.go
new file mode 100644
index 0000000000..9e0f637dd2
--- /dev/null
+++ b/src/internal/cpu/cortexm/exce_thumb.go
@@ -0,0 +1,37 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package cortexm
+
+// Cortex-M exception numbers.
+const (
+	Reset       = 1  // IRQ number:
+	NMI         = 2  // -14
+	HardFault   = 3  // -13
+	MemManage   = 4  // -12
+	BusFault    = 5  // -11
+	UsageFault  = 6  // -10
+	SecureFault = 7  //  -9
+	_           = 8  //  -8
+	_           = 9  //  -7
+	_           = 10 //  -6
+	SVCall      = 11 //  -5
+	DebugMon    = 12 //  -4
+	_           = 13 //  -3
+	PendSV      = 14 //  -2
+	SysTick     = 15 //  -1
+)
+
+// Exception number for the first external interrupt.
+const IRQ0 = 16
+
+// EXC_RETURN fields
+const (
+	ExcReturnBase    = 0xFFFFFFE0 // EXC_RETURN base value
+	ExcReturnMode    = 0xF        // Selects bits responsible for return mode:
+	ExcReturnHandler = 0x01       // - return to handler mode, use MSP
+	ExcReturnMSP     = 0x09       // - return to thread mode, use MSP
+	ExcReturnPSP     = 0x0D       // - return to thread mode, use PSP
+	ExcReturnNoFPU   = 0x10       // Basic frame on the stack (no FPU state)
+)
diff --git a/src/internal/cpu/cortexm/fpu/doc.go b/src/internal/cpu/cortexm/fpu/doc.go
new file mode 100644
index 0000000000..7d2c6433c5
--- /dev/null
+++ b/src/internal/cpu/cortexm/fpu/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package fpu gives access to the Floating Point Unit registers.
+package fpu
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/fpu/periph_thumb.go b/src/internal/cpu/cortexm/fpu/periph_thumb.go
new file mode 100644
index 0000000000..612d8394d8
--- /dev/null
+++ b/src/internal/cpu/cortexm/fpu/periph_thumb.go
@@ -0,0 +1,49 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  FPU  0xe000ED88  -  -  Floating Point Unit registers
+// Registers:
+//  0x000  32  CPACR   Coprocessor Access Control Register
+//  0x1AC  32  FPCCR   Floating-point Context Control Register
+//  0x1B0  32  FPCAR   Floating-point Context Address Register
+//  0x1B4  32  FPDSCR  Floating-point Default Status Control Register
+package fpu
+
+const (
+	CP10 CPACR = 3 << 20 //+ Access privileges for coprocessor 10.
+	CP11 CPACR = 3 << 22 //+ Access privileges for coprocessor 11.
+
+	CPACDENY CPACR = 0
+	CPACPRIV CPACR = 1
+	CPACFULL CPACR = 3
+)
+
+const (
+	CP10n = 20
+	CP11n = 22
+)
+
+const (
+	LSPACT FPCCR = 1 << 0  //+
+	USER   FPCCR = 1 << 1  //+
+	THREAD FPCCR = 1 << 3  //+
+	HFRDY  FPCCR = 1 << 4  //+
+	MMRDY  FPCCR = 1 << 5  //+
+	BFRDY  FPCCR = 1 << 6  //+
+	MONRDY FPCCR = 1 << 8  //+
+	LSPEN  FPCCR = 1 << 30 //+
+	ASPEN  FPCCR = 1 << 31 //+
+)
+
+const (
+	ADDRESS FPCAR = 0x3fffffff << 2 //+
+)
+
+const (
+	RMode FPDSCR = 3 << 22 //+
+	FZ    FPDSCR = 1 << 24 //+
+	DN    FPDSCR = 1 << 25 //+
+	AHP   FPDSCR = 1 << 26 //+
+)
diff --git a/src/internal/cpu/cortexm/fpu/xperiph_thumb.go b/src/internal/cpu/cortexm/fpu/xperiph_thumb.go
new file mode 100644
index 0000000000..b84e3c2bc3
--- /dev/null
+++ b/src/internal/cpu/cortexm/fpu/xperiph_thumb.go
@@ -0,0 +1,150 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package fpu
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	CPACR  RCPACR
+	_      [106]uint32
+	FPCCR  RFPCCR
+	FPCAR  RFPCAR
+	FPDSCR RFPDSCR
+}
+
+func FPU() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xe000ED88))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type CPACR uint32
+
+type RCPACR struct{ mmio.U32 }
+
+func (r *RCPACR) LoadBits(mask CPACR) CPACR { return CPACR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCPACR) StoreBits(mask, b CPACR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCPACR) SetBits(mask CPACR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RCPACR) ClearBits(mask CPACR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RCPACR) Load() CPACR               { return CPACR(r.U32.Load()) }
+func (r *RCPACR) Store(b CPACR)             { r.U32.Store(uint32(b)) }
+
+type RMCPACR struct{ mmio.UM32 }
+
+func (rm RMCPACR) Load() CPACR   { return CPACR(rm.UM32.Load()) }
+func (rm RMCPACR) Store(b CPACR) { rm.UM32.Store(uint32(b)) }
+
+func CP10_(p *Periph) RMCPACR {
+	return RMCPACR{mmio.UM32{&p.CPACR.U32, uint32(CP10)}}
+}
+
+func CP11_(p *Periph) RMCPACR {
+	return RMCPACR{mmio.UM32{&p.CPACR.U32, uint32(CP11)}}
+}
+
+type FPCCR uint32
+
+type RFPCCR struct{ mmio.U32 }
+
+func (r *RFPCCR) LoadBits(mask FPCCR) FPCCR { return FPCCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RFPCCR) StoreBits(mask, b FPCCR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RFPCCR) SetBits(mask FPCCR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RFPCCR) ClearBits(mask FPCCR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RFPCCR) Load() FPCCR               { return FPCCR(r.U32.Load()) }
+func (r *RFPCCR) Store(b FPCCR)             { r.U32.Store(uint32(b)) }
+
+type RMFPCCR struct{ mmio.UM32 }
+
+func (rm RMFPCCR) Load() FPCCR   { return FPCCR(rm.UM32.Load()) }
+func (rm RMFPCCR) Store(b FPCCR) { rm.UM32.Store(uint32(b)) }
+
+func LSPACT_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(LSPACT)}}
+}
+
+func USER_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(USER)}}
+}
+
+func THREAD_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(THREAD)}}
+}
+
+func HFRDY_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(HFRDY)}}
+}
+
+func MMRDY_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(MMRDY)}}
+}
+
+func BFRDY_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(BFRDY)}}
+}
+
+func MONRDY_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(MONRDY)}}
+}
+
+func LSPEN_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(LSPEN)}}
+}
+
+func ASPEN_(p *Periph) RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(ASPEN)}}
+}
+
+type FPCAR uint32
+
+type RFPCAR struct{ mmio.U32 }
+
+func (r *RFPCAR) LoadBits(mask FPCAR) FPCAR { return FPCAR(r.U32.LoadBits(uint32(mask))) }
+func (r *RFPCAR) StoreBits(mask, b FPCAR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RFPCAR) SetBits(mask FPCAR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RFPCAR) ClearBits(mask FPCAR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RFPCAR) Load() FPCAR               { return FPCAR(r.U32.Load()) }
+func (r *RFPCAR) Store(b FPCAR)             { r.U32.Store(uint32(b)) }
+
+type RMFPCAR struct{ mmio.UM32 }
+
+func (rm RMFPCAR) Load() FPCAR   { return FPCAR(rm.UM32.Load()) }
+func (rm RMFPCAR) Store(b FPCAR) { rm.UM32.Store(uint32(b)) }
+
+func ADDRESS_(p *Periph) RMFPCAR {
+	return RMFPCAR{mmio.UM32{&p.FPCAR.U32, uint32(ADDRESS)}}
+}
+
+type FPDSCR uint32
+
+type RFPDSCR struct{ mmio.U32 }
+
+func (r *RFPDSCR) LoadBits(mask FPDSCR) FPDSCR { return FPDSCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RFPDSCR) StoreBits(mask, b FPDSCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RFPDSCR) SetBits(mask FPDSCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RFPDSCR) ClearBits(mask FPDSCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RFPDSCR) Load() FPDSCR                { return FPDSCR(r.U32.Load()) }
+func (r *RFPDSCR) Store(b FPDSCR)              { r.U32.Store(uint32(b)) }
+
+type RMFPDSCR struct{ mmio.UM32 }
+
+func (rm RMFPDSCR) Load() FPDSCR   { return FPDSCR(rm.UM32.Load()) }
+func (rm RMFPDSCR) Store(b FPDSCR) { rm.UM32.Store(uint32(b)) }
+
+func RMode_(p *Periph) RMFPDSCR {
+	return RMFPDSCR{mmio.UM32{&p.FPDSCR.U32, uint32(RMode)}}
+}
+
+func FZ_(p *Periph) RMFPDSCR {
+	return RMFPDSCR{mmio.UM32{&p.FPDSCR.U32, uint32(FZ)}}
+}
+
+func DN_(p *Periph) RMFPDSCR {
+	return RMFPDSCR{mmio.UM32{&p.FPDSCR.U32, uint32(DN)}}
+}
+
+func AHP_(p *Periph) RMFPDSCR {
+	return RMFPDSCR{mmio.UM32{&p.FPDSCR.U32, uint32(AHP)}}
+}
diff --git a/src/internal/cpu/cortexm/mpu/doc.go b/src/internal/cpu/cortexm/mpu/doc.go
new file mode 100644
index 0000000000..ee9e171bef
--- /dev/null
+++ b/src/internal/cpu/cortexm/mpu/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package mpu provides interface to configure Cortex-M Memory Protection Unit
+package mpu
diff --git a/src/internal/cpu/cortexm/mpu/mpu_thumb.go b/src/internal/cpu/cortexm/mpu/mpu_thumb.go
new file mode 100644
index 0000000000..4bbb3e73df
--- /dev/null
+++ b/src/internal/cpu/cortexm/mpu/mpu_thumb.go
@@ -0,0 +1,119 @@
+package mpu
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type regs struct {
+	typ  mmio.U32
+	ctrl mmio.U32
+	rnr  mmio.U32
+	rbar mmio.U32
+	rasr mmio.U32
+}
+
+func p() *regs { return (*regs)(unsafe.Pointer(uintptr(0xE000ED90))) }
+
+// Type returns information about MPU unit:
+// i - number of supported instruction regions,
+// d - number of supported data regions.
+// s - true if separate instruction and data regions are supported.
+func Type() (i, d int, s bool) {
+	typ := p().typ.Load()
+	i = int(typ>>16) & 0xf
+	d = int(typ>>8) & 0xf
+	s = (typ&1 != 0)
+	return
+}
+
+type Flags uint32
+
+const (
+	// If ENABLE is set MPU is enabled.
+	ENABLE Flags = 1 << 0
+	// If HFNMIENA is not set the MPU will be disabled during HardFault, NMI
+	// and FAULTMASK handlers.
+	HFNMIENA Flags = 1 << 1
+	// If PRIVDEF is set the default memory map is used as background region for
+	// privileged software access.
+	PRIVDEFENA Flags = 1 << 2
+)
+
+// Set sets the flags specified by fl.
+func Set(fl Flags) { p().ctrl.SetBits(uint32(fl)) }
+
+// Clear clears the flags specified by fl.
+func Clear(fl Flags) { p().ctrl.ClearBits(uint32(fl)) }
+
+// State returns the current state.
+func State() Flags { return Flags(p().ctrl.Load()) }
+
+// Select selects the region number n.
+func Select(n int) { p().rnr.Store(uint32(n)) }
+
+type Attr uint32
+
+const (
+	ENA Attr = 1 << 0 // Enables region
+
+	B Attr = 1 << 16 // Bufferable
+	C Attr = 1 << 17 // Cacheable
+	S Attr = 1 << 18 // Shareable
+
+	// Access permissons.
+	Amask Attr = 7 << 24 // Use to extract access permission bits
+	A____ Attr = 0 << 24 // No access
+	Ar___ Attr = 5 << 24 // Priv-RO
+	Arw__ Attr = 1 << 24 // Priv-RW
+	Ar_r_ Attr = 6 << 24 // Priv-RO, Unpriv-RO
+	Arwr_ Attr = 2 << 24 // Priv-RW, Unpriv-RO
+	Arwrw Attr = 3 << 24 // Priv-RW, Unpriv-RW
+
+	XN Attr = 1 << 28 // Instruction access disable
+)
+
+func SIZE(exp int) Attr {
+	return Attr(exp-1) & 0x1f << 1
+}
+
+func (a Attr) SIZE() (exp int) {
+	return int(a>>1)&0x1f + 1
+}
+
+func SRD(srd uint8) Attr {
+	return Attr(srd) << 8
+}
+
+func (a Attr) SRD() uint8 {
+	return uint8(a >> 8)
+}
+
+func TEX(tex int) Attr {
+	return Attr(tex&7) << 19
+}
+
+func (a Attr) TEX() int {
+	return int(a>>19) & 7
+}
+
+const VALID = 1 << 4
+
+func SetRegion(base uintptr, attr Attr) {
+	p().rbar.Store(uint32(base))
+	p().rasr.Store(uint32(attr))
+}
+
+func Region() (base uintptr, attr Attr) {
+	return uintptr(p().rbar.Load()), Attr(p().rasr.Load())
+}
+
+/*
+TODO: Implement SetRegions using STM instrucion.
+type BaseAttr struct {
+	RBAR uintptr
+	RASR Attr
+}
+
+func SetRegions(bas [4]BaseAttr)
+*/
diff --git a/src/internal/cpu/cortexm/nvic/doc.go b/src/internal/cpu/cortexm/nvic/doc.go
new file mode 100644
index 0000000000..a5118b7bf2
--- /dev/null
+++ b/src/internal/cpu/cortexm/nvic/doc.go
@@ -0,0 +1,31 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package nvic gives access to the registers of the Nested Vectored Interrupt
+// Controller
+//
+// This package can not manage system exceptions. Use scb package instead.
+//
+// NVIC combines level and pulse sensing of interrupt signals. It is important
+// to understand this behavior to avoid subtle bugs in device drivers code.
+//
+// An interrupt can be in four states:
+//	1. Inactive: the interrupt is not active and not pending.
+//	2. Pending: the interrupt is waiting to be serviced by the processor.
+//	3. Active: the interrupt is being serviced by the processor.
+//	4. Active and pending: the interrupt is being serviced by the processor
+//	   and there is a pending interrupt from the same source.
+//
+// In simple terms, it can be assumed that level sensing is used in inactive
+// state, pulse sensing is used in active state. This behavior has two
+// important consequences:
+//
+// If the interrupt signal remains active after return from the interrupt
+// handler, the interrupt state becomes pending and the handler will be
+// executed again.
+//
+// If the interrupt signal was deasserted and next asserted again in active
+// state, the interrupt state changes to active and pending, the handler will
+// be executed again after return.
+package nvic
diff --git a/src/internal/cpu/cortexm/nvic/periph_thumb.go b/src/internal/cpu/cortexm/nvic/periph_thumb.go
new file mode 100644
index 0000000000..b802c68b35
--- /dev/null
+++ b/src/internal/cpu/cortexm/nvic/periph_thumb.go
@@ -0,0 +1,21 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// BUG: Cortex-M0 supports only 32-bit access on the Private Peripheral Bus but
+// the IPRs are defined as 8-bit registers. We prefer 8-bit access because it
+// allows changing the priority of different interrupts concurrently. If we
+// ever will support ARMv6-M all code that uses IPRs need to be fixed.
+
+// Instances:
+//  NVIC  0xE000E100  -  -  Nested Vectored Interrupt Controller
+// Registers:
+//  0x000  32  ISER[16]  Interrupt Set-Enable Registers
+//  0x080  32  ICER[16]  Interrupt Clear-Enable Registers
+//  0x100  32  ISPR[16]  Interrupt Set-Pending Registers
+//  0x180  32  ICPR[16]  Interrupt Clear-Pending Registers
+//  0x200  32  IABR[16]  Interrupt Active Bit Registers
+//  0x280  32  ITNS[16]  Interrupt Target Non-secure Registers
+//  0x300   8  IPR[496]  Interrupt Priority Registers
+//  0xE00  32  STIR      Software Trigger Interrupt Register
+package nvic
diff --git a/src/internal/cpu/cortexm/nvic/xperiph_thumb.go b/src/internal/cpu/cortexm/nvic/xperiph_thumb.go
new file mode 100644
index 0000000000..a08f34e3eb
--- /dev/null
+++ b/src/internal/cpu/cortexm/nvic/xperiph_thumb.go
@@ -0,0 +1,160 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package nvic
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	ISER [16]RISER
+	_    [16]uint32
+	ICER [16]RICER
+	_    [16]uint32
+	ISPR [16]RISPR
+	_    [16]uint32
+	ICPR [16]RICPR
+	_    [16]uint32
+	IABR [16]RIABR
+	_    [16]uint32
+	ITNS [16]RITNS
+	_    [16]uint32
+	IPR  [496]RIPR
+	_    [580]uint32
+	STIR RSTIR
+}
+
+func NVIC() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000E100))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type ISER uint32
+
+type RISER struct{ mmio.U32 }
+
+func (r *RISER) LoadBits(mask ISER) ISER { return ISER(r.U32.LoadBits(uint32(mask))) }
+func (r *RISER) StoreBits(mask, b ISER)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RISER) SetBits(mask ISER)       { r.U32.SetBits(uint32(mask)) }
+func (r *RISER) ClearBits(mask ISER)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RISER) Load() ISER              { return ISER(r.U32.Load()) }
+func (r *RISER) Store(b ISER)            { r.U32.Store(uint32(b)) }
+
+type RMISER struct{ mmio.UM32 }
+
+func (rm RMISER) Load() ISER   { return ISER(rm.UM32.Load()) }
+func (rm RMISER) Store(b ISER) { rm.UM32.Store(uint32(b)) }
+
+type ICER uint32
+
+type RICER struct{ mmio.U32 }
+
+func (r *RICER) LoadBits(mask ICER) ICER { return ICER(r.U32.LoadBits(uint32(mask))) }
+func (r *RICER) StoreBits(mask, b ICER)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICER) SetBits(mask ICER)       { r.U32.SetBits(uint32(mask)) }
+func (r *RICER) ClearBits(mask ICER)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RICER) Load() ICER              { return ICER(r.U32.Load()) }
+func (r *RICER) Store(b ICER)            { r.U32.Store(uint32(b)) }
+
+type RMICER struct{ mmio.UM32 }
+
+func (rm RMICER) Load() ICER   { return ICER(rm.UM32.Load()) }
+func (rm RMICER) Store(b ICER) { rm.UM32.Store(uint32(b)) }
+
+type ISPR uint32
+
+type RISPR struct{ mmio.U32 }
+
+func (r *RISPR) LoadBits(mask ISPR) ISPR { return ISPR(r.U32.LoadBits(uint32(mask))) }
+func (r *RISPR) StoreBits(mask, b ISPR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RISPR) SetBits(mask ISPR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RISPR) ClearBits(mask ISPR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RISPR) Load() ISPR              { return ISPR(r.U32.Load()) }
+func (r *RISPR) Store(b ISPR)            { r.U32.Store(uint32(b)) }
+
+type RMISPR struct{ mmio.UM32 }
+
+func (rm RMISPR) Load() ISPR   { return ISPR(rm.UM32.Load()) }
+func (rm RMISPR) Store(b ISPR) { rm.UM32.Store(uint32(b)) }
+
+type ICPR uint32
+
+type RICPR struct{ mmio.U32 }
+
+func (r *RICPR) LoadBits(mask ICPR) ICPR { return ICPR(r.U32.LoadBits(uint32(mask))) }
+func (r *RICPR) StoreBits(mask, b ICPR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICPR) SetBits(mask ICPR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RICPR) ClearBits(mask ICPR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RICPR) Load() ICPR              { return ICPR(r.U32.Load()) }
+func (r *RICPR) Store(b ICPR)            { r.U32.Store(uint32(b)) }
+
+type RMICPR struct{ mmio.UM32 }
+
+func (rm RMICPR) Load() ICPR   { return ICPR(rm.UM32.Load()) }
+func (rm RMICPR) Store(b ICPR) { rm.UM32.Store(uint32(b)) }
+
+type IABR uint32
+
+type RIABR struct{ mmio.U32 }
+
+func (r *RIABR) LoadBits(mask IABR) IABR { return IABR(r.U32.LoadBits(uint32(mask))) }
+func (r *RIABR) StoreBits(mask, b IABR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RIABR) SetBits(mask IABR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RIABR) ClearBits(mask IABR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RIABR) Load() IABR              { return IABR(r.U32.Load()) }
+func (r *RIABR) Store(b IABR)            { r.U32.Store(uint32(b)) }
+
+type RMIABR struct{ mmio.UM32 }
+
+func (rm RMIABR) Load() IABR   { return IABR(rm.UM32.Load()) }
+func (rm RMIABR) Store(b IABR) { rm.UM32.Store(uint32(b)) }
+
+type ITNS uint32
+
+type RITNS struct{ mmio.U32 }
+
+func (r *RITNS) LoadBits(mask ITNS) ITNS { return ITNS(r.U32.LoadBits(uint32(mask))) }
+func (r *RITNS) StoreBits(mask, b ITNS)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RITNS) SetBits(mask ITNS)       { r.U32.SetBits(uint32(mask)) }
+func (r *RITNS) ClearBits(mask ITNS)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RITNS) Load() ITNS              { return ITNS(r.U32.Load()) }
+func (r *RITNS) Store(b ITNS)            { r.U32.Store(uint32(b)) }
+
+type RMITNS struct{ mmio.UM32 }
+
+func (rm RMITNS) Load() ITNS   { return ITNS(rm.UM32.Load()) }
+func (rm RMITNS) Store(b ITNS) { rm.UM32.Store(uint32(b)) }
+
+type IPR uint8
+
+type RIPR struct{ mmio.U8 }
+
+func (r *RIPR) LoadBits(mask IPR) IPR { return IPR(r.U8.LoadBits(uint8(mask))) }
+func (r *RIPR) StoreBits(mask, b IPR) { r.U8.StoreBits(uint8(mask), uint8(b)) }
+func (r *RIPR) SetBits(mask IPR)      { r.U8.SetBits(uint8(mask)) }
+func (r *RIPR) ClearBits(mask IPR)    { r.U8.ClearBits(uint8(mask)) }
+func (r *RIPR) Load() IPR             { return IPR(r.U8.Load()) }
+func (r *RIPR) Store(b IPR)           { r.U8.Store(uint8(b)) }
+
+type RMIPR struct{ mmio.UM8 }
+
+func (rm RMIPR) Load() IPR   { return IPR(rm.UM8.Load()) }
+func (rm RMIPR) Store(b IPR) { rm.UM8.Store(uint8(b)) }
+
+type STIR uint32
+
+type RSTIR struct{ mmio.U32 }
+
+func (r *RSTIR) LoadBits(mask STIR) STIR { return STIR(r.U32.LoadBits(uint32(mask))) }
+func (r *RSTIR) StoreBits(mask, b STIR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSTIR) SetBits(mask STIR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RSTIR) ClearBits(mask STIR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RSTIR) Load() STIR              { return STIR(r.U32.Load()) }
+func (r *RSTIR) Store(b STIR)            { r.U32.Store(uint32(b)) }
+
+type RMSTIR struct{ mmio.UM32 }
+
+func (rm RMSTIR) Load() STIR   { return STIR(rm.UM32.Load()) }
+func (rm RMSTIR) Store(b STIR) { rm.UM32.Store(uint32(b)) }
diff --git a/src/internal/cpu/cortexm/pft/doc.go b/src/internal/cpu/cortexm/pft/doc.go
new file mode 100644
index 0000000000..730d0d07c6
--- /dev/null
+++ b/src/internal/cpu/cortexm/pft/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package pft provides an access to the Processor features registers.
+package pft
diff --git a/src/internal/cpu/cortexm/pft/periph_thumb.go b/src/internal/cpu/cortexm/pft/periph_thumb.go
new file mode 100644
index 0000000000..8e24cb41a5
--- /dev/null
+++ b/src/internal/cpu/cortexm/pft/periph_thumb.go
@@ -0,0 +1,112 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  PFT  0xE000ED78  -  -  Processor features registers
+// Registers:
+//  0x00 32  CLIDR   Cache Level ID
+//  0x00 32  CTR     Cache Type
+//  0x00 32  CCSIDR  Cache Size ID
+//  0x00 32  CSSELR  Cache Size Selection
+package pft
+
+const (
+	CL1I  CLIDR = 1 << 0    //+ Instruction L1 cache implemented.
+	CL1D  CLIDR = 1 << 1    //+ Data cache L1 implemented.
+	CL1U  CLIDR = 1 << 2    //+ Unified L1 cache.
+	CL2I  CLIDR = 1 << 3    //+ Instruction L2 cache implemented.
+	CL2D  CLIDR = 1 << 4    //+ Data cache L2 implemented.
+	CL2U  CLIDR = 1 << 5    //+ Unified L2 cache.
+	CL3I  CLIDR = 1 << 6    //+ Instruction L3 cache implemented.
+	CL3D  CLIDR = 1 << 7    //+ Data cache L3 implemented.
+	CL3U  CLIDR = 1 << 8    //+ Unified L3 cache.
+	CL4I  CLIDR = 1 << 9    //+ Instruction L4 cache implemented.
+	CL4D  CLIDR = 1 << 10   //+ Data cache L4 implemented.
+	CL4U  CLIDR = 1 << 11   //+ Unified L4 cache.
+	CL5I  CLIDR = 1 << 12   //+ Instruction L5 cache implemented.
+	CL5D  CLIDR = 1 << 13   //+ Data cache L5 implemented.
+	CL5U  CLIDR = 1 << 14   //+ Unified L5 cache.
+	CL6I  CLIDR = 1 << 15   //+ Instruction L6 cache implemented.
+	CL6D  CLIDR = 1 << 16   //+ Data cache L6 implemented.
+	CL6U  CLIDR = 1 << 17   //+ Unified L6 cache.
+	CL7I  CLIDR = 1 << 18   //+ Instruction L7 cache implemented.
+	CL7D  CLIDR = 1 << 19   //+ Data cache L7 implemented.
+	CL7U  CLIDR = 1 << 20   //+ Unified L7 cache.
+	LoUIS CLIDR = 0x7 << 21 //+
+	LoC   CLIDR = 0x7 << 24 //+ Level of Coherency.
+	LoU   CLIDR = 0x7 << 27 //+ Level of Unification.
+)
+
+const (
+	CL1In  = 0
+	CL1Dn  = 1
+	CL1Un  = 2
+	CL2In  = 3
+	CL2Dn  = 4
+	CL2Un  = 5
+	CL3In  = 6
+	CL3Dn  = 7
+	CL3Un  = 8
+	CL4In  = 9
+	CL4Dn  = 10
+	CL4Un  = 11
+	CL5In  = 12
+	CL5Dn  = 13
+	CL5Un  = 14
+	CL6In  = 15
+	CL6Dn  = 16
+	CL6Un  = 17
+	CL7In  = 18
+	CL7Dn  = 19
+	CL7Un  = 20
+	LoUISn = 21
+	LoCn   = 24
+	LoUn   = 27
+)
+
+const (
+	IMinLine CTR = 0xf << 0  //+ Smallest cache line of all the I-caches.
+	DMinLine CTR = 0xf << 16 //+ Smallest cache line of all the D/U-caches.
+	ERG      CTR = 0xf << 20 //+ Exclusives Reservation Granule.
+	CWG      CTR = 0xf << 24 //+ Cache Writeback Granule.
+	Format   CTR = 0x7 << 29 //+ Register format (4: ARMv7 format).
+)
+
+const (
+	IMinLinen = 0
+	DMinLinen = 16
+	ERGn      = 20
+	CWGn      = 24
+	Formatn   = 29
+)
+
+const (
+	LineSize      CCSIDR = 0x7 << 0     //+ Number of words in cache line (log2(n)-2).
+	Associativity CCSIDR = 0x3ff << 3   //+ Number of ways - 1.
+	NumSets       CCSIDR = 0x7fff << 13 //+ Number of sets - 1.
+	WA            CCSIDR = 1 << 28      //+ Write allocation support.
+	RA            CCSIDR = 1 << 29      //+ Read allocation support.
+	WB            CCSIDR = 1 << 30      //+ Write-Back support.
+	WT            CCSIDR = 1 << 31      //+ Write-Through support.
+)
+
+const (
+	LineSizen      = 0
+	Associativityn = 3
+	NumSetsn       = 13
+	WAn            = 28
+	RAn            = 29
+	WBn            = 30
+	WTn            = 31
+)
+
+const (
+	InD   CSSELR = 1 << 0   //+ Selection of 1:instruction or 0:data cache.
+	Level CSSELR = 0x7 << 1 //+ Cache level selected (0: level1).
+)
+
+const (
+	InDn   = 0
+	Leveln = 1
+)
diff --git a/src/internal/cpu/cortexm/pft/xperiph_thumb.go b/src/internal/cpu/cortexm/pft/xperiph_thumb.go
new file mode 100644
index 0000000000..306c0f3eed
--- /dev/null
+++ b/src/internal/cpu/cortexm/pft/xperiph_thumb.go
@@ -0,0 +1,237 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package pft
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	CLIDR  RCLIDR
+	CTR    RCTR
+	CCSIDR RCCSIDR
+	CSSELR RCSSELR
+}
+
+func PFT() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000ED78))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type CLIDR uint32
+
+type RCLIDR struct{ mmio.U32 }
+
+func (r *RCLIDR) LoadBits(mask CLIDR) CLIDR { return CLIDR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCLIDR) StoreBits(mask, b CLIDR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCLIDR) SetBits(mask CLIDR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RCLIDR) ClearBits(mask CLIDR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RCLIDR) Load() CLIDR               { return CLIDR(r.U32.Load()) }
+func (r *RCLIDR) Store(b CLIDR)             { r.U32.Store(uint32(b)) }
+
+type RMCLIDR struct{ mmio.UM32 }
+
+func (rm RMCLIDR) Load() CLIDR   { return CLIDR(rm.UM32.Load()) }
+func (rm RMCLIDR) Store(b CLIDR) { rm.UM32.Store(uint32(b)) }
+
+func CL1I_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL1I)}}
+}
+
+func CL1D_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL1D)}}
+}
+
+func CL1U_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL1U)}}
+}
+
+func CL2I_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL2I)}}
+}
+
+func CL2D_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL2D)}}
+}
+
+func CL2U_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL2U)}}
+}
+
+func CL3I_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL3I)}}
+}
+
+func CL3D_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL3D)}}
+}
+
+func CL3U_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL3U)}}
+}
+
+func CL4I_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL4I)}}
+}
+
+func CL4D_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL4D)}}
+}
+
+func CL4U_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL4U)}}
+}
+
+func CL5I_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL5I)}}
+}
+
+func CL5D_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL5D)}}
+}
+
+func CL5U_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL5U)}}
+}
+
+func CL6I_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL6I)}}
+}
+
+func CL6D_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL6D)}}
+}
+
+func CL6U_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL6U)}}
+}
+
+func CL7I_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL7I)}}
+}
+
+func CL7D_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL7D)}}
+}
+
+func CL7U_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL7U)}}
+}
+
+func LoUIS_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(LoUIS)}}
+}
+
+func LoC_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(LoC)}}
+}
+
+func LoU_(p *Periph) RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(LoU)}}
+}
+
+type CTR uint32
+
+type RCTR struct{ mmio.U32 }
+
+func (r *RCTR) LoadBits(mask CTR) CTR { return CTR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCTR) StoreBits(mask, b CTR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCTR) SetBits(mask CTR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCTR) ClearBits(mask CTR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCTR) Load() CTR             { return CTR(r.U32.Load()) }
+func (r *RCTR) Store(b CTR)           { r.U32.Store(uint32(b)) }
+
+type RMCTR struct{ mmio.UM32 }
+
+func (rm RMCTR) Load() CTR   { return CTR(rm.UM32.Load()) }
+func (rm RMCTR) Store(b CTR) { rm.UM32.Store(uint32(b)) }
+
+func IMinLine_(p *Periph) RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(IMinLine)}}
+}
+
+func DMinLine_(p *Periph) RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(DMinLine)}}
+}
+
+func ERG_(p *Periph) RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(ERG)}}
+}
+
+func CWG_(p *Periph) RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(CWG)}}
+}
+
+func Format_(p *Periph) RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(Format)}}
+}
+
+type CCSIDR uint32
+
+type RCCSIDR struct{ mmio.U32 }
+
+func (r *RCCSIDR) LoadBits(mask CCSIDR) CCSIDR { return CCSIDR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCCSIDR) StoreBits(mask, b CCSIDR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCCSIDR) SetBits(mask CCSIDR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RCCSIDR) ClearBits(mask CCSIDR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RCCSIDR) Load() CCSIDR                { return CCSIDR(r.U32.Load()) }
+func (r *RCCSIDR) Store(b CCSIDR)              { r.U32.Store(uint32(b)) }
+
+type RMCCSIDR struct{ mmio.UM32 }
+
+func (rm RMCCSIDR) Load() CCSIDR   { return CCSIDR(rm.UM32.Load()) }
+func (rm RMCCSIDR) Store(b CCSIDR) { rm.UM32.Store(uint32(b)) }
+
+func LineSize_(p *Periph) RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(LineSize)}}
+}
+
+func Associativity_(p *Periph) RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(Associativity)}}
+}
+
+func NumSets_(p *Periph) RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(NumSets)}}
+}
+
+func WA_(p *Periph) RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(WA)}}
+}
+
+func RA_(p *Periph) RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(RA)}}
+}
+
+func WB_(p *Periph) RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(WB)}}
+}
+
+func WT_(p *Periph) RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(WT)}}
+}
+
+type CSSELR uint32
+
+type RCSSELR struct{ mmio.U32 }
+
+func (r *RCSSELR) LoadBits(mask CSSELR) CSSELR { return CSSELR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCSSELR) StoreBits(mask, b CSSELR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCSSELR) SetBits(mask CSSELR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RCSSELR) ClearBits(mask CSSELR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RCSSELR) Load() CSSELR                { return CSSELR(r.U32.Load()) }
+func (r *RCSSELR) Store(b CSSELR)              { r.U32.Store(uint32(b)) }
+
+type RMCSSELR struct{ mmio.UM32 }
+
+func (rm RMCSSELR) Load() CSSELR   { return CSSELR(rm.UM32.Load()) }
+func (rm RMCSSELR) Store(b CSSELR) { rm.UM32.Store(uint32(b)) }
+
+func InD_(p *Periph) RMCSSELR {
+	return RMCSSELR{mmio.UM32{&p.CSSELR.U32, uint32(InD)}}
+}
+
+func Level_(p *Periph) RMCSSELR {
+	return RMCSSELR{mmio.UM32{&p.CSSELR.U32, uint32(Level)}}
+}
diff --git a/src/internal/cpu/cortexm/scb/doc.go b/src/internal/cpu/cortexm/scb/doc.go
new file mode 100644
index 0000000000..990699bad6
--- /dev/null
+++ b/src/internal/cpu/cortexm/scb/doc.go
@@ -0,0 +1,14 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package scb gives access to the registers of the System Control Block.
+//
+// Notes:
+//
+// 1. Cortex-M0 does not implement SHPR1, CFSR, HFSR, MMFR, BFAR, AFSR
+// registers.
+//
+// 2. Cortex-M0 supports only word access to SHPR2, SHPR3 so this package does
+// not provide a byte access to the individual fields.
+package scb
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/scb/periph_thumb.go b/src/internal/cpu/cortexm/scb/periph_thumb.go
new file mode 100644
index 0000000000..a9de6b48f3
--- /dev/null
+++ b/src/internal/cpu/cortexm/scb/periph_thumb.go
@@ -0,0 +1,172 @@
+// Instances:
+//  SCB  0xE000ED00  -  -  System Control Block
+// Registers:
+//  0x00  32  CPUID  CPUID Base Register
+//  0x04  32  ICSR   Interrupt Control and State Register
+//  0x08  32  VTOR   Vector Table Offset Register
+//  0x0C  32  AIRCR  Application Interrupt and Reset Control Register
+//  0x10  32  SCR    System Control Register
+//  0x14  32  CCR    Configuration and Control Register
+//  0x18  32  SHPR1  System Handler Priority Register 1
+//  0x1C  32  SHPR2  System Handler Priority Register 2
+//  0x20  32  SHPR3  System Handler Priority Register 3
+//  0x24  32  SHCSR  System Handler Control and State Register
+//  0x28  32  CFSR   Configurable Fault Status Register
+//  0x2C  32  HFSR   HardFault Status Register
+//  0x34  32  MMFR   MemManage Fault Address Register
+//  0x38  32  BFAR   BusFault Address Register
+//  0x3C  32  AFSR   Auxiliary Fault Status Register
+package scb
+
+const (
+	Revision    CPUID = 0xf << 0   //+
+	PartNo      CPUID = 0xfff << 4 //+
+	Constant    CPUID = 0xf << 16  //+
+	Variant     CPUID = 0xf << 20  //+
+	Implementer CPUID = 0xff << 24 //+
+)
+
+const (
+	Revisionn    = 0
+	PartNon      = 4
+	Constantn    = 16
+	Variantn     = 20
+	Implementern = 24
+)
+
+const (
+	VECTACTIVE  ICSR = 0x1ff << 0  //+ Active exception number (0: thread mode)
+	RETTOBASE   ICSR = 1 << 11     //+ No preempted active exceptions
+	VECTPENDING ICSR = 0x1ff << 12 //+ Highest priority pending exception number
+	ISRPENDING  ICSR = 1 << 22     //+ Int. pending flag, excluding NMI, Faults
+	PENDSTCLR   ICSR = 1 << 25     //+ SysTick exception clear-pending bit
+	PENDSTSET   ICSR = 1 << 26     //+ SysTick exception set-pending bit
+	PENDSVCLR   ICSR = 1 << 27     //+ PendSV clear-pending bit
+	PENDSVSET   ICSR = 1 << 28     //+ PendSV set-pending bit
+	NMIPENDSET  ICSR = 1 << 31     //+ NMI set-pending bit
+)
+
+const (
+	VECTACTIVEn  = 0
+	VECTPENDINGn = 12
+)
+
+const (
+	TBLOFF VTOR = 0x1ffffff << 7 //+
+)
+
+const (
+	VECTRESET     AIRCR = 1 << 0       //+
+	VECTCLRACTIVE AIRCR = 1 << 1       //+
+	SYSRESETREQ   AIRCR = 1 << 2       //+
+	PRIGROUP      AIRCR = 7 << 8       //+
+	ENDIANNESS    AIRCR = 1 << 15      //+
+	VECTKEY       AIRCR = 0xffff << 16 //+
+)
+
+const (
+	VECTRESETn     = 0
+	VECTCLRACTIVEn = 1
+	SYSRESETREQn   = 2
+	PRIGROUPn      = 8
+	ENDIANNESSn    = 15
+	VECTKEYn       = 16
+)
+
+const (
+	SLEEPONEXIT SCR = 1 << 1 //+
+	SLEEPDEEP   SCR = 1 << 2 //+
+	SEVONPEND   SCR = 1 << 4 //+
+)
+
+const (
+	NONBASETHRDENA CCR = 1 << 0  //+
+	USERSETMPEND   CCR = 1 << 1  //+
+	UNALIGN_TRP    CCR = 1 << 3  //+
+	DIV_0_TRP      CCR = 1 << 4  //+
+	BFHFNMIGN      CCR = 1 << 8  //+
+	STKALIGN       CCR = 1 << 9  //+ Stack 8 byte aligned on exception entry
+	DC             CCR = 1 << 16 //+ Enable data cache
+	IC             CCR = 1 << 17 //+ Enable instruction cache
+	BP             CCR = 1 << 18 //+ Branch prediction is enabled
+)
+
+const (
+	PRI_MemManage  SHPR1 = 0xff << 0  //+
+	PRI_BusFault   SHPR1 = 0xff << 8  //+
+	PRI_UsageFault SHPR1 = 0xff << 16 //+
+)
+
+const (
+	PRI_MemManagen  = 0
+	PRI_BusFaultn   = 8
+	PRI_UsageFaultn = 16
+)
+
+const (
+	PRI_SVCall SHPR2 = 0xff << 24 //+
+)
+
+const (
+	PRI_SVCalln = 24
+)
+
+const (
+	PRI_PendSV  SHPR3 = 0xff << 16 //+
+	PRI_SysTick SHPR3 = 0xff << 24 //+
+)
+
+const (
+	PRI_PendSVn  = 16
+	PRI_SysTickn = 24
+)
+
+const (
+	MEMFAULTACT    SHCSR = 1 << 0  //+
+	BUSFAULTACT    SHCSR = 1 << 1  //+
+	USGFAULTACT    SHCSR = 1 << 3  //+
+	SVCALLACT      SHCSR = 1 << 7  //+
+	MONITORACT     SHCSR = 1 << 8  //+
+	PENDSVACT      SHCSR = 1 << 10 //+
+	SYSTICKACT     SHCSR = 1 << 11 //+
+	USGFAULTPENDED SHCSR = 1 << 12 //+
+	MEMFAULTPENDED SHCSR = 1 << 13 //+
+	BUSFAULTPENDED SHCSR = 1 << 14 //+
+	SVCALLPENDED   SHCSR = 1 << 15 //+
+	MEMFAULTENA    SHCSR = 1 << 16 //+
+	BUSFAULTENA    SHCSR = 1 << 17 //+
+	USGFAULTENA    SHCSR = 1 << 18 //+
+)
+
+const (
+	// MFSR
+	IACCVIOL  CFSR = 1 << 0 //+
+	DACCVIOL  CFSR = 1 << 1 //+
+	MUNSTKERR CFSR = 1 << 3 //+
+	MSTKERR   CFSR = 1 << 4 //+
+	MLSPERR   CFSR = 1 << 5 //+
+	MMARVALID CFSR = 1 << 7 //+
+
+	// BFSR
+	IBUSERR     CFSR = 1 << 8  //+
+	PRECISERR   CFSR = 1 << 9  //+
+	IMPRECISERR CFSR = 1 << 10 //+
+	UNSTKERR    CFSR = 1 << 11 //+
+	STKERR      CFSR = 1 << 12 //+
+	LSPERR      CFSR = 1 << 13 //+
+	BFARVALID   CFSR = 1 << 15 //+
+
+	// UFSR
+	UNDEFINSTR CFSR = 1 << 16 //+
+	INVSTATE   CFSR = 1 << 17 //+
+	INVPC      CFSR = 1 << 18 //+
+	NOCP       CFSR = 1 << 19 //+
+	UNALIGNED  CFSR = 1 << 24 //+
+	DIVBYZERO  CFSR = 1 << 25 //+
+)
+
+const (
+	VECTTBL  HFSR = 1 << 1  //+
+	FORCED   HFSR = 1 << 30 //+
+	DEBUGEVT HFSR = 1 << 31 //+
+)
diff --git a/src/internal/cpu/cortexm/scb/xperiph_thumb.go b/src/internal/cpu/cortexm/scb/xperiph_thumb.go
new file mode 100644
index 0000000000..a3a39df0e5
--- /dev/null
+++ b/src/internal/cpu/cortexm/scb/xperiph_thumb.go
@@ -0,0 +1,573 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package scb
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	CPUID RCPUID
+	ICSR  RICSR
+	VTOR  RVTOR
+	AIRCR RAIRCR
+	SCR   RSCR
+	CCR   RCCR
+	SHPR1 RSHPR1
+	SHPR2 RSHPR2
+	SHPR3 RSHPR3
+	SHCSR RSHCSR
+	CFSR  RCFSR
+	HFSR  RHFSR
+	_     uint32
+	MMFR  RMMFR
+	BFAR  RBFAR
+	AFSR  RAFSR
+}
+
+func SCB() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000ED00))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type CPUID uint32
+
+type RCPUID struct{ mmio.U32 }
+
+func (r *RCPUID) LoadBits(mask CPUID) CPUID { return CPUID(r.U32.LoadBits(uint32(mask))) }
+func (r *RCPUID) StoreBits(mask, b CPUID)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCPUID) SetBits(mask CPUID)        { r.U32.SetBits(uint32(mask)) }
+func (r *RCPUID) ClearBits(mask CPUID)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RCPUID) Load() CPUID               { return CPUID(r.U32.Load()) }
+func (r *RCPUID) Store(b CPUID)             { r.U32.Store(uint32(b)) }
+
+type RMCPUID struct{ mmio.UM32 }
+
+func (rm RMCPUID) Load() CPUID   { return CPUID(rm.UM32.Load()) }
+func (rm RMCPUID) Store(b CPUID) { rm.UM32.Store(uint32(b)) }
+
+func Revision_(p *Periph) RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(Revision)}}
+}
+
+func PartNo_(p *Periph) RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(PartNo)}}
+}
+
+func Constant_(p *Periph) RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(Constant)}}
+}
+
+func Variant_(p *Periph) RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(Variant)}}
+}
+
+func Implementer_(p *Periph) RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(Implementer)}}
+}
+
+type ICSR uint32
+
+type RICSR struct{ mmio.U32 }
+
+func (r *RICSR) LoadBits(mask ICSR) ICSR { return ICSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RICSR) StoreBits(mask, b ICSR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICSR) SetBits(mask ICSR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RICSR) ClearBits(mask ICSR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RICSR) Load() ICSR              { return ICSR(r.U32.Load()) }
+func (r *RICSR) Store(b ICSR)            { r.U32.Store(uint32(b)) }
+
+type RMICSR struct{ mmio.UM32 }
+
+func (rm RMICSR) Load() ICSR   { return ICSR(rm.UM32.Load()) }
+func (rm RMICSR) Store(b ICSR) { rm.UM32.Store(uint32(b)) }
+
+func VECTACTIVE_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(VECTACTIVE)}}
+}
+
+func RETTOBASE_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(RETTOBASE)}}
+}
+
+func VECTPENDING_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(VECTPENDING)}}
+}
+
+func ISRPENDING_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(ISRPENDING)}}
+}
+
+func PENDSTCLR_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(PENDSTCLR)}}
+}
+
+func PENDSTSET_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(PENDSTSET)}}
+}
+
+func PENDSVCLR_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(PENDSVCLR)}}
+}
+
+func PENDSVSET_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(PENDSVSET)}}
+}
+
+func NMIPENDSET_(p *Periph) RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(NMIPENDSET)}}
+}
+
+type VTOR uint32
+
+type RVTOR struct{ mmio.U32 }
+
+func (r *RVTOR) LoadBits(mask VTOR) VTOR { return VTOR(r.U32.LoadBits(uint32(mask))) }
+func (r *RVTOR) StoreBits(mask, b VTOR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RVTOR) SetBits(mask VTOR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RVTOR) ClearBits(mask VTOR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RVTOR) Load() VTOR              { return VTOR(r.U32.Load()) }
+func (r *RVTOR) Store(b VTOR)            { r.U32.Store(uint32(b)) }
+
+type RMVTOR struct{ mmio.UM32 }
+
+func (rm RMVTOR) Load() VTOR   { return VTOR(rm.UM32.Load()) }
+func (rm RMVTOR) Store(b VTOR) { rm.UM32.Store(uint32(b)) }
+
+func TBLOFF_(p *Periph) RMVTOR {
+	return RMVTOR{mmio.UM32{&p.VTOR.U32, uint32(TBLOFF)}}
+}
+
+type AIRCR uint32
+
+type RAIRCR struct{ mmio.U32 }
+
+func (r *RAIRCR) LoadBits(mask AIRCR) AIRCR { return AIRCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RAIRCR) StoreBits(mask, b AIRCR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RAIRCR) SetBits(mask AIRCR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RAIRCR) ClearBits(mask AIRCR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RAIRCR) Load() AIRCR               { return AIRCR(r.U32.Load()) }
+func (r *RAIRCR) Store(b AIRCR)             { r.U32.Store(uint32(b)) }
+
+type RMAIRCR struct{ mmio.UM32 }
+
+func (rm RMAIRCR) Load() AIRCR   { return AIRCR(rm.UM32.Load()) }
+func (rm RMAIRCR) Store(b AIRCR) { rm.UM32.Store(uint32(b)) }
+
+func VECTRESET_(p *Periph) RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(VECTRESET)}}
+}
+
+func VECTCLRACTIVE_(p *Periph) RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(VECTCLRACTIVE)}}
+}
+
+func SYSRESETREQ_(p *Periph) RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(SYSRESETREQ)}}
+}
+
+func PRIGROUP_(p *Periph) RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(PRIGROUP)}}
+}
+
+func ENDIANNESS_(p *Periph) RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(ENDIANNESS)}}
+}
+
+func VECTKEY_(p *Periph) RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(VECTKEY)}}
+}
+
+type SCR uint32
+
+type RSCR struct{ mmio.U32 }
+
+func (r *RSCR) LoadBits(mask SCR) SCR { return SCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RSCR) StoreBits(mask, b SCR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSCR) SetBits(mask SCR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RSCR) ClearBits(mask SCR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RSCR) Load() SCR             { return SCR(r.U32.Load()) }
+func (r *RSCR) Store(b SCR)           { r.U32.Store(uint32(b)) }
+
+type RMSCR struct{ mmio.UM32 }
+
+func (rm RMSCR) Load() SCR   { return SCR(rm.UM32.Load()) }
+func (rm RMSCR) Store(b SCR) { rm.UM32.Store(uint32(b)) }
+
+func SLEEPONEXIT_(p *Periph) RMSCR {
+	return RMSCR{mmio.UM32{&p.SCR.U32, uint32(SLEEPONEXIT)}}
+}
+
+func SLEEPDEEP_(p *Periph) RMSCR {
+	return RMSCR{mmio.UM32{&p.SCR.U32, uint32(SLEEPDEEP)}}
+}
+
+func SEVONPEND_(p *Periph) RMSCR {
+	return RMSCR{mmio.UM32{&p.SCR.U32, uint32(SEVONPEND)}}
+}
+
+type CCR uint32
+
+type RCCR struct{ mmio.U32 }
+
+func (r *RCCR) LoadBits(mask CCR) CCR { return CCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCCR) StoreBits(mask, b CCR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCCR) SetBits(mask CCR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCCR) ClearBits(mask CCR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCCR) Load() CCR             { return CCR(r.U32.Load()) }
+func (r *RCCR) Store(b CCR)           { r.U32.Store(uint32(b)) }
+
+type RMCCR struct{ mmio.UM32 }
+
+func (rm RMCCR) Load() CCR   { return CCR(rm.UM32.Load()) }
+func (rm RMCCR) Store(b CCR) { rm.UM32.Store(uint32(b)) }
+
+func NONBASETHRDENA_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(NONBASETHRDENA)}}
+}
+
+func USERSETMPEND_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(USERSETMPEND)}}
+}
+
+func UNALIGN_TRP_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(UNALIGN_TRP)}}
+}
+
+func DIV_0_TRP_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(DIV_0_TRP)}}
+}
+
+func BFHFNMIGN_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(BFHFNMIGN)}}
+}
+
+func STKALIGN_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(STKALIGN)}}
+}
+
+func DC_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(DC)}}
+}
+
+func IC_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(IC)}}
+}
+
+func BP_(p *Periph) RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(BP)}}
+}
+
+type SHPR1 uint32
+
+type RSHPR1 struct{ mmio.U32 }
+
+func (r *RSHPR1) LoadBits(mask SHPR1) SHPR1 { return SHPR1(r.U32.LoadBits(uint32(mask))) }
+func (r *RSHPR1) StoreBits(mask, b SHPR1)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSHPR1) SetBits(mask SHPR1)        { r.U32.SetBits(uint32(mask)) }
+func (r *RSHPR1) ClearBits(mask SHPR1)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RSHPR1) Load() SHPR1               { return SHPR1(r.U32.Load()) }
+func (r *RSHPR1) Store(b SHPR1)             { r.U32.Store(uint32(b)) }
+
+type RMSHPR1 struct{ mmio.UM32 }
+
+func (rm RMSHPR1) Load() SHPR1   { return SHPR1(rm.UM32.Load()) }
+func (rm RMSHPR1) Store(b SHPR1) { rm.UM32.Store(uint32(b)) }
+
+func PRI_MemManage_(p *Periph) RMSHPR1 {
+	return RMSHPR1{mmio.UM32{&p.SHPR1.U32, uint32(PRI_MemManage)}}
+}
+
+func PRI_BusFault_(p *Periph) RMSHPR1 {
+	return RMSHPR1{mmio.UM32{&p.SHPR1.U32, uint32(PRI_BusFault)}}
+}
+
+func PRI_UsageFault_(p *Periph) RMSHPR1 {
+	return RMSHPR1{mmio.UM32{&p.SHPR1.U32, uint32(PRI_UsageFault)}}
+}
+
+type SHPR2 uint32
+
+type RSHPR2 struct{ mmio.U32 }
+
+func (r *RSHPR2) LoadBits(mask SHPR2) SHPR2 { return SHPR2(r.U32.LoadBits(uint32(mask))) }
+func (r *RSHPR2) StoreBits(mask, b SHPR2)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSHPR2) SetBits(mask SHPR2)        { r.U32.SetBits(uint32(mask)) }
+func (r *RSHPR2) ClearBits(mask SHPR2)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RSHPR2) Load() SHPR2               { return SHPR2(r.U32.Load()) }
+func (r *RSHPR2) Store(b SHPR2)             { r.U32.Store(uint32(b)) }
+
+type RMSHPR2 struct{ mmio.UM32 }
+
+func (rm RMSHPR2) Load() SHPR2   { return SHPR2(rm.UM32.Load()) }
+func (rm RMSHPR2) Store(b SHPR2) { rm.UM32.Store(uint32(b)) }
+
+func PRI_SVCall_(p *Periph) RMSHPR2 {
+	return RMSHPR2{mmio.UM32{&p.SHPR2.U32, uint32(PRI_SVCall)}}
+}
+
+type SHPR3 uint32
+
+type RSHPR3 struct{ mmio.U32 }
+
+func (r *RSHPR3) LoadBits(mask SHPR3) SHPR3 { return SHPR3(r.U32.LoadBits(uint32(mask))) }
+func (r *RSHPR3) StoreBits(mask, b SHPR3)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSHPR3) SetBits(mask SHPR3)        { r.U32.SetBits(uint32(mask)) }
+func (r *RSHPR3) ClearBits(mask SHPR3)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RSHPR3) Load() SHPR3               { return SHPR3(r.U32.Load()) }
+func (r *RSHPR3) Store(b SHPR3)             { r.U32.Store(uint32(b)) }
+
+type RMSHPR3 struct{ mmio.UM32 }
+
+func (rm RMSHPR3) Load() SHPR3   { return SHPR3(rm.UM32.Load()) }
+func (rm RMSHPR3) Store(b SHPR3) { rm.UM32.Store(uint32(b)) }
+
+func PRI_PendSV_(p *Periph) RMSHPR3 {
+	return RMSHPR3{mmio.UM32{&p.SHPR3.U32, uint32(PRI_PendSV)}}
+}
+
+func PRI_SysTick_(p *Periph) RMSHPR3 {
+	return RMSHPR3{mmio.UM32{&p.SHPR3.U32, uint32(PRI_SysTick)}}
+}
+
+type SHCSR uint32
+
+type RSHCSR struct{ mmio.U32 }
+
+func (r *RSHCSR) LoadBits(mask SHCSR) SHCSR { return SHCSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RSHCSR) StoreBits(mask, b SHCSR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSHCSR) SetBits(mask SHCSR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RSHCSR) ClearBits(mask SHCSR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RSHCSR) Load() SHCSR               { return SHCSR(r.U32.Load()) }
+func (r *RSHCSR) Store(b SHCSR)             { r.U32.Store(uint32(b)) }
+
+type RMSHCSR struct{ mmio.UM32 }
+
+func (rm RMSHCSR) Load() SHCSR   { return SHCSR(rm.UM32.Load()) }
+func (rm RMSHCSR) Store(b SHCSR) { rm.UM32.Store(uint32(b)) }
+
+func MEMFAULTACT_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(MEMFAULTACT)}}
+}
+
+func BUSFAULTACT_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(BUSFAULTACT)}}
+}
+
+func USGFAULTACT_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(USGFAULTACT)}}
+}
+
+func SVCALLACT_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(SVCALLACT)}}
+}
+
+func MONITORACT_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(MONITORACT)}}
+}
+
+func PENDSVACT_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(PENDSVACT)}}
+}
+
+func SYSTICKACT_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(SYSTICKACT)}}
+}
+
+func USGFAULTPENDED_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(USGFAULTPENDED)}}
+}
+
+func MEMFAULTPENDED_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(MEMFAULTPENDED)}}
+}
+
+func BUSFAULTPENDED_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(BUSFAULTPENDED)}}
+}
+
+func SVCALLPENDED_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(SVCALLPENDED)}}
+}
+
+func MEMFAULTENA_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(MEMFAULTENA)}}
+}
+
+func BUSFAULTENA_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(BUSFAULTENA)}}
+}
+
+func USGFAULTENA_(p *Periph) RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(USGFAULTENA)}}
+}
+
+type CFSR uint32
+
+type RCFSR struct{ mmio.U32 }
+
+func (r *RCFSR) LoadBits(mask CFSR) CFSR { return CFSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCFSR) StoreBits(mask, b CFSR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCFSR) SetBits(mask CFSR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RCFSR) ClearBits(mask CFSR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RCFSR) Load() CFSR              { return CFSR(r.U32.Load()) }
+func (r *RCFSR) Store(b CFSR)            { r.U32.Store(uint32(b)) }
+
+type RMCFSR struct{ mmio.UM32 }
+
+func (rm RMCFSR) Load() CFSR   { return CFSR(rm.UM32.Load()) }
+func (rm RMCFSR) Store(b CFSR) { rm.UM32.Store(uint32(b)) }
+
+func IACCVIOL_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(IACCVIOL)}}
+}
+
+func DACCVIOL_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(DACCVIOL)}}
+}
+
+func MUNSTKERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(MUNSTKERR)}}
+}
+
+func MSTKERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(MSTKERR)}}
+}
+
+func MLSPERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(MLSPERR)}}
+}
+
+func MMARVALID_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(MMARVALID)}}
+}
+
+func IBUSERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(IBUSERR)}}
+}
+
+func PRECISERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(PRECISERR)}}
+}
+
+func IMPRECISERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(IMPRECISERR)}}
+}
+
+func UNSTKERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(UNSTKERR)}}
+}
+
+func STKERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(STKERR)}}
+}
+
+func LSPERR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(LSPERR)}}
+}
+
+func BFARVALID_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(BFARVALID)}}
+}
+
+func UNDEFINSTR_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(UNDEFINSTR)}}
+}
+
+func INVSTATE_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(INVSTATE)}}
+}
+
+func INVPC_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(INVPC)}}
+}
+
+func NOCP_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(NOCP)}}
+}
+
+func UNALIGNED_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(UNALIGNED)}}
+}
+
+func DIVBYZERO_(p *Periph) RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(DIVBYZERO)}}
+}
+
+type HFSR uint32
+
+type RHFSR struct{ mmio.U32 }
+
+func (r *RHFSR) LoadBits(mask HFSR) HFSR { return HFSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RHFSR) StoreBits(mask, b HFSR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RHFSR) SetBits(mask HFSR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RHFSR) ClearBits(mask HFSR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RHFSR) Load() HFSR              { return HFSR(r.U32.Load()) }
+func (r *RHFSR) Store(b HFSR)            { r.U32.Store(uint32(b)) }
+
+type RMHFSR struct{ mmio.UM32 }
+
+func (rm RMHFSR) Load() HFSR   { return HFSR(rm.UM32.Load()) }
+func (rm RMHFSR) Store(b HFSR) { rm.UM32.Store(uint32(b)) }
+
+func VECTTBL_(p *Periph) RMHFSR {
+	return RMHFSR{mmio.UM32{&p.HFSR.U32, uint32(VECTTBL)}}
+}
+
+func FORCED_(p *Periph) RMHFSR {
+	return RMHFSR{mmio.UM32{&p.HFSR.U32, uint32(FORCED)}}
+}
+
+func DEBUGEVT_(p *Periph) RMHFSR {
+	return RMHFSR{mmio.UM32{&p.HFSR.U32, uint32(DEBUGEVT)}}
+}
+
+type MMFR uint32
+
+type RMMFR struct{ mmio.U32 }
+
+func (r *RMMFR) LoadBits(mask MMFR) MMFR { return MMFR(r.U32.LoadBits(uint32(mask))) }
+func (r *RMMFR) StoreBits(mask, b MMFR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RMMFR) SetBits(mask MMFR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RMMFR) ClearBits(mask MMFR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RMMFR) Load() MMFR              { return MMFR(r.U32.Load()) }
+func (r *RMMFR) Store(b MMFR)            { r.U32.Store(uint32(b)) }
+
+type RMMMFR struct{ mmio.UM32 }
+
+func (rm RMMMFR) Load() MMFR   { return MMFR(rm.UM32.Load()) }
+func (rm RMMMFR) Store(b MMFR) { rm.UM32.Store(uint32(b)) }
+
+type BFAR uint32
+
+type RBFAR struct{ mmio.U32 }
+
+func (r *RBFAR) LoadBits(mask BFAR) BFAR { return BFAR(r.U32.LoadBits(uint32(mask))) }
+func (r *RBFAR) StoreBits(mask, b BFAR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RBFAR) SetBits(mask BFAR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RBFAR) ClearBits(mask BFAR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RBFAR) Load() BFAR              { return BFAR(r.U32.Load()) }
+func (r *RBFAR) Store(b BFAR)            { r.U32.Store(uint32(b)) }
+
+type RMBFAR struct{ mmio.UM32 }
+
+func (rm RMBFAR) Load() BFAR   { return BFAR(rm.UM32.Load()) }
+func (rm RMBFAR) Store(b BFAR) { rm.UM32.Store(uint32(b)) }
+
+type AFSR uint32
+
+type RAFSR struct{ mmio.U32 }
+
+func (r *RAFSR) LoadBits(mask AFSR) AFSR { return AFSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RAFSR) StoreBits(mask, b AFSR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RAFSR) SetBits(mask AFSR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RAFSR) ClearBits(mask AFSR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RAFSR) Load() AFSR              { return AFSR(r.U32.Load()) }
+func (r *RAFSR) Store(b AFSR)            { r.U32.Store(uint32(b)) }
+
+type RMAFSR struct{ mmio.UM32 }
+
+func (rm RMAFSR) Load() AFSR   { return AFSR(rm.UM32.Load()) }
+func (rm RMAFSR) Store(b AFSR) { rm.UM32.Store(uint32(b)) }
diff --git a/src/internal/cpu/cortexm/scid/doc.go b/src/internal/cpu/cortexm/scid/doc.go
new file mode 100644
index 0000000000..1a851fad8b
--- /dev/null
+++ b/src/internal/cpu/cortexm/scid/doc.go
@@ -0,0 +1,7 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package scid gives access to the System Control and ID registers. Note:
+// Cortex-M0 does not implement ACTLR.
+package scid
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/scid/periph_thumb.go b/src/internal/cpu/cortexm/scid/periph_thumb.go
new file mode 100644
index 0000000000..3736a9cd8e
--- /dev/null
+++ b/src/internal/cpu/cortexm/scid/periph_thumb.go
@@ -0,0 +1,19 @@
+// Instances:
+//  SCID  0xE000E000  -  -  System Control and ID registers
+// Registers:
+//  0x00  32  MCR    Master Control register, Reserved
+//  0x04  32  ICTR   Interrupt Controller Type Register
+//  0x08  32  ACTLR  Auxiliary Control Register
+package scid
+
+const (
+	INTLINESNUM ICTR = 15 << 0 //+ The number of IRQs = 32*(INTLINESNUM+1)
+)
+
+const (
+	DISMCYCINT ACTLR = 1 << 0 //+
+	DISDEFWBUF ACTLR = 1 << 1 //+
+	DISFOLD    ACTLR = 1 << 2 //+
+	DISFPCA    ACTLR = 1 << 8 //+
+	DISOOFP    ACTLR = 1 << 9 //+
+)
diff --git a/src/internal/cpu/cortexm/scid/xperiph_thumb.go b/src/internal/cpu/cortexm/scid/xperiph_thumb.go
new file mode 100644
index 0000000000..13a62bf586
--- /dev/null
+++ b/src/internal/cpu/cortexm/scid/xperiph_thumb.go
@@ -0,0 +1,92 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package scid
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	MCR   RMCR
+	ICTR  RICTR
+	ACTLR RACTLR
+}
+
+func SCID() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000E000))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type MCR uint32
+
+type RMCR struct{ mmio.U32 }
+
+func (r *RMCR) LoadBits(mask MCR) MCR { return MCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RMCR) StoreBits(mask, b MCR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RMCR) SetBits(mask MCR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RMCR) ClearBits(mask MCR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RMCR) Load() MCR             { return MCR(r.U32.Load()) }
+func (r *RMCR) Store(b MCR)           { r.U32.Store(uint32(b)) }
+
+type RMMCR struct{ mmio.UM32 }
+
+func (rm RMMCR) Load() MCR   { return MCR(rm.UM32.Load()) }
+func (rm RMMCR) Store(b MCR) { rm.UM32.Store(uint32(b)) }
+
+type ICTR uint32
+
+type RICTR struct{ mmio.U32 }
+
+func (r *RICTR) LoadBits(mask ICTR) ICTR { return ICTR(r.U32.LoadBits(uint32(mask))) }
+func (r *RICTR) StoreBits(mask, b ICTR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICTR) SetBits(mask ICTR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RICTR) ClearBits(mask ICTR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RICTR) Load() ICTR              { return ICTR(r.U32.Load()) }
+func (r *RICTR) Store(b ICTR)            { r.U32.Store(uint32(b)) }
+
+type RMICTR struct{ mmio.UM32 }
+
+func (rm RMICTR) Load() ICTR   { return ICTR(rm.UM32.Load()) }
+func (rm RMICTR) Store(b ICTR) { rm.UM32.Store(uint32(b)) }
+
+func INTLINESNUM_(p *Periph) RMICTR {
+	return RMICTR{mmio.UM32{&p.ICTR.U32, uint32(INTLINESNUM)}}
+}
+
+type ACTLR uint32
+
+type RACTLR struct{ mmio.U32 }
+
+func (r *RACTLR) LoadBits(mask ACTLR) ACTLR { return ACTLR(r.U32.LoadBits(uint32(mask))) }
+func (r *RACTLR) StoreBits(mask, b ACTLR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RACTLR) SetBits(mask ACTLR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RACTLR) ClearBits(mask ACTLR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RACTLR) Load() ACTLR               { return ACTLR(r.U32.Load()) }
+func (r *RACTLR) Store(b ACTLR)             { r.U32.Store(uint32(b)) }
+
+type RMACTLR struct{ mmio.UM32 }
+
+func (rm RMACTLR) Load() ACTLR   { return ACTLR(rm.UM32.Load()) }
+func (rm RMACTLR) Store(b ACTLR) { rm.UM32.Store(uint32(b)) }
+
+func DISMCYCINT_(p *Periph) RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISMCYCINT)}}
+}
+
+func DISDEFWBUF_(p *Periph) RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISDEFWBUF)}}
+}
+
+func DISFOLD_(p *Periph) RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISFOLD)}}
+}
+
+func DISFPCA_(p *Periph) RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISFPCA)}}
+}
+
+func DISOOFP_(p *Periph) RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISOOFP)}}
+}
diff --git a/src/internal/cpu/cortexm/stackframe_thumb.go b/src/internal/cpu/cortexm/stackframe_thumb.go
new file mode 100644
index 0000000000..6986e4bb37
--- /dev/null
+++ b/src/internal/cpu/cortexm/stackframe_thumb.go
@@ -0,0 +1,46 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package cortexm
+
+// Masks to select PSR subregisters/bits
+const (
+	APSR  = 0xF80F0000 // Application Program Status Register:
+	GE    = 0x000F0000 // - greater than or equal flags (SIMD instructions)
+	Q     = 0x08000000 // - saturation flag (DSP instructions)
+	V     = 0x10000000 // - overflow condition flag
+	C     = 0x20000000 // - carry condition flag
+	Z     = 0x40000000 // - zero condition flag
+	N     = 0x80000000 // - negative condition flag
+	IPSR  = 0x000001FF // Interrupt Program Status Register (exception number)
+	EPSR  = 0x0700FC00 // Execution Program Status Register:
+	SR    = 0x00000200 // - stack realigned (in stacked PSR only)
+	ITICI = 0x0600FC00 // - interrupt-continue for LDM/STM or IT block
+	T     = 0x01000000 // - execution mode (0: ARM, 1: Thumb)
+)
+
+// StackFrame represents the ARMv7-M stack frame
+type StackFrame struct {
+	R   [4]uintptr
+	R12 uintptr
+	LR  uintptr
+	PC  uintptr
+	PSR uint32
+}
+
+// StackFrameF represents the ARMv7-M extended stack frame with
+// single-precision floating-point registers
+type StackFrameF struct {
+	StackFrame
+	S     [16]float32
+	FPSCR uint32
+}
+
+// StackFrameD represents the ARMv7-M extended stack frame with
+// double-precision floating-point registers
+type StackFrameD struct {
+	StackFrame
+	D     [8]float64
+	FPSCR uint32
+}
diff --git a/src/internal/cpu/cortexm/systick/doc.go b/src/internal/cpu/cortexm/systick/doc.go
new file mode 100644
index 0000000000..8edbbb905f
--- /dev/null
+++ b/src/internal/cpu/cortexm/systick/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package systick gives access to the registers of the System Timer.
+package systick
diff --git a/src/internal/cpu/cortexm/systick/periph_thumb.go b/src/internal/cpu/cortexm/systick/periph_thumb.go
new file mode 100644
index 0000000000..ee8abb6f84
--- /dev/null
+++ b/src/internal/cpu/cortexm/systick/periph_thumb.go
@@ -0,0 +1,33 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  SYSTICK  0xE000E010  -  -  System Timer registers
+// Registers:
+//	0x00  32  CSR   Control and Status Register (any read clears COUNTFLAG)
+//	0x04  32  RVR   Reload Value Register.
+//	0x08  32  CVR   Current Value Register.
+//	0x0C  32  CALIB Calibration Value Register.
+package systick
+
+const (
+	ENABLE    CSR = 1 << 0  //+ Enable counter.
+	TICKINT   CSR = 1 << 1  //+ Generate exceptions.
+	CLKSOURCE CSR = 1 << 2  //+ Clock source: 0:external, 1:CPU.
+	COUNTFLAG CSR = 1 << 16 //+ 1:Timer counted to 0 since last read of CSR
+)
+
+const (
+	RELOAD RVR = 1<<24 - 1 //+ Loaded into CVR when the counter reaches 0.
+)
+
+const (
+	CURRENT CVR = 1<<24 - 1 //+ Read: counter value, write: clears to zero.
+)
+
+const (
+	TENMS CALIB = 1<<24 - 1 //+
+	SKEW  CALIB = 1 << 30   //+
+	NOREF CALIB = 1 << 31   //+
+)
diff --git a/src/internal/cpu/cortexm/systick/xperiph_thumb.go b/src/internal/cpu/cortexm/systick/xperiph_thumb.go
new file mode 100644
index 0000000000..f1de37e2b6
--- /dev/null
+++ b/src/internal/cpu/cortexm/systick/xperiph_thumb.go
@@ -0,0 +1,121 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package systick
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	CSR   RCSR
+	RVR   RRVR
+	CVR   RCVR
+	CALIB RCALIB
+}
+
+func SYSTICK() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000E010))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type CSR uint32
+
+type RCSR struct{ mmio.U32 }
+
+func (r *RCSR) LoadBits(mask CSR) CSR { return CSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCSR) StoreBits(mask, b CSR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCSR) SetBits(mask CSR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCSR) ClearBits(mask CSR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCSR) Load() CSR             { return CSR(r.U32.Load()) }
+func (r *RCSR) Store(b CSR)           { r.U32.Store(uint32(b)) }
+
+type RMCSR struct{ mmio.UM32 }
+
+func (rm RMCSR) Load() CSR   { return CSR(rm.UM32.Load()) }
+func (rm RMCSR) Store(b CSR) { rm.UM32.Store(uint32(b)) }
+
+func ENABLE_(p *Periph) RMCSR {
+	return RMCSR{mmio.UM32{&p.CSR.U32, uint32(ENABLE)}}
+}
+
+func TICKINT_(p *Periph) RMCSR {
+	return RMCSR{mmio.UM32{&p.CSR.U32, uint32(TICKINT)}}
+}
+
+func CLKSOURCE_(p *Periph) RMCSR {
+	return RMCSR{mmio.UM32{&p.CSR.U32, uint32(CLKSOURCE)}}
+}
+
+func COUNTFLAG_(p *Periph) RMCSR {
+	return RMCSR{mmio.UM32{&p.CSR.U32, uint32(COUNTFLAG)}}
+}
+
+type RVR uint32
+
+type RRVR struct{ mmio.U32 }
+
+func (r *RRVR) LoadBits(mask RVR) RVR { return RVR(r.U32.LoadBits(uint32(mask))) }
+func (r *RRVR) StoreBits(mask, b RVR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RRVR) SetBits(mask RVR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RRVR) ClearBits(mask RVR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RRVR) Load() RVR             { return RVR(r.U32.Load()) }
+func (r *RRVR) Store(b RVR)           { r.U32.Store(uint32(b)) }
+
+type RMRVR struct{ mmio.UM32 }
+
+func (rm RMRVR) Load() RVR   { return RVR(rm.UM32.Load()) }
+func (rm RMRVR) Store(b RVR) { rm.UM32.Store(uint32(b)) }
+
+func RELOAD_(p *Periph) RMRVR {
+	return RMRVR{mmio.UM32{&p.RVR.U32, uint32(RELOAD)}}
+}
+
+type CVR uint32
+
+type RCVR struct{ mmio.U32 }
+
+func (r *RCVR) LoadBits(mask CVR) CVR { return CVR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCVR) StoreBits(mask, b CVR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCVR) SetBits(mask CVR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCVR) ClearBits(mask CVR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCVR) Load() CVR             { return CVR(r.U32.Load()) }
+func (r *RCVR) Store(b CVR)           { r.U32.Store(uint32(b)) }
+
+type RMCVR struct{ mmio.UM32 }
+
+func (rm RMCVR) Load() CVR   { return CVR(rm.UM32.Load()) }
+func (rm RMCVR) Store(b CVR) { rm.UM32.Store(uint32(b)) }
+
+func CURRENT_(p *Periph) RMCVR {
+	return RMCVR{mmio.UM32{&p.CVR.U32, uint32(CURRENT)}}
+}
+
+type CALIB uint32
+
+type RCALIB struct{ mmio.U32 }
+
+func (r *RCALIB) LoadBits(mask CALIB) CALIB { return CALIB(r.U32.LoadBits(uint32(mask))) }
+func (r *RCALIB) StoreBits(mask, b CALIB)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCALIB) SetBits(mask CALIB)        { r.U32.SetBits(uint32(mask)) }
+func (r *RCALIB) ClearBits(mask CALIB)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RCALIB) Load() CALIB               { return CALIB(r.U32.Load()) }
+func (r *RCALIB) Store(b CALIB)             { r.U32.Store(uint32(b)) }
+
+type RMCALIB struct{ mmio.UM32 }
+
+func (rm RMCALIB) Load() CALIB   { return CALIB(rm.UM32.Load()) }
+func (rm RMCALIB) Store(b CALIB) { rm.UM32.Store(uint32(b)) }
+
+func TENMS_(p *Periph) RMCALIB {
+	return RMCALIB{mmio.UM32{&p.CALIB.U32, uint32(TENMS)}}
+}
+
+func SKEW_(p *Periph) RMCALIB {
+	return RMCALIB{mmio.UM32{&p.CALIB.U32, uint32(SKEW)}}
+}
+
+func NOREF_(p *Periph) RMCALIB {
+	return RMCALIB{mmio.UM32{&p.CALIB.U32, uint32(NOREF)}}
+}
diff --git a/src/internal/cpu/cpu_arm.go b/src/internal/cpu/cpu_armt.go
similarity index 96%
rename from src/internal/cpu/cpu_arm.go
rename to src/internal/cpu/cpu_armt.go
index b624526860..56bf48983a 100644
--- a/src/internal/cpu/cpu_arm.go
+++ b/src/internal/cpu/cpu_armt.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build arm thumb,!noos
+
 package cpu
 
 const CacheLinePadSize = 32
diff --git a/src/internal/cpu/cpu_noos_thumb.go b/src/internal/cpu/cpu_noos_thumb.go
new file mode 100644
index 0000000000..3b6b0474ea
--- /dev/null
+++ b/src/internal/cpu/cpu_noos_thumb.go
@@ -0,0 +1,9 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package cpu
+
+const CacheLinePadSize = 32 // Cortex-M7
+
+func doinit() {}
diff --git a/src/internal/cpu/riscv/clint/doc.go b/src/internal/cpu/riscv/clint/doc.go
new file mode 100644
index 0000000000..e99cfd1f38
--- /dev/null
+++ b/src/internal/cpu/riscv/clint/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package clint provides access to the registers of the Core Local Interrupter.
+package clint
\ No newline at end of file
diff --git a/src/internal/cpu/riscv/clint/periph_riscv64.go b/src/internal/cpu/riscv/clint/periph_riscv64.go
new file mode 100644
index 0000000000..0ab4d550e9
--- /dev/null
+++ b/src/internal/cpu/riscv/clint/periph_riscv64.go
@@ -0,0 +1,11 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  CLINT  0x2000000  -  -  Core Local Interrupter
+// Registers:
+//  0x0000 32  MSIP[99]      Hart Software Interrupt Register
+//  0x4000 64  MTIMECMP[99]  Hart Time Comparator Register
+//  0xBFF8 64  MTIME         Timer Register
+package clint
diff --git a/src/internal/cpu/riscv/clint/xperiph_riscv64.go b/src/internal/cpu/riscv/clint/xperiph_riscv64.go
new file mode 100644
index 0000000000..4f874111ec
--- /dev/null
+++ b/src/internal/cpu/riscv/clint/xperiph_riscv64.go
@@ -0,0 +1,70 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package clint
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	MSIP     [2]RMSIP
+	_        [4094]uint32
+	MTIMECMP [2]RMTIMECMP
+	_        [8186]uint32
+	MTIME    RMTIME
+}
+
+func CLINT() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0x2000000))) }
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+type MSIP uint32
+
+type RMSIP struct{ mmio.U32 }
+
+func (r *RMSIP) LoadBits(mask MSIP) MSIP { return MSIP(r.U32.LoadBits(uint32(mask))) }
+func (r *RMSIP) StoreBits(mask, b MSIP)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RMSIP) SetBits(mask MSIP)       { r.U32.SetBits(uint32(mask)) }
+func (r *RMSIP) ClearBits(mask MSIP)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RMSIP) Load() MSIP              { return MSIP(r.U32.Load()) }
+func (r *RMSIP) Store(b MSIP)            { r.U32.Store(uint32(b)) }
+
+type RMMSIP struct{ mmio.UM32 }
+
+func (rm RMMSIP) Load() MSIP   { return MSIP(rm.UM32.Load()) }
+func (rm RMMSIP) Store(b MSIP) { rm.UM32.Store(uint32(b)) }
+
+type MTIMECMP uint64
+
+type RMTIMECMP struct{ mmio.U64 }
+
+func (r *RMTIMECMP) LoadBits(mask MTIMECMP) MTIMECMP { return MTIMECMP(r.U64.LoadBits(uint64(mask))) }
+func (r *RMTIMECMP) StoreBits(mask, b MTIMECMP)      { r.U64.StoreBits(uint64(mask), uint64(b)) }
+func (r *RMTIMECMP) SetBits(mask MTIMECMP)           { r.U64.SetBits(uint64(mask)) }
+func (r *RMTIMECMP) ClearBits(mask MTIMECMP)         { r.U64.ClearBits(uint64(mask)) }
+func (r *RMTIMECMP) Load() MTIMECMP                  { return MTIMECMP(r.U64.Load()) }
+func (r *RMTIMECMP) Store(b MTIMECMP)                { r.U64.Store(uint64(b)) }
+
+type RMMTIMECMP struct{ mmio.UM64 }
+
+func (rm RMMTIMECMP) Load() MTIMECMP   { return MTIMECMP(rm.UM64.Load()) }
+func (rm RMMTIMECMP) Store(b MTIMECMP) { rm.UM64.Store(uint64(b)) }
+
+type MTIME uint64
+
+type RMTIME struct{ mmio.U64 }
+
+func (r *RMTIME) LoadBits(mask MTIME) MTIME { return MTIME(r.U64.LoadBits(uint64(mask))) }
+func (r *RMTIME) StoreBits(mask, b MTIME)   { r.U64.StoreBits(uint64(mask), uint64(b)) }
+func (r *RMTIME) SetBits(mask MTIME)        { r.U64.SetBits(uint64(mask)) }
+func (r *RMTIME) ClearBits(mask MTIME)      { r.U64.ClearBits(uint64(mask)) }
+func (r *RMTIME) Load() MTIME               { return MTIME(r.U64.Load()) }
+func (r *RMTIME) Store(b MTIME)             { r.U64.Store(uint64(b)) }
+
+type RMMTIME struct{ mmio.UM64 }
+
+func (rm RMMTIME) Load() MTIME   { return MTIME(rm.UM64.Load()) }
+func (rm RMMTIME) Store(b MTIME) { rm.UM64.Store(uint64(b)) }
diff --git a/src/internal/cpu/riscv/plic/periph_riscv64.go b/src/internal/cpu/riscv/plic/periph_riscv64.go
new file mode 100644
index 0000000000..3040abdf4f
--- /dev/null
+++ b/src/internal/cpu/riscv/plic/periph_riscv64.go
@@ -0,0 +1,30 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package plic provides access to the registers of the Platform-Level Interrupt Controller.
+package plic
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	PRIO [1024]mmio.U32 // interrupt source priorities
+	PEND [32]mmio.U32   // interrupt pending bits
+	_    [992]uint32
+	EN   [15872][32]mmio.U32 // interrupt enable bits
+	_    [14336]uint32
+	TC   [15872]struct {
+		THR mmio.U32 // priority threshold
+		CC  mmio.U32 // claim/complete
+		_   [1022]uint32
+	}
+}
+
+func PLIC() *Periph {
+	// BUG: The 0x0C000000 address is valid for Qemu Virt, Kendryte K210, some
+	// SiFive cores but generally it's implementation specific.
+	return (*Periph)(unsafe.Pointer(uintptr(0x0C000000)))
+}
diff --git a/src/internal/poll/fd_noos.go b/src/internal/poll/fd_noos.go
new file mode 100644
index 0000000000..059021af14
--- /dev/null
+++ b/src/internal/poll/fd_noos.go
@@ -0,0 +1,15 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package poll
+
+type FD struct {
+	// Lock sysfd and serialize access to Read and Write methods.
+	fdmu fdMutex
+
+	// Whether this is a file rather than a network socket.
+	isFile bool
+}
+
+func (fd *FD) destroy() error { return nil }
diff --git a/src/internal/syscall/unix/at_sysnum_fstatat64_linux.go b/src/internal/syscall/unix/at_sysnum_fstatat64_linux.go
index c6ea206c12..ab2f05e88c 100644
--- a/src/internal/syscall/unix/at_sysnum_fstatat64_linux.go
+++ b/src/internal/syscall/unix/at_sysnum_fstatat64_linux.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build arm mips mipsle 386
+// +build arm thumb mips mipsle 386
 
 package unix
 
diff --git a/src/internal/syscall/unix/sysnum_linux_arm.go b/src/internal/syscall/unix/sysnum_linux_armt.go
similarity index 87%
rename from src/internal/syscall/unix/sysnum_linux_arm.go
rename to src/internal/syscall/unix/sysnum_linux_armt.go
index acaec05879..c679443e4f 100644
--- a/src/internal/syscall/unix/sysnum_linux_arm.go
+++ b/src/internal/syscall/unix/sysnum_linux_armt.go
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build linux
+// +build arm thumb
+
 package unix
 
 const (
diff --git a/src/math/big/arith_thumb.s b/src/math/big/arith_thumb.s
new file mode 100644
index 0000000000..f0b4631b36
--- /dev/null
+++ b/src/math/big/arith_thumb.s
@@ -0,0 +1,283 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !math_big_pure_go
+
+#include "textflag.h"
+
+// This file provides fast assembly versions for the elementary
+// arithmetic operations on vectors implemented in arith.go.
+
+// func addVV(z, x, y []Word) (c Word)
+TEXT ·addVV(SB),NOSPLIT,$0
+	MOVW   z+0(FP), R1
+	MOVW   z_len+4(FP), R4
+	MOVW   x+12(FP), R2
+	MOVW   y+24(FP), R3
+	ADD    R4<<2, R1, R4
+	ADD.S  $0, R0  // clear carry flag
+	B      E1
+L1:
+	MOVW.P  4(R2), R5
+	MOVW.P  4(R3), R6
+	ADC.S   R6, R5
+	MOVW.P  R5, 4(R1)
+E1:
+	TEQ  R1, R4
+	BNE  L1
+
+	MOVW.CC  $0, R0
+	MOVW.CS  $1, R0
+	MOVW     R0, c+36(FP)
+	RET
+
+
+// func subVV(z, x, y []Word) (c Word)
+// (same as addVV except for SBC instead of ADC and label names)
+TEXT ·subVV(SB),NOSPLIT,$0
+	MOVW   z+0(FP), R1
+	MOVW   z_len+4(FP), R4
+	MOVW   x+12(FP), R2
+	MOVW   y+24(FP), R3
+	ADD    R4<<2, R1, R4
+	SUB.S  $0, R0  // clear borrow flag
+	B      E2
+L2:
+	MOVW.P  4(R2), R5
+	MOVW.P  4(R3), R6
+	SBC.S   R6, R5
+	MOVW.P  R5, 4(R1)
+E2:
+	TEQ  R1, R4
+	BNE  L2
+
+	MOVW.CS  $0, R0
+	MOVW.CC  $1, R0
+	MOVW     R0, c+36(FP)
+	RET
+
+
+// func addVW(z, x []Word, y Word) (c Word)
+TEXT ·addVW(SB),NOSPLIT,$0
+	MOVW  z+0(FP), R1
+	MOVW  z_len+4(FP), R4
+	MOVW  x+12(FP), R2
+	MOVW  y+24(FP), R3
+	ADD   R4<<2, R1, R4
+	TEQ   R1, R4
+	BNE   L3a
+	MOVW  R3, c+28(FP)
+	RET
+L3a:
+	MOVW.P  4(R2), R5
+	ADD.S   R3, R5
+	MOVW.P  R5, 4(R1)
+	B       E3
+L3:
+	MOVW.P  4(R2), R5
+	ADC.S   $0, R5
+	MOVW.P  R5, 4(R1)
+E3:
+	TEQ  R1, R4
+	BNE  L3
+
+	MOVW.CC  $0, R0
+	MOVW.CS  $1, R0
+	MOVW     R0, c+28(FP)
+	RET
+
+
+// func subVW(z, x []Word, y Word) (c Word)
+TEXT ·subVW(SB),NOSPLIT,$0
+	MOVW  z+0(FP), R1
+	MOVW  z_len+4(FP), R4
+	MOVW  x+12(FP), R2
+	MOVW  y+24(FP), R3
+	ADD   R4<<2, R1, R4
+	TEQ   R1, R4
+	BNE   L4a
+	MOVW  R3, c+28(FP)
+	RET
+L4a:
+	MOVW.P  4(R2), R5
+	SUB.S   R3, R5
+	MOVW.P  R5, 4(R1)
+	B       E4
+L4:
+	MOVW.P  4(R2), R5
+	SBC.S   $0, R5
+	MOVW.P  R5, 4(R1)
+E4:
+	TEQ  R1, R4
+	BNE  L4
+
+	MOVW.CS  $0, R0
+	MOVW.CC  $1, R0
+	MOVW     R0, c+28(FP)
+	RET
+
+
+// func shlVU(z, x []Word, s uint) (c Word)
+TEXT ·shlVU(SB),NOSPLIT,$0
+	MOVW  z_len+4(FP), R5
+	TEQ   $0, R5
+	BEQ   X7
+
+	MOVW  z+0(FP), R1
+	MOVW  x+12(FP), R2
+	ADD   R5<<2, R2, R2
+	ADD   R5<<2, R1, R5
+	MOVW  s+24(FP), R3
+	TEQ   $0, R3  // shift 0 is special
+	BEQ   Y7
+	ADD   $4, R1  // stop one word early
+	MOVW  $32, R4
+	SUB   R3, R4
+
+	MOVW.W  -4(R2), R6
+	MOVW    R6<<R3, R7
+	MOVW    R6>>R4, R6
+	MOVW    R6, c+28(FP)
+	B       E7
+
+L7:
+	MOVW.W  -4(R2), R6
+	MOVW    R6>>R4, R0
+	ORR     R0, R7
+	MOVW.W  R7, -4(R5)
+	MOVW    R6<<R3, R7
+E7:
+	TEQ  R1, R5
+	BNE  L7
+
+	MOVW  R7, -4(R5)
+	RET
+
+Y7: // copy loop, because shift 0 == shift 32
+	MOVW.W  -4(R2), R6
+	MOVW.W  R6, -4(R5)
+	TEQ     R1, R5
+	BNE     Y7
+
+X7:
+	MOVW  $0, R1
+	MOVW  R1, c+28(FP)
+	RET
+
+
+// func shrVU(z, x []Word, s uint) (c Word)
+TEXT ·shrVU(SB),NOSPLIT,$0
+	MOVW  z_len+4(FP), R5
+	TEQ   $0, R5
+	BEQ   X6
+
+	MOVW  z+0(FP), R1
+	MOVW  x+12(FP), R2
+	ADD   R5<<2, R1, R5
+	MOVW  s+24(FP), R3
+	TEQ   $0, R3  // shift 0 is special
+	BEQ   Y6
+	SUB   $4, R5  // stop one word early
+	MOVW  $32, R4
+	SUB   R3, R4
+
+	// first word
+	MOVW.P  4(R2), R6
+	MOVW    R6>>R3, R7
+	MOVW    R6<<R4, R6
+	MOVW    R6, c+28(FP)
+	B       E6
+
+	// word loop
+L6:
+	MOVW.P  4(R2), R6
+	MOVW    R6<<R4, R0
+	ORR     R0, R7
+	MOVW.P  R7, 4(R1)
+	MOVW    R6>>R3, R7
+E6:
+	TEQ  R1, R5
+	BNE  L6
+
+	MOVW  R7, 0(R1)
+	RET
+
+Y6: // copy loop, because shift 0 == shift 32
+	MOVW.P  4(R2), R6
+	MOVW.P  R6, 4(R1)
+	TEQ     R1, R5
+	BNE     Y6
+
+X6:
+	MOVW  $0, R1
+	MOVW  R1, c+28(FP)
+	RET
+
+
+// func mulAddVWW(z, x []Word, y, r Word) (c Word)
+TEXT ·mulAddVWW(SB),NOSPLIT,$0
+	MOVW  $0, R0
+	MOVW  z+0(FP), R1
+	MOVW  z_len+4(FP), R5
+	MOVW  x+12(FP), R2
+	MOVW  y+24(FP), R3
+	MOVW  r+28(FP), R4
+	ADD   R5<<2, R1, R5
+	B     E8
+
+	// word loop
+L8:
+	MOVW.P  4(R2), R6
+	MULLU   R6, R3, (R7, R6)
+	ADD.S   R4, R6
+	ADC     R0, R7
+	MOVW.P  R6, 4(R1)
+	MOVW    R7, R4
+E8:
+	TEQ  R1, R5
+	BNE  L8
+
+	MOVW  R4, c+32(FP)
+	RET
+
+
+// func addMulVVW(z, x []Word, y Word) (c Word)
+TEXT ·addMulVVW(SB),NOSPLIT,$0
+	MOVW  $0, R0
+	MOVW  z+0(FP), R1
+	MOVW  z_len+4(FP), R5
+	MOVW  x+12(FP), R2
+	MOVW  y+24(FP), R3
+	ADD   R5<<2, R1, R5
+	MOVW  $0, R4
+	B     E9
+
+	// word loop
+L9:
+	MOVW.P  4(R2), R6
+	MULLU   R6, R3, (R7, R6)
+	ADD.S   R4, R6
+	ADC     R0, R7
+	MOVW    0(R1), R4
+	ADD.S   R4, R6
+	ADC     R0, R7
+	MOVW.P  R6, 4(R1)
+	MOVW    R7, R4
+E9:
+	TEQ  R1, R5
+	BNE  L9
+
+	MOVW  R4, c+28(FP)
+	RET
+
+
+
+// func mulWW(x, y Word) (z1, z0 Word)
+TEXT ·mulWW(SB),NOSPLIT,$0
+	MOVW   x+0(FP), R1
+	MOVW   y+4(FP), R2
+	MULLU  R1, R2, (R4, R3)
+	MOVW   R4, z1+8(FP)
+	MOVW   R3, z0+12(FP)
+	RET
diff --git a/src/math/sqrt_thumb.s b/src/math/sqrt_thumb.s
new file mode 100644
index 0000000000..b3fd31250e
--- /dev/null
+++ b/src/math/sqrt_thumb.s
@@ -0,0 +1,22 @@
+// Copyright 2011 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+#define REGTMP R7
+
+// func Sqrt(x float64) float64
+TEXT ·Sqrt(SB),NOSPLIT,$0
+	MOVB	runtime·goarm(SB), REGTMP
+	CMP	$0x7D, REGTMP
+	BNE	softfloat
+	MOVD	x+0(FP),F0
+	SQRTD	F0,F0
+	MOVD	F0,ret+8(FP)
+	RET
+softfloat:
+	// Tail call to Go implementation.
+	// Can't use JMP, as in softfloat mode SQRTD is rewritten
+	// to a CALL, which makes this function have a frame.
+	RET	·sqrt(SB)
diff --git a/src/math/stubs_thumb.s b/src/math/stubs_thumb.s
new file mode 100644
index 0000000000..31bf872e43
--- /dev/null
+++ b/src/math/stubs_thumb.s
@@ -0,0 +1,110 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT ·Acos(SB), NOSPLIT, $0
+	B ·acos(SB)
+
+TEXT ·Acosh(SB), NOSPLIT, $0
+	B ·acosh(SB)
+
+TEXT ·Asin(SB), NOSPLIT, $0
+	B ·asin(SB)
+
+TEXT ·Asinh(SB), NOSPLIT, $0
+	B ·asinh(SB)
+
+TEXT ·Atan(SB), NOSPLIT, $0
+	B ·atan(SB)
+
+TEXT ·Atan2(SB), NOSPLIT, $0
+	B ·atan2(SB)
+
+TEXT ·Atanh(SB), NOSPLIT, $0
+	B ·atanh(SB)
+
+TEXT ·Cbrt(SB), NOSPLIT, $0
+	B ·cbrt(SB)
+
+TEXT ·Cos(SB), NOSPLIT, $0
+	B ·cos(SB)
+
+TEXT ·Cosh(SB), NOSPLIT, $0
+	B ·cosh(SB)
+
+TEXT ·Erf(SB), NOSPLIT, $0
+	B ·erf(SB)
+
+TEXT ·Erfc(SB), NOSPLIT, $0
+	B ·erfc(SB)
+
+TEXT ·Exp2(SB), NOSPLIT, $0
+	B ·exp2(SB)
+
+TEXT ·Exp(SB), NOSPLIT, $0
+	B ·exp(SB)
+
+TEXT ·Expm1(SB), NOSPLIT, $0
+	B ·expm1(SB)
+
+TEXT ·Floor(SB), NOSPLIT, $0
+	B ·floor(SB)
+
+TEXT ·Ceil(SB), NOSPLIT, $0
+	B ·ceil(SB)
+
+TEXT ·Trunc(SB), NOSPLIT, $0
+	B ·trunc(SB)
+
+TEXT ·Frexp(SB), NOSPLIT, $0
+	B ·frexp(SB)
+
+TEXT ·Hypot(SB), NOSPLIT, $0
+	B ·hypot(SB)
+
+TEXT ·Ldexp(SB), NOSPLIT, $0
+	B ·ldexp(SB)
+
+TEXT ·Log10(SB), NOSPLIT, $0
+	B ·log10(SB)
+
+TEXT ·Log2(SB), NOSPLIT, $0
+	B ·log2(SB)
+
+TEXT ·Log1p(SB), NOSPLIT, $0
+	B ·log1p(SB)
+
+TEXT ·Log(SB), NOSPLIT, $0
+	B ·log(SB)
+
+TEXT ·Max(SB), NOSPLIT, $0
+	B ·max(SB)
+
+TEXT ·Min(SB), NOSPLIT, $0
+	B ·min(SB)
+
+TEXT ·Mod(SB), NOSPLIT, $0
+	B ·mod(SB)
+
+TEXT ·Modf(SB), NOSPLIT, $0
+	B ·modf(SB)
+
+TEXT ·Pow(SB), NOSPLIT, $0
+	JMP ·pow(SB)
+
+TEXT ·Remainder(SB), NOSPLIT, $0
+	B ·remainder(SB)
+
+TEXT ·Sin(SB), NOSPLIT, $0
+	B ·sin(SB)
+
+TEXT ·Sinh(SB), NOSPLIT, $0
+	B ·sinh(SB)
+
+TEXT ·Tan(SB), NOSPLIT, $0
+	B ·tan(SB)
+
+TEXT ·Tanh(SB), NOSPLIT, $0
+	B ·tanh(SB)
diff --git a/src/os/dir_noos.go b/src/os/dir_noos.go
new file mode 100644
index 0000000000..2f74995f27
--- /dev/null
+++ b/src/os/dir_noos.go
@@ -0,0 +1,13 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import "syscall"
+
+type dirInfo struct{}
+
+func (f *File) readdirnames(n int) (names []string, err error) {
+	return nil, syscall.ENOTSUP
+}
diff --git a/src/os/endian_little.go b/src/os/endian_little.go
index 3efc5e0d8d..333a1027ce 100644
--- a/src/os/endian_little.go
+++ b/src/os/endian_little.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 //
-// +build 386 amd64 arm arm64 ppc64le mips64le mipsle riscv64 wasm
+// +build 386 amd64 arm arm64 ppc64le mips64le mipsle riscv64 thumb wasm
 
 package os
 
diff --git a/src/os/exec_noos.go b/src/os/exec_noos.go
new file mode 100644
index 0000000000..1ddd762514
--- /dev/null
+++ b/src/os/exec_noos.go
@@ -0,0 +1,34 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import (
+	"syscall"
+	"time"
+)
+
+type ProcessState struct{}
+
+func findProcess(pid int) (p *Process, err error) {
+	return &Process{Pid: pid}, nil
+}
+
+func startProcess(name string, argv []string, attr *ProcAttr) (p *Process, err error) {
+	return nil, &PathError{"fork/exec", name, syscall.ENOTSUP}
+}
+
+func (p *Process) release() error               { return syscall.ENOTSUP }
+func (p *Process) kill() error                  { return syscall.ENOTSUP }
+func (p *Process) wait() (*ProcessState, error) { return nil, syscall.ENOTSUP }
+func (p *Process) signal(sig Signal) error      { return syscall.ENOTSUP }
+
+func (p *ProcessState) userTime() time.Duration   { return 0 }
+func (p *ProcessState) systemTime() time.Duration { return 0 }
+func (p *ProcessState) exited() bool              { return false }
+func (p *ProcessState) success() bool             { return false }
+func (p *ProcessState) sys() interface{}          { return nil }
+func (p *ProcessState) sysUsage() interface{}     { return nil }
+
+func executable() (string, error) { return "", ErrNotExist }
diff --git a/src/os/file_noos.go b/src/os/file_noos.go
new file mode 100644
index 0000000000..9774b646aa
--- /dev/null
+++ b/src/os/file_noos.go
@@ -0,0 +1,248 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import (
+	_ "embedded/rtos"
+	"io"
+	"io/fs"
+	"syscall"
+	"time"
+)
+
+func tempDir() string {
+	return "/tmp"
+}
+
+type file struct {
+	f          fs.File
+	name       string
+	dirinfo    *dirInfo // nil unless directory being read
+	appendMode bool
+}
+
+func openFileNolog(name string, flag int, perm FileMode) (*File, error) {
+	f, err := openFile(name, flag, perm)
+	if err != nil {
+		return nil, &PathError{Op: "open", Path: name, Err: err}
+	}
+	return &File{&file{f: f, name: name}}, nil
+}
+
+// NewFile is not supported by GOOS=noos.
+func NewFile(fd uintptr, name string) *File {
+	return nil
+}
+
+func (f *File) readdir(n int, mode readdirMode) (names []string, dirents []DirEntry, fi []FileInfo, err error) {
+	{
+		ff, ok := f.f.(interface {
+			ReadDir(n int) ([]fs.DirEntry, error)
+		})
+		if !ok {
+			err = syscall.ENOTSUP
+			goto error
+		}
+		dirents, err := ff.ReadDir(n)
+		if err != nil {
+			goto error
+		}
+		switch mode {
+		case readdirName:
+			names = make([]string, len(dirents))
+			for i, de := range dirents {
+				names[i] = de.Name()
+			}
+			return names, nil, nil, err
+		case readdirDirEntry:
+			return nil, dirents, nil, err
+		default:
+			fi = make([]FileInfo, len(dirents))
+			for i, de := range dirents {
+				fi[i], err = de.Info()
+				if err != nil {
+					goto error
+				}
+			}
+			return nil, nil, fi, err
+		}
+	}
+error:
+	return nil, nil, nil, f.wrapErr("readdir", err)
+}
+
+func (f *File) checkValid(op string) error {
+	if f == nil {
+		return ErrInvalid
+	}
+	return nil
+}
+
+func (f *File) read(p []byte) (n int, err error) {
+	return f.f.Read(p)
+}
+
+func (f *File) pread(b []byte, off int64) (n int, err error) {
+	if ff, ok := f.f.(interface {
+		ReadAt(b []byte, off int64) (n int, err error)
+	}); ok {
+		return ff.ReadAt(b, off)
+	}
+	return 0, syscall.ENOTSUP
+}
+
+func (f *File) write(p []byte) (n int, err error) {
+	if ff, ok := f.f.(io.Writer); ok {
+		return ff.Write(p)
+	}
+	return 0, syscall.ENOTSUP
+}
+
+func (f *File) pwrite(b []byte, off int64) (n int, err error) {
+	if ff, ok := f.f.(interface {
+		WriteAt(b []byte, off int64) (n int, err error)
+	}); ok {
+		return ff.WriteAt(b, off)
+	}
+	return 0, syscall.ENOTSUP
+}
+
+func (f *File) seek(offset int64, whence int) (ret int64, err error) {
+	if ff, ok := f.f.(interface {
+		Seek(offset int64, whence int) (ret int64, err error)
+	}); ok {
+		return ff.Seek(offset, whence)
+	}
+	return 0, syscall.ENOTSUP
+}
+
+// See docs in file.go:(*File).Chmod.
+func (f *File) chmod(mode FileMode) (err error) {
+	err = syscall.ENOTSUP
+	if ff, ok := f.f.(interface {
+		Chmod(mode fs.FileMode) error
+	}); ok {
+		err = ff.Chmod(mode)
+		if err == nil {
+			return nil
+		}
+	}
+	return f.wrapErr("chmod", err)
+}
+
+func (f *File) setDeadline(t time.Time) (err error) {
+	err = syscall.ENOTSUP
+	if ff, ok := f.f.(interface {
+		SetDeadline(t time.Time) error
+	}); ok {
+		err = ff.SetDeadline(t)
+		if err == nil {
+			return nil
+		}
+	}
+	return f.wrapErr("setDeadline", err)
+}
+
+func (f *File) setReadDeadline(t time.Time) (err error) {
+	err = syscall.ENOTSUP
+	if ff, ok := f.f.(interface {
+		SetReadDeadline(t time.Time) error
+	}); ok {
+		err = ff.SetReadDeadline(t)
+		if err == nil {
+			return nil
+		}
+	}
+	return f.wrapErr("setReadDeadline", err)
+}
+
+func (f *File) setWriteDeadline(t time.Time) (err error) {
+	err = syscall.ENOTSUP
+	if ff, ok := f.f.(interface {
+		SetWriteDeadline(t time.Time) error
+	}); ok {
+		err = ff.SetWriteDeadline(t)
+		if err == nil {
+			return nil
+		}
+	}
+	return f.wrapErr("setWriteDeadline", err)
+}
+
+// Close closes the File, rendering it unusable for I/O.
+// On files that support SetDeadline, any pending I/O operations will
+// be canceled and return immediately with an error.
+// Close will return an error if it has already been called.
+func (f *File) Close() error {
+	if f == nil {
+		return ErrInvalid
+	}
+	if err := f.f.Close(); err != nil {
+		return f.wrapErr("close", err)
+	}
+	return nil
+}
+
+// Stat returns the FileInfo structure describing file.
+// If there is an error, it will be of type *PathError.
+func (f *File) Stat() (fi FileInfo, err error) {
+	err = syscall.ENOTSUP
+	if ff, ok := f.f.(interface {
+		Stat() (FileInfo, error)
+	}); ok {
+		fi, err = ff.Stat()
+		if err == nil {
+			return fi, nil
+		}
+	}
+	return nil, f.wrapErr("stat", err)
+}
+
+func epipecheck(file *File, e error) {}
+
+// fixLongPath is a noop on non-Windows platforms.
+func fixLongPath(path string) string {
+	return path
+}
+
+func syscallMode(i FileMode) uint32 {
+	return uint32(i.Perm())
+}
+
+type rawConn struct{}
+
+func (c *rawConn) Control(f func(uintptr)) error {
+	return syscall.ENOTSUP
+}
+
+func (c *rawConn) Read(f func(uintptr) bool) error {
+	return syscall.ENOTSUP
+}
+
+func (c *rawConn) Write(f func(uintptr) bool) error {
+	return syscall.ENOTSUP
+}
+
+func newRawConn(file *File) (*rawConn, error) {
+	return nil, syscall.ENOTSUP
+}
+
+func hostname() (name string, err error) {
+	return "", syscall.ENOTSUP
+}
+
+func ignoringEINTR(fn func() error) error {
+	return fn()
+}
+
+// provided by package embedded/rtos
+
+func openFile(name string, flag int, perm fs.FileMode) (f fs.File, err error)
+func chmod(name string, mode FileMode) error
+func rename(oldname, newname string) error
+
+// Remove removes the named file or (empty) directory.
+// If there is an error, it will be of type *PathError.
+func Remove(name string) error
diff --git a/src/os/path_unix.go b/src/os/path_unix.go
index c99a8240c5..459d215470 100644
--- a/src/os/path_unix.go
+++ b/src/os/path_unix.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build aix darwin dragonfly freebsd js,wasm linux netbsd openbsd solaris
+// +build aix darwin dragonfly freebsd js,wasm linux netbsd noos openbsd solaris
 
 package os
 
diff --git a/src/os/rawconn.go b/src/os/rawconn.go
index 9e11cda8c9..9d8f4dc501 100644
--- a/src/os/rawconn.go
+++ b/src/os/rawconn.go
@@ -3,6 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build !plan9
+// +build !noos
 
 package os
 
diff --git a/src/os/removeall_at.go Del b/src/os/removeall_at.go Del
new file mode 100644
index 0000000000..cbee970684
--- /dev/null
+++ b/src/os/removeall_at.go Del	
@@ -0,0 +1,192 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris
+
+package os
+
+import (
+	"internal/syscall/unix"
+	"io"
+	"syscall"
+)
+
+func removeAll(path string) error {
+	if path == "" {
+		// fail silently to retain compatibility with previous behavior
+		// of RemoveAll. See issue 28830.
+		return nil
+	}
+
+	// The rmdir system call does not permit removing ".",
+	// so we don't permit it either.
+	if endsWithDot(path) {
+		return &PathError{Op: "RemoveAll", Path: path, Err: syscall.EINVAL}
+	}
+
+	// Simple case: if Remove works, we're done.
+	err := Remove(path)
+	if err == nil || IsNotExist(err) {
+		return nil
+	}
+
+	// RemoveAll recurses by deleting the path base from
+	// its parent directory
+	parentDir, base := splitPath(path)
+
+	parent, err := Open(parentDir)
+	if IsNotExist(err) {
+		// If parent does not exist, base cannot exist. Fail silently
+		return nil
+	}
+	if err != nil {
+		return err
+	}
+	defer parent.Close()
+
+	if err := removeAllFrom(parent, base); err != nil {
+		if pathErr, ok := err.(*PathError); ok {
+			pathErr.Path = parentDir + string(PathSeparator) + pathErr.Path
+			err = pathErr
+		}
+		return err
+	}
+	return nil
+}
+
+func removeAllFrom(parent *File, base string) error {
+	parentFd := int(parent.Fd())
+	// Simple case: if Unlink (aka remove) works, we're done.
+	err := unix.Unlinkat(parentFd, base, 0)
+	if err == nil || IsNotExist(err) {
+		return nil
+	}
+
+	// EISDIR means that we have a directory, and we need to
+	// remove its contents.
+	// EPERM or EACCES means that we don't have write permission on
+	// the parent directory, but this entry might still be a directory
+	// whose contents need to be removed.
+	// Otherwise just return the error.
+	if err != syscall.EISDIR && err != syscall.EPERM && err != syscall.EACCES {
+		return &PathError{"unlinkat", base, err}
+	}
+
+	// Is this a directory we need to recurse into?
+	var statInfo syscall.Stat_t
+	statErr := unix.Fstatat(parentFd, base, &statInfo, unix.AT_SYMLINK_NOFOLLOW)
+	if statErr != nil {
+		if IsNotExist(statErr) {
+			return nil
+		}
+		return &PathError{"fstatat", base, statErr}
+	}
+	if statInfo.Mode&syscall.S_IFMT != syscall.S_IFDIR {
+		// Not a directory; return the error from the unix.Unlinkat.
+		return &PathError{"unlinkat", base, err}
+	}
+
+	// Remove the directory's entries.
+	var recurseErr error
+	for {
+		const reqSize = 1024
+		var respSize int
+
+		// Open the directory to recurse into
+		file, err := openFdAt(parentFd, base)
+		if err != nil {
+			if IsNotExist(err) {
+				return nil
+			}
+			recurseErr = &PathError{"openfdat", base, err}
+			break
+		}
+
+		for {
+			numErr := 0
+
+			names, readErr := file.Readdirnames(reqSize)
+			// Errors other than EOF should stop us from continuing.
+			if readErr != nil && readErr != io.EOF {
+				file.Close()
+				if IsNotExist(readErr) {
+					return nil
+				}
+				return &PathError{"readdirnames", base, readErr}
+			}
+
+			respSize = len(names)
+			for _, name := range names {
+				err := removeAllFrom(file, name)
+				if err != nil {
+					if pathErr, ok := err.(*PathError); ok {
+						pathErr.Path = base + string(PathSeparator) + pathErr.Path
+					}
+					numErr++
+					if recurseErr == nil {
+						recurseErr = err
+					}
+				}
+			}
+
+			// If we can delete any entry, break to start new iteration.
+			// Otherwise, we discard current names, get next entries and try deleting them.
+			if numErr != reqSize {
+				break
+			}
+		}
+
+		// Removing files from the directory may have caused
+		// the OS to reshuffle it. Simply calling Readdirnames
+		// again may skip some entries. The only reliable way
+		// to avoid this is to close and re-open the
+		// directory. See issue 20841.
+		file.Close()
+
+		// Finish when the end of the directory is reached
+		if respSize < reqSize {
+			break
+		}
+	}
+
+	// Remove the directory itself.
+	unlinkError := unix.Unlinkat(parentFd, base, unix.AT_REMOVEDIR)
+	if unlinkError == nil || IsNotExist(unlinkError) {
+		return nil
+	}
+
+	if recurseErr != nil {
+		return recurseErr
+	}
+	return &PathError{"unlinkat", base, unlinkError}
+}
+
+// openFdAt opens path relative to the directory in fd.
+// Other than that this should act like openFileNolog.
+// This acts like openFileNolog rather than OpenFile because
+// we are going to (try to) remove the file.
+// The contents of this file are not relevant for test caching.
+func openFdAt(dirfd int, name string) (*File, error) {
+	var r int
+	for {
+		var e error
+		r, e = unix.Openat(dirfd, name, O_RDONLY|syscall.O_CLOEXEC, 0)
+		if e == nil {
+			break
+		}
+
+		// See comment in openFileNolog.
+		if e == syscall.EINTR {
+			continue
+		}
+
+		return nil, e
+	}
+
+	if !supportsCloseOnExec {
+		syscall.CloseOnExec(r)
+	}
+
+	return newFile(uintptr(r), name, kindOpenFile), nil
+}
diff --git a/src/os/stat_noos.go b/src/os/stat_noos.go
new file mode 100644
index 0000000000..bde6b353e3
--- /dev/null
+++ b/src/os/stat_noos.go
@@ -0,0 +1,15 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import "syscall"
+
+func statNolog(name string) (FileInfo, error) {
+	return nil, &PathError{"stat", name, syscall.ENOTSUP}
+}
+
+func lstatNolog(name string) (FileInfo, error) {
+	return nil, &PathError{"lstat", name, syscall.ENOTSUP}
+}
diff --git a/src/os/types_noos.go b/src/os/types_noos.go
new file mode 100644
index 0000000000..51089d8cd8
--- /dev/null
+++ b/src/os/types_noos.go
@@ -0,0 +1,31 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import (
+	"syscall"
+	"time"
+)
+
+type fileStat struct {
+	name    string
+	size    int64
+	mode    FileMode
+	modTime time.Time
+	sys     interface{}
+}
+
+func (fs *fileStat) Size() int64        { return fs.size }
+func (fs *fileStat) Mode() FileMode     { return fs.mode }
+func (fs *fileStat) ModTime() time.Time { return fs.modTime }
+func (fs *fileStat) Sys() interface{}   { return fs.sys }
+
+func sameFile(fs1, fs2 *fileStat) bool {
+	a := fs1.sys.(*syscall.Dir)
+	b := fs2.sys.(*syscall.Dir)
+	return a.Qid.Path == b.Qid.Path && a.Type == b.Type && a.Dev == b.Dev
+}
+
+const badFd = -1
diff --git a/src/os/types_unix.go b/src/os/types_unix.go
index c0259ae0e8..8a595aaa98 100644
--- a/src/os/types_unix.go
+++ b/src/os/types_unix.go
@@ -4,6 +4,7 @@
 
 // +build !windows
 // +build !plan9
+// +build !noos
 
 package os
 
diff --git a/src/reflect/asm_thumb.s b/src/reflect/asm_thumb.s
new file mode 100644
index 0000000000..16b4b45d97
--- /dev/null
+++ b/src/reflect/asm_thumb.s
@@ -0,0 +1,40 @@
+// Copyright 2012 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "funcdata.h"
+
+#define REGCTXT R11
+
+// makeFuncStub is jumped to by the code generated by MakeFunc.
+// See the comment on the declaration of makeFuncStub in makefunc.go
+// for more details.
+// No argsize here, gc generates argsize info at call site.
+TEXT ·makeFuncStub(SB),(NOSPLIT|WRAPPER),$16
+	NO_LOCAL_POINTERS
+	MOVW	REGCTXT, 4(R13)
+	MOVW	$argframe+0(FP), R1
+	MOVW	R1, 8(R13)
+	MOVW	$0, R1
+	MOVB	R1, 16(R13)
+	ADD	$16, R13, R1
+	MOVW	R1, 12(R13)
+	BL	·callReflect(SB)
+	RET
+
+// methodValueCall is the code half of the function returned by makeMethodValue.
+// See the comment on the declaration of methodValueCall in makefunc.go
+// for more details.
+// No argsize here, gc generates argsize info at call site.
+TEXT ·methodValueCall(SB),(NOSPLIT|WRAPPER),$16
+	NO_LOCAL_POINTERS
+	MOVW	REGCTXT, 4(R13)
+	MOVW	$argframe+0(FP), R1
+	MOVW	R1, 8(R13)
+	MOVW	$0, R1
+	MOVB	R1, 16(R13)
+	ADD	$16, R13, R1
+	MOVW	R1, 12(R13)
+	BL	·callMethod(SB)
+	RET
diff --git a/src/runtime/asm_riscv64.h b/src/runtime/asm_riscv64.h
new file mode 100644
index 0000000000..551ad69bc7
--- /dev/null
+++ b/src/runtime/asm_riscv64.h
@@ -0,0 +1,251 @@
+#define LR  RA // link register
+#define TMP T6 // REG_TMP
+
+// CSRs
+
+#define fflags   0x001
+#define frm      0x002
+#define fcsr     0x003 // fflags + frm
+
+#define mstatus  0x300
+#define medeleg  0x302
+#define mideleg  0x303
+#define mie      0x304
+#define mtvec    0x305
+
+#define mscratch 0x340
+#define mepc     0x341
+#define mcause   0x342
+#define mtval    0x343
+#define mip      0x344
+
+#define mhartid  0xF14
+
+// mstatus field offsets
+
+#define MIEn  3
+#define MPIEn 7
+#define MPPn 11
+#define FSn  13
+
+// mip/mie bits
+
+#define MSI (1<<3)
+#define MTI (1<<7)
+#define SEI (1<<9)
+#define MEI (1<<11)
+
+// core peripherals
+
+#define CLINT_BASE 0x2000000 // true for Qemu Virt, K210, some SiFive cores
+#define msip     (CLINT_BASE + 0x0000)
+#define mtimecmp (CLINT_BASE + 0x4000)
+#define mtime    (CLINT_BASE + 0xBFF8)
+
+#define PLIC_BASE 0x0C000000 // true for Qemu Virt, K210, some SiFive cores
+#define PLIC_EN   (PLIC_BASE + 0x002000)
+#define PLIC_TC   (PLIC_BASE + 0x200000)
+
+// instructinos not implemented by assembly
+
+#define WFI   WORD $0x10500073
+#define MRET  WORD $0x30200073
+#define FENCE WORD $0x0ff0000f
+
+#define CSRW(RS,CSR)     WORD $(0x1073 + RS<<15 + CSR<<20)
+#define CSRR(CSR,RD)     WORD $(0x2073 + RD<<7 + CSR<<20)
+#define CSRS(RS,CSR)     WORD $(0x2073 + RS<<15 + CSR<<20)
+#define CSRC(RS,CSR)     WORD $(0x3073 + RS<<15 + CSR<<20)
+#define CSRRW(RS,CSR,RD) WORD $(0x1073 + RD<<7 + RS<<15 + CSR<<20)
+
+#define CSRWI(U5,CSR)     WORD $(0x5073 + U5<<15 + CSR<<20)
+#define CSRSI(U5,CSR)     WORD $(0x6073 + U5<<15 + CSR<<20)
+#define CSRCI(U5,CSR)     WORD $(0x7073 + U5<<15 + CSR<<20)
+#define CSRRWI(U5,CSR,RD) WORD $(0x5073 + RD<<7 + U5<<15 + CSR<<20)
+#define CSRRCI(U5,CSR,RD) WORD $(0x7073 + RD<<7 + U5<<15 + CSR<<20)
+
+#define LRW(RA, RD)     WORD $(0x1600202F + RD<<7 + RA<<15)
+#define LRD(RA, RD)     WORD $(0x1600302F + RD<<7 + RA<<15)
+#define SCW(RS, RD, RA) WORD $(0x1E00202F + RD<<7 + RA<<15 + RS<<20)
+#define SCD(RS, RD, RA) WORD $(0x1E00302F + RD<<7 + RA<<15 + RS<<20)
+
+// register numbers for above CSR* macros
+
+#define zero 0
+#define lr   1
+#define x2   2
+#define gp   3
+#define tp   4
+#define t0   5
+#define t1   6
+#define t2   7
+#define s0   8
+#define s1   9
+#define a0  10
+#define a1  11
+#define a2  12
+#define a3  13
+#define a4  14
+#define a5  15
+#define a6  16
+#define a7  17
+#define s2  18
+#define s3  19
+#define s4  20
+#define s5  21
+#define s6  22
+#define s7  23
+#define s8  24
+#define s9  25
+#define s10 26
+#define G   27
+#define t3  28
+#define t4  29
+#define t5  30
+#define tmp 31
+
+// trap context on the stack
+#define _LR (0*8)
+#define _A0 (1*8)
+#define _mstatus (2*8) // cannot be zero, see: tasker_noos_riscv64.s:/SCW
+#define _mepc (3*8)
+#define _mie (4*8) // cannot be zero, see: tasker_noos_riscv64.s:/SCW
+#define trapCtxSize (5*8)
+
+#define SAVE_GPRS(base,offset) \
+\ // LR saved separately
+\ // SP saved separately
+MOV  GP, (0*8+offset)(base) \
+MOV  TP, (1*8+offset)(base) \
+MOV  T0, (2*8+offset)(base) \
+MOV  T1, (3*8+offset)(base) \
+MOV  T2, (4*8+offset)(base) \
+MOV  S0, (5*8+offset)(base) \
+MOV  S1, (6*8+offset)(base) \
+\ // A0 saved separately
+MOV  A1, (7*8+offset)(base) \
+MOV  A2, (8*8+offset)(base) \
+MOV  A3, (9*8+offset)(base) \
+MOV  A4, (10*8+offset)(base) \
+MOV  A5, (11*8+offset)(base) \
+MOV  A6, (12*8+offset)(base) \
+MOV  A7, (13*8+offset)(base) \
+MOV  S2, (14*8+offset)(base) \
+MOV  S3, (15*8+offset)(base) \
+MOV  S4, (16*8+offset)(base) \
+MOV  S5, (17*8+offset)(base) \
+MOV  S6, (18*8+offset)(base) \
+MOV  S7, (19*8+offset)(base) \
+MOV  S8, (20*8+offset)(base) \
+MOV  S9, (21*8+offset)(base) \
+MOV  S10, (22*8+offset)(base) \
+\ // g saved separately
+MOV  T3, (23*8+offset)(base) \
+MOV  T4, (24*8+offset)(base) \
+MOV  T5, (25*8+offset)(base) \
+MOV  TMP, (26*8+offset)(base)
+
+#define RESTORE_GPRS(base,offset) \
+\ // LR loaded separately
+\ // SP loaded separately
+MOV  (0*8+offset)(base), GP \
+MOV  (1*8+offset)(base), TP \
+MOV  (2*8+offset)(base), T0 \
+MOV  (3*8+offset)(base), T1 \
+MOV  (4*8+offset)(base), T2 \
+MOV  (5*8+offset)(base), S0 \
+MOV  (6*8+offset)(base), S1 \
+\ // A0 loaded separately
+MOV  (7*8+offset)(base), A1 \
+MOV  (8*8+offset)(base), A2 \
+MOV  (9*8+offset)(base), A3 \
+MOV  (10*8+offset)(base), A4 \
+MOV  (11*8+offset)(base), A5 \
+MOV  (12*8+offset)(base), A6 \
+MOV  (13*8+offset)(base), A7 \
+MOV  (14*8+offset)(base), S2 \
+MOV  (15*8+offset)(base), S3 \
+MOV  (16*8+offset)(base), S4 \
+MOV  (17*8+offset)(base), S5 \
+MOV  (18*8+offset)(base), S6 \
+MOV  (19*8+offset)(base), S7 \
+MOV  (20*8+offset)(base), S8 \
+MOV  (21*8+offset)(base), S9 \
+MOV  (22*8+offset)(base), S10 \
+\ // g loaded separately
+MOV  (23*8+offset)(base), T3 \
+MOV  (24*8+offset)(base), T4 \
+MOV  (25*8+offset)(base), T5 \
+MOV  (26*8+offset)(base), TMP
+
+#define SAVE_FPRS(base,offset) \
+CSRR  (fcsr, tmp) \
+MOV   TMP, (0*8+offset)(base) \
+MOVD  F0, (1*8+offset)(base) \
+MOVD  F1, (2*8+offset)(base) \
+MOVD  F2, (3*8+offset)(base) \
+MOVD  F3, (4*8+offset)(base) \
+MOVD  F4, (5*8+offset)(base) \
+MOVD  F5, (6*8+offset)(base) \
+MOVD  F6, (7*8+offset)(base) \
+MOVD  F7, (8*8+offset)(base) \
+MOVD  F8, (9*8+offset)(base) \
+MOVD  F9, (10*8+offset)(base) \
+MOVD  F10, (11*8+offset)(base) \
+MOVD  F11, (12*8+offset)(base) \
+MOVD  F12, (13*8+offset)(base) \
+MOVD  F13, (14*8+offset)(base) \
+MOVD  F14, (15*8+offset)(base) \
+MOVD  F15, (16*8+offset)(base) \
+MOVD  F16, (17*8+offset)(base) \
+MOVD  F17, (18*8+offset)(base) \
+MOVD  F18, (19*8+offset)(base) \
+MOVD  F19, (20*8+offset)(base) \
+MOVD  F20, (21*8+offset)(base) \
+MOVD  F21, (22*8+offset)(base) \
+MOVD  F22, (23*8+offset)(base) \
+MOVD  F23, (24*8+offset)(base) \
+MOVD  F24, (25*8+offset)(base) \
+MOVD  F25, (26*8+offset)(base) \
+MOVD  F26, (27*8+offset)(base) \
+MOVD  F27, (28*8+offset)(base) \
+MOVD  F28, (29*8+offset)(base) \
+MOVD  F29, (30*8+offset)(base) \
+MOVD  F30, (31*8+offset)(base) \
+MOVD  F31, (32*8+offset)(base)
+
+#define RESTORE_FPRS(base,offset) \
+MOV   (0*8+offset)(base), TMP \
+CSRW  (tmp, fcsr) \
+MOVD  (1*8+offset)(base), F0 \
+MOVD  (2*8+offset)(base), F1 \
+MOVD  (3*8+offset)(base), F2 \
+MOVD  (4*8+offset)(base), F3 \
+MOVD  (5*8+offset)(base), F4 \
+MOVD  (6*8+offset)(base), F5 \
+MOVD  (7*8+offset)(base), F6 \
+MOVD  (8*8+offset)(base), F7 \
+MOVD  (9*8+offset)(base), F8 \
+MOVD  (10*8+offset)(base), F9 \
+MOVD  (11*8+offset)(base), F10 \
+MOVD  (12*8+offset)(base), F11 \
+MOVD  (13*8+offset)(base), F12 \
+MOVD  (14*8+offset)(base), F13 \
+MOVD  (15*8+offset)(base), F14 \
+MOVD  (16*8+offset)(base), F15 \
+MOVD  (17*8+offset)(base), F16 \
+MOVD  (18*8+offset)(base), F17 \
+MOVD  (19*8+offset)(base), F18 \
+MOVD  (20*8+offset)(base), F19 \
+MOVD  (21*8+offset)(base), F20 \
+MOVD  (22*8+offset)(base), F21 \
+MOVD  (23*8+offset)(base), F22 \
+MOVD  (24*8+offset)(base), F23 \
+MOVD  (25*8+offset)(base), F24 \
+MOVD  (26*8+offset)(base), F25 \
+MOVD  (27*8+offset)(base), F26 \
+MOVD  (28*8+offset)(base), F27 \
+MOVD  (29*8+offset)(base), F26 \
+MOVD  (30*8+offset)(base), F29 \
+MOVD  (31*8+offset)(base), F30 \
+MOVD  (32*8+offset)(base), F31
diff --git a/src/runtime/asm_riscv64.s b/src/runtime/asm_riscv64.s
index 01b42dc3de..ae0d6c89a5 100644
--- a/src/runtime/asm_riscv64.s
+++ b/src/runtime/asm_riscv64.s
@@ -6,6 +6,8 @@
 #include "funcdata.h"
 #include "textflag.h"
 
+#ifndef GOOS_noos
+
 // func rt0_go()
 TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	// X2 = stack; A0 = argc; A1 = argv
@@ -82,6 +84,8 @@ TEXT runtime·cputicks(SB),NOSPLIT,$0-8
 	MOV	A0, ret+0(FP)
 	RET
 
+#endif
+
 // systemstack_switch is a dummy routine that systemstack leaves at the bottom
 // of the G stack. We need to distinguish the routine that
 // lives at the bottom of the G stack from the one that lives
diff --git a/src/runtime/asm_thumb.s b/src/runtime/asm_thumb.s
new file mode 100644
index 0000000000..9375b3cdaf
--- /dev/null
+++ b/src/runtime/asm_thumb.s
@@ -0,0 +1,773 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "funcdata.h"
+#include "textflag.h"
+
+
+#ifndef GOOS_noos
+
+// using NOFRAME means do not save LR on stack.
+// argc is in R0, argv is in R1.
+TEXT runtime·rt0_go(SB),NOSPLIT|NOFRAME,$0
+	MOVW  $0xcafebabe, R12
+
+	// copy arguments forward on an even stack
+	// use R13 instead of SP to avoid linker rewriting the offsets
+	SUB   $64, R13  // plenty of scratch
+	AND   $~7, R13
+	MOVW  R0, 60(R13)  // save argc, argv away
+	MOVW  R1, 64(R13)
+
+	// set up g register
+	// g is R10
+	MOVW  $runtime·g0(SB), g
+	MOVW  $runtime·m0(SB), R8
+
+	// save m->g0 = g0
+	MOVW  g, m_g0(R8)
+	// save g->m = m0
+	MOVW  R8, g_m(g)
+
+	// create istack out of the OS stack
+	// (1MB of system stack is available on iOS and Android)
+	MOVW  $(-64*1024+104)(R13), R0
+	MOVW  R0, (g_stack+stack_lo)(g)
+	MOVW  R13, (g_stack+stack_hi)(g)
+	ADD   $const__StackGuard, R0
+	MOVW  R0, g_stackguard0(g)
+	MOVW  R0, g_stackguard1(g)
+
+	BL  runtime·emptyfunc(SB)  // fault if stack check is wrong
+
+	BL  runtime·check(SB)
+
+	// saved argc, argv
+	MOVW  60(R13), R0
+	MOVW  R0, 4(R13)
+	MOVW  64(R13), R1
+	MOVW  R1, 8(R13)
+	BL    runtime·args(SB)
+	BL    runtime·checkgoarm(SB)
+	BL    runtime·osinit(SB)
+	BL    runtime·schedinit(SB)
+
+	// create a new goroutine to start program
+	MOVW       $0, R0
+	MOVW       $8, R1
+	MOVW       $runtime·mainPC(SB), R2
+	MOVM.DB.W  [R0-R2], (R13)
+	BL         runtime·newproc(SB)
+	MOVW       $12(R13), R13
+
+	// start this M
+	BL  runtime·mstart(SB)
+
+	MOVW  $1234, R0
+	MOVW  $1000, R1
+	MOVW  R0, (R1)  // fail hard
+
+#endif
+
+
+DATA runtime·mainPC+0(SB)/4,$runtime·main(SB)
+GLOBL runtime·mainPC(SB),RODATA,$4
+
+TEXT runtime·breakpoint(SB),NOSPLIT,$0-0
+	// gdb won't skip this breakpoint instruction automatically,
+	// so you must manually "set $pc+=4" to skip it and continue.
+#ifdef GOOS_noos
+	BKPT
+#else
+	UNDEF  $1  // undefined instruction that gdb understands is a software breakpoint
+	//WORD 0xA000F7F0 // T32 UNDEF $0
+#endif
+	RET
+
+TEXT runtime·asminit(SB),NOSPLIT,$0-0
+	// disable runfast (flush-to-zero) mode of vfp if runtime.goarm > 5
+	// MOVB	runtime·goarm(SB), REGTMP
+	// CMP	$5, REGTMP
+	// BLE	4(PC)
+	// WORD	$0xeef1ba10	// vmrs REGTMP, fpscr
+	// BIC	$(1<<24), REGTMP
+	// WORD	$0xeee1ba10	// vmsr fpscr, REGTMP
+	RET
+
+//
+//  go-routine
+//
+
+// void gosave(Gobuf*)
+// save state in Gobuf; setjmp
+TEXT runtime·gosave(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  buf+0(FP), R0
+	MOVW  R13, gobuf_sp(R0)
+	MOVW  LR, gobuf_pc(R0)
+	MOVW  g, gobuf_g(R0)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, gobuf_lr(R0)
+	MOVW  REGTMP, gobuf_ret(R0)
+	// Assert ctxt is zero. See func save.
+	MOVW  gobuf_ctxt(R0), R0
+	CMP   R0, REGTMP
+	B.EQ   2(PC)
+	CALL  runtime·badctxt(SB)
+	RET
+
+TEXT ·asmcgocall(SB),NOSPLIT,$0-12
+	BKPT
+	B   -1(PC)
+
+// void gogo(Gobuf*)
+// restore state from Gobuf; longjmp
+TEXT runtime·gogo(SB),NOSPLIT,$8-4
+	MOVW  buf+0(FP), R1
+	MOVW  gobuf_g(R1), R0
+	BL    setg<>(SB)
+
+	// NOTE: We updated g above, and we are about to update SP.
+	// Until LR and PC are also updated, the g/SP/LR/PC quadruple
+	// are out of sync and must not be used as the basis of a traceback.
+	// Sigprof skips the traceback when SP is not within g's bounds,
+	// and when the PC is inside this function, runtime.gogo.
+	// Since we are about to update SP, until we complete runtime.gogo
+	// we must not leave this function. In particular, no calls
+	// after this point: it must be straight-line code until the
+	// final B instruction.
+	// See large comment in sigprof for more details.
+	MOVW  gobuf_sp(R1), R13  // restore SP==R13
+	MOVW  gobuf_lr(R1), LR
+	MOVW  gobuf_ret(R1), R0
+	MOVW  gobuf_ctxt(R1), REGCTXT
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, gobuf_sp(R1)  // clear to help garbage collector
+	MOVW  REGTMP, gobuf_ret(R1)
+	MOVW  REGTMP, gobuf_lr(R1)
+	MOVW  REGTMP, gobuf_ctxt(R1)
+	MOVW  gobuf_pc(R1), REGTMP
+	CMP   REGTMP, REGTMP  // set condition codes for == test, needed by stack split
+	B     (REGTMP)
+
+// func mcall(fn func(*g))
+// Switch to m->g0's stack, call fn(g).
+// Fn must never return. It should gogo(&g->sched)
+// to keep running g.
+TEXT runtime·mcall(SB),NOSPLIT|NOFRAME,$0-4
+	// Save caller state in g->sched.
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, (g_sched+gobuf_lr)(g)
+	MOVW  g, (g_sched+gobuf_g)(g)
+
+	// Switch to m->g0 & its stack, call fn.
+	MOVW  g, R1
+	MOVW  g_m(g), R8
+	MOVW  m_g0(R8), R0
+	BL    setg<>(SB)
+	CMP   g, R1
+	B.NE  2(PC)
+	B     runtime·badmcall(SB)
+	MOVW  fn+0(FP), R0
+	MOVW  (g_sched+gobuf_sp)(g), R13
+	SUB   $8, R13
+	MOVW  R1, 4(R13)
+	MOVW  R0, REGCTXT
+	MOVW  0(R0), R0
+	BL    (R0)
+	B     runtime·badmcall2(SB)
+	RET
+
+// systemstack_switch is a dummy routine that systemstack leaves at the bottom
+// of the G stack. We need to distinguish the routine that
+// lives at the bottom of the G stack from the one that lives
+// at the top of the system stack because the one at the top of
+// the system stack terminates the stack walk (see topofstack()).
+TEXT runtime·systemstack_switch(SB),NOSPLIT,$0-0
+	MOVW  $0, R0
+	BL    (R0)  // clobber lr to ensure push {lr} is kept
+	RET
+
+// func systemstack(fn func())
+TEXT runtime·systemstack(SB),NOSPLIT,$0-4
+	MOVW  fn+0(FP), R0  // R0 = fn
+	MOVW  g_m(g), R1    // R1 = m
+
+	MOVW  m_gsignal(R1), R2  // R2 = gsignal
+	CMP   g, R2
+	B.EQ  noswitch
+
+	MOVW  m_g0(R1), R2  // R2 = g0
+	CMP   g, R2
+	B.EQ  noswitch
+
+	MOVW  m_curg(R1), R3
+	CMP   g, R3
+	B.EQ  switch
+
+	// Bad: g is not gsignal, not g0, not curg. What is it?
+	// Hide call from linker nosplit analysis.
+	MOVW  $runtime·badsystemstack(SB), R0
+	BL    (R0)
+	B     runtime·abort(SB)
+
+switch:
+	// save our state in g->sched. Pretend to
+	// be systemstack_switch if the G stack is scanned.
+	MOVW  $runtime·systemstack_switch(SB), R3
+	ADD   $4, R3, R3  // get past push {lr}
+	MOVW  R3, (g_sched+gobuf_pc)(g)
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_lr)(g)
+	MOVW  g, (g_sched+gobuf_g)(g)
+
+	// switch to g0
+	MOVW  R0, R5
+	MOVW  R2, R0
+	BL    setg<>(SB)
+	MOVW  R5, R0
+	MOVW  (g_sched+gobuf_sp)(R2), R3
+	// make it look like mstart called systemstack on g0, to stop traceback
+	SUB   $4, R3, R3
+	MOVW  $runtime·mstart(SB), R4
+	MOVW  R4, 0(R3)
+	MOVW  R3, R13
+
+	// call target function
+	MOVW  R0, REGCTXT
+	MOVW  0(R0), R0
+	BL    (R0)
+
+	// switch back to g
+	MOVW  g_m(g), R1
+	MOVW  m_curg(R1), R0
+	BL    setg<>(SB)
+	MOVW  (g_sched+gobuf_sp)(g), R13
+	MOVW  $0, R3
+	MOVW  R3, (g_sched+gobuf_sp)(g)
+	RET
+
+noswitch:
+	// Using a tail call here cleans up tracebacks since we won't stop
+	// at an intermediate systemstack.
+	MOVW    R0, REGCTXT
+	MOVW    0(R0), R0
+	MOVW.P  4(R13), R14  // restore LR
+	B       (R0)
+
+//
+//  support for morestack
+//
+
+// Called during function prolog when more stack is needed.
+// R3 prolog's LR
+// using NOFRAME means do not save LR on stack.
+
+// The traceback routines see morestack on a g0 as being
+// the top of a stack (for example, morestack calling newstack
+// calling the scheduler calling newm calling gc), so we must
+// record an argument size. For that purpose, it has no arguments.
+TEXT runtime·morestack(SB),NOSPLIT|NOFRAME,$0-0
+	// Cannot grow scheduler stack (m->g0).
+	MOVW  g_m(g), R8
+	MOVW  m_g0(R8), R4
+	CMP   g, R4
+	BNE   3(PC)
+	BL    runtime·badmorestackg0(SB)
+	B     runtime·abort(SB)
+
+	// Cannot grow signal stack (m->gsignal).
+	MOVW  m_gsignal(R8), R4
+	CMP   g, R4
+	BNE   3(PC)
+	BL    runtime·badmorestackgsignal(SB)
+	B     runtime·abort(SB)
+
+	// Called from f.
+	// Set g->sched to context in f.
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  R3, (g_sched+gobuf_lr)(g)
+	MOVW  REGCTXT, (g_sched+gobuf_ctxt)(g)
+
+	// Called from f.
+	// Set m->morebuf to f's caller.
+	MOVW  R3, (m_morebuf+gobuf_pc)(R8)   // f's caller's PC
+	MOVW  R13, (m_morebuf+gobuf_sp)(R8)  // f's caller's SP
+	MOVW  g, (m_morebuf+gobuf_g)(R8)
+
+	// Call newstack on m->g0's stack.
+	MOVW    m_g0(R8), R0
+	BL      setg<>(SB)
+	MOVW    (g_sched+gobuf_sp)(g), R13
+	MOVW    $0, R0
+	MOVW.W  R0, -4(R13)  // create a call frame on g0 (saved LR)
+	BL      runtime·newstack(SB)
+
+	// Not reached, but make sure the return PC from the call to newstack
+	// is still in this function, and not the beginning of the next.
+	RET
+
+TEXT runtime·morestack_noctxt(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, REGCTXT
+	B     runtime·morestack(SB)
+
+// reflectcall: call a function with the given argument list
+// func call(argtype *_type, f *FuncVal, arg *byte, argsize, retoffset uint32).
+// we don't have variable-sized frames, so we use a small number
+// of constant-sized-frame functions to encode a few bits of size in the pc.
+// Caution: ugly multiline assembly macros in your future!
+
+#define DISPATCH(NAME,MAXSIZE) \
+	CMP   $MAXSIZE, R0; \
+	B.HI  3(PC); \
+	MOVW  $NAME(SB), R1; \
+	B     (R1)
+
+TEXT ·reflectcall(SB),NOSPLIT|NOFRAME,$0-20
+	MOVW                              argsize+12(FP), R0
+	DISPATCH(runtime·call16,          16)
+	DISPATCH(runtime·call32,          32)
+	DISPATCH(runtime·call64,          64)
+	DISPATCH(runtime·call128,         128)
+	DISPATCH(runtime·call256,         256)
+	DISPATCH(runtime·call512,         512)
+	DISPATCH(runtime·call1024,        1024)
+	DISPATCH(runtime·call2048,        2048)
+	DISPATCH(runtime·call4096,        4096)
+	DISPATCH(runtime·call8192,        8192)
+	DISPATCH(runtime·call16384,       16384)
+	DISPATCH(runtime·call32768,       32768)
+	DISPATCH(runtime·call65536,       65536)
+	DISPATCH(runtime·call131072,      131072)
+	DISPATCH(runtime·call262144,      262144)
+	DISPATCH(runtime·call524288,      524288)
+	DISPATCH(runtime·call1048576,     1048576)
+	DISPATCH(runtime·call2097152,     2097152)
+	DISPATCH(runtime·call4194304,     4194304)
+	DISPATCH(runtime·call8388608,     8388608)
+	DISPATCH(runtime·call16777216,    16777216)
+	DISPATCH(runtime·call33554432,    33554432)
+	DISPATCH(runtime·call67108864,    67108864)
+	DISPATCH(runtime·call134217728,   134217728)
+	DISPATCH(runtime·call268435456,   268435456)
+	DISPATCH(runtime·call536870912,   536870912)
+	DISPATCH(runtime·call1073741824,  1073741824)
+	MOVW                              $runtime·badreflectcall(SB), R1
+	B                                 (R1)
+
+#define CALLFN(NAME,MAXSIZE) \
+TEXT NAME(SB), WRAPPER, $MAXSIZE-20; \
+	NO_LOCAL_POINTERS;  \
+/*                  copy arguments to stack */ \
+	MOVW     argptr+8(FP), R0; \
+	MOVW     argsize+12(FP), R2; \
+	ADD      $4, R13, R1; \
+	CMP      $0, R2; \
+	B.EQ     5(PC); \
+	MOVBU.P  1(R0), R5; \
+	MOVB.P   R5, 1(R1); \
+	SUB      $1, R2, R2; \
+	B        -5(PC); \
+/*                  call function */ \
+	MOVW    f+4(FP), REGCTXT; \
+	MOVW    (REGCTXT), R0; \
+	PCDATA  $PCDATA_StackMapIndex, $0; \
+	BL      (R0); \
+/*                  copy return values back */ \
+	MOVW  argtype+0(FP), R4; \
+	MOVW  argptr+8(FP), R0; \
+	MOVW  argsize+12(FP), R2; \
+	MOVW  retoffset+16(FP), R3; \
+	ADD   $4, R13, R1; \
+	ADD   R3, R1; \
+	ADD   R3, R0; \
+	SUB   R3, R2; \
+	BL    callRet<>(SB); \
+	RET
+
+// callRet copies return values back at the end of call*. This is a
+// separate function so it can allocate stack space for the arguments
+// to reflectcallmove. It does not follow the Go ABI; it expects its
+// arguments in registers.
+TEXT callRet<>(SB), NOSPLIT, $16-0
+	MOVW  R4, 4(R13)
+	MOVW  R0, 8(R13)
+	MOVW  R1, 12(R13)
+	MOVW  R2, 16(R13)
+	BL    runtime·reflectcallmove(SB)
+	RET
+
+	CALLFN(·call16,          16)
+	CALLFN(·call32,          32)
+	CALLFN(·call64,          64)
+	CALLFN(·call128,         128)
+	CALLFN(·call256,         256)
+	CALLFN(·call512,         512)
+	CALLFN(·call1024,        1024)
+	CALLFN(·call2048,        2048)
+	CALLFN(·call4096,        4096)
+	CALLFN(·call8192,        8192)
+	CALLFN(·call16384,       16384)
+	CALLFN(·call32768,       32768)
+	CALLFN(·call65536,       65536)
+	CALLFN(·call131072,      131072)
+	CALLFN(·call262144,      262144)
+	CALLFN(·call524288,      524288)
+	CALLFN(·call1048576,     1048576)
+	CALLFN(·call2097152,     2097152)
+	CALLFN(·call4194304,     4194304)
+	CALLFN(·call8388608,     8388608)
+	CALLFN(·call16777216,    16777216)
+	CALLFN(·call33554432,    33554432)
+	CALLFN(·call67108864,    67108864)
+	CALLFN(·call134217728,   134217728)
+	CALLFN(·call268435456,   268435456)
+	CALLFN(·call536870912,   536870912)
+	CALLFN(·call1073741824,  1073741824)
+
+// void jmpdefer(fn, sp);
+// called from deferreturn.
+// 1. grab stored LR for caller
+// 2. sub 4 bytes to get back to BL deferreturn
+// 3. B to fn
+// TODO(rsc): Push things on stack and then use pop
+// to load all registers simultaneously, so that a profiling
+// interrupt can never see mismatched SP/LR/PC.
+// (And double-check that pop is atomic in that way.)
+TEXT runtime·jmpdefer(SB),NOSPLIT,$0-8
+	MOVW  0(R13), LR
+	MOVW  $-4(LR), LR  // BL deferreturn
+	MOVW  fv+0(FP), REGCTXT
+	MOVW  argp+4(FP), R13
+	MOVW  $-4(R13), R13  // SP is 4 below argp, due to saved LR
+	MOVW  0(REGCTXT), R1
+	B     (R1)
+
+// Save state of caller into g->sched. Smashes REGTMP.
+TEXT gosave<>(SB),NOSPLIT|NOFRAME,$0
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, (g_sched+gobuf_lr)(g)
+	MOVW  REGTMP, (g_sched+gobuf_ret)(g)
+	MOVW  REGTMP, (g_sched+gobuf_ctxt)(g)
+	// Assert ctxt is zero. See func save.
+	MOVW  (g_sched+gobuf_ctxt)(g), REGTMP
+	CMP   $0, REGTMP
+	B.EQ  2(PC)
+	CALL  runtime·badctxt(SB)
+	RET
+
+// void setg(G*); set g. for use by needm.
+TEXT runtime·setg(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  gg+0(FP), R0
+	B     setg<>(SB)
+
+TEXT setg<>(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  R0, g
+	RET
+
+TEXT runtime·emptyfunc(SB),0,$0-0
+	RET
+
+TEXT runtime·abort(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+
+// AES hashing not implemented for ARM
+TEXT runtime·memhash(SB),NOSPLIT|NOFRAME,$0-16
+	JMP	runtime·memhashFallback(SB)
+TEXT runtime·strhash(SB),NOSPLIT|NOFRAME,$0-12
+	JMP	runtime·strhashFallback(SB)
+TEXT runtime·memhash32(SB),NOSPLIT|NOFRAME,$0-12
+	JMP	runtime·memhash32Fallback(SB)
+TEXT runtime·memhash64(SB),NOSPLIT|NOFRAME,$0-12
+	JMP	runtime·memhash64Fallback(SB)
+
+TEXT runtime·return0(SB),NOSPLIT,$0
+	MOVW  $0, R0
+	RET
+
+TEXT runtime·procyield(SB),NOSPLIT|NOFRAME,$0
+	MOVW  cycles+0(FP), R1
+yieldloop:
+	YIELD
+	CMP  $0, R1
+	RET.EQ
+	SUB  $1, R1
+	B    yieldloop
+
+// Called from cgo wrappers, this function returns g->m->curg.stack.hi.
+// Must obey the gcc calling convention.
+//TEXT _cgo_topofstack(SB),NOSPLIT,$8
+//	// R11 (REGCTXT) and g register are clobbered by load_g. They are
+//	// callee-save in the gcc calling convention, so save them here.
+//	MOVW  REGCTXT, saveR11-4(SP)
+//	MOVW  g, saveG-8(SP)
+//
+//	BL    runtime·load_g(SB)
+//	MOVW  g_m(g), R0
+//	MOVW  m_curg(R0), R0
+//	MOVW  (g_stack+stack_hi)(R0), R0
+//
+//	MOVW  saveG-8(SP), g
+//	MOVW  saveR11-4(SP), REGCTXT
+//	RET
+
+// The top-most function running on a goroutine
+// returns to goexit+PCQuantum.
+TEXT runtime·goexit(SB),NOSPLIT|NOFRAME|TOPFRAME,$0-0
+	NOP2
+	BL  runtime·goexit1(SB)  // does not return
+	// traceback from goexit1 must hit code range of goexit
+	NOP2
+
+// x -> x/1000000, x%1000000, called from Go with args, results on stack.
+TEXT runtime·usplit(SB),NOSPLIT,$0-12
+	MOVW  x+0(FP), R0
+	CALL  runtime·usplitR0(SB)
+	MOVW  R0, q+4(FP)
+	MOVW  R1, r+8(FP)
+	RET
+
+// R0, R1 = R0/1000000, R0%1000000
+TEXT runtime·usplitR0(SB),NOSPLIT,$0
+	// magic multiply to avoid software divide without available m.
+	// see output of go tool compile -S for x/1000000.
+	MOVW   R0, R3
+	MOVW   $1125899907, R1
+	MULLU  R1, R0, (R0, R1)
+	MOVW   R0>>18, R0
+	MOVW   $1000000, R1
+	MUL    R0, R1
+	SUB    R1, R3, R1
+	RET
+
+// This is called from .init_array and follows the platform, not Go, ABI.
+TEXT runtime·addmoduledata(SB),NOSPLIT,$0-0
+	MOVW  R9, saver9-4(SP)        // The access to global variables below implicitly uses R9, which is callee-save
+	MOVW  REGCTXT, saver11-8(SP)  // Likewise, R11 is the REGCTXT register, but callee-save in C ABI
+	MOVW  runtime·lastmoduledatap(SB), R1
+	MOVW  R0, moduledata_next(R1)
+	MOVW  R0, runtime·lastmoduledatap(SB)
+	MOVW  saver11-8(SP), REGCTXT
+	MOVW  saver9-4(SP), R9
+	RET
+
+TEXT ·checkASM(SB),NOSPLIT,$0-1
+	MOVW  $1, R3
+	MOVB  R3, ret+0(FP)
+	RET
+
+// gcWriteBarrier performs a heap pointer write and informs the GC.
+
+// gcWriteBarrier does NOT follow the Go ABI. It takes two arguments:
+// - R2 is the destination of the write
+// - R3 is the value being written at R2
+// It clobbers condition codes.
+// It does not clobber any other general-purpose registers,
+// but may clobber others (e.g., floating point registers).
+// The act of CALLing gcWriteBarrier will clobber R14 (LR).
+TEXT runtime·gcWriteBarrier(SB),NOSPLIT|NOFRAME,$0
+	// Save the registers clobbered by the fast path.
+	MOVM.DB.W  [R0,R1], (R13)
+	MOVW       g_m(g), R0
+	MOVW       m_p(R0), R0
+	MOVW       (p_wbBuf+wbBuf_next)(R0), R1
+	// Increment wbBuf.next position.
+	ADD   $8, R1
+	MOVW  R1, (p_wbBuf+wbBuf_next)(R0)
+	MOVW  (p_wbBuf+wbBuf_end)(R0), R0
+	CMP   R1, R0
+	// Record the write.
+	MOVW  R3, -8(R1)  // Record value
+	MOVW  (R2), R0    // TODO: This turns bad writes into bad reads.
+	MOVW  R0, -4(R1)  // Record *slot
+	// Is the buffer full? (flags set in CMP above)
+	B.EQ  flush
+ret:
+	MOVM.IA.W  (R13), [R0,R1]
+	// Do the write.
+	MOVW  R3, (R2)
+	RET
+
+flush:
+	// Save all general purpose registers since these could be
+	// clobbered by wbBufFlush and were not saved by the caller.
+
+	// R0 and R1 were saved at entry.
+	// R7 is linker temp, so no need to save.
+	// R10 is g, so preserved.
+	// R13 is stack pointer.
+	// R15 is PC.
+
+	// This also sets up R2 and R3 as the arguments to wbBufFlush.
+	MOVM.DB.W  [R2-R6,R8-R9,R11-R12], (R13)
+	// Save R14 (LR) because the fast path above doesn't save it,
+	// but needs it to RET. This is after the MOVM so it appears below
+	// the arguments in the stack frame.
+	MOVW.W  LR, -4(R13)
+
+	// This takes arguments R2 and R3.
+	CALL  runtime·wbBufFlush(SB)
+
+	MOVW.P     4(R13), LR
+	MOVM.IA.W  (R13), [R2-R6,R8-R9,R11-R12]
+	JMP        ret
+
+// Note: these functions use a special calling convention to save generated code space.
+// Arguments are passed in registers, but the space for those arguments are allocated
+// in the caller's stack frame. These stubs write the args into that stack space and
+// then tail call to the corresponding runtime handler.
+// The tail call makes these stubs disappear in backtraces.
+TEXT runtime·panicIndex(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtime·goPanicIndex(SB)
+TEXT runtime·panicIndexU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtime·goPanicIndexU(SB)
+TEXT runtime·panicSliceAlen(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtime·goPanicSliceAlen(SB)
+TEXT runtime·panicSliceAlenU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtime·goPanicSliceAlenU(SB)
+TEXT runtime·panicSliceAcap(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtime·goPanicSliceAcap(SB)
+TEXT runtime·panicSliceAcapU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtime·goPanicSliceAcapU(SB)
+TEXT runtime·panicSliceB(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtime·goPanicSliceB(SB)
+TEXT runtime·panicSliceBU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtime·goPanicSliceBU(SB)
+TEXT runtime·panicSlice3Alen(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtime·goPanicSlice3Alen(SB)
+TEXT runtime·panicSlice3AlenU(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtime·goPanicSlice3AlenU(SB)
+TEXT runtime·panicSlice3Acap(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtime·goPanicSlice3Acap(SB)
+TEXT runtime·panicSlice3AcapU(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtime·goPanicSlice3AcapU(SB)
+TEXT runtime·panicSlice3B(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtime·goPanicSlice3B(SB)
+TEXT runtime·panicSlice3BU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtime·goPanicSlice3BU(SB)
+TEXT runtime·panicSlice3C(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtime·goPanicSlice3C(SB)
+TEXT runtime·panicSlice3CU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtime·goPanicSlice3CU(SB)
+
+// Extended versions for 64-bit indexes.
+TEXT runtime·panicExtendIndex(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtime·goPanicExtendIndex(SB)
+TEXT runtime·panicExtendIndexU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtime·goPanicExtendIndexU(SB)
+TEXT runtime·panicExtendSliceAlen(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtime·goPanicExtendSliceAlen(SB)
+TEXT runtime·panicExtendSliceAlenU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtime·goPanicExtendSliceAlenU(SB)
+TEXT runtime·panicExtendSliceAcap(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtime·goPanicExtendSliceAcap(SB)
+TEXT runtime·panicExtendSliceAcapU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtime·goPanicExtendSliceAcapU(SB)
+TEXT runtime·panicExtendSliceB(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtime·goPanicExtendSliceB(SB)
+TEXT runtime·panicExtendSliceBU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtime·goPanicExtendSliceBU(SB)
+TEXT runtime·panicExtendSlice3Alen(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtime·goPanicExtendSlice3Alen(SB)
+TEXT runtime·panicExtendSlice3AlenU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtime·goPanicExtendSlice3AlenU(SB)
+TEXT runtime·panicExtendSlice3Acap(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtime·goPanicExtendSlice3Acap(SB)
+TEXT runtime·panicExtendSlice3AcapU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtime·goPanicExtendSlice3AcapU(SB)
+TEXT runtime·panicExtendSlice3B(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtime·goPanicExtendSlice3B(SB)
+TEXT runtime·panicExtendSlice3BU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtime·goPanicExtendSlice3BU(SB)
+TEXT runtime·panicExtendSlice3C(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtime·goPanicExtendSlice3C(SB)
+TEXT runtime·panicExtendSlice3CU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtime·goPanicExtendSlice3CU(SB)
diff --git a/src/runtime/cgo.go b/src/runtime/cgo.go
index 395d54a66e..538cf91e28 100644
--- a/src/runtime/cgo.go
+++ b/src/runtime/cgo.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build !noos
+
 package runtime
 
 import "unsafe"
diff --git a/src/runtime/cgo_noos.go b/src/runtime/cgo_noos.go
new file mode 100644
index 0000000000..4fb6d23e34
--- /dev/null
+++ b/src/runtime/cgo_noos.go
@@ -0,0 +1,30 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+const iscgo = false
+
+var (
+	_cgo_thread_start             unsafe.Pointer
+	_cgo_notify_runtime_init_done unsafe.Pointer
+	_cgo_set_context_function     unsafe.Pointer
+	_cgo_yield                    unsafe.Pointer
+)
+
+var (
+	cgo_yield      = &_cgo_yield
+	cgoAlwaysFalse bool
+	cgoHasExtraM   bool
+)
+
+type cgoCallers [1]uintptr
+
+func cgoCheckSliceCopy(typ *_type, dst, src unsafe.Pointer, n int) {}
+func cgoCheckWriteBarrier(dst *uintptr, src uintptr)            {}
+func cgocall(fn unsafe.Pointer, arg unsafe.Pointer) int32       { return 0 }
+
+func cgoCheckMemmove(typ *_type, dst unsafe.Pointer, src unsafe.Pointer, off uintptr, size uintptr) {}
diff --git a/src/runtime/cgocall.go b/src/runtime/cgocall.go
index 20cacd6043..a87825038c 100644
--- a/src/runtime/cgocall.go
+++ b/src/runtime/cgocall.go
@@ -82,6 +82,8 @@
 // crosscall2 restores the callee-save registers for gcc and returns
 // to GoF, which unpacks any result values and returns to f.
 
+// +build !noos
+
 package runtime
 
 import (
diff --git a/src/runtime/cgocheck.go b/src/runtime/cgocheck.go
index 516045c163..a8dda030b9 100644
--- a/src/runtime/cgocheck.go
+++ b/src/runtime/cgocheck.go
@@ -5,6 +5,8 @@
 // Code to check that pointer writes follow the cgo rules.
 // These functions are invoked via the write barrier when debug.cgocheck > 1.
 
+// +build !noos
+
 package runtime
 
 import (
diff --git a/src/runtime/const_noos_kendryte.go b/src/runtime/const_noos_kendryte.go
new file mode 100644
index 0000000000..4f1d3b5811
--- /dev/null
+++ b/src/runtime/const_noos_kendryte.go
@@ -0,0 +1,32 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build noos
+// +build k210
+
+package runtime
+
+const (
+	_OS                        = 0
+	noos                       = true
+	noosScaleDown              = 2
+	noosStackCacheSize         = 16 * 1024
+	noosNumStackOrders         = 3
+	noosHeapAddrBits           = 23 // enough for 8 MiB K210 SRAM
+	noosLogHeapArenaBytes      = 17 // 128 KiB
+	noosArenaBaseOffset        = 0x80000000
+	noosMinPhysPageSize        = 256
+	noosSpanSetInitSpineCap    = 64
+	noosStackMin               = 2048
+	noosStackSystem            = 0
+	noosStackGuard             = 928
+	noosFinBlockSize           = 2 * 1024
+	noosSweepMinHeapDistance   = 8 * 1024
+	noosDefaultHeapMinimum     = 64 * 1024
+	noosHeapGoalInc            = 8 * 1024
+	noosGCSweepBlockEntries    = 1024
+	noosGCSweepBufInitSpineCap = 64
+	noosGCBitsChunkBytes       = 16 * 1024
+	noosSemTabSize             = 113
+)
diff --git a/src/runtime/const_noos_nrf52.go b/src/runtime/const_noos_nrf52.go
new file mode 100644
index 0000000000..bb4d5c92ca
--- /dev/null
+++ b/src/runtime/const_noos_nrf52.go
@@ -0,0 +1,32 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build noos
+// +build nrf52840
+
+package runtime
+
+const (
+	_OS                        = 0
+	noos                       = true
+	noosScaleDown              = 16
+	noosStackCacheSize         = 4 * 1024
+	noosNumStackOrders         = 2
+	noosHeapAddrBits           = 18 // enough for 256 KiB of nRF52840
+	noosLogHeapArenaBytes      = 14 // 16 KiB
+	noosArenaBaseOffset        = 0x20000000
+	noosMinPhysPageSize        = 256
+	noosSpanSetInitSpineCap    = 8
+	noosStackMin               = 1024
+	noosStackSystem            = 27 * 4 // register stacking at exception entry
+	noosStackGuard             = 464
+	noosFinBlockSize           = 256
+	noosSweepMinHeapDistance   = 1024
+	noosDefaultHeapMinimum     = 8 * 1024
+	noosHeapGoalInc            = 1024
+	noosGCSweepBlockEntries    = 64
+	noosGCSweepBufInitSpineCap = 32
+	noosGCBitsChunkBytes       = 2 * 1024
+	noosSemTabSize             = 31
+)
diff --git a/src/runtime/const_noos_stm32h7.go b/src/runtime/const_noos_stm32h7.go
new file mode 100644
index 0000000000..85f785e655
--- /dev/null
+++ b/src/runtime/const_noos_stm32h7.go
@@ -0,0 +1,32 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build noos
+// +build stm32h7x3
+
+package runtime
+
+const (
+	_OS                        = 0
+	noos                       = true
+	noosScaleDown              = 16
+	noosStackCacheSize         = 8 * 1024
+	noosNumStackOrders         = 2
+	noosHeapAddrBits           = 19         // enough for 512 KiB AXI SRAM (AHB SRAM not supported)
+	noosLogHeapArenaBytes      = 14         // 16 KiB
+	noosArenaBaseOffset        = 0x24000000 // the begginning of AXI SRAM
+	noosMinPhysPageSize        = 256
+	noosSpanSetInitSpineCap    = 8
+	noosStackMin               = 1024
+	noosStackSystem            = 27 * 4 // register stacking at exception entry
+	noosStackGuard             = 464
+	noosFinBlockSize           = 256
+	noosSweepMinHeapDistance   = 1024
+	noosDefaultHeapMinimum     = 8 * 1024
+	noosHeapGoalInc            = 1024
+	noosGCSweepBlockEntries    = 64
+	noosGCSweepBufInitSpineCap = 32
+	noosGCBitsChunkBytes       = 2 * 1024
+	noosSemTabSize             = 31
+)
diff --git a/src/runtime/const_noos_stm32x4.go b/src/runtime/const_noos_stm32x4.go
new file mode 100644
index 0000000000..3cd1ab2cea
--- /dev/null
+++ b/src/runtime/const_noos_stm32x4.go
@@ -0,0 +1,32 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build noos
+// +build stm32f215 stm32f407 stm32f412 stm32l4x6
+
+package runtime
+
+const (
+	_OS                        = 0
+	noos                       = true
+	noosScaleDown              = 16
+	noosStackCacheSize         = 4 * 1024
+	noosNumStackOrders         = 2
+	noosHeapAddrBits           = 19 // enough for 320 KiB of STM32L496
+	noosLogHeapArenaBytes      = 14 // 16 KiB
+	noosArenaBaseOffset        = 0x20000000
+	noosMinPhysPageSize        = 256
+	noosSpanSetInitSpineCap    = 8
+	noosStackMin               = 1024
+	noosStackSystem            = 27 * 4 // register stacking at exception entry
+	noosStackGuard             = 464
+	noosFinBlockSize           = 256
+	noosSweepMinHeapDistance   = 1024
+	noosDefaultHeapMinimum     = 8 * 1024
+	noosHeapGoalInc            = 1024
+	noosGCSweepBlockEntries    = 64
+	noosGCSweepBufInitSpineCap = 32
+	noosGCBitsChunkBytes       = 2 * 1024
+	noosSemTabSize             = 31
+)
diff --git a/src/runtime/const_os.go b/src/runtime/const_os.go
new file mode 100644
index 0000000000..64ee3c95da
--- /dev/null
+++ b/src/runtime/const_os.go
@@ -0,0 +1,31 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package runtime
+
+const (
+	_OS                        = 1
+	noos                       = false
+	noosScaleDown              = 1
+	noosStackCacheSize         = 0
+	noosNumStackOrders         = 0
+	noosHeapAddrBits           = 0
+	noosLogHeapArenaBytes      = 0
+	noosArenaBaseOffset        = 0
+	noosMinPhysPageSize        = 0
+	noosSpanSetInitSpineCap    = 0
+	noosStackMin               = 0
+	noosStackSystem            = 0
+	noosStackGuard             = 0
+	noosFinBlockSize           = 0
+	noosSweepMinHeapDistance   = 0
+	noosDefaultHeapMinimum     = 0
+	noosHeapGoalInc            = 0
+	noosGCSweepBlockEntries    = 0
+	noosGCSweepBufInitSpineCap = 0
+	noosGCBitsChunkBytes       = 0
+	noosSemTabSize             = 31
+)
diff --git a/src/runtime/cpumtx.go b/src/runtime/cpumtx.go
new file mode 100644
index 0000000000..f3698ca054
--- /dev/null
+++ b/src/runtime/cpumtx.go
@@ -0,0 +1,28 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos !thumb
+
+package runtime
+
+import "runtime/internal/atomic"
+
+// simple spinlock
+type cpumtx struct {
+	v uint32
+}
+
+//go:nosplit
+func (l *cpumtx) lock() {
+	for {
+		if atomic.Cas(&l.v, 0, 1) {
+			return
+		}
+	}
+}
+
+//go:nosplit
+func (l *cpumtx) unlock() {
+	atomic.Store(&l.v, 0)
+}
diff --git a/src/runtime/cpumtx_noos_thumb.go b/src/runtime/cpumtx_noos_thumb.go
new file mode 100644
index 0000000000..1f27582c88
--- /dev/null
+++ b/src/runtime/cpumtx_noos_thumb.go
@@ -0,0 +1,8 @@
+package runtime
+
+// for now noos/thumb supports only single CPU
+
+type cpumtx struct{}
+
+func (mt *cpumtx) lock()   {}
+func (mt *cpumtx) unlock() {}
diff --git a/src/runtime/cputicks.go b/src/runtime/cputicks.go
index 7beb57ea12..969a73f895 100644
--- a/src/runtime/cputicks.go
+++ b/src/runtime/cputicks.go
@@ -3,6 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build !arm
+// +build !thumb
 // +build !arm64
 // +build !mips64
 // +build !mips64le
diff --git a/src/runtime/debug.go b/src/runtime/debug.go
index f411b22676..9db6bbaa12 100644
--- a/src/runtime/debug.go
+++ b/src/runtime/debug.go
@@ -45,6 +45,9 @@ func NumCPU() int {
 
 // NumCgoCall returns the number of cgo calls made by the current process.
 func NumCgoCall() int64 {
+	if noos {
+		return 0
+	}
 	var n int64
 	for mp := (*m)(atomic.Loadp(unsafe.Pointer(&allm))); mp != nil; mp = mp.alllink {
 		n += int64(mp.ncgocall)
diff --git a/src/runtime/debug/stubs.go b/src/runtime/debug/stubs.go
index 2cba136044..82ab5ba0fd 100644
--- a/src/runtime/debug/stubs.go
+++ b/src/runtime/debug/stubs.go
@@ -15,3 +15,4 @@ func setMaxStack(int) int
 func setGCPercent(int32) int32
 func setPanicOnFault(bool) bool
 func setMaxThreads(int) int
+func setGCTrace(int32)
diff --git a/src/runtime/defs_linux_arm.go b/src/runtime/defs_linux_armt.go
similarity index 98%
rename from src/runtime/defs_linux_arm.go
rename to src/runtime/defs_linux_armt.go
index 5bc0916f8b..0681886e10 100644
--- a/src/runtime/defs_linux_arm.go
+++ b/src/runtime/defs_linux_armt.go
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build linux
+// +build arm thumb
+
 package runtime
 
 // Constants
diff --git a/src/runtime/duff_thumb.s b/src/runtime/duff_thumb.s
new file mode 100644
index 0000000000..ba8235b740
--- /dev/null
+++ b/src/runtime/duff_thumb.s
@@ -0,0 +1,523 @@
+// Code generated by mkduff.go; DO NOT EDIT.
+// Run go generate from src/runtime to update.
+// See mkduff.go for comments.
+
+#include "textflag.h"
+
+TEXT runtime·duffzero(SB), NOSPLIT, $0-0
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	RET
+
+TEXT runtime·duffcopy(SB), NOSPLIT, $0-0
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	RET
diff --git a/src/runtime/env_noos.go b/src/runtime/env_noos.go
new file mode 100644
index 0000000000..2a8c6701c5
--- /dev/null
+++ b/src/runtime/env_noos.go
@@ -0,0 +1,30 @@
+// Copyright 2012 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/atomic"
+	"unsafe"
+)
+
+var _cgo_setenv unsafe.Pointer   // pointer to C function
+var _cgo_unsetenv unsafe.Pointer // pointer to C function
+
+//go:nosplit
+func gogetenv(key string) string { return "" }
+
+// The following functions should be defined in the runtime/debug package but
+// for now they are here because the runtime/debug imports time package, not
+// ported yet.
+
+// SetGCTrace is the alternate way to set GODEBUG=gctrace=n at the runtime.
+func SetGCTrace(in int32) {
+	atomic.Store((*uint32)(unsafe.Pointer(&debug.gctrace)), uint32(in))
+}
+
+// SetGCPercent is the alternate way to set GOGC=percent at the runtime.
+func SetGCPercent(percent int) int {
+	return int(setGCPercent(int32(percent)))
+}
diff --git a/src/runtime/export_thumb_test.go b/src/runtime/export_thumb_test.go
new file mode 100644
index 0000000000..b8a89fc0d2
--- /dev/null
+++ b/src/runtime/export_thumb_test.go
@@ -0,0 +1,9 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Export guts for testing.
+
+package runtime
+
+var Usplit = usplit
diff --git a/src/runtime/gcinfo_test.go b/src/runtime/gcinfo_test.go
index 0808b416f0..8571158128 100644
--- a/src/runtime/gcinfo_test.go
+++ b/src/runtime/gcinfo_test.go
@@ -156,7 +156,7 @@ type BigStruct struct {
 
 func infoBigStruct() []byte {
 	switch runtime.GOARCH {
-	case "386", "arm", "mips", "mipsle":
+	case "386", "arm", "thumb", "mips", "mipsle":
 		return []byte{
 			typePointer,                                                // q *int
 			typeScalar, typeScalar, typeScalar, typeScalar, typeScalar, // w byte; e [17]byte
diff --git a/src/runtime/go_tls.h b/src/runtime/go_tls.h
index a47e798d9d..e42865c429 100644
--- a/src/runtime/go_tls.h
+++ b/src/runtime/go_tls.h
@@ -6,6 +6,12 @@
 #define LR R14
 #endif
 
+#ifdef GOARCH_thumb
+#define REGTMP  R7
+#define REGCTXT R11
+#define LR      R14
+#endif
+
 #ifdef GOARCH_amd64
 #define	get_tls(r)	MOVQ TLS, r
 #define	g(r)	0(r)(TLS*1)
diff --git a/src/runtime/hash32.go b/src/runtime/hash32.go
index 966f70e1aa..8ef67de9c9 100644
--- a/src/runtime/hash32.go
+++ b/src/runtime/hash32.go
@@ -6,7 +6,7 @@
 //   xxhash: https://code.google.com/p/xxhash/
 // cityhash: https://code.google.com/p/cityhash/
 
-// +build 386 arm mips mipsle
+// +build 386 arm thumb mips mipsle
 
 package runtime
 
diff --git a/src/runtime/heapdump.go b/src/runtime/heapdump.go
index 2d531571aa..9d4341128a 100644
--- a/src/runtime/heapdump.go
+++ b/src/runtime/heapdump.go
@@ -9,6 +9,8 @@
 // The format of the dumped file is described at
 // https://golang.org/s/go15heapdump.
 
+// +build !noos
+
 package runtime
 
 import (
diff --git a/src/runtime/iface.go b/src/runtime/iface.go
index 0504b89363..2ce1d2fa96 100644
--- a/src/runtime/iface.go
+++ b/src/runtime/iface.go
@@ -10,7 +10,7 @@ import (
 	"unsafe"
 )
 
-const itabInitSize = 512
+const itabInitSize = 512 * _OS
 
 var (
 	itabLock      mutex                               // lock for accessing itab table
@@ -245,6 +245,11 @@ imethods:
 func itabsinit() {
 	lockInit(&itabLock, lockRankItab)
 	lock(&itabLock)
+	if noos {
+		// allocate starter table
+		itabTable = (*itabTableType)(mallocgc((2+126)*sys.PtrSize, nil, true))
+		itabTable.size = 126
+	}
 	for _, md := range activeModules() {
 		for _, i := range md.itablinks {
 			itabAdd(i)
diff --git a/src/runtime/internal/atomic/asm_thumb.s b/src/runtime/internal/atomic/asm_thumb.s
new file mode 100644
index 0000000000..c0427dc8eb
--- /dev/null
+++ b/src/runtime/internal/atomic/asm_thumb.s
@@ -0,0 +1,156 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT ·Load(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  (R0), R1
+	DMB   MB_ISH
+	MOVW  R1, ret+4(FP)
+	RET
+
+TEXT ·Load8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  (R0), R1
+	DMB    MB_ISH
+	MOVB   R1, ret+4(FP)
+	RET
+
+TEXT ·Store(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R1
+	MOVW  v+4(FP), R2
+	DMB   MB_ISH
+	MOVW  R2, (R1)
+	DMB   MB_ISH
+	RET
+
+TEXT ·Store8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R1
+	MOVBU  v+4(FP), R2
+	DMB    MB_ISH
+	MOVB   R2, (R1)
+	DMB    MB_ISH
+	RET
+
+TEXT ·Cas(SB),NOSPLIT|NOFRAME,$0
+	MOVW  ptr+0(FP), R1
+	MOVW  old+4(FP), R2
+	MOVW  new+8(FP), R3
+casl:
+	LDREX  (R1), R0
+	CMP    R0, R2
+	BNE    end
+	DMB    MB_ISHST
+	STREX  R3, (R1), R0
+	CMP    $0, R0
+	BNE    casl
+end:
+	MOVW.NE    $0, R0
+	MOVW.P.EQ  $1, R0
+	DMB.EQ     MB_ISH
+	MOVB       R0, ret+12(FP)
+	RET
+
+TEXT ·Xadd(SB),NOSPLIT|NOFRAME,$0-12
+	MOVW  val+0(FP), R1
+	MOVW  delta+4(FP), R2
+loop:
+	LDREX  (R1), R0
+	DMB    MB_ISHST
+	ADD    R2, R0
+	STREX  R0, (R1), R3
+	CMP    $0, R3
+	BNE    loop
+	DMB    MB_ISH
+	MOVW   R0, ret+8(FP)
+	RET
+
+TEXT ·Or(SB),NOSPLIT|NOFRAME,$0-8
+	MOVW  addr+0(FP), R1
+	MOVW  v+4(FP), R2
+loop:
+	LDREX  (R1), R0
+	DMB    MB_ISHST
+	ORR    R2, R0
+	STREX  R0, (R1), R3
+	CMP    $0, R3
+	BNE    loop
+	DMB    MB_ISH
+	RET
+
+TEXT ·And(SB),NOSPLIT|NOFRAME,$0-8
+	MOVW  addr+0(FP), R1
+	MOVW  v+4(FP), R2
+loop:
+	LDREX  (R1), R0
+	DMB    MB_ISHST
+	AND    R2, R0
+	STREX  R0, (R1), R3
+	CMP    $0, R3
+	BNE    loop
+	DMB    MB_ISH
+	RET
+
+// stubs
+
+TEXT ·Loadp(SB),NOSPLIT|NOFRAME,$0-8
+	B   ·Load(SB)
+
+TEXT ·LoadAcq(SB),NOSPLIT|NOFRAME,$0-8
+	B   ·Load(SB)
+
+TEXT ·LoadAcquintptr(SB),NOSPLIT|NOFRAME,$0-8
+	B 	·Load(SB)
+
+TEXT ·Casuintptr(SB),NOSPLIT,$0-13
+	B	·Cas(SB)
+
+TEXT ·Casp1(SB),NOSPLIT,$0-13
+	B   ·Cas(SB)
+
+TEXT ·CasRel(SB),NOSPLIT,$0-13
+	B   ·Cas(SB)
+
+TEXT ·Loaduintptr(SB),NOSPLIT,$0-8
+	B	·Load(SB)
+
+TEXT ·Loaduint(SB),NOSPLIT,$0-8
+	B	·Load(SB)
+
+TEXT ·Storeuintptr(SB),NOSPLIT,$0-8
+	B	·Store(SB)
+
+TEXT ·StorepNoWB(SB),NOSPLIT,$0-8
+	B   ·Store(SB)
+
+TEXT ·StoreRel(SB),NOSPLIT,$0-8
+	B   ·Store(SB)
+
+TEXT ·StoreReluintptr(SB),NOSPLIT,$0-8
+	B	·Store(SB)
+
+TEXT ·Loadint64(SB),NOSPLIT,$0-12
+	B   ·Load64(SB)
+
+TEXT ·Xadduintptr(SB),NOSPLIT,$0-12
+	B	·Xadd(SB)
+
+TEXT ·Xaddint64(SB),NOSPLIT,$0-20
+	B   ·Xadd64(SB)
+
+TEXT ·Cas64(SB),NOSPLIT,$0-21
+	B   ·goCas64(SB)
+
+TEXT ·Xadd64(SB),NOSPLIT,$0-20
+	B   ·goXadd64(SB)
+
+TEXT ·Xchg64(SB),NOSPLIT,$0-20
+	B   ·goXchg64(SB)
+
+TEXT ·Load64(SB),NOSPLIT,$0-12
+	B   ·goLoad64(SB)
+
+TEXT ·Store64(SB),NOSPLIT,$0-12
+	B   ·goStore64(SB)
diff --git a/src/runtime/internal/atomic/atomic_thumb.go b/src/runtime/internal/atomic/atomic_thumb.go
new file mode 100644
index 0000000000..3dbe61f973
--- /dev/null
+++ b/src/runtime/internal/atomic/atomic_thumb.go
@@ -0,0 +1,217 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package atomic
+
+import (
+	"internal/cpu"
+	"runtime/internal/sys"
+	"unsafe"
+)
+
+// Export some functions via linkname to assembly in sync/atomic.
+//go:linkname Xchg
+//go:linkname Xchguintptr
+
+type spinlock struct {
+	v uint32
+}
+
+//go:nosplit
+func (l *spinlock) lock() {
+	for {
+		if Cas(&l.v, 0, 1) {
+			return
+		}
+	}
+}
+
+//go:nosplit
+func (l *spinlock) unlock() {
+	Store(&l.v, 0)
+}
+
+const _MCU = sys.GoosNoos
+
+var locktab [57*(1-_MCU) + 29*_MCU]struct {
+	l   spinlock
+	pad [cpu.CacheLinePadSize - unsafe.Sizeof(spinlock{})]byte
+}
+
+func addrLock(addr *uint64) *spinlock {
+	return &locktab[(uintptr(unsafe.Pointer(addr))>>3)%uintptr(len(locktab))].l
+}
+
+//go:nosplit
+func goCas64(addr *uint64, old, new uint64) bool {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	var ok bool
+	addrLock(addr).lock()
+	if *addr == old {
+		*addr = new
+		ok = true
+	}
+	addrLock(addr).unlock()
+	return ok
+}
+
+//go:nosplit
+func goXadd64(addr *uint64, delta int64) uint64 {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	var r uint64
+	addrLock(addr).lock()
+	r = *addr + uint64(delta)
+	*addr = r
+	addrLock(addr).unlock()
+	return r
+}
+
+//go:nosplit
+func goXchg64(addr *uint64, v uint64) uint64 {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	var r uint64
+	addrLock(addr).lock()
+	r = *addr
+	*addr = v
+	addrLock(addr).unlock()
+	return r
+}
+
+//go:nosplit
+func goLoad64(addr *uint64) uint64 {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	var r uint64
+	addrLock(addr).lock()
+	r = *addr
+	addrLock(addr).unlock()
+	return r
+}
+
+//go:nosplit
+func goStore64(addr *uint64, v uint64) {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	addrLock(addr).lock()
+	*addr = v
+	addrLock(addr).unlock()
+}
+
+//go:nosplit
+func Or8(addr *uint8, v uint8) {
+	// Align down to 4 bytes and use 32-bit CAS.
+	uaddr := uintptr(unsafe.Pointer(addr))
+	addr32 := (*uint32)(unsafe.Pointer(uaddr &^ 3))
+	word := uint32(v) << ((uaddr & 3) * 8) // little endian
+	for {
+		old := *addr32
+		if Cas(addr32, old, old|word) {
+			return
+		}
+	}
+}
+
+//go:nosplit
+func And8(addr *uint8, v uint8) {
+	// Align down to 4 bytes and use 32-bit CAS.
+	uaddr := uintptr(unsafe.Pointer(addr))
+	addr32 := (*uint32)(unsafe.Pointer(uaddr &^ 3))
+	word := uint32(v) << ((uaddr & 3) * 8)    // little endian
+	mask := uint32(0xFF) << ((uaddr & 3) * 8) // little endian
+	word |= ^mask
+	for {
+		old := *addr32
+		if Cas(addr32, old, old&word) {
+			return
+		}
+	}
+}
+
+//go:nosplit
+func Or(addr *uint32, v uint32)
+
+//go:nosplit
+func And(addr *uint32, v uint32)
+
+//go:noescape
+func Load(addr *uint32) uint32
+
+// NO go:noescape annotation; *addr escapes if result escapes (#31525)
+func Loadp(addr unsafe.Pointer) unsafe.Pointer
+
+//go:noescape
+func Load64(addr *uint64) uint64
+
+//go:noescape
+func Load8(addr *uint8) uint8
+
+//go:noescape
+func LoadAcq(addr *uint32) uint32
+
+//go:noescape
+func LoadAcquintptr(ptr *uintptr) uintptr
+
+// Not noescape -- it installs a pointer to addr.
+func StorepNoWB(addr unsafe.Pointer, v unsafe.Pointer)
+
+//go:noescape
+func Store(addr *uint32, v uint32)
+
+//go:noescape
+func StoreRel(addr *uint32, v uint32)
+
+//go:noescape
+func StoreReluintptr(addr *uintptr, v uintptr)
+
+//go:noescape
+func Store64(addr *uint64, v uint64)
+
+//go:noescape
+func Store8(addr *uint8, v uint8)
+
+//go:noescape
+func Cas64(addr *uint64, old, new uint64) bool
+
+//go:noescape
+func CasRel(addr *uint32, old, new uint32) bool
+
+//go:noescape
+func Xadd(val *uint32, delta int32) uint32
+
+//go:noescape
+func Xadd64(addr *uint64, delta int64) uint64
+
+//go:noescape
+func Xchg64(addr *uint64, v uint64) uint64
+
+//go:nosplit
+func Xadduintptr(val *uintptr, delta uintptr) uintptr
+
+//go:nosplit
+func Xchg(addr *uint32, v uint32) uint32 {
+	for {
+		old := *addr
+		if Cas(addr, old, v) {
+			return old
+		}
+	}
+}
+
+//go:nosplit
+func Xchguintptr(addr *uintptr, v uintptr) uintptr {
+	return uintptr(Xchg((*uint32)(unsafe.Pointer(addr)), uint32(v)))
+}
diff --git a/src/runtime/internal/sys/arch_thumb.go b/src/runtime/internal/sys/arch_thumb.go
new file mode 100644
index 0000000000..606ca16c1f
--- /dev/null
+++ b/src/runtime/internal/sys/arch_thumb.go
@@ -0,0 +1,16 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sys
+
+const (
+	ArchFamily          = ARM
+	BigEndian           = false
+	DefaultPhysPageSize = 1024
+	PCQuantum           = 2
+	Int64Align          = 4
+	MinFrameSize        = 4
+)
+
+type Uintreg uint32
diff --git a/src/runtime/internal/sys/zgoarch_386.go b/src/runtime/internal/sys/zgoarch_386.go
index c286d0df2b..85dfdb6b44 100644
--- a/src/runtime/internal/sys/zgoarch_386.go
+++ b/src/runtime/internal/sys/zgoarch_386.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_amd64.go b/src/runtime/internal/sys/zgoarch_amd64.go
index d21c1d7d2a..8ed3ec6899 100644
--- a/src/runtime/internal/sys/zgoarch_amd64.go
+++ b/src/runtime/internal/sys/zgoarch_amd64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_arm.go b/src/runtime/internal/sys/zgoarch_arm.go
index 9085fb0ea8..b23027e402 100644
--- a/src/runtime/internal/sys/zgoarch_arm.go
+++ b/src/runtime/internal/sys/zgoarch_arm.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_arm64.go b/src/runtime/internal/sys/zgoarch_arm64.go
index ed7ef2ebcb..1f90f77914 100644
--- a/src/runtime/internal/sys/zgoarch_arm64.go
+++ b/src/runtime/internal/sys/zgoarch_arm64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_arm64be.go b/src/runtime/internal/sys/zgoarch_arm64be.go
index faf3111053..3f717d4bd8 100644
--- a/src/runtime/internal/sys/zgoarch_arm64be.go
+++ b/src/runtime/internal/sys/zgoarch_arm64be.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_armbe.go b/src/runtime/internal/sys/zgoarch_armbe.go
index cb28301e0b..0393a4d176 100644
--- a/src/runtime/internal/sys/zgoarch_armbe.go
+++ b/src/runtime/internal/sys/zgoarch_armbe.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips.go b/src/runtime/internal/sys/zgoarch_mips.go
index 315dea1c84..b5b85f2337 100644
--- a/src/runtime/internal/sys/zgoarch_mips.go
+++ b/src/runtime/internal/sys/zgoarch_mips.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips64.go b/src/runtime/internal/sys/zgoarch_mips64.go
index 5258cbfbe7..f6be8bfb3c 100644
--- a/src/runtime/internal/sys/zgoarch_mips64.go
+++ b/src/runtime/internal/sys/zgoarch_mips64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips64le.go b/src/runtime/internal/sys/zgoarch_mips64le.go
index 1721698518..640550a5e1 100644
--- a/src/runtime/internal/sys/zgoarch_mips64le.go
+++ b/src/runtime/internal/sys/zgoarch_mips64le.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips64p32.go b/src/runtime/internal/sys/zgoarch_mips64p32.go
index 44c4624da9..181fd59061 100644
--- a/src/runtime/internal/sys/zgoarch_mips64p32.go
+++ b/src/runtime/internal/sys/zgoarch_mips64p32.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips64p32le.go b/src/runtime/internal/sys/zgoarch_mips64p32le.go
index eb63225430..f35e05b89d 100644
--- a/src/runtime/internal/sys/zgoarch_mips64p32le.go
+++ b/src/runtime/internal/sys/zgoarch_mips64p32le.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mipsle.go b/src/runtime/internal/sys/zgoarch_mipsle.go
index e0ebfbf038..b4a8fc7ea9 100644
--- a/src/runtime/internal/sys/zgoarch_mipsle.go
+++ b/src/runtime/internal/sys/zgoarch_mipsle.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_ppc.go b/src/runtime/internal/sys/zgoarch_ppc.go
index ef26aa3201..e8dd9c90c0 100644
--- a/src/runtime/internal/sys/zgoarch_ppc.go
+++ b/src/runtime/internal/sys/zgoarch_ppc.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_ppc64.go b/src/runtime/internal/sys/zgoarch_ppc64.go
index 32c2d46d4c..a3c812c93f 100644
--- a/src/runtime/internal/sys/zgoarch_ppc64.go
+++ b/src/runtime/internal/sys/zgoarch_ppc64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_ppc64le.go b/src/runtime/internal/sys/zgoarch_ppc64le.go
index 3a6e56763c..20ebdbe286 100644
--- a/src/runtime/internal/sys/zgoarch_ppc64le.go
+++ b/src/runtime/internal/sys/zgoarch_ppc64le.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_riscv.go b/src/runtime/internal/sys/zgoarch_riscv.go
index d8f6b49093..944b1f7704 100644
--- a/src/runtime/internal/sys/zgoarch_riscv.go
+++ b/src/runtime/internal/sys/zgoarch_riscv.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_riscv64.go b/src/runtime/internal/sys/zgoarch_riscv64.go
index 0ba843b5ac..74ac8c102b 100644
--- a/src/runtime/internal/sys/zgoarch_riscv64.go
+++ b/src/runtime/internal/sys/zgoarch_riscv64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_s390.go b/src/runtime/internal/sys/zgoarch_s390.go
index 20a1b234a6..8f0777d2b5 100644
--- a/src/runtime/internal/sys/zgoarch_s390.go
+++ b/src/runtime/internal/sys/zgoarch_s390.go
@@ -28,4 +28,5 @@ const GoarchS390 = 1
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_s390x.go b/src/runtime/internal/sys/zgoarch_s390x.go
index ffdda0c827..c88d6d82fe 100644
--- a/src/runtime/internal/sys/zgoarch_s390x.go
+++ b/src/runtime/internal/sys/zgoarch_s390x.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 1
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_sparc.go b/src/runtime/internal/sys/zgoarch_sparc.go
index b4949510d5..487d43b557 100644
--- a/src/runtime/internal/sys/zgoarch_sparc.go
+++ b/src/runtime/internal/sys/zgoarch_sparc.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 1
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_sparc64.go b/src/runtime/internal/sys/zgoarch_sparc64.go
index 0f6df411ce..0924b1a031 100644
--- a/src/runtime/internal/sys/zgoarch_sparc64.go
+++ b/src/runtime/internal/sys/zgoarch_sparc64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 1
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_thumb.go b/src/runtime/internal/sys/zgoarch_thumb.go
new file mode 100644
index 0000000000..2cc7172b31
--- /dev/null
+++ b/src/runtime/internal/sys/zgoarch_thumb.go
@@ -0,0 +1,32 @@
+// Code generated by gengoos.go using 'go generate'. DO NOT EDIT.
+
+// +build thumb
+
+package sys
+
+const GOARCH = `thumb`
+
+const Goarch386 = 0
+const GoarchAmd64 = 0
+const GoarchAmd64p32 = 0
+const GoarchArm = 0
+const GoarchArmbe = 0
+const GoarchArm64 = 0
+const GoarchArm64be = 0
+const GoarchPpc64 = 0
+const GoarchPpc64le = 0
+const GoarchMips = 0
+const GoarchMipsle = 0
+const GoarchMips64 = 0
+const GoarchMips64le = 0
+const GoarchMips64p32 = 0
+const GoarchMips64p32le = 0
+const GoarchPpc = 0
+const GoarchRiscv = 0
+const GoarchRiscv64 = 0
+const GoarchS390 = 0
+const GoarchS390x = 0
+const GoarchSparc = 0
+const GoarchSparc64 = 0
+const GoarchThumb = 1
+const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_wasm.go b/src/runtime/internal/sys/zgoarch_wasm.go
index e69afb0cb3..06a191ee74 100644
--- a/src/runtime/internal/sys/zgoarch_wasm.go
+++ b/src/runtime/internal/sys/zgoarch_wasm.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 1
diff --git a/src/runtime/internal/sys/zgoos_aix.go b/src/runtime/internal/sys/zgoos_aix.go
index 0631d02aa5..1f9d113c89 100644
--- a/src/runtime/internal/sys/zgoos_aix.go
+++ b/src/runtime/internal/sys/zgoos_aix.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_android.go b/src/runtime/internal/sys/zgoos_android.go
index d356a40bec..23da68e1cf 100644
--- a/src/runtime/internal/sys/zgoos_android.go
+++ b/src/runtime/internal/sys/zgoos_android.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_darwin.go b/src/runtime/internal/sys/zgoos_darwin.go
index 6aa2db7e3a..94ade598b2 100644
--- a/src/runtime/internal/sys/zgoos_darwin.go
+++ b/src/runtime/internal/sys/zgoos_darwin.go
@@ -19,6 +19,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_dragonfly.go b/src/runtime/internal/sys/zgoos_dragonfly.go
index 88ee1174f1..17b3e4a9e7 100644
--- a/src/runtime/internal/sys/zgoos_dragonfly.go
+++ b/src/runtime/internal/sys/zgoos_dragonfly.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_freebsd.go b/src/runtime/internal/sys/zgoos_freebsd.go
index 8de2ec0559..f91be0d9dc 100644
--- a/src/runtime/internal/sys/zgoos_freebsd.go
+++ b/src/runtime/internal/sys/zgoos_freebsd.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_hurd.go b/src/runtime/internal/sys/zgoos_hurd.go
index 183ccb02a1..4a33f2afca 100644
--- a/src/runtime/internal/sys/zgoos_hurd.go
+++ b/src/runtime/internal/sys/zgoos_hurd.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_illumos.go b/src/runtime/internal/sys/zgoos_illumos.go
index d04134e1df..aced21ac14 100644
--- a/src/runtime/internal/sys/zgoos_illumos.go
+++ b/src/runtime/internal/sys/zgoos_illumos.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_js.go b/src/runtime/internal/sys/zgoos_js.go
index 1d9279ab38..9236fed37b 100644
--- a/src/runtime/internal/sys/zgoos_js.go
+++ b/src/runtime/internal/sys/zgoos_js.go
@@ -18,6 +18,7 @@ const GoosJs = 1
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_linux.go b/src/runtime/internal/sys/zgoos_linux.go
index 0f718d704f..ad654085ea 100644
--- a/src/runtime/internal/sys/zgoos_linux.go
+++ b/src/runtime/internal/sys/zgoos_linux.go
@@ -19,6 +19,7 @@ const GoosJs = 0
 const GoosLinux = 1
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_netbsd.go b/src/runtime/internal/sys/zgoos_netbsd.go
index 2ae149ff13..6384fb4da8 100644
--- a/src/runtime/internal/sys/zgoos_netbsd.go
+++ b/src/runtime/internal/sys/zgoos_netbsd.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 1
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_noos.go b/src/runtime/internal/sys/zgoos_noos.go
new file mode 100644
index 0000000000..96895ada8b
--- /dev/null
+++ b/src/runtime/internal/sys/zgoos_noos.go
@@ -0,0 +1,26 @@
+// Code generated by gengoos.go using 'go generate'. DO NOT EDIT.
+
+// +build noos
+
+package sys
+
+const GOOS = `noos`
+
+const GoosAix = 0
+const GoosAndroid = 0
+const GoosDarwin = 0
+const GoosDragonfly = 0
+const GoosFreebsd = 0
+const GoosHurd = 0
+const GoosIllumos = 0
+const GoosIos = 0
+const GoosJs = 0
+const GoosLinux = 0
+const GoosNacl = 0
+const GoosNetbsd = 0
+const GoosNoos = 1
+const GoosOpenbsd = 0
+const GoosPlan9 = 0
+const GoosSolaris = 0
+const GoosWindows = 0
+const GoosZos = 0
diff --git a/src/runtime/internal/sys/zgoos_openbsd.go b/src/runtime/internal/sys/zgoos_openbsd.go
index 7d4d61e4ca..109870e277 100644
--- a/src/runtime/internal/sys/zgoos_openbsd.go
+++ b/src/runtime/internal/sys/zgoos_openbsd.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 1
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_plan9.go b/src/runtime/internal/sys/zgoos_plan9.go
index 30aec46df3..807bd605d9 100644
--- a/src/runtime/internal/sys/zgoos_plan9.go
+++ b/src/runtime/internal/sys/zgoos_plan9.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 1
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_solaris.go b/src/runtime/internal/sys/zgoos_solaris.go
index 4bb8c99bce..b2b6524336 100644
--- a/src/runtime/internal/sys/zgoos_solaris.go
+++ b/src/runtime/internal/sys/zgoos_solaris.go
@@ -19,6 +19,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 1
diff --git a/src/runtime/internal/sys/zgoos_windows.go b/src/runtime/internal/sys/zgoos_windows.go
index d1f4290204..6585bfaaf8 100644
--- a/src/runtime/internal/sys/zgoos_windows.go
+++ b/src/runtime/internal/sys/zgoos_windows.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_zos.go b/src/runtime/internal/sys/zgoos_zos.go
index d22be46fc0..4bdb65d143 100644
--- a/src/runtime/internal/sys/zgoos_zos.go
+++ b/src/runtime/internal/sys/zgoos_zos.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/lfstack_32bit.go b/src/runtime/lfstack_32bit.go
index f07ff1c06b..908bf9d415 100644
--- a/src/runtime/lfstack_32bit.go
+++ b/src/runtime/lfstack_32bit.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 arm mips mipsle
+// +build 386 arm mips mipsle thumb
 
 package runtime
 
diff --git a/src/runtime/lock_futex.go b/src/runtime/lock_futex.go
index 91467fdfae..cb03da23ea 100644
--- a/src/runtime/lock_futex.go
+++ b/src/runtime/lock_futex.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build dragonfly freebsd linux
+// +build dragonfly freebsd linux noos
 
 package runtime
 
diff --git a/src/runtime/malloc.go b/src/runtime/malloc.go
index f20ded5bf7..76d00b9fdf 100644
--- a/src/runtime/malloc.go
+++ b/src/runtime/malloc.go
@@ -136,7 +136,7 @@ const (
 	_FixAllocChunk = 16 << 10 // Chunk size for FixAlloc
 
 	// Per-P, per order stack segment cache size.
-	_StackCacheSize = 32 * 1024
+	_StackCacheSize = 32*1024*_OS + noosStackCacheSize
 
 	// Number of orders that get caching. Order 0 is FixedStack
 	// and each successive order is twice as large.
@@ -150,7 +150,7 @@ const (
 	//   windows/32       | 4KB        | 3
 	//   windows/64       | 8KB        | 2
 	//   plan9            | 4KB        | 3
-	_NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9
+	_NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9 - noosNumStackOrders
 
 	// heapAddrBits is the number of bits in a heap address. On
 	// amd64, addresses are sign-extended beyond heapAddrBits. On
@@ -207,7 +207,7 @@ const (
 	// arenaBaseOffset to offset into the top 4 GiB.
 	//
 	// WebAssembly currently has a limit of 4GB linear memory.
-	heapAddrBits = (_64bit*(1-sys.GoarchWasm)*(1-sys.GoosIos*sys.GoarchArm64))*48 + (1-_64bit+sys.GoarchWasm)*(32-(sys.GoarchMips+sys.GoarchMipsle)) + 33*sys.GoosIos*sys.GoarchArm64
+	heapAddrBits = (_64bit*(1-sys.GoarchWasm)*(1-sys.GoosIos*sys.GoarchArm64)*_OS)*48 + (1-_64bit+sys.GoarchWasm)*_OS*(32-(sys.GoarchMips+sys.GoarchMipsle)) + 33*sys.GoosIos*sys.GoarchArm64 + noosHeapAddrBits
 
 	// maxAlloc is the maximum size of an allocation. On 64-bit,
 	// it's theoretically possible to allocate 1<<heapAddrBits bytes. On
@@ -247,7 +247,7 @@ const (
 	// logHeapArenaBytes is log_2 of heapArenaBytes. For clarity,
 	// prefer using heapArenaBytes where possible (we need the
 	// constant to compute some other constants).
-	logHeapArenaBytes = (6+20)*(_64bit*(1-sys.GoosWindows)*(1-sys.GoarchWasm)) + (2+20)*(_64bit*sys.GoosWindows) + (2+20)*(1-_64bit) + (2+20)*sys.GoarchWasm
+	logHeapArenaBytes = (6+20)*(_64bit*(1-sys.GoosWindows)*(1-sys.GoarchWasm)*_OS) + (2+20)*(_64bit*sys.GoosWindows) + (2+20)*(1-_64bit)*_OS + noosLogHeapArenaBytes + (2+20)*sys.GoarchWasm
 
 	// heapArenaBitmapBytes is the size of each heap arena's bitmap.
 	heapArenaBitmapBytes = heapArenaBytes / (sys.PtrSize * 8 / 2)
@@ -302,7 +302,7 @@ const (
 	//
 	// On other platforms, the user address space is contiguous
 	// and starts at 0, so no offset is necessary.
-	arenaBaseOffset = 0xffff800000000000*sys.GoarchAmd64 + 0x0a00000000000000*sys.GoosAix
+	arenaBaseOffset = 0xffff800000000000*sys.GoarchAmd64 + 0x0a00000000000000*sys.GoosAix + noosArenaBaseOffset
 	// A typed version of this constant that will make it into DWARF (for viewcore).
 	arenaBaseOffsetUintptr = uintptr(arenaBaseOffset)
 
@@ -484,7 +484,9 @@ func mallocinit() {
 	lockInit(&globalAlloc.mutex, lockRankGlobalAlloc)
 
 	// Create initial arena growth hints.
-	if sys.PtrSize == 8 {
+	if noos {
+		mheap_.arena.init(sysReserveMaxArena())
+	} else if sys.PtrSize == 8 {
 		// On a 64-bit machine, we pick the following hints
 		// because:
 		//
@@ -637,6 +639,9 @@ func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr) {
 		size = n
 		goto mapped
 	}
+	if noos {
+		return nil, 0
+	}
 
 	// Try to grow the heap at a hint address.
 	for h.arenaHints != nil {
@@ -1320,6 +1325,10 @@ func persistentalloc(size, align uintptr, sysStat *sysMemStat) unsafe.Pointer {
 // See issue 9174.
 //go:systemstack
 func persistentalloc1(size, align uintptr, sysStat *sysMemStat) *notInHeap {
+	if noos {
+		return sysPersistentAlloc(size, align, sysStat)
+	}
+
 	const (
 		maxBlock = 64 << 10 // VM reservation granularity is 64K on windows
 	)
diff --git a/src/runtime/mem_noos.go b/src/runtime/mem_noos.go
new file mode 100644
index 0000000000..57d3b434b6
--- /dev/null
+++ b/src/runtime/mem_noos.go
@@ -0,0 +1,129 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+// Simple memory allocator that emulates OS allocator
+//
+// There are sysReserveMaxArena and sysPersistentAlloc functions specific to
+// noos target.
+//
+// sysReserveMaxArena allocates maximum arena space heapArenaBytes aligned for
+// mheap. It is intended to be run only once by mallocinit.
+//
+// sysReserve allocates down from sysMem.end always returning _PageSize aligned
+// memory.
+//
+// sysPersistentAlloc is fast and memory efficient implementation of
+// persistentalloc1. It uses sysReserve for _PageSize alignned allocations.
+// Otherwise it allocates from sysMem.start.
+
+var sysMem struct {
+	free, nodma           pamem
+	arenaStart, arenaSize uintptr
+	mx                    mutex
+}
+
+type pamem struct {
+	start, end uintptr
+}
+
+//go:nosplit
+func (m *pamem) allocPages(size uintptr) unsafe.Pointer {
+	var p uintptr
+	if m.end-m.start >= size {
+		p = m.end - size
+		m.end = p
+	}
+	return unsafe.Pointer(p)
+}
+
+//go:nosplit
+func (m *pamem) alloc(size, align uintptr) unsafe.Pointer {
+	var p uintptr
+	align-- // align must be power of two
+	astart := (m.start + align) &^ align
+	if m.end-astart >= size {
+		p = astart
+		m.start = astart + size
+	}
+	return unsafe.Pointer(p)
+}
+
+// sysReserveMaxArena is for initial reservation, must not be called
+// concurrently with any other reservation. TODO: Consider replace this by
+// direct initialization of mheap_.arena.
+//go:nosplit
+func sysReserveMaxArena() (addr, size uintptr) {
+	return sysMem.arenaStart, sysMem.arenaSize
+}
+
+//go:nosplit
+func sysReserve1(size uintptr) unsafe.Pointer {
+	lock(&sysMem.mx)
+	p := sysMem.free.allocPages(size)
+	if p == nil {
+		p = sysMem.nodma.allocPages(size)
+	}
+	unlock(&sysMem.mx)
+	return p
+}
+
+//go:nosplit
+func sysReserve(v unsafe.Pointer, size uintptr) unsafe.Pointer {
+	size += (_PageSize - 1)
+	size &^= (_PageSize - 1)
+	return sysReserve1(size)
+}
+
+//go:nosplit
+func sysAlloc(size uintptr, sysStat *sysMemStat) unsafe.Pointer {
+	size += (_PageSize - 1)
+	size &^= (_PageSize - 1)
+	p := sysReserve1(size)
+	if p != nil {
+		sysStat.add(int64(size))
+	}
+	return p
+}
+
+// align must be power of two
+//go:nosplit
+func sysPersistentAlloc(size, align uintptr, sysStat *sysMemStat) (p *notInHeap) {
+	if size&(_PageSize-1) == 0 {
+		p = (*notInHeap)(sysReserve1(size))
+	} else {
+		if align == 0 {
+			align = 8
+		}
+		lock(&sysMem.mx)
+		p = (*notInHeap)(sysMem.free.alloc(size, align))
+		if p == nil {
+			p = (*notInHeap)(sysMem.nodma.alloc(size, align))
+		}
+		unlock(&sysMem.mx)
+	}
+	if p == nil {
+		throw("runtime: cannot allocate memory")
+	}
+	sysStat.add(int64(size))
+	return
+}
+
+//go:nosplit
+func sysMap(v unsafe.Pointer, n uintptr, sysStat *sysMemStat) {
+	sysStat.add(int64(n))
+}
+
+//go:nosplit
+func sysFree(v unsafe.Pointer, n uintptr, sysStat *sysMemStat) {
+	sysStat.add(-int64(n))
+}
+
+func sysUnused(v unsafe.Pointer, n uintptr)   {}
+func sysUsed(v unsafe.Pointer, n uintptr)     {}
+func sysFault(v unsafe.Pointer, n uintptr)    {}
+func sysHugePage(v unsafe.Pointer, n uintptr) {}
diff --git a/src/runtime/memclr_thumb.s b/src/runtime/memclr_thumb.s
new file mode 100644
index 0000000000..7326b8be34
--- /dev/null
+++ b/src/runtime/memclr_thumb.s
@@ -0,0 +1,88 @@
+// Inferno's libkern/memset-arm.s
+// https://bitbucket.org/inferno-os/inferno-os/src/master/libkern/memset-arm.s
+//
+//         Copyright © 1994-1999 Lucent Technologies Inc. All rights reserved.
+//         Revisions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com).  All rights reserved.
+//         Portions Copyright 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+#include "textflag.h"
+
+#define TO	R8
+#define TOE	R11
+#define N	R12
+#define TMP	R12				/* N and TMP don't overlap */
+
+// func memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)
+TEXT runtime·memclrNoHeapPointers(SB),NOSPLIT,$0-8
+	MOVW	ptr+0(FP), TO
+	MOVW	n+4(FP), N
+	MOVW	$0, R0
+
+	ADD	N, TO, TOE	/* to end pointer */
+
+	CMP	$4, N		/* need at least 4 bytes to copy */
+	BLT	_1tail
+
+_4align:				/* align on 4 */
+	AND.S	$3, TO, TMP
+	BEQ	_4aligned
+
+	MOVBU.P	R0, 1(TO)		/* implicit write back */
+	B	_4align
+
+_4aligned:
+	SUB	$31, TOE, TMP	/* do 32-byte chunks if possible */
+	CMP	TMP, TO
+	BHS	_4tail
+
+	MOVW	R0, R1			/* replicate */
+	MOVW	R0, R2
+	MOVW	R0, R3
+	MOVW	R0, R4
+	MOVW	R0, R5
+	MOVW	R0, R6
+	MOVW	R0, R7
+
+_f32loop:
+	CMP	TMP, TO
+	BHS	_4tail
+
+	MOVM.IA.W [R0-R7], (TO)
+	B	_f32loop
+
+_4tail:
+	SUB	$3, TOE, TMP	/* do remaining words if possible */
+_4loop:
+	CMP	TMP, TO
+	BHS	_1tail
+
+	MOVW.P	R0, 4(TO)		/* implicit write back */
+	B	_4loop
+
+_1tail:
+	CMP	TO, TOE
+	BEQ	_return
+
+	MOVBU.P	R0, 1(TO)		/* implicit write back */
+	B	_1tail
+
+_return:
+	RET
diff --git a/src/runtime/memmove_noos_riscv64.s b/src/runtime/memmove_noos_riscv64.s
new file mode 100755
index 0000000000..187e44e438
--- /dev/null
+++ b/src/runtime/memmove_noos_riscv64.s
@@ -0,0 +1,103 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+// See memmove Go doc for important implementation constraints.
+
+// void runtime·memmove(void*, void*, uintptr)
+TEXT runtime·memmove(SB),NOSPLIT,$-0-24
+	MOV  n+16(FP), A2
+	BEQ  A2, ZERO, done
+	MOV  to+0(FP), A0
+	MOV  from+8(FP), A1
+
+	// check alignment
+	// The code below requires all of the "to", "from", "n" are aligned
+	// to use multi-byte load/stores. TODO: relax this requirement.
+	OR   A0, A1, S0
+	OR   A2, S0
+	AND  $7, S0
+	AND  $3, S0, S1
+	AND  $1, S0, S2
+
+	// check the relative position of the "to" and "from"
+	BLTU  A1, A0, backward
+
+	ADD  A0, A2
+	BEQ  S0, ZERO, forwardLoop8
+	BEQ  S1, ZERO, forwardLoop4
+	BEQ  S2, ZERO, forwardLoop2
+
+forwardLoop1:
+	MOVBU  (A1), S0
+	ADD    $1, A1
+	MOVB   S0, (A0)
+	ADD    $1, A0
+	BNE    A0, A2, forwardLoop1
+done:
+	RET
+
+forwardLoop8:
+	MOV  (A1), S0
+	ADD  $8, A1
+	MOV  S0, (A0)
+	ADD  $8, A0
+	BNE  A0, A2, forwardLoop8
+	RET
+
+forwardLoop4:
+	MOVWU  (A1), S0
+	ADD    $4, A1
+	MOVW   S0, (A0)
+	ADD    $4, A0
+	BNE    A0, A2, forwardLoop4
+	RET
+
+forwardLoop2:
+	MOVHU  (A1), S0
+	ADD    $2, A1
+	MOVH   S0, (A0)
+	ADD    $2, A0
+	BNE    A0, A2, forwardLoop2
+	RET
+
+backward:
+	ADD  A2, A1
+	ADD  A0, A2
+	BEQ  S0, ZERO, backwardLoop8
+	BEQ  S1, ZERO, backwardLoop4
+	BEQ  S2, ZERO, backwardLoop2
+
+backwardLoop1:
+	ADD    $-1, A1
+	MOVBU  (A1), S0
+	ADD    $-1, A2
+	MOVB   S0, (A2)
+	BNE    A0, A2, backwardLoop1
+	RET
+
+backwardLoop8:
+	ADD  $-8, A1
+	MOV  (A1), S0
+	ADD  $-8, A2
+	MOV  S0, (A2)
+	BNE  A0, A2, backwardLoop8
+	RET
+
+backwardLoop4:
+	ADD    $-4, A1
+	MOVWU  (A1), S0
+	ADD    $-4, A2
+	MOVW   S0, (A2)
+	BNE    A0, A2, backwardLoop4
+	RET
+
+backwardLoop2:
+	ADD    $-2, A1
+	MOVHU  (A1), S0
+	ADD    $-2, A2
+	MOVH   S0, (A2)
+	BNE    A0, A2, backwardLoop2
+	RET
diff --git a/src/runtime/memmove_riscv64.s b/src/runtime/memmove_riscv64.s
index 5dec8d0a33..aca9ca1a45 100644
--- a/src/runtime/memmove_riscv64.s
+++ b/src/runtime/memmove_riscv64.s
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build !noos
+
 #include "textflag.h"
 
 // See memmove Go doc for important implementation constraints.
diff --git a/src/runtime/memmove_thumb.s b/src/runtime/memmove_thumb.s
new file mode 100644
index 0000000000..1790cd583d
--- /dev/null
+++ b/src/runtime/memmove_thumb.s
@@ -0,0 +1,108 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+// The ARMv7-M architecture supports unaligned MOVW, MOVH. An implementation can
+// be configured to force alignment faults but no real one does this.
+
+// Writes are often buffered so this code aligns reads and allows unaligned
+// writes. Ensure CCR.UNALIGN_TRP=0.
+
+// TODO: better performance (eg. full aligned copy using MOVM)
+
+// func memmove(to, from unsafe.Pointer, n uintptr)
+TEXT runtime·memmove(SB), NOSPLIT|NOFRAME, $0-12
+	MOVW  to+0(FP), R0
+	MOVW  from+4(FP), R1
+	MOVW  n+8(FP), R2
+
+	CMP  $4, R2
+	BHI  more
+
+// Fast path for 0, 1, 2, 3, 4 bytes.
+
+	TBB    R2, R15
+	HWORD  $0x0305
+	HWORD  $0x0906
+	HWORD  $0x0E
+
+	MOVBU  (R1), R3  // n == 1 (TBB offset 0x03)
+	MOVBU  R3, (R0)
+	RET        // n == 0 (TBB offset 0x05)
+
+	MOVHU  (R1), R3  // n == 2 (TBB offset 0x06)
+	MOVHU  R3, (R0)
+	RET    
+
+	MOVHU  (R1), R3  // n == 3 (TBB offset 0x09)
+	MOVBU  2(R1),R2
+	MOVHU  R3, (R0)
+	MOVBU  R2, 2(R0)
+	RET    
+
+	MOVW  (R1), R3  // n == 4 (TBB offset 0x0E)
+	MOVW  R3, (R0)
+end:
+	RET  
+
+more: // now we are sure that n > 4
+	CMP  R0, R1
+	BLO  backward
+
+// Forward copy
+
+fhead: // head copy (up to 3 bytes) to make src (R1) word aligned
+	TST      $3, R1
+	BEQ      fwords
+	MOVBU.P  1(R1), R3
+	MOVBU.P  R3, 1(R0)
+	SUB      $1, R2
+	B        fhead
+
+fwords: // copy words
+	SUB.S   $4, R2
+	BLT     fwend
+	MOVW.P  4(R1), R3
+	MOVW.P  R3, 4(R0)
+	B       fwords
+fwend:
+	ADD.S  $4, R2
+
+ftail: // tail copy (up to 3 bytes)
+	BEQ      end
+	MOVBU.P  1(R1), R3
+	MOVBU.P  R3, 1(R0)
+	SUB.S    $1, R2
+	B        ftail
+
+// Backward copy
+
+backward:
+	ADD  R2, R0
+	ADD  R2, R1
+
+btail: // tail copy (up to 3 bytes) to make src (R1) word aligned
+	TST      $3, R1
+	BEQ      bwords
+	MOVBU.W  -1(R1), R3
+	MOVBU.W  R3, -1(R0)
+	SUB      $1, R2
+	B        btail
+
+bwords: // copy words
+	SUB.S   $4, R2
+	BLT     bwend
+	MOVW.W  -4(R1), R3
+	MOVW.W  R3, -4(R0)
+	B       bwords
+bwend:
+	ADD.S  $4, R2
+
+bhead: // head copy (up to 3 bytes)
+	BEQ      end
+	MOVBU.W  -1(R1), R3
+	MOVBU.W  R3, -1(R0)
+	SUB.S    $1, R2
+	B        bhead
diff --git a/src/runtime/mfixalloc.go b/src/runtime/mfixalloc.go
index 293c16b38b..41a1f99534 100644
--- a/src/runtime/mfixalloc.go
+++ b/src/runtime/mfixalloc.go
@@ -8,7 +8,9 @@
 
 package runtime
 
-import "unsafe"
+import (
+	"unsafe"
+)
 
 // FixAlloc is a simple free-list allocator for fixed size objects.
 // Malloc uses a FixAlloc wrapped around sysAlloc to manage its
@@ -77,8 +79,19 @@ func (f *fixalloc) alloc() unsafe.Pointer {
 		return v
 	}
 	if uintptr(f.nchunk) < f.size {
-		f.chunk = uintptr(persistentalloc(_FixAllocChunk, 0, f.stat))
-		f.nchunk = _FixAllocChunk
+		if noos {
+			nchunk := f.size
+			if nchunk == unsafe.Sizeof(mspan{}) {
+				nchunk *= 16 * (1 + _64bit) // more for mspan{} allocator
+			} else {
+				nchunk *= 2 * (1 + _64bit)
+			}
+			f.nchunk = uint32(nchunk)
+			f.chunk = uintptr(persistentalloc(nchunk, 0, f.stat))
+		} else {
+			f.chunk = uintptr(persistentalloc(_FixAllocChunk, 0, f.stat))
+			f.nchunk = _FixAllocChunk
+		}
 	}
 
 	v := unsafe.Pointer(f.chunk)
diff --git a/src/runtime/mgc.go b/src/runtime/mgc.go
index 185d3201ca..e4904c4957 100644
--- a/src/runtime/mgc.go
+++ b/src/runtime/mgc.go
@@ -137,7 +137,7 @@ import (
 const (
 	_DebugGC         = 0
 	_ConcurrentSweep = true
-	_FinBlockSize    = 4 * 1024
+	_FinBlockSize    = 4*1024*_OS + noosFinBlockSize
 
 	// debugScanConservative enables debug logging for stack
 	// frames that are scanned conservatively.
@@ -146,7 +146,7 @@ const (
 	// sweepMinHeapDistance is a lower bound on the heap distance
 	// (in bytes) reserved for concurrent sweeping between GC
 	// cycles.
-	sweepMinHeapDistance = 1024 * 1024
+	sweepMinHeapDistance = 1024*1024*_OS + noosSweepMinHeapDistance
 )
 
 // heapminimum is the minimum heap size at which to trigger GC.
@@ -164,7 +164,7 @@ const (
 var heapminimum uint64 = defaultHeapMinimum
 
 // defaultHeapMinimum is the value of heapminimum for GOGC==100.
-const defaultHeapMinimum = 4 << 20
+const defaultHeapMinimum = 4<<20*_OS + noosDefaultHeapMinimum
 
 // Initialized from $GOGC.  GOGC=off means no GC.
 var gcpercent int32
@@ -204,7 +204,11 @@ func readgogc() int32 {
 	if n, ok := atoi32(p); ok {
 		return n
 	}
-	return 100
+	if noos {
+		return 50
+	} else {
+		return 100
+	}
 }
 
 // gcenable is called after the bulk of the runtime initialization,
@@ -215,8 +219,10 @@ func gcenable() {
 	// Kick off sweeping and scavenging.
 	c := make(chan int, 2)
 	go bgsweep(c)
-	go bgscavenge(c)
-	<-c
+	if !noos {
+		go bgscavenge(c)
+		<-c
+	}
 	<-c
 	memstats.enablegc = true // now that runtime is initialized, GC is okay
 }
@@ -445,8 +451,8 @@ func (c *gcControllerState) startCycle() {
 	// GOGC. Assist is proportional to this distance, so enforce a
 	// minimum distance, even if it means going over the GOGC goal
 	// by a tiny bit.
-	if memstats.next_gc < memstats.heap_live+1024*1024 {
-		memstats.next_gc = memstats.heap_live + 1024*1024
+	if memstats.next_gc < memstats.heap_live+1024*1024*_OS+noosHeapGoalInc {
+		memstats.next_gc = memstats.heap_live + 1024*1024*_OS + noosHeapGoalInc
 	}
 
 	// Compute the background mark utilization goal. In general,
@@ -1796,10 +1802,17 @@ func gcMarkTermination(nextTriggerRatio float64) {
 			}
 			print(string(fmtNSAsMS(sbuf[:], uint64(ns))))
 		}
-		print(" ms cpu, ",
-			work.heap0>>20, "->", work.heap1>>20, "->", work.heap2>>20, " MB, ",
-			work.heapGoal>>20, " MB goal, ",
-			work.maxprocs, " P")
+		if !noos {
+			print(" ms cpu, ",
+				work.heap0>>20, "->", work.heap1>>20, "->", work.heap2>>20, " MB, ",
+				work.heapGoal>>20, " MB goal, ",
+				work.maxprocs, " P")
+		} else {
+			print(" ms cpu, ",
+				work.heap0>>10, "->", work.heap1>>10, "->", work.heap2>>10, " KB, ",
+				work.heapGoal>>10, " KB goal, ",
+				work.maxprocs, " P")
+		}
 		if work.userForced {
 			print(" (forced)")
 		}
diff --git a/src/runtime/mgcmark.go b/src/runtime/mgcmark.go
index 46fae5de72..21d990bed2 100644
--- a/src/runtime/mgcmark.go
+++ b/src/runtime/mgcmark.go
@@ -46,7 +46,7 @@ const (
 	//
 	// Must be a multiple of the pageInUse bitmap element size and
 	// must also evenly divide pagesPerArena.
-	pagesPerSpanRoot = 512
+	pagesPerSpanRoot = 512*_OS + pagesPerArena/2*(1-_OS)
 )
 
 // gcMarkRootPrepare queues root scanning jobs (stacks, globals, and
diff --git a/src/runtime/mgcwork.go b/src/runtime/mgcwork.go
index b3a068661e..f2df6d7460 100644
--- a/src/runtime/mgcwork.go
+++ b/src/runtime/mgcwork.go
@@ -11,7 +11,7 @@ import (
 )
 
 const (
-	_WorkbufSize = 2048 // in bytes; larger values result in less contention
+	_WorkbufSize = 2048*_OS + _PageSize*(1-_OS) // in bytes; larger values result in less contention
 
 	// workbufAlloc is the number of bytes to allocate at a time
 	// for new workbufs. This must be a multiple of pageSize and
@@ -19,7 +19,7 @@ const (
 	//
 	// Larger values reduce workbuf allocation overhead. Smaller
 	// values reduce heap fragmentation.
-	workbufAlloc = 32 << 10
+	workbufAlloc = 32<<10*_OS + 2*_WorkbufSize*(1-_OS)
 )
 
 func init() {
diff --git a/src/runtime/mheap.go b/src/runtime/mheap.go
index 1855330da5..8537891881 100644
--- a/src/runtime/mheap.go
+++ b/src/runtime/mheap.go
@@ -19,7 +19,7 @@ const (
 	// minPhysPageSize is a lower-bound on the physical page size. The
 	// true physical page size may be larger than this. In contrast,
 	// sys.PhysPageSize is an upper-bound on the physical page size.
-	minPhysPageSize = 4096
+	minPhysPageSize = 4096*_OS + noosMinPhysPageSize
 
 	// maxPhysPageSize is the maximum page size the runtime supports.
 	maxPhysPageSize = 512 << 10
@@ -42,8 +42,8 @@ const (
 	// roughly 100µs.
 	//
 	// Must be a multiple of the pageInUse bitmap element size and
-	// must also evenly divide pagesPerArena.
-	pagesPerReclaimerChunk = 512
+	// must also evenly divid pagesPerArena.
+	pagesPerReclaimerChunk = 512*_OS + pagesPerArena*(1-_OS)
 
 	// physPageAlignedStacks indicates whether stack allocations must be
 	// physical page aligned. This is a requirement for MAP_STACK on
@@ -498,7 +498,7 @@ func recordspan(vh unsafe.Pointer, p unsafe.Pointer) {
 	assertLockHeld(&h.lock)
 
 	if len(h.allspans) >= cap(h.allspans) {
-		n := 64 * 1024 / sys.PtrSize
+		n := 8 * _PageSize / sys.PtrSize
 		if n < cap(h.allspans)*3/2 {
 			n = cap(h.allspans) * 3 / 2
 		}
@@ -1394,6 +1394,9 @@ func (h *mheap) grow(npage uintptr) bool {
 	// By scavenging inline we deal with the failure to allocate out of
 	// memory fragments by scavenging the memory fragments that are least
 	// likely to be re-used.
+	if noos {
+		return true
+	}
 	if retained := heapRetained(); retained+uint64(totalGrowth) > h.scavengeGoal {
 		todo := totalGrowth
 		if overage := uintptr(retained + uint64(totalGrowth) - h.scavengeGoal); todo > overage {
@@ -1518,6 +1521,9 @@ func (h *mheap) scavengeAll() {
 //go:linkname runtime_debug_freeOSMemory runtime/debug.freeOSMemory
 func runtime_debug_freeOSMemory() {
 	GC()
+	if noos {
+		return
+	}
 	systemstack(func() { mheap_.scavengeAll() })
 }
 
@@ -1883,7 +1889,7 @@ func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8) {
 	return b.bytep(n / 8), 1 << (n % 8)
 }
 
-const gcBitsChunkBytes = uintptr(64 << 10)
+const gcBitsChunkBytes = uintptr(64<<10)*_OS + noosGCBitsChunkBytes
 const gcBitsHeaderBytes = unsafe.Sizeof(gcBitsHeader{})
 
 type gcBitsHeader struct {
diff --git a/src/runtime/mkduff.go b/src/runtime/mkduff.go
index 94ae75fbfe..c5da0792ed 100644
--- a/src/runtime/mkduff.go
+++ b/src/runtime/mkduff.go
@@ -35,6 +35,7 @@ func main() {
 	gen("amd64", notags, zeroAMD64, copyAMD64)
 	gen("386", notags, zero386, copy386)
 	gen("arm", notags, zeroARM, copyARM)
+	gen("thumb", notags, zeroARM, copyARM)
 	gen("arm64", notags, zeroARM64, copyARM64)
 	gen("ppc64x", tagsPPC64x, zeroPPC64x, copyPPC64x)
 	gen("mips64x", tagsMIPS64x, zeroMIPS64x, copyMIPS64x)
diff --git a/src/runtime/mkpreempt.go b/src/runtime/mkpreempt.go
index 1d614dd003..713d3c3d28 100644
--- a/src/runtime/mkpreempt.go
+++ b/src/runtime/mkpreempt.go
@@ -85,6 +85,7 @@ var arches = map[string]func(){
 	"ppc64x":  genPPC64,
 	"riscv64": genRISCV64,
 	"s390x":   genS390X,
+	"thumb":   genThumb,
 	"wasm":    genWasm,
 }
 var beLe = map[string]bool{"mips64x": true, "mipsx": true, "ppc64x": true}
@@ -558,6 +559,50 @@ func genS390X() {
 	p("JMP (R10)")
 }
 
+func genThumb() {
+	// Add integer registers R0-R12.
+	// R13 (SP), R14 (LR), R15 (PC) are special and not saved here.
+	var l = layout{sp: "R13", stack: 4} // add LR slot
+	for i := 0; i <= 12; i++ {
+		reg := fmt.Sprintf("R%d", i)
+		if i == 10 {
+			continue // R10 is g register, no need to save/restore
+		}
+		l.add("MOVW", reg, 4)
+	}
+	// Add flag register.
+	l.addSpecial(
+		"MOVW APSR, R0\nMOVW R0, %d(R13)",
+		"MOVW %d(R13), R0\nMOVW R0, APSR",
+		4)
+
+	// Add floating point registers F0-F15 and flag register.
+	var lfp = layout{stack: l.stack, sp: "R13"}
+	lfp.addSpecial(
+		"MOVW FPSCR, R0\nMOVW R0, %d(R13)",
+		"MOVW %d(R13), R0\nMOVW R0, FPSCR",
+		4)
+	for i := 0; i <= 15; i++ {
+		reg := fmt.Sprintf("F%d", i)
+		lfp.add("MOVD", reg, 8)
+	}
+
+	p("MOVW.W R14, -%d(R13)", lfp.stack) // allocate frame, save LR
+	l.save()
+	p("MOVB ·goarm(SB), R0\nCMP $0x10, R0\nBLT nofp") // test goarm, and skip FP registers if goarm != 0xXF, 0xXD.
+	lfp.save()
+	label("nofp:")
+	p("CALL ·asyncPreempt2(SB)")
+	p("MOVB ·goarm(SB), R0\nCMP $0x10, R0\nBLT nofp2") // test goarm, and skip FP registers if goarm != 0xXF, 0xXD.
+	lfp.restore()
+	label("nofp2:")
+	l.restore()
+
+	p("MOVW %d(R13), R14", lfp.stack)     // sigctxt.pushCall pushes LR on stack, restore it
+	p("MOVW.P %d(R13), R15", lfp.stack+4) // load PC, pop frame (including the space pushed by sigctxt.pushCall)
+	p("UNDEF")                            // shouldn't get here
+}
+
 func genWasm() {
 	p("// No async preemption on wasm")
 	p("UNDEF")
diff --git a/src/runtime/mksizeclasses.go b/src/runtime/mksizeclasses.go
index b92d1fed5f..948e19cd45 100644
--- a/src/runtime/mksizeclasses.go
+++ b/src/runtime/mksizeclasses.go
@@ -50,6 +50,8 @@ func main() {
 	fmt.Fprintln(&b, "// Code generated by mksizeclasses.go; DO NOT EDIT.")
 	fmt.Fprintln(&b, "//go:generate go run mksizeclasses.go")
 	fmt.Fprintln(&b)
+	fmt.Fprintln(&b, "// +build !noos")
+	fmt.Fprintln(&b)
 	fmt.Fprintln(&b, "package runtime")
 	classes := makeClasses()
 
diff --git a/src/runtime/mksizeclasses_mcu.go b/src/runtime/mksizeclasses_mcu.go
new file mode 100644
index 0000000000..4a352dd75d
--- /dev/null
+++ b/src/runtime/mksizeclasses_mcu.go
@@ -0,0 +1,330 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build ignore
+
+// Generate tables for small malloc size classes.
+//
+// See malloc.go for overview.
+//
+// The size classes are chosen so that rounding an allocation
+// request up to the next size class wastes at most 12.5% (1.125x).
+//
+// Each size class has its own page count that gets allocated
+// and chopped up when new objects of the size class are needed.
+// That page count is chosen so that chopping up the run of
+// pages into objects of the given size wastes at most 12.5% (1.125x)
+// of the memory. It is not necessary that the cutoff here be
+// the same as above.
+//
+// The two sources of waste multiply, so the worst possible case
+// for the above constraints would be that allocations of some
+// size might have a 26.6% (1.266x) overhead.
+// In practice, only one of the wastes comes into play for a
+// given size (sizes < 512 waste mainly on the round-up,
+// sizes > 512 waste mainly on the page chopping).
+// For really small sizes, alignment constraints force the
+// overhead higher.
+
+package main
+
+import (
+	"bytes"
+	"flag"
+	"fmt"
+	"go/format"
+	"io"
+	"io/ioutil"
+	"log"
+	"os"
+)
+
+// Generate msize.go
+
+var stdout = flag.Bool("stdout", false, "write to stdout instead of sizeclasses.go")
+
+func main() {
+	flag.Parse()
+
+	var b bytes.Buffer
+	fmt.Fprintln(&b, "// Code generated by mksizeclasses_mcu.go; DO NOT EDIT.")
+	fmt.Fprintln(&b, "//go:generate go run mksizeclasses_mcu.go")
+	fmt.Fprintln(&b)
+	fmt.Fprintln(&b, "// +build noos,thumb")
+	fmt.Fprintln(&b)
+	fmt.Fprintln(&b, "package runtime")
+	classes := makeClasses()
+
+	printComment(&b, classes)
+
+	printClasses(&b, classes)
+
+	out, err := format.Source(b.Bytes())
+	if err != nil {
+		log.Fatal(err)
+	}
+	if *stdout {
+		_, err = os.Stdout.Write(out)
+	} else {
+		err = ioutil.WriteFile("sizeclasses_mcu.go", out, 0666)
+	}
+	if err != nil {
+		log.Fatal(err)
+	}
+}
+
+const (
+	// Constants that we use and will transfer to the runtime.
+	maxSmallSize = 1 << 9
+	smallSizeDiv = 8
+	smallSizeMax = 256
+	largeSizeDiv = 128
+	pageShift    = 8
+
+	// Derived constants.
+	pageSize = 1 << pageShift
+)
+
+type class struct {
+	size   int // max size
+	npages int // number of pages
+
+	mul    int
+	shift  uint
+	shift2 uint
+	mask   int
+}
+
+func powerOfTwo(x int) bool {
+	return x != 0 && x&(x-1) == 0
+}
+
+func makeClasses() []class {
+	var classes []class
+
+	classes = append(classes, class{}) // class #0 is a dummy entry
+
+	align := 8
+	for size := align; size <= maxSmallSize; size += align {
+		if powerOfTwo(size) { // bump alignment once in a while
+			if size >= 2048 {
+				align = 256
+			} else if size >= 128 {
+				align = size / 8
+			} else if size >= 16 {
+				align = 16 // required for x86 SSE instructions, if we want to use them
+			}
+		}
+		if !powerOfTwo(align) {
+			panic("incorrect alignment")
+		}
+
+		// Make the allocnpages big enough that
+		// the leftover is less than 1/8 of the total,
+		// so wasted space is at most 12.5%.
+		allocsize := pageSize
+		for allocsize%size > allocsize/8 {
+			allocsize += pageSize
+		}
+		npages := allocsize / pageSize
+
+		// If the previous sizeclass chose the same
+		// allocation size and fit the same number of
+		// objects into the page, we might as well
+		// use just this size instead of having two
+		// different sizes.
+		if len(classes) > 1 && npages == classes[len(classes)-1].npages && allocsize/size == allocsize/classes[len(classes)-1].size {
+			classes[len(classes)-1].size = size
+			continue
+		}
+		classes = append(classes, class{size: size, npages: npages})
+	}
+
+	// Increase object sizes if we can fit the same number of larger objects
+	// into the same number of pages. For example, we choose size 8448 above
+	// with 6 objects in 7 pages. But we can well use object size 9472,
+	// which is also 6 objects in 7 pages but +1024 bytes (+12.12%).
+	// We need to preserve at least largeSizeDiv alignment otherwise
+	// sizeToClass won't work.
+	for i := range classes {
+		if i == 0 {
+			continue
+		}
+		c := &classes[i]
+		psize := c.npages * pageSize
+		new_size := (psize / (psize / c.size)) &^ (largeSizeDiv - 1)
+		if new_size > c.size {
+			c.size = new_size
+		}
+	}
+
+	if len(classes) != 19 {
+		panic("number of size classes has changed")
+	}
+
+	for i := range classes {
+		computeDivMagic(&classes[i])
+	}
+
+	return classes
+}
+
+// computeDivMagic computes some magic constants to implement
+// the division required to compute object number from span offset.
+// n / c.size is implemented as n >> c.shift * c.mul >> c.shift2
+// for all 0 <= n < c.npages * pageSize
+func computeDivMagic(c *class) {
+	// divisor
+	d := c.size
+	if d == 0 {
+		return
+	}
+
+	// maximum input value for which the formula needs to work.
+	max := c.npages*pageSize - 1
+
+	if powerOfTwo(d) {
+		// If the size is a power of two, heapBitsForObject can divide even faster by masking.
+		// Compute this mask.
+		if max >= 1<<16 {
+			panic("max too big for power of two size")
+		}
+		c.mask = 1<<16 - d
+	}
+
+	// Compute pre-shift by factoring power of 2 out of d.
+	for d%2 == 0 {
+		c.shift++
+		d >>= 1
+		max >>= 1
+	}
+
+	// Find the smallest k that works.
+	// A small k allows us to fit the math required into 32 bits
+	// so we can use 32-bit multiplies and shifts on 32-bit platforms.
+nextk:
+	for k := uint(0); ; k++ {
+		mul := (int(1)<<k + d - 1) / d //  ⌈2^k / d⌉
+
+		// Test to see if mul works.
+		for n := 0; n <= max; n++ {
+			if n*mul>>k != n/d {
+				continue nextk
+			}
+		}
+		if mul >= 1<<16 {
+			panic("mul too big")
+		}
+		if uint64(mul)*uint64(max) >= 1<<32 {
+			panic("mul*max too big")
+		}
+		c.mul = mul
+		c.shift2 = k
+		break
+	}
+
+	// double-check.
+	for n := 0; n <= max; n++ {
+		if n*c.mul>>c.shift2 != n/d {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad multiply magic")
+		}
+		// Also check the exact computations that will be done by the runtime,
+		// for both 32 and 64 bit operations.
+		if uint32(n)*uint32(c.mul)>>uint8(c.shift2) != uint32(n/d) {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad 32-bit multiply magic")
+		}
+		if uint64(n)*uint64(c.mul)>>uint8(c.shift2) != uint64(n/d) {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad 64-bit multiply magic")
+		}
+	}
+}
+
+func printComment(w io.Writer, classes []class) {
+	fmt.Fprintf(w, "// %-5s  %-9s  %-10s  %-7s  %-10s  %-9s\n", "class", "bytes/obj", "bytes/span", "objects", "tail waste", "max waste")
+	prevSize := 0
+	for i, c := range classes {
+		if i == 0 {
+			continue
+		}
+		spanSize := c.npages * pageSize
+		objects := spanSize / c.size
+		tailWaste := spanSize - c.size*(spanSize/c.size)
+		maxWaste := float64((c.size-prevSize-1)*objects+tailWaste) / float64(spanSize)
+		prevSize = c.size
+		fmt.Fprintf(w, "// %5d  %9d  %10d  %7d  %10d  %8.2f%%\n", i, c.size, spanSize, objects, tailWaste, 100*maxWaste)
+	}
+	fmt.Fprintf(w, "\n")
+}
+
+func printClasses(w io.Writer, classes []class) {
+	fmt.Fprintln(w, "const (")
+	fmt.Fprintf(w, "_MaxSmallSize = %d\n", maxSmallSize)
+	fmt.Fprintf(w, "smallSizeDiv = %d\n", smallSizeDiv)
+	fmt.Fprintf(w, "smallSizeMax = %d\n", smallSizeMax)
+	fmt.Fprintf(w, "largeSizeDiv = %d\n", largeSizeDiv)
+	fmt.Fprintf(w, "_NumSizeClasses = %d\n", len(classes))
+	fmt.Fprintf(w, "_PageShift = %d\n", pageShift)
+	fmt.Fprintln(w, ")")
+
+	fmt.Fprint(w, "var class_to_size = [_NumSizeClasses]uint16 {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "%d,", c.size)
+	}
+	fmt.Fprintln(w, "}")
+
+	fmt.Fprint(w, "var class_to_allocnpages = [_NumSizeClasses]uint8 {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "%d,", c.npages)
+	}
+	fmt.Fprintln(w, "}")
+
+	fmt.Fprintln(w, "type divMagic struct {")
+	fmt.Fprintln(w, "  shift uint8")
+	fmt.Fprintln(w, "  shift2 uint8")
+	fmt.Fprintln(w, "  mul uint16")
+	fmt.Fprintln(w, "  baseMask uint16")
+	fmt.Fprintln(w, "}")
+	fmt.Fprint(w, "var class_to_divmagic = [_NumSizeClasses]divMagic {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "{%d,%d,%d,%d},", c.shift, c.shift2, c.mul, c.mask)
+	}
+	fmt.Fprintln(w, "}")
+
+	// map from size to size class, for small sizes.
+	sc := make([]int, smallSizeMax/smallSizeDiv+1)
+	for i := range sc {
+		size := i * smallSizeDiv
+		for j, c := range classes {
+			if c.size >= size {
+				sc[i] = j
+				break
+			}
+		}
+	}
+	fmt.Fprint(w, "var size_to_class8 = [smallSizeMax/smallSizeDiv+1]uint8 {")
+	for _, v := range sc {
+		fmt.Fprintf(w, "%d,", v)
+	}
+	fmt.Fprintln(w, "}")
+
+	// map from size to size class, for large sizes.
+	sc = make([]int, (maxSmallSize-smallSizeMax)/largeSizeDiv+1)
+	for i := range sc {
+		size := smallSizeMax + i*largeSizeDiv
+		for j, c := range classes {
+			if c.size >= size {
+				sc[i] = j
+				break
+			}
+		}
+	}
+	fmt.Fprint(w, "var size_to_class128 = [(_MaxSmallSize-smallSizeMax)/largeSizeDiv+1]uint8 {")
+	for _, v := range sc {
+		fmt.Fprintf(w, "%d,", v)
+	}
+	fmt.Fprintln(w, "}")
+}
diff --git a/src/runtime/mksizeclasses_mcu64.go b/src/runtime/mksizeclasses_mcu64.go
new file mode 100644
index 0000000000..b48fd71de7
--- /dev/null
+++ b/src/runtime/mksizeclasses_mcu64.go
@@ -0,0 +1,330 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build ignore
+
+// Generate tables for small malloc size classes.
+//
+// See malloc.go for overview.
+//
+// The size classes are chosen so that rounding an allocation
+// request up to the next size class wastes at most 12.5% (1.125x).
+//
+// Each size class has its own page count that gets allocated
+// and chopped up when new objects of the size class are needed.
+// That page count is chosen so that chopping up the run of
+// pages into objects of the given size wastes at most 12.5% (1.125x)
+// of the memory. It is not necessary that the cutoff here be
+// the same as above.
+//
+// The two sources of waste multiply, so the worst possible case
+// for the above constraints would be that allocations of some
+// size might have a 26.6% (1.266x) overhead.
+// In practice, only one of the wastes comes into play for a
+// given size (sizes < 512 waste mainly on the round-up,
+// sizes > 512 waste mainly on the page chopping).
+// For really small sizes, alignment constraints force the
+// overhead higher.
+
+package main
+
+import (
+	"bytes"
+	"flag"
+	"fmt"
+	"go/format"
+	"io"
+	"io/ioutil"
+	"log"
+	"os"
+)
+
+// Generate msize.go
+
+var stdout = flag.Bool("stdout", false, "write to stdout instead of sizeclasses.go")
+
+func main() {
+	flag.Parse()
+
+	var b bytes.Buffer
+	fmt.Fprintln(&b, "// Code generated by mksizeclasses_mcu64.go; DO NOT EDIT.")
+	fmt.Fprintln(&b, "//go:generate go run mksizeclasses_mcu64.go")
+	fmt.Fprintln(&b)
+	fmt.Fprintln(&b, "// +build noos,riscv64")
+	fmt.Fprintln(&b)
+	fmt.Fprintln(&b, "package runtime")
+	classes := makeClasses()
+
+	printComment(&b, classes)
+
+	printClasses(&b, classes)
+
+	out, err := format.Source(b.Bytes())
+	if err != nil {
+		log.Fatal(err)
+	}
+	if *stdout {
+		_, err = os.Stdout.Write(out)
+	} else {
+		err = ioutil.WriteFile("sizeclasses_mcu64.go", out, 0666)
+	}
+	if err != nil {
+		log.Fatal(err)
+	}
+}
+
+const (
+	// Constants that we use and will transfer to the runtime.
+	maxSmallSize = 1 << 12
+	smallSizeDiv = 8
+	smallSizeMax = 256
+	largeSizeDiv = 128
+	pageShift    = 11
+
+	// Derived constants.
+	pageSize = 1 << pageShift
+)
+
+type class struct {
+	size   int // max size
+	npages int // number of pages
+
+	mul    int
+	shift  uint
+	shift2 uint
+	mask   int
+}
+
+func powerOfTwo(x int) bool {
+	return x != 0 && x&(x-1) == 0
+}
+
+func makeClasses() []class {
+	var classes []class
+
+	classes = append(classes, class{}) // class #0 is a dummy entry
+
+	align := 8
+	for size := align; size <= maxSmallSize; size += align {
+		if powerOfTwo(size) { // bump alignment once in a while
+			if size >= 2048 {
+				align = 256
+			} else if size >= 128 {
+				align = size / 8
+			} else if size >= 16 {
+				align = 16 // required for x86 SSE instructions, if we want to use them
+			}
+		}
+		if !powerOfTwo(align) {
+			panic("incorrect alignment")
+		}
+
+		// Make the allocnpages big enough that
+		// the leftover is less than 1/8 of the total,
+		// so wasted space is at most 12.5%.
+		allocsize := pageSize
+		for allocsize%size > allocsize/8 {
+			allocsize += pageSize
+		}
+		npages := allocsize / pageSize
+
+		// If the previous sizeclass chose the same
+		// allocation size and fit the same number of
+		// objects into the page, we might as well
+		// use just this size instead of having two
+		// different sizes.
+		if len(classes) > 1 && npages == classes[len(classes)-1].npages && allocsize/size == allocsize/classes[len(classes)-1].size {
+			classes[len(classes)-1].size = size
+			continue
+		}
+		classes = append(classes, class{size: size, npages: npages})
+	}
+
+	// Increase object sizes if we can fit the same number of larger objects
+	// into the same number of pages. For example, we choose size 8448 above
+	// with 6 objects in 7 pages. But we can well use object size 9472,
+	// which is also 6 objects in 7 pages but +1024 bytes (+12.12%).
+	// We need to preserve at least largeSizeDiv alignment otherwise
+	// sizeToClass won't work.
+	for i := range classes {
+		if i == 0 {
+			continue
+		}
+		c := &classes[i]
+		psize := c.npages * pageSize
+		new_size := (psize / (psize / c.size)) &^ (largeSizeDiv - 1)
+		if new_size > c.size {
+			c.size = new_size
+		}
+	}
+
+	if false && len(classes) != 67 {
+		panic("number of size classes has changed")
+	}
+
+	for i := range classes {
+		computeDivMagic(&classes[i])
+	}
+
+	return classes
+}
+
+// computeDivMagic computes some magic constants to implement
+// the division required to compute object number from span offset.
+// n / c.size is implemented as n >> c.shift * c.mul >> c.shift2
+// for all 0 <= n <= c.npages * pageSize
+func computeDivMagic(c *class) {
+	// divisor
+	d := c.size
+	if d == 0 {
+		return
+	}
+
+	// maximum input value for which the formula needs to work.
+	max := c.npages * pageSize
+
+	if powerOfTwo(d) {
+		// If the size is a power of two, heapBitsForObject can divide even faster by masking.
+		// Compute this mask.
+		if max >= 1<<16 {
+			panic("max too big for power of two size")
+		}
+		c.mask = 1<<16 - d
+	}
+
+	// Compute pre-shift by factoring power of 2 out of d.
+	for d%2 == 0 {
+		c.shift++
+		d >>= 1
+		max >>= 1
+	}
+
+	// Find the smallest k that works.
+	// A small k allows us to fit the math required into 32 bits
+	// so we can use 32-bit multiplies and shifts on 32-bit platforms.
+nextk:
+	for k := uint(0); ; k++ {
+		mul := (int(1)<<k + d - 1) / d //  ⌈2^k / d⌉
+
+		// Test to see if mul works.
+		for n := 0; n <= max; n++ {
+			if n*mul>>k != n/d {
+				continue nextk
+			}
+		}
+		if mul >= 1<<16 {
+			panic("mul too big")
+		}
+		if uint64(mul)*uint64(max) >= 1<<32 {
+			panic("mul*max too big")
+		}
+		c.mul = mul
+		c.shift2 = k
+		break
+	}
+
+	// double-check.
+	for n := 0; n <= max; n++ {
+		if n*c.mul>>c.shift2 != n/d {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad multiply magic")
+		}
+		// Also check the exact computations that will be done by the runtime,
+		// for both 32 and 64 bit operations.
+		if uint32(n)*uint32(c.mul)>>uint8(c.shift2) != uint32(n/d) {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad 32-bit multiply magic")
+		}
+		if uint64(n)*uint64(c.mul)>>uint8(c.shift2) != uint64(n/d) {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad 64-bit multiply magic")
+		}
+	}
+}
+
+func printComment(w io.Writer, classes []class) {
+	fmt.Fprintf(w, "// %-5s  %-9s  %-10s  %-7s  %-10s  %-9s\n", "class", "bytes/obj", "bytes/span", "objects", "tail waste", "max waste")
+	prevSize := 0
+	for i, c := range classes {
+		if i == 0 {
+			continue
+		}
+		spanSize := c.npages * pageSize
+		objects := spanSize / c.size
+		tailWaste := spanSize - c.size*(spanSize/c.size)
+		maxWaste := float64((c.size-prevSize-1)*objects+tailWaste) / float64(spanSize)
+		prevSize = c.size
+		fmt.Fprintf(w, "// %5d  %9d  %10d  %7d  %10d  %8.2f%%\n", i, c.size, spanSize, objects, tailWaste, 100*maxWaste)
+	}
+	fmt.Fprintf(w, "\n")
+}
+
+func printClasses(w io.Writer, classes []class) {
+	fmt.Fprintln(w, "const (")
+	fmt.Fprintf(w, "_MaxSmallSize = %d\n", maxSmallSize)
+	fmt.Fprintf(w, "smallSizeDiv = %d\n", smallSizeDiv)
+	fmt.Fprintf(w, "smallSizeMax = %d\n", smallSizeMax)
+	fmt.Fprintf(w, "largeSizeDiv = %d\n", largeSizeDiv)
+	fmt.Fprintf(w, "_NumSizeClasses = %d\n", len(classes))
+	fmt.Fprintf(w, "_PageShift = %d\n", pageShift)
+	fmt.Fprintln(w, ")")
+
+	fmt.Fprint(w, "var class_to_size = [_NumSizeClasses]uint16 {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "%d,", c.size)
+	}
+	fmt.Fprintln(w, "}")
+
+	fmt.Fprint(w, "var class_to_allocnpages = [_NumSizeClasses]uint8 {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "%d,", c.npages)
+	}
+	fmt.Fprintln(w, "}")
+
+	fmt.Fprintln(w, "type divMagic struct {")
+	fmt.Fprintln(w, "  shift uint8")
+	fmt.Fprintln(w, "  shift2 uint8")
+	fmt.Fprintln(w, "  mul uint16")
+	fmt.Fprintln(w, "  baseMask uint16")
+	fmt.Fprintln(w, "}")
+	fmt.Fprint(w, "var class_to_divmagic = [_NumSizeClasses]divMagic {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "{%d,%d,%d,%d},", c.shift, c.shift2, c.mul, c.mask)
+	}
+	fmt.Fprintln(w, "}")
+
+	// map from size to size class, for small sizes.
+	sc := make([]int, smallSizeMax/smallSizeDiv+1)
+	for i := range sc {
+		size := i * smallSizeDiv
+		for j, c := range classes {
+			if c.size >= size {
+				sc[i] = j
+				break
+			}
+		}
+	}
+	fmt.Fprint(w, "var size_to_class8 = [smallSizeMax/smallSizeDiv+1]uint8 {")
+	for _, v := range sc {
+		fmt.Fprintf(w, "%d,", v)
+	}
+	fmt.Fprintln(w, "}")
+
+	// map from size to size class, for large sizes.
+	sc = make([]int, (maxSmallSize-smallSizeMax)/largeSizeDiv+1)
+	for i := range sc {
+		size := smallSizeMax + i*largeSizeDiv
+		for j, c := range classes {
+			if c.size >= size {
+				sc[i] = j
+				break
+			}
+		}
+	}
+	fmt.Fprint(w, "var size_to_class128 = [(_MaxSmallSize-smallSizeMax)/largeSizeDiv+1]uint8 {")
+	for _, v := range sc {
+		fmt.Fprintf(w, "%d,", v)
+	}
+	fmt.Fprintln(w, "}")
+}
diff --git a/src/runtime/mpagealloc.go b/src/runtime/mpagealloc.go
index dac1f39969..2d20725a3f 100644
--- a/src/runtime/mpagealloc.go
+++ b/src/runtime/mpagealloc.go
@@ -57,7 +57,7 @@ const (
 	// in the bitmap at once.
 	pallocChunkPages    = 1 << logPallocChunkPages
 	pallocChunkBytes    = pallocChunkPages * pageSize
-	logPallocChunkPages = 9
+	logPallocChunkPages = 9*_OS + (logHeapArenaBytes-pageShift)*(1-_OS)
 	logPallocChunkBytes = logPallocChunkPages + pageShift
 
 	// The number of radix bits for each level.
@@ -214,6 +214,7 @@ type pageAlloc struct {
 	// 32           | 0       | 10      | 128 KiB
 	// 33 (iOS)     | 0       | 11      | 256 KiB
 	// 48           | 13      | 13      | 1 MiB
+	// 20 (MCU)     | 0       | 6       | 512 B
 	//
 	// There's no reason to use the L1 part of chunks on 32-bit, the
 	// address space is small so the L2 is small. For platforms with a
diff --git a/src/runtime/mpagealloc_32bit.go b/src/runtime/mpagealloc_32bit.go
index 331dadade9..80be6136f8 100644
--- a/src/runtime/mpagealloc_32bit.go
+++ b/src/runtime/mpagealloc_32bit.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 arm mips mipsle wasm ios,arm64
+// +build 386 arm mips mipsle wasm ios,arm64 !noos,thumb
 
 // wasm is a treated as a 32-bit architecture for the purposes of the page
 // allocator, even though it has 64-bit pointers. This is because any wasm
diff --git a/src/runtime/mpagealloc_64bit.go b/src/runtime/mpagealloc_64bit.go
index ffacb46c18..9b3f0a7484 100644
--- a/src/runtime/mpagealloc_64bit.go
+++ b/src/runtime/mpagealloc_64bit.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build amd64 !ios,arm64 mips64 mips64le ppc64 ppc64le riscv64 s390x
+// +build amd64 !ios,arm64 mips64 mips64le ppc64 ppc64le !noos,riscv64 s390x
 
 // See mpagealloc_32bit.go for why ios/arm64 is excluded here.
 
diff --git a/src/runtime/mpagealloc_mcu.go b/src/runtime/mpagealloc_mcu.go
new file mode 100644
index 0000000000..4ea2656d60
--- /dev/null
+++ b/src/runtime/mpagealloc_mcu.go
@@ -0,0 +1,96 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build noos,thumb noos,riscv64
+
+package runtime
+
+import "unsafe"
+
+const (
+	// The number of levels in the radix tree.
+	summaryLevels = 2
+
+	// Constants for testing.
+	pageAlloc32Bit = 0
+	pageAlloc64Bit = 0
+
+	// Number of bits needed to represent all indices into the L1 of the
+	// chunks map.
+	//
+	// See (*pageAlloc).chunks for more details. Update the documentation
+	// there should this number change.
+	pallocChunksL1Bits = 0
+)
+
+// See comment in mpagealloc_64bit.go.
+var levelBits = [summaryLevels]uint{
+	summaryL0Bits,
+	summaryLevelBits,
+}
+
+// See comment in mpagealloc_64bit.go.
+var levelShift = [summaryLevels]uint{
+	heapAddrBits - summaryL0Bits,
+	heapAddrBits - summaryL0Bits - 1*summaryLevelBits,
+}
+
+// See comment in mpagealloc_64bit.go.
+var levelLogPages = [summaryLevels]uint{
+	logPallocChunkPages + 1*summaryLevelBits,
+	logPallocChunkPages,
+}
+
+// See mpagealloc_64bit.go for details.
+func (p *pageAlloc) sysInit() {
+	// Calculate how much memory all our entries will take up.
+	//
+	// This should be around 12 KiB or less.
+	totalSize := uintptr(0)
+	for l := 0; l < summaryLevels; l++ {
+		totalSize += (uintptr(1) << (heapAddrBits - levelShift[l])) * pallocSumBytes
+	}
+	totalSize = alignUp(totalSize, physPageSize)
+
+	// Reserve memory for all levels in one go.
+	reservation := sysAlloc(totalSize, p.sysStat)
+	if reservation == nil {
+		throw("failed to reserve page summary memory")
+	}
+
+	// Iterate over the reservation and cut it up into slices.
+	//
+	// Maintain i as the byte offset from reservation where
+	// the new slice should start.
+	for l, shift := range levelShift {
+		entries := 1 << (heapAddrBits - shift)
+
+		// Put this reservation into a slice.
+		sl := notInHeapSlice{(*notInHeap)(reservation), 0, entries}
+		p.summary[l] = *(*[]pallocSum)(unsafe.Pointer(&sl))
+
+		reservation = add(reservation, uintptr(entries)*pallocSumBytes)
+	}
+}
+
+// See mpagealloc_64bit.go for details.
+func (p *pageAlloc) sysGrow(base, limit uintptr) {
+	if base%pallocChunkBytes != 0 || limit%pallocChunkBytes != 0 {
+		print("runtime: base = ", hex(base), ", limit = ", hex(limit), "\n")
+		throw("sysGrow bounds not aligned to pallocChunkBytes")
+	}
+
+	// Walk up the tree and update the summary slices.
+	for l := len(p.summary) - 1; l >= 0; l-- {
+		// Figure out what part of the summary array this new address space needs.
+		// Note that we need to align the ranges to the block width (1<<levelBits[l])
+		// at this level because the full block is needed to compute the summary for
+		// the next level.
+		lo, hi := addrsToSummaryRange(l, base, limit)
+		_, hi = blockAlignSummaryRange(l, lo, hi)
+		if hi > len(p.summary[l]) {
+			p.summary[l] = p.summary[l][:hi]
+		}
+	}
+}
diff --git a/src/runtime/mprof.go b/src/runtime/mprof.go
index 128498d69b..50d211a9b6 100644
--- a/src/runtime/mprof.go
+++ b/src/runtime/mprof.go
@@ -5,6 +5,8 @@
 // Malloc profiling.
 // Patterned after tcmalloc's algorithms; shorter code.
 
+// +build !noos
+
 package runtime
 
 import (
diff --git a/src/runtime/mprof_noos.go b/src/runtime/mprof_noos.go
new file mode 100644
index 0000000000..11ad0d2a43
--- /dev/null
+++ b/src/runtime/mprof_noos.go
@@ -0,0 +1,31 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+const (
+	blockprofilerate = 0
+	mutexprofilerate = 0
+)
+
+var (
+	proflock       mutex
+	MemProfileRate int = 0
+)
+
+//go:notinheap
+type bucket struct{}
+
+func blockevent(cycles int64, skip int)                     {}
+func tracealloc(p unsafe.Pointer, size uintptr, typ *_type) {}
+func mProf_Malloc(p unsafe.Pointer, size uintptr)           {}
+func mProf_PostSweep()                                      {}
+func mProf_NextCycle()                                      {}
+func mProf_Flush()                                          {}
+func tracegc()                                              {}
+func tracefree(p unsafe.Pointer, size uintptr)              {}
+func mProf_Free(b *bucket, size uintptr)                    {}
+func mutexevent(cycles int64, skip int)                     {}
diff --git a/src/runtime/mq.go b/src/runtime/mq.go
new file mode 100644
index 0000000000..573815d9c6
--- /dev/null
+++ b/src/runtime/mq.go
@@ -0,0 +1,245 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/atomic"
+)
+
+// A mq represents a queue of threads.
+type mq struct {
+	_mqprivate
+}
+
+type _mqprivate struct {
+	first muintptr
+	last  muintptr
+	n     uint
+	mx    cpumtx
+}
+
+//go:nosplit
+func (q *mq) lock() { q.mx.lock() }
+
+//go:nosplit
+func (q *mq) unlock() { q.mx.unlock() }
+
+// atomicLen returns the approximate number of elements in the q. It returns an
+// exact value if called by the only mutator of q.
+//go:nosplit
+func (q *mq) atomicLen() int { return int(atomic.Loaduint(&q.n)) }
+
+// push inserts m at the end of q
+//go:nosplit
+func (q *mq) push(m *m) {
+	if q.n == 0 {
+		q.first.set(m)
+	} else {
+		msetnext1(q.last.ptr(), m)
+	}
+	q.last.set(m)
+	q.n++
+}
+
+// pop removes the first m from the beginning of q and returns it
+//go:nosplit
+func (q *mq) pop() *m {
+	var ret *m
+	if q.n != 0 {
+		q.n--
+		ret = q.first.ptr()
+		q.first.set(mnext1(ret))
+	}
+	return ret
+}
+
+// A mcl represents a circular list of threads.
+type mcl struct {
+	_mclprivate
+	//_ [(cpu.CacheLinePadSize - unsafe.Sizeof(_mclprivate{}))]byte
+}
+
+type _mclprivate struct {
+	cur muintptr
+	n   uint
+	mx  cpumtx
+}
+
+//go:nosplit
+func (q *mcl) lock() { q.mx.lock() }
+
+//go:nosplit
+func (q *mcl) unlock() { q.mx.unlock() }
+
+// push inserts m just before the current m.
+//go:nosplit
+func (q *mcl) push(m *m) {
+	if q.n == 0 {
+		msetnext1(m, m)
+		msetprev1(m, m)
+		q.cur.set(m)
+	} else {
+		cur := q.cur.ptr()
+		prev := mprev1(cur)
+		msetprev1(m, prev)
+		msetnext1(m, cur)
+		msetnext1(prev, m)
+		msetprev1(cur, m)
+	}
+	q.n++
+}
+
+// find finds and returns the pointer to the first m in q that matches the
+// provided key. As a side effect it rotates the q so the m.next becomes the
+// current element.
+//go:nosplit
+func (q *mcl) find(key uintptr) *m {
+	if q.n == 0 {
+		return nil
+	}
+	cur := q.cur.ptr()
+	first := cur
+	for {
+		if mkey(cur) == key {
+			q.cur.set(mnext1(cur))
+			return cur
+		}
+		cur = mnext1(cur)
+		if cur == first {
+			return nil
+		}
+	}
+}
+
+// remove removes m from q. Remove is fast (O(1)) but the caller must ensure
+// that m belongs to q, othervise the effect of remove is unpredictable.
+//go:nosplit
+func (q *mcl) remove(m *m) {
+	q.n--
+	if q.n == 0 {
+		return
+	}
+	prev := mprev1(m)
+	next := mnext1(m)
+	if m == q.cur.ptr() {
+		q.cur.set(next)
+	}
+	msetnext1(prev, next)
+	msetprev1(next, prev)
+}
+
+// A msl represents a sorted list of threads.
+type msl struct {
+	_mslprivate
+}
+
+type _mslprivate struct {
+	head muintptr
+	n    uint
+	mx   cpumtx
+}
+
+//go:nosplit
+func (q *msl) lock() { q.mx.lock() }
+
+//go:nosplit
+func (q *msl) unlock() { q.mx.unlock() }
+
+// insertbyval inserts m into q just before the first item with greater value
+//go:nosplit
+func (q *msl) insertbyval(m *m) {
+	if q.n == 0 {
+		msetnext2(m, m)
+		msetprev2(m, m)
+		q.head.set(m)
+	} else {
+		val := mval(m)
+		// search from the last to the first because mval(m) is strongly
+		// corelated with the current (monotonic) time
+		last := mprev2(q.head.ptr())
+		cur := last
+		for {
+			if mval(cur) <= val {
+				break
+			}
+			cur = mprev2(cur)
+			if cur == last {
+				q.head.set(m)
+				break
+			}
+		}
+		next := mnext2(cur)
+		msetprev2(m, cur)
+		msetnext2(m, next)
+		msetnext2(cur, m)
+		msetprev2(next, m)
+
+	}
+	q.n++
+}
+
+// first returns the pointer to the first m in q.
+//go:nosplit
+func (q *msl) first() *m {
+	if q.n == 0 {
+		return nil
+	}
+	return q.head.ptr()
+}
+
+// remove removes m from q. Remove is fast (O(1)) but the caller must ensure
+// that m belongs to q, othervise the effect of remove is unpredictable.
+//go:nosplit
+func (q *msl) remove(m *m) {
+	q.n--
+	if q.n == 0 {
+		return
+	}
+	prev := mprev2(m)
+	next := mnext2(m)
+	if m == q.head.ptr() {
+		q.head.set(next)
+	}
+	msetnext2(prev, next)
+	msetprev2(next, prev)
+}
+
+// m fields used
+
+//go:nosplit
+func mprev1(m *m) *m { return muintptr(m.tls[0]).ptr() }
+
+//go:nosplit
+func mnext1(m *m) *m { return muintptr(m.tls[1]).ptr() }
+
+//go:nosplit
+func mprev2(m *m) *m { return muintptr(m.tls[2]).ptr() }
+
+//go:nosplit
+func mnext2(m *m) *m { return muintptr(m.tls[3]).ptr() }
+
+//go:nosplit
+func mval(m *m) int64 { return int64(m.ncgocall) }
+
+//go:nosplit
+func mkey(m *m) uintptr { return m.mqkey }
+
+//go:nosplit
+func msetprev1(m, prev *m) { (*muintptr)(&m.tls[0]).set(prev) }
+
+//go:nosplit
+func msetnext1(m, next *m) { (*muintptr)(&m.tls[1]).set(next) }
+
+//go:nosplit
+func msetprev2(m, prev *m) { (*muintptr)(&m.tls[2]).set(prev) }
+
+//go:nosplit
+func msetnext2(m, next *m) { (*muintptr)(&m.tls[3]).set(next) }
+
+//go:nosplit
+func msetval(m *m, val int64) { m.ncgocall = uint64(val) }
+
+//go:nosplit
+func msetkey(m *m, key uintptr) { m.mqkey = key }
diff --git a/src/runtime/mq_test.go b/src/runtime/mq_test.go
new file mode 100644
index 0000000000..dd76fb46b0
--- /dev/null
+++ b/src/runtime/mq_test.go
@@ -0,0 +1,312 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "runtime/internal/atomic"
+
+const fillkeys = "ABCDEFGHIJ"
+
+var (
+	testms [len(fillkeys)]m // global to avoid GC (mq uses uintptr pointers)
+	testm  m
+	run    uint32
+)
+
+func mqfill(q *mq) {
+	for i := range testms {
+		m := &testms[i]
+		msetkey(m, uintptr(fillkeys[i]))
+		q.push(m)
+	}
+}
+
+func mclfill(q *mcl) {
+	for i := range testms {
+		m := &testms[i]
+		msetkey(m, uintptr(fillkeys[i]))
+		q.push(m)
+	}
+}
+
+func (q *mcl) removebykey(key uintptr) *m {
+	m := q.find(key)
+	if m != nil {
+		q.remove(m)
+	}
+	return m
+}
+
+func (q *msl) pople(val int64) *m {
+	m := q.first()
+	if m == nil || mval(m) > val {
+		return nil
+	}
+	q.remove(m)
+	return m
+}
+
+func (q *mq) str() string {
+	if q.n == 0 {
+		return ""
+	}
+	var buf [len(testms)]byte
+	n := 0
+	for m := q.first.ptr(); ; m = mnext1(m) {
+		buf[n] = byte(mkey(m))
+		n++
+		if m == q.last.ptr() {
+			break
+		}
+	}
+	return string(buf[:n])
+}
+
+func (q *mcl) str() string {
+	if q.n == 0 {
+		return ""
+	}
+	var buf [len(testms)]byte
+	cur := q.cur.ptr()
+	m := cur
+	n := 0
+	for {
+		buf[n] = byte(mkey(m))
+		n++
+		m = mnext1(m)
+		if m == cur {
+			break
+		}
+	}
+	return string(buf[:n])
+}
+
+func (q *msl) str() string {
+	if q.n == 0 {
+		return ""
+	}
+	var buf [len(testms)]byte
+	first := q.head.ptr()
+	m := first
+	n := 0
+	for {
+		buf[n] = byte(mval(m) >> 30)
+		n++
+		m = mnext2(m)
+		if m == first {
+			break
+		}
+	}
+	return string(buf[:n])
+}
+
+var mvi int
+
+func mv(val byte) *m {
+	m := &testms[mvi]
+	mvi++
+	msetval(m, int64(val)<<30)
+	return m
+}
+
+func MQTest() string {
+	if !atomic.Cas(&run, 0, 1) {
+		return "" // run only once
+	}
+
+	q := new(mq)
+
+	mqfill(q)
+	if q.str() != fillkeys {
+		return "mq: fill"
+	}
+	for i := 0; i < len(fillkeys); i++ {
+		m := q.pop()
+		if mkey(m) != uintptr(fillkeys[i]) || q.str() != fillkeys[i+1:] {
+			return "mq: pop"
+		}
+	}
+	if m := q.pop(); m != nil && q.str() != "" {
+		return "mq: pop from empty"
+	}
+	mqfill(q)
+	if q.str() != fillkeys {
+		return "mq fill"
+	}
+	order := fillkeys
+	for i := 0; i < len(fillkeys); i++ {
+		q.push(q.pop())
+		order = order[1:] + order[:1]
+		if q.str() != order {
+			return "mq: push(pop)"
+		}
+	}
+	if order != fillkeys {
+		return "mq: order != fillkeys"
+	}
+
+	cl := new(mcl)
+
+	mclfill(cl)
+	if cl.str() != fillkeys {
+		return "mcl fill"
+	}
+
+	var m *m
+	if m = cl.removebykey('x'); m != nil || cl.str() != "ABCDEFGHIJ" {
+		return "mcl: removebykey, key unknown"
+	}
+	if m = cl.removebykey('A'); mkey(m) != 'A' || cl.str() != "BCDEFGHIJ" {
+		return "mcl: removebykey A"
+	}
+	if m = cl.removebykey('E'); mkey(m) != 'E' || cl.str() != "FGHIJBCD" {
+		return "mcl: removebykey E"
+	}
+	if m = cl.removebykey('D'); mkey(m) != 'D' || cl.str() != "FGHIJBC" {
+		return "mcl: removebykey D"
+	}
+	if cl.push(m); cl.str() != "FGHIJBCD" {
+		return "mcl: push D"
+	}
+	msetkey(&testm, 'G')
+	if cl.push(&testm); cl.str() != "FGHIJBCDG" {
+		return "mcl: push G"
+	}
+	if m = cl.removebykey('G'); mkey(m) != 'G' || cl.str() != "HIJBCDGF" {
+		return "mcl: removebykey G"
+	}
+	if m = cl.removebykey('G'); mkey(m) != 'G' || cl.str() != "FHIJBCD" {
+		return "mcl: removebykey G"
+	}
+	if cl.push(&testm); cl.str() != "FHIJBCDG" {
+		return "mcl: push G"
+	}
+	if m = cl.removebykey('D'); mkey(m) != 'D' || cl.str() != "GFHIJBC" {
+		return "mcl: removebykey D"
+	}
+	if cl.remove(&testm); cl.str() != "FHIJBC" {
+		return "mcl: remove G"
+	}
+	if cl.push(m); cl.str() != "FHIJBCD" {
+		return "mcl: push D"
+	}
+	if m = cl.removebykey('H'); mkey(m) != 'H' || cl.str() != "IJBCDF" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('I'); mkey(m) != 'I' || cl.str() != "JBCDF" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('C'); mkey(m) != 'C' || cl.str() != "DFJB" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('B'); mkey(m) != 'B' || cl.str() != "DFJ" {
+		return "mcl: removebykey"
+	}
+	if cl.push(m); cl.str() != "DFJB" {
+		return "mcl: push"
+	}
+	if cl.push(&testm); cl.str() != "DFJBG" {
+		return "mcl: push G"
+	}
+	if cl.remove(&testm); cl.str() != "DFJB" {
+		return "mcl: remove G"
+	}
+	if m = cl.removebykey('F'); mkey(m) != 'F' || cl.str() != "JBD" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('B'); mkey(m) != 'B' || cl.str() != "DJ" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('J'); mkey(m) != 'J' || cl.str() != "D" {
+		return "mcl: removebykey"
+	}
+	if cl.push(m); cl.str() != "DJ" {
+		return "mcl: push"
+	}
+	if m = cl.removebykey('D'); mkey(m) != 'D' || cl.str() != "J" {
+		return "mcl: removebykey"
+	}
+	if cl.push(m); cl.str() != "JD" {
+		return "mq: push"
+	}
+	if m = cl.removebykey('x'); m != nil || cl.str() != "JD" {
+		return "mcl: removebykey, key unknown"
+	}
+	if m = cl.removebykey('J'); mkey(m) != 'J' || cl.str() != "D" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('x'); m != nil || cl.str() != "D" {
+		return "mcl: removebykey, key unknown"
+	}
+	if m = cl.removebykey('D'); mkey(m) != 'D' || cl.str() != "" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('D'); m != nil || cl.str() != "" {
+		return "mcl: removebykey, key unknown"
+	}
+
+	sl := new(msl)
+
+	sl.insertbyval(mv('3'))
+	sl.insertbyval(mv('0'))
+	sl.insertbyval(mv('9'))
+	sl.insertbyval(mv('1'))
+	sl.insertbyval(mv('7'))
+	sl.insertbyval(mv('5'))
+	sl.insertbyval(mv('2'))
+	sl.insertbyval(mv('8'))
+	sl.insertbyval(mv('4'))
+	sl.insertbyval(mv('6'))
+	if sl.str() != "0123456789" {
+		return "msl: insertbyval"
+	}
+	if m = sl.pople('2' << 30); mval(m) != '0'<<30 || sl.str() != "123456789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('2' << 30); mval(m) != '1'<<30 || sl.str() != "23456789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('2' << 30); mval(m) != '2'<<30 || sl.str() != "3456789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('2' << 30); m != nil || sl.str() != "3456789" {
+		println(m, sl.str())
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '3'<<30 || sl.str() != "456789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '4'<<30 || sl.str() != "56789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '5'<<30 || sl.str() != "6789" {
+		return "msl: pople"
+	}
+	msetval(&testm, '7'<<30)
+	if sl.insertbyval(&testm); sl.str() != "67789" {
+		return "msl: insertbyval 7"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '6'<<30 || sl.str() != "7789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '7'<<30 || sl.str() != "789" {
+		return "msl: pople"
+	}
+	if sl.remove(&testm); sl.str() != "89" {
+		return "msl: remove 7"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '8'<<30 || sl.str() != "9" {
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); m != nil || sl.str() != "9" {
+		return "msl: pople"
+	}
+	if m = sl.pople('9' << 30); mval(m) != '9'<<30 || sl.str() != "" {
+		return "msl: pople"
+	}
+	if m = sl.pople('9' << 30); m != nil || sl.str() != "" {
+		return "msl: pople"
+	}
+	return ""
+}
diff --git a/src/runtime/mspanset.go b/src/runtime/mspanset.go
index 10d2596c38..282012b282 100644
--- a/src/runtime/mspanset.go
+++ b/src/runtime/mspanset.go
@@ -52,8 +52,8 @@ type spanSet struct {
 }
 
 const (
-	spanSetBlockEntries = 512 // 4KB on 64-bit
-	spanSetInitSpineCap = 256 // Enough for 1GB heap on 64-bit
+	spanSetBlockEntries = 512 / noosScaleDown // 4KB on 64-bit
+	spanSetInitSpineCap = 256 / noosScaleDown // Enough for 1GB heap on 64-bit
 )
 
 type spanSetBlock struct {
diff --git a/src/runtime/mstats.go b/src/runtime/mstats.go
index 6defaedabe..75e353fb1f 100644
--- a/src/runtime/mstats.go
+++ b/src/runtime/mstats.go
@@ -71,8 +71,8 @@ type mstats struct {
 	// Protected by mheap or stopping the world during GC.
 	last_gc_unix    uint64 // last gc (in unix time)
 	pause_total_ns  uint64
-	pause_ns        [256]uint64 // circular buffer of recent gc pause lengths
-	pause_end       [256]uint64 // circular buffer of recent gc end times (nanoseconds since 1970)
+	pause_ns        [256 / noosScaleDown]uint64 // circular buffer of recent gc pause lengths
+	pause_end       [256 / noosScaleDown]uint64 // circular buffer of recent gc end times (nanoseconds since 1970)
 	numgc           uint32
 	numforcedgc     uint32  // number of user-forced GCs
 	gc_cpu_fraction float64 // fraction of CPU time used by GC
@@ -380,7 +380,7 @@ type MemStats struct {
 	// general, PauseNs[N%256] records the time paused in the most
 	// recent N%256th GC cycle. There may be multiple pauses per
 	// GC cycle; this is the sum of all pauses during a cycle.
-	PauseNs [256]uint64
+	PauseNs [256 / noosScaleDown]uint64
 
 	// PauseEnd is a circular buffer of recent GC pause end times,
 	// as nanoseconds since 1970 (the UNIX epoch).
@@ -388,7 +388,7 @@ type MemStats struct {
 	// This buffer is filled the same way as PauseNs. There may be
 	// multiple pauses per GC cycle; this records the end of the
 	// last pause in a cycle.
-	PauseEnd [256]uint64
+	PauseEnd [256 / noosScaleDown]uint64
 
 	// NumGC is the number of completed GC cycles.
 	NumGC uint32
@@ -425,7 +425,7 @@ type MemStats struct {
 	// BySize[N-1].Size < S ≤ BySize[N].Size.
 	//
 	// This does not report allocations larger than BySize[60].Size.
-	BySize [61]struct {
+	BySize [61*_OS + _NumSizeClasses*(1-_OS)]struct {
 		// Size is the maximum byte size of an object in this
 		// size class.
 		Size uint32
diff --git a/src/runtime/netpoll_stub.go b/src/runtime/netpoll_stub.go
index 3599f2d01b..c995c4cfa4 100644
--- a/src/runtime/netpoll_stub.go
+++ b/src/runtime/netpoll_stub.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build plan9
+// +build noos plan9
 
 package runtime
 
diff --git a/src/runtime/os_linux_noauxv.go b/src/runtime/os_linux_noauxv.go
index 895b4cd5f4..f2415e811b 100644
--- a/src/runtime/os_linux_noauxv.go
+++ b/src/runtime/os_linux_noauxv.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build !arm,!arm64,!mips,!mipsle,!mips64,!mips64le,!s390x,!ppc64,!ppc64le
+// +build !arm,!thumb,!arm64,!mips,!mipsle,!mips64,!mips64le,!s390x,!ppc64,!ppc64le
 
 package runtime
 
diff --git a/src/runtime/os_linux_novdso.go b/src/runtime/os_linux_novdso.go
index 155f415e71..db6f179151 100644
--- a/src/runtime/os_linux_novdso.go
+++ b/src/runtime/os_linux_novdso.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build !386,!amd64,!arm,!arm64,!mips64,!mips64le,!ppc64,!ppc64le
+// +build !386,!amd64,!arm,!arm64,!mips64,!mips64le,!ppc64,!ppc64le,!thumb
 
 package runtime
 
diff --git a/src/runtime/os_linux_thumb.go b/src/runtime/os_linux_thumb.go
new file mode 100644
index 0000000000..2970ad52e1
--- /dev/null
+++ b/src/runtime/os_linux_thumb.go
@@ -0,0 +1,41 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "internal/cpu"
+
+const (
+	_HWCAP_VFPv3 = 1 << 13 // introduced in 2.6.30
+	_HWCAP_IDIVT = 1 << 18
+)
+
+func checkgoarm() {
+	if cpu.HWCap&_HWCAP_IDIVT == 0 {
+		print("runtime: hardware division not supported, cannot run GOARCH=thumb binary")
+		exit(1)
+	}
+	if goarm&0xF >= 0xD && cpu.HWCap&_HWCAP_VFPv3 == 0 {
+		print("runtime: VFPv3 not supported, cannot run GOARM=7F/7D binary\n")
+		exit(1)
+	}
+}
+
+func archauxv(tag, val uintptr) {
+	switch tag {
+	case _AT_HWCAP:
+		cpu.HWCap = uint(val)
+	case _AT_HWCAP2:
+		cpu.HWCap2 = uint(val)
+	}
+}
+
+func osArchInit() {}
+
+//go:nosplit
+func cputicks() int64 {
+	// Currently cputicks() is used in blocking profiler and to seed fastrand().
+	// nanotime() is a poor approximation of CPU ticks that is enough for the profiler.
+	return nanotime()
+}
diff --git a/src/runtime/os_noos.go b/src/runtime/os_noos.go
new file mode 100644
index 0000000000..222855b6b0
--- /dev/null
+++ b/src/runtime/os_noos.go
@@ -0,0 +1,191 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"unsafe"
+)
+
+const (
+	_NSIG             = 0
+	preemptMSupported = false
+)
+
+type gsignalStack struct{}
+type sigset struct{}
+
+//go:nosplit
+func setThreadCPUProfiler(hz int32) {
+	for {
+		breakpoint()
+	}
+}
+
+//go:nosplit
+func unminit() {
+	for {
+		breakpoint()
+	}
+}
+
+func clearSignalHandlers()           {}
+func sigdisable(uint32)              {}
+func sigenable(uint32)               {}
+func sigignore(uint32)               {}
+func signame(sig uint32) string      { return "" }
+func sigblock(exiting bool)          {}
+func msigrestore(sigmask sigset)     {}
+func initsig(preinit bool)           {}
+func setProcessCPUProfiler(hz int32) {}
+func mpreinit(mp *m)                 {}
+func sigsave(p *sigset)              {}
+func goenvs()                        {}
+func minit()                         {}
+func mdestroy(mp *m)                 {}
+func preemptM(mp *m)                 {}
+
+//go:nosplit
+func crash() {
+	for {
+		breakpoint()
+	}
+}
+
+//go:nosplit
+func setsystim(nanotime func() int64, setalarm func(ns int64)) {
+	if nanotime != nil {
+		thetasker.newnanotime = nanotime
+	} else {
+		thetasker.newnanotime = dummyNanotime
+	}
+	if setalarm != nil {
+		thetasker.newsetalarm = setalarm
+	} else {
+		thetasker.newsetalarm = dummySetalarm
+
+	}
+	setsystim1()
+}
+
+//go:nosplit
+func setsyswriter(w func(fd int, p []byte) int) {
+	if w != nil {
+		thetasker.newwrite = w
+	} else {
+		thetasker.newwrite = defaultWrite
+	}
+	setsyswriter1()
+}
+
+//go:nosplit
+func usleep(usec uint32) {
+	nanosleep(int64(usec) * 1000)
+}
+
+//go:nosplit
+func getRandomData(r []byte) {
+	// BUG: true random data required
+	extendRandom(r, 0)
+}
+
+//go:nosplit
+func osinit() {
+	ncpu = 1 // for now only single CPU is supported (see identcurcpu, cpuid)
+	physPageSize = _PageSize
+}
+
+//go:nosplit
+func isr() bool {
+	gp := getg()
+	return gp == gp.m.gsignal
+	/*
+		allcpu := thetasker.allcpu
+		for i := 0; i < len(allcpu); i++ {
+			if getg() == &allcpu[i].gh {
+				return true
+			}
+		}
+		return false
+	*/
+}
+
+var timestart struct {
+	sec  int64
+	nsec int32
+	lock rwmutex
+}
+
+// The noos has separate implementation of time_now for better performance (only
+// one syscall) and to ensure that the monotonic and the wall components both
+// point to the same instant in time (the RTC setting code can rely on this).
+
+//go:linkname time_now time.now
+func time_now() (sec int64, nsec int32, mono int64) {
+	timestart.lock.rlock()
+	sec = timestart.sec
+	nsec = timestart.nsec
+	timestart.lock.runlock()
+	mono = nanotime()
+	s := mono / 1e9
+	ns := int32(mono - s*1e9)
+	sec += s
+	nsec += ns
+	if nsec >= 1e9 {
+		sec++
+		nsec -= 1e9
+	}
+	return
+}
+
+//go:linkname time_move time.move
+func time_move(sec int64, nsec int32) (sec0 int64, nsec0 int32) {
+	timestart.lock.lock()
+	sec0 = timestart.sec + sec
+	nsec0 = timestart.nsec + nsec
+	if nsec0 < 0 {
+		nsec0 += 1e9
+		sec0--
+	} else if nsec0 >= 1e9 {
+		nsec0 -= 1e9
+		sec0++
+	}
+	timestart.sec = sec0
+	timestart.nsec = nsec0
+	timestart.lock.unlock()
+	return
+}
+
+func setsystim1()
+func setsyswriter1()
+func newosproc(mp *m)
+func exit(code int32)
+func osyield()
+
+//go:noescape
+func write(fd uintptr, p unsafe.Pointer, n int32) int32
+
+//go:noescape
+func futexsleep(addr *uint32, val uint32, ns int64)
+
+//go:noescape
+func futexwakeup(addr *uint32, cnt uint32)
+
+//go:noescape
+func exitThread(wait *uint32)
+
+// syscalls not used by runtime
+
+func setprivlevel(newlevel int) (oldlevel, errno int)
+func irqenabled(irq int) (enabled, errno int)
+func setirqenabled(irq, enabled int) (errno int)
+func irqctl(irq, ctl, ctxid int) (enabled, prio, errno int)
+func nanosleep(ns int64)
+func nanotime() int64
+
+// faketime is the simulated time in nanoseconds since 1970 for the
+// playground.
+//
+// Zero means not to use faketime.
+var faketime int64
diff --git a/src/runtime/os_noos_thumb.go b/src/runtime/os_noos_thumb.go
new file mode 100644
index 0000000000..6c7fd3c1c3
--- /dev/null
+++ b/src/runtime/os_noos_thumb.go
@@ -0,0 +1,12 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+//go:nosplit
+func cputicks() int64 {
+	// Currently cputicks() is used in blocking profiler and to seed fastrand().
+	// nanotime() is a poor approximation of CPU ticks that is enough for the profiler.
+	return nanotime()
+}
diff --git a/src/runtime/panic.go b/src/runtime/panic.go
index 5b2ccdd874..bea0592213 100644
--- a/src/runtime/panic.go
+++ b/src/runtime/panic.go
@@ -1149,6 +1149,9 @@ func recovery(gp *g) {
 	// this time returning 1. The calling function will
 	// jump to the standard return epilogue.
 	gp.sched.sp = sp
+	if GOARCH == "thumb" {
+		pc |= 1
+	}
 	gp.sched.pc = pc
 	gp.sched.lr = 0
 	gp.sched.ret = 1
@@ -1409,5 +1412,5 @@ func shouldPushSigpanic(gp *g, pc, lr uintptr) bool {
 //
 //go:nosplit
 func isAbortPC(pc uintptr) bool {
-	return pc == funcPC(abort) || ((GOARCH == "arm" || GOARCH == "arm64") && pc == funcPC(abort)+sys.PCQuantum)
+	return pc == funcPC(abort) || ((GOARCH == "arm" || GOARCH == "arm64" || GOARCH == "thumb") && pc == funcPC(abort)+sys.PCQuantum)
 }
diff --git a/src/runtime/panic32.go b/src/runtime/panic32.go
index aea8401a37..990fbf5b1b 100644
--- a/src/runtime/panic32.go
+++ b/src/runtime/panic32.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 arm mips mipsle
+// +build 386 arm mips mipsle thumb
 
 package runtime
 
diff --git a/src/runtime/preempt_thumb.s b/src/runtime/preempt_thumb.s
new file mode 100644
index 0000000000..1b22eb99e5
--- /dev/null
+++ b/src/runtime/preempt_thumb.s
@@ -0,0 +1,83 @@
+// Code generated by mkpreempt.go; DO NOT EDIT.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+TEXT ·asyncPreempt(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW.W R14, -188(R13)
+	MOVW R0, 4(R13)
+	MOVW R1, 8(R13)
+	MOVW R2, 12(R13)
+	MOVW R3, 16(R13)
+	MOVW R4, 20(R13)
+	MOVW R5, 24(R13)
+	MOVW R6, 28(R13)
+	MOVW R7, 32(R13)
+	MOVW R8, 36(R13)
+	MOVW R9, 40(R13)
+	MOVW R11, 44(R13)
+	MOVW R12, 48(R13)
+	MOVW APSR, R0
+	MOVW R0, 52(R13)
+	MOVB ·goarm(SB), R0
+	CMP $0x10, R0
+	BLT nofp
+	MOVW FPSCR, R0
+	MOVW R0, 56(R13)
+	MOVD F0, 60(R13)
+	MOVD F1, 68(R13)
+	MOVD F2, 76(R13)
+	MOVD F3, 84(R13)
+	MOVD F4, 92(R13)
+	MOVD F5, 100(R13)
+	MOVD F6, 108(R13)
+	MOVD F7, 116(R13)
+	MOVD F8, 124(R13)
+	MOVD F9, 132(R13)
+	MOVD F10, 140(R13)
+	MOVD F11, 148(R13)
+	MOVD F12, 156(R13)
+	MOVD F13, 164(R13)
+	MOVD F14, 172(R13)
+	MOVD F15, 180(R13)
+nofp:
+	CALL ·asyncPreempt2(SB)
+	MOVB ·goarm(SB), R0
+	CMP $0x10, R0
+	BLT nofp2
+	MOVD 180(R13), F15
+	MOVD 172(R13), F14
+	MOVD 164(R13), F13
+	MOVD 156(R13), F12
+	MOVD 148(R13), F11
+	MOVD 140(R13), F10
+	MOVD 132(R13), F9
+	MOVD 124(R13), F8
+	MOVD 116(R13), F7
+	MOVD 108(R13), F6
+	MOVD 100(R13), F5
+	MOVD 92(R13), F4
+	MOVD 84(R13), F3
+	MOVD 76(R13), F2
+	MOVD 68(R13), F1
+	MOVD 60(R13), F0
+	MOVW 56(R13), R0
+	MOVW R0, FPSCR
+nofp2:
+	MOVW 52(R13), R0
+	MOVW R0, APSR
+	MOVW 48(R13), R12
+	MOVW 44(R13), R11
+	MOVW 40(R13), R9
+	MOVW 36(R13), R8
+	MOVW 32(R13), R7
+	MOVW 28(R13), R6
+	MOVW 24(R13), R5
+	MOVW 20(R13), R4
+	MOVW 16(R13), R3
+	MOVW 12(R13), R2
+	MOVW 8(R13), R1
+	MOVW 4(R13), R0
+	MOVW 188(R13), R14
+	MOVW.P 192(R13), R15
+	UNDEF
diff --git a/src/runtime/print.go b/src/runtime/print.go
index 64055a34cc..8be52727c0 100644
--- a/src/runtime/print.go
+++ b/src/runtime/print.go
@@ -26,7 +26,7 @@ func bytes(s string) (ret []byte) {
 var (
 	// printBacklog is a circular buffer of messages written with the builtin
 	// print* functions, for use in postmortem analysis of core dumps.
-	printBacklog      [512]byte
+	printBacklog      [256 * (1 + _OS)]byte
 	printBacklogIndex int
 )
 
@@ -64,6 +64,9 @@ var debuglock mutex
 // For both these reasons, let a thread acquire the printlock 'recursively'.
 
 func printlock() {
+	if isr() {
+		return
+	}
 	mp := getg().m
 	mp.locks++ // do not reschedule between printlock++ and lock(&debuglock).
 	mp.printlock++
@@ -74,6 +77,9 @@ func printlock() {
 }
 
 func printunlock() {
+	if isr() {
+		return
+	}
 	mp := getg().m
 	mp.printlock--
 	if mp.printlock == 0 {
@@ -196,7 +202,7 @@ func printcomplex(c complex128) {
 }
 
 func printuint(v uint64) {
-	var buf [100]byte
+	var buf [24]byte
 	i := len(buf)
 	for i--; i > 0; i-- {
 		buf[i] = byte(v%10 + '0')
@@ -218,7 +224,7 @@ func printint(v int64) {
 
 func printhex(v uint64) {
 	const dig = "0123456789abcdef"
-	var buf [100]byte
+	var buf [20]byte
 	i := len(buf)
 	for i--; i > 0; i-- {
 		buf[i] = dig[v%16]
diff --git a/src/runtime/proc.go b/src/runtime/proc.go
index cf9770587a..1b79ced984 100644
--- a/src/runtime/proc.go
+++ b/src/runtime/proc.go
@@ -122,7 +122,9 @@ func main() {
 	// Max stack size is 1 GB on 64-bit, 250 MB on 32-bit.
 	// Using decimal instead of binary GB and MB because
 	// they look nicer in the stack overflow failure message.
-	if sys.PtrSize == 8 {
+	if noos {
+		maxstacksize = 256000 / noosScaleDown
+	} else if sys.PtrSize == 8 {
 		maxstacksize = 1000000000
 	} else {
 		maxstacksize = 250000000
@@ -1362,7 +1364,7 @@ func mexit(osStack bool) {
 	g := getg()
 	m := g.m
 
-	if m == &m0 {
+	if m == &m0 && !noos {
 		// This is the main thread. Just wedge it.
 		//
 		// On Linux, exiting the main thread puts the process
@@ -1763,7 +1765,15 @@ func allocm(_p_ *p, fn func(), id int64) *m {
 			systemstack(func() {
 				stackfree(freem.g0.stack)
 			})
+			cleanm := freem
 			freem = freem.freelink
+			if cleanm == &m0 {
+				// ensure nothing will hang on m0
+				cleanm.g0 = nil
+				cleanm.curg = nil
+				cleanm.freelink = nil
+				// TODO: can we zero allink? any other pointer fields?
+			}
 		}
 		sched.freem = newList
 		unlock(&sched.lock)
@@ -1777,6 +1787,8 @@ func allocm(_p_ *p, fn func(), id int64) *m {
 	// Windows and Plan 9 will layout sched stack on OS stack.
 	if iscgo || mStackIsSystemAllocated() {
 		mp.g0 = malg(-1)
+	} else if noos {
+		mp.g0 = malg(2 * _StackMin)
 	} else {
 		mp.g0 = malg(8192 * sys.StackGuardMultiplier)
 	}
@@ -4194,9 +4206,9 @@ func gfput(_p_ *p, gp *g) {
 
 	_p_.gFree.push(gp)
 	_p_.gFree.n++
-	if _p_.gFree.n >= 64 {
+	if _p_.gFree.n >= 64*_OS+5*16/noosScaleDown {
 		lock(&sched.gFree.lock)
-		for _p_.gFree.n >= 32 {
+		for _p_.gFree.n >= 32*_OS+3*16/noosScaleDown {
 			_p_.gFree.n--
 			gp = _p_.gFree.pop()
 			if gp.stack.lo == 0 {
@@ -4217,7 +4229,7 @@ retry:
 	if _p_.gFree.empty() && (!sched.gFree.stack.empty() || !sched.gFree.noStack.empty()) {
 		lock(&sched.gFree.lock)
 		// Move a batch of free Gs to the P.
-		for _p_.gFree.n < 32 {
+		for _p_.gFree.n < 32*_OS+3*16/noosScaleDown {
 			// Prefer Gs with stacks.
 			gp := sched.gFree.stack.pop()
 			if gp == nil {
@@ -4431,7 +4443,7 @@ func sigprof(pc, sp, lr uintptr, gp *g, mp *m) {
 	// As a workaround, create a counter of SIGPROFs while in critical section
 	// to store the count, and pass it to sigprof.add() later when SIGPROF is
 	// received from somewhere else (with _LostSIGPROFDuringAtomic64 as pc).
-	if GOARCH == "mips" || GOARCH == "mipsle" || GOARCH == "arm" {
+	if GOARCH == "mips" || GOARCH == "mipsle" || GOARCH == "arm" || GOARCH == "thumb" {
 		if f := findfunc(pc); f.valid() {
 			if hasPrefix(funcname(f), "runtime/internal/atomic") {
 				cpuprof.lostAtomic++
@@ -5921,7 +5933,7 @@ func runqget(_p_ *p) (gp *g, inheritTime bool) {
 // Batch is a ring buffer starting at batchHead.
 // Returns number of grabbed goroutines.
 // Can be executed by any P.
-func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32 {
+func runqgrab(_p_ *p, batch *[256 / noosScaleDown]guintptr, batchHead uint32, stealRunNextG bool) uint32 {
 	for {
 		h := atomic.LoadAcq(&_p_.runqhead) // load-acquire, synchronize with other consumers
 		t := atomic.LoadAcq(&_p_.runqtail) // load-acquire, synchronize with the producer
diff --git a/src/runtime/rt0_linux_thumb.s b/src/runtime/rt0_linux_thumb.s
new file mode 100644
index 0000000000..6cdeb88294
--- /dev/null
+++ b/src/runtime/rt0_linux_thumb.s
@@ -0,0 +1,29 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT _rt0_thumb_linux(SB),NOSPLIT|NOFRAME,$0
+	MOVW  (R13), R0    // argc
+	MOVW  $4(R13), R1  // argv
+	MOVW  $_rt0_thumb_linux1(SB), R4
+	B     (R4)
+
+TEXT _rt0_thumb_linux1(SB),NOSPLIT|NOFRAME,$0
+	// We first need to detect the kernel ABI, and warn the user
+	// if the system only supports OABI.
+	// The strategy here is to call some EABI syscall to see if
+	// SIGILL is received.
+	// If you get a SIGILL here, you have the wrong kernel.
+
+	// Save argc and argv (syscall will clobber at least R0).
+	MOVM.DB.W  [R0-R1], (R13)
+
+	// do an EABI syscall
+	MOVW  $20, R7  // sys_getpid
+	SWI   $0       // this will trigger SIGILL on OABI systems
+
+	MOVM.IA.W  (R13), [R0-R1]
+	B          runtime·rt0_go(SB)
+
diff --git a/src/runtime/rt0_noos_riscv64.s b/src/runtime/rt0_noos_riscv64.s
new file mode 100644
index 0000000000..303a2e9b43
--- /dev/null
+++ b/src/runtime/rt0_noos_riscv64.s
@@ -0,0 +1,290 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "funcdata.h"
+#include "textflag.h"
+#include "asm_riscv64.h"
+
+
+// Prefer X8-X15 registers (S0, S1, A0-A5) to allow compressed instructions if
+// compiler will support C extension. By the convention An registers are
+// preffered for addresses and addressing related things, Sn for numbers and
+// other things..
+//
+// The gdb and objdump use C ABI names for registers (Tn, An, Sn, ...). In most
+// cases there is imposible to make them print Xn names so we use C ABI names
+// in RISC-V assembly except LR (X1), X2 (stack pointer), g (X27) and TMP (X31).
+
+
+#define handlerStackSize 4*1024 // size of stack usesd by trap handlers
+#define persistAllocMin 128*1024
+
+#define intEnabled (MTI+MSI+SEI+MEI)
+
+DATA runtime·waitInit+0(SB)/4, $-1
+GLOBL runtime·waitInit(SB), NOPTR, $4
+
+// _rt0_riscv64_noos initializs all running cores
+TEXT _rt0_riscv64_noos(SB),NOSPLIT|NOFRAME,$0
+	// Disable interrupts globally, enable FPU (Kendryte K210 supports only
+	// FS=0(off)/3(dirty), this is a weakness of the Rocket Chip Generator used
+	// to generate K210 cores).
+	MOV   $0x7FFF, S0
+	CSRC  (s0, mstatus)
+	MOV   $(1<<FSn), S0  // set FS to init
+	CSRS  (s0, mstatus)
+
+	// enable interrupts locally
+	MOV   $intEnabled, S0
+	CSRW  (s0, mie)
+
+	// Set a temporary trap handler.
+	MOV   $·defaultExceptionHandler(SB), S0
+	CSRW  (s0, mtvec)
+
+	// Clear some other CSRs
+	CSRWI  (0, mideleg)
+	CSRWI  (0, medeleg)
+	CSRWI  (0, mscratch)
+	CSRWI  (0, fcsr)
+
+	//MOV  ZERO, X1
+	//...
+	//MOV  ZERO, X32
+	//
+	//FCVTDL  ZERO, F0
+	//...
+	//FCVTDL  ZERO, F31
+
+	// park excess harts
+	CSRR  (mhartid, s0)  // from now S0 used only to provide hart id
+	MOV   $const_maxHarts, S1
+	BGEU  S0, S1, parkHart
+
+	// ensure handler stacks are 16 byte aligned (may be required in the future)
+	MOV  $runtime·end(SB), A0
+	ADD  $(handlerStackSize+15), A0
+	AND  $~15, A0
+
+	// set handler SP for this hart
+	MOV  $handlerStackSize, A1
+	MUL  S0, A1, A2
+	ADD  A2, A0, X2
+
+	// clear msip register
+	MOV   $msip, A0
+	SLL   $2, S0, A1
+	ADD   A1, A0
+	MOVW  ZERO, (A0)
+
+	// set mtimecmp to maximum value
+	MOV  $mtimecmp, A0
+	SLL  $3, S0, A1
+	ADD  A1, A0
+	MOV  $-1, A1
+	MOV  A1, (A0)
+
+	BEQ  ZERO, S0, clear
+
+	// Other harts have to wait for hart0 to initaialize all shared components
+	MOV   $·waitInit(SB), A0
+	MOVW  (A0), S1
+	BNE   ZERO, S1, -1(PC)
+	JMP   cleared
+
+clear:
+	// clear the mtime register
+	MOV  $mtime, A0
+	MOV  ZERO, (A0)
+
+	// clear the BSS and the whole unallocated memory
+	ADD   $-24, X2
+	MOV   $runtime·bss(SB), A0
+	MOV   $runtime·ramend(SB), A1
+	SUB   A0, A1
+	MOV   ZERO, 0(X2)
+	MOV   A0, 8(X2)
+	MOV   A1, 16(X2)
+	CALL  runtime·memclrNoHeapPointers(SB)
+	MOV   $runtime·nodmastart(SB), A0
+	MOV   $runtime·nodmaend(SB), A1
+	SUB   A0, A1
+	MOV   A0, 8(X2)
+	MOV   A1, 16(X2)
+	CALL  runtime·memclrNoHeapPointers(SB)
+	ADD   $24, X2
+
+cleared:
+	// setup handler stack in harts[mhartid].gh
+	MOV  $runtime·harts(SB), g
+	MOV  $cpuctx__size, A0
+	MUL  S0, A0
+	ADD  A0, g  // gh is the first field of the cpuctx struct
+	ADD  $-handlerStackSize, X2, A0
+	MOV  A0, (g_stack+stack_lo)(g)
+	MOV  X2, (g_stack+stack_hi)(g)
+
+	// setup gh and mh
+	ADD  $cpuctx_mh, g, A0
+	MOV  g, m_g0(A0)       // harts[mhartid].mh.g0 = harts[mhartid].gh
+	MOV  g, m_gsignal(A0)  // harts[mhartid].mh.gsignal = harts[mhartid].gh (to easily check for handler mode)
+	MOV  A0, g_m(g)        // harts[mhartid].gh.m = harts[mhartid].mh
+
+	// as we have SP and g set we can set the real trap handler in mtvec
+	MOV   $·trapHandler(SB), A0
+	CSRW  (a0, mtvec)
+
+	// hart0 runs the m0
+	BNE  ZERO, S0, 2(PC)
+	JMP  runtime·rt0_go(SB)
+
+	MOV  S0, g_goid(g)  // set cpuctx.gh.goid to mhartid
+
+	// set thetasker.allcpu.len to hartid+1 if lower
+	MOV  cpuctx_t(g), A0         // &thetasker
+	ADD  $(tasker_allcpu+8), A0  // &thetasker.allcpu.len
+	ADD  $1, S0
+again:
+	LRD   (a0, s1)
+	BGEU  S1, S0, runScheduler
+	SCD   (s0, s1, a0)
+	BNE   ZERO, S1, again
+
+runScheduler:
+	// prepare trap context (only mstatus and mie are required on the stack)
+	ADD   $-trapCtxSize, X2
+	CSRR  (mstatus, s0)
+	OR    $(1<<MPIEn), S0
+	MOV   S0, _mstatus(X2)
+	MOV   $intEnabled, S0
+	MOV   S0, _mie(X2)
+	MOV   $0xFF, S0
+	CSRC  (s0, mie)  // disable MTI+MSI before enter the scheduler
+
+	// enter scheduler
+	MOV   $1, S0
+	MOVB  S0, (cpuctx_schedule)(g)
+	JMP   ·enterScheduler(SB)
+
+parkHart:
+	WFI
+	JMP  -1(PC)
+
+
+// rt0_go is known as top level function
+TEXT runtime·rt0_go(SB),NOSPLIT|NOFRAME,$0
+
+	// set up m0 (bootstrap thread), temporarily use harts[0].gh as g
+	MOV  $runtime·m0(SB), A0
+	MOV  g, m_g0(A0)  // m0.g0 = harts[0].gh
+	MOV  A0, g_m(g)   // harts[0].gh.m = m0
+
+	CALL  runtime·check(SB)
+	CALL  runtime·osinit(SB)
+
+	// initialize sysMem
+
+	// calculate the beginning of free memory (just after handler stacks)
+	MOV  $runtime·end(SB), A0
+	ADD  $(const_maxHarts*handlerStackSize+15), A0
+	AND  $~15, A0
+	MOV  $runtime·ramend(SB), A1
+	SUB  A0, A1, A5  // size of available memory (DMA capable)
+
+	// estimate the space need for non-heap allocations
+	SRL  $(const__PageShift+1), A5, A4
+	MOV  $mspan__size, A2
+	MUL  A2, A4
+	ADD  $persistAllocMin, A4
+
+	MOV  $runtime·nodmastart(SB), A2
+	MOV  $runtime·nodmaend(SB), A3
+	SUB  A2, A3, S0  // size of non-DMA memory
+
+	// we prefer the non-DMA memory for non-heap objects to preserve as much as
+	// possible of the DMA capable memory for heap allocations
+	SUB  S0, A4
+
+	// reduce the arena by the remain of the non-heap space that did not fit in
+	// the non-DMA memory, properly align the arena
+	BLT  A4, ZERO, 2(PC)
+	SUB  A4, A5
+	AND  $~(const_heapArenaBytes-1), A5
+	SUB  A5, A1
+
+	// save {free.start,free.end,nodma.start,nodma.end,arenaStart,arenaSize}
+	MOV  $runtime·sysMem(SB), S0
+	MOV  A0, 0(S0)
+	MOV  A1, 8(S0)
+	MOV  A2, 16(S0)
+	MOV  A3, 24(S0)
+	MOV  A1, 32(S0)
+	MOV  A5, 40(S0)
+
+	// initialize noos tasker and Go scheduler
+	CALL  runtime·taskerinit(SB)
+	CALL  runtime·schedinit(SB)
+
+	// run other harts
+	MOV   $·waitInit(SB), A0
+	MOVW  ZERO, (A0)
+
+	// allocate g0 for m0
+	MOV   $(2*const__StackMin), A0
+	ADD   $-24, X2
+	MOV   ZERO, 0(X2)
+	MOV   A0, 8(X2)
+	CALL  runtime·malg(SB)
+	MOV   16(X2), A0  // newg in A0
+	ADD   $24, X2
+
+	// stackguard check during newproc requires valid stackguard1 but malg
+	// sets it to 0xFFFFFFFFFFFFFFFF (mstart fixes this but is called later)
+	MOV  g_stackguard0(A0), A1
+	MOV  A1, g_stackguard1(A0)
+
+	MOV  $runtime·m0(SB), A1
+	MOV  A0, m_g0(A1)  // m0.g0 = newg
+	MOV  A1, g_m(A0)   // newg.m = m0
+
+	// newg stack pointer to X2
+	MOV  (g_stack+stack_hi)(A0), X2
+
+	// newg to g
+	MOV  g, A1
+	MOV  A0, g
+
+	// fix harts[0].gh
+	ADD   $cpuctx_mh, A1, A0
+	MOV   A0, g_m(A1)  // harts[0].gh.m = harts[0].mh
+	CSRW  (a1, mscratch)
+
+	// switch to the user mode
+	MOV    $(1<<MPIEn), S0
+	CSRS   (s0, mstatus)
+	AUIPC  $0, A0
+	ADD    $16, A0  // A0 must point just after MRET
+	CSRW   (a0, mepc)
+	MRET
+
+	// K210: Don't be surprised the MIE is cleared though we set the MPIE before
+	// MRET. It's described in RISC-V 1.9.1 spec (changed in 1.10 and fixed in
+	// Rocket Chip by 29414f3a239174201938a345ac8565726892fdbb commit). Despite
+	// this, machine mode interrupts are globally enabled because we are now in
+	// user mode.
+
+	MOV   $runtime·mainPC(SB), A0  // entry
+	ADD   $-24, X2
+	MOV   A0, 16(X2)
+	MOV   ZERO, 8(X2)
+	MOV   ZERO, 0(X2)
+	CALL  runtime·newproc(SB)
+	ADD   $24, X2
+
+	// start this M
+	CALL  runtime·mstart(SB)
+
+	UNDEF  // fail
diff --git a/src/runtime/rt0_noos_thumb.s b/src/runtime/rt0_noos_thumb.s
new file mode 100644
index 0000000000..12342a9de0
--- /dev/null
+++ b/src/runtime/rt0_noos_thumb.s
@@ -0,0 +1,165 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "funcdata.h"
+#include "textflag.h"
+
+
+TEXT _rt0_thumb_noos(SB),NOSPLIT|NOFRAME,$0
+
+	// initialize data and BSS
+
+	MOVW       R13, R0
+	MOVW       $runtime·romdata(SB), R1
+	MOVW       $runtime·bss(SB), R3
+	MOVW       $runtime·ramend(SB), R4
+	SUB        R0, R3, R2
+	SUB        R3, R4
+	MOVM.DB.W  [R0-R4], (R13)  // push: to,from,n for memmove, ptr,n for memclr
+	SUB        $4, R13
+	BL         runtime·memmove(SB)  // copy data to RAM
+	ADD        $12, R13
+	BL         runtime·memclrNoHeapPointers(SB)  // clear BSS and unallocated memory
+	MOVW       $runtime·nodmastart(SB), R0
+	MOVW       $runtime·nodmaend(SB), R1
+	SUB        R0, R1
+	MOVW       R0, 4(R13)
+	MOVW       R1, 8(R13)
+	BL         runtime·memclrNoHeapPointers(SB)  // clear non-DMA memory
+	ADD        $12, R13
+
+	B   runtime·rt0_go(SB)  // rt0_go is known as top of a goroutine stack
+
+
+#define PALLOC_MIN 20*1024
+#define FPU_CTRL_BASE 0xE000ED88
+#define FPU_CPACR 0x000
+#define FPU_FPCCR 0x1AC
+
+TEXT runtime·rt0_go(SB),NOSPLIT|NOFRAME,$0
+
+	// setup main stack in cpu0.gh
+	MOVW  $runtime·cpu0(SB), R0      // gh is the first field of the cpuctx struct
+	MOVW  $runtime·ramstart(SB), R1  // main stack starts at the beggining of memory
+	MOVW  R1, (g_stack+stack_lo)(R0)
+	MOVW  R13, (g_stack+stack_hi)(R0)
+	ADD   $const__StackGuard, R1
+	MOVW  R1, g_stackguard0(R0)
+	MOVW  R1, g_stackguard1(R0)
+
+	// set up m0 (bootstrap thread), temporarily use cpu0.gh as g
+	MOVW  $runtime·m0(SB), R1
+	MOVW  R0, m_g0(R1)  // m0.g0 = cpu0.gh
+	MOVW  R1, g_m(R0)   // cpu0.gh.m = m0
+
+	MOVW  R0, g  // we use R0 above instead of g for shorter encoding
+
+	// enable FPU if GOARM is xF or xD
+	MOVB  runtime·goarm(SB), R0
+	AND   $0xD, R0
+	CMP   $0xD, R0
+	BNE   skipFPU
+	MOVW  $FPU_CTRL_BASE, R0  // address of CPACR
+	MOVW  $3<<20, R1
+	MOVW  R1, FPU_CPACR(R0)  // full access to CP10
+	SLL   $10, R1
+	MOVW  R1, FPU_FPCCR(R0)  // set LSPEN and ASPEN
+skipFPU:
+
+	//BL  runtime·emptyfunc(SB)  // fault if stack check is wrong
+	BL  runtime·check(SB)
+	BL  runtime·osinit(SB)
+
+	// initialize sysMem
+
+	MOVW  $runtime·end(SB), R0
+	MOVW  $runtime·ramend(SB), R1
+	SUB   R0, R1, R5  // size of available memory (DMA capable)
+
+	// estimate the space need for non-heap allocations
+	MOVW  R5>>(const__PageShift+2), R4
+	MOVW  $mspan__size, R2
+	MUL   R2, R4
+	ADD   $PALLOC_MIN, R4
+
+	MOVW  $runtime·nodmastart(SB), R2
+	MOVW  $runtime·nodmaend(SB), R3
+	SUB   R2, R3, R6  // size of non-DMA memory
+
+	// we prefer the non-DMA memory for non-heap objects to preserve as much as
+	// possible of the DMA capable memory for heap allocations
+	SUB.S  R6, R4
+
+	// reduce the arena by the remain of the non-heap space that did not fit in
+	// the non-DMA memory, properly align the arena
+	SUB.HI  R4, R5
+	BIC     $(const_heapArenaBytes-1), R5
+	SUB     R5, R1
+	MOVW    R1, R4
+
+	// save {free.start,free.end,nodma.start,nodma.end,arenaStart,arenaSize}
+	MOVW     $runtime·sysMem(SB), R6
+	MOVM.IA  [R0-R5], (R6)
+
+	// initialize noos tasker and Go scheduler
+
+	BL  runtime·taskerinit(SB)
+	BL  runtime·schedinit(SB)
+
+	// allocate g0 for m0 and leave gh
+
+	SUB        $4, R13
+	MOVW       $0, R0
+	MOVW       $(2*const__StackMin), R1
+	MOVM.DB.W  [R0-R1], (R13)
+	BL         runtime·malg(SB)
+	MOVW       8(R13), R0  // newg in R0
+	ADD        $12, R13
+
+	// stackguard check during newproc requires valid stackguard1 but
+	// malg sets it to 0xFFFFFFFF (mstart fixes this but is called later)
+	MOVW  g_stackguard0(R0), R1
+	MOVW  R1, g_stackguard1(R0)
+
+	MOVW  $runtime·m0(SB), R1
+	MOVW  R0, m_g0(R1)  // m0.g0 = newg
+	MOVW  R1, g_m(R0)   // newg.m = m0
+
+	MOVW  (g_stack+stack_hi)(R0), R1
+	MOVW  R1, PSP
+
+	MOVW  g, R2
+	MOVW  R0, g
+
+	// fix cpu0.gh, cpu0.mh
+
+	ADD   $cpuctx_mh, R2, R1  // R2 points to cpu0 (and to cpu0.gh at the same time)
+	MOVW  R2, m_g0(R1)        // cpu0.mh.g0 = cpu0.gh
+	MOVW  R2, m_gsignal(R1)   // cpu0.mh.gsignal = cpu0.gh (to easily check for handler mode)
+	MOVW  R1, g_m(R2)         // cpu0.gh.m = cpu0.mh
+
+	// leave the main stack and the privileged mode
+	DSB
+	MOVW  CONTROL, R0
+	ORR   $2, R0  // use PSP as stack pointer
+	MOVW  R0, CONTROL
+	ISB
+	ORR   $1, R0  // go to unprivileged mode
+	MOVW  R0, CONTROL
+	ISB
+
+	// create a new goroutine to start program
+	MOVW       $0, R0
+	MOVW       $0, R1
+	MOVW       $runtime·mainPC(SB), R2
+	MOVM.DB.W  [R0-R2], (R13)
+	BL         runtime·newproc(SB)
+	ADD        $12, R13
+
+	// start this M
+	BL  runtime·mstart(SB)
+
+	UNDEF  // fail
diff --git a/src/runtime/runtime2.go b/src/runtime/runtime2.go
index 9a032d8658..7a2c1307db 100644
--- a/src/runtime/runtime2.go
+++ b/src/runtime/runtime2.go
@@ -558,6 +558,9 @@ type m struct {
 	vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call)
 	vdsoPC uintptr // PC for traceback while in VDSO call
 
+	mOS
+	mqkey uintptr // to allow run mq tests, TODO: move to mOS
+
 	// preemptGen counts the number of completed preemption
 	// signals. This is used to detect when a preemption is
 	// requested, but fails. Accessed atomically.
@@ -569,8 +572,6 @@ type m struct {
 
 	dlogPerM
 
-	mOS
-
 	// Up to 10 locks held by this m, maintained by the lock ranking code.
 	locksHeldLen int
 	locksHeld    [10]heldLockInfo
@@ -589,7 +590,7 @@ type p struct {
 	raceprocctx uintptr
 
 	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
-	deferpoolbuf [5][32]*_defer
+	deferpoolbuf [5][32 / noosScaleDown]*_defer
 
 	// Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.
 	goidcache    uint64
@@ -598,7 +599,7 @@ type p struct {
 	// Queue of runnable goroutines. Accessed without lock.
 	runqhead uint32
 	runqtail uint32
-	runq     [256]guintptr
+	runq     [256 / noosScaleDown]guintptr
 	// runnext, if non-nil, is a runnable G that was ready'd by
 	// the current G and should be run next instead of what's in
 	// runq if there's time remaining in the running G's time
@@ -617,7 +618,7 @@ type p struct {
 	}
 
 	sudogcache []*sudog
-	sudogbuf   [128]*sudog
+	sudogbuf   [128 / noosScaleDown]*sudog
 
 	// Cache of mspan objects from the heap.
 	mspancache struct {
@@ -627,7 +628,7 @@ type p struct {
 		// slice updates is tricky, moreso than just managing the length
 		// ourselves.
 		len int
-		buf [128]*mspan
+		buf [128 / noosScaleDown]*mspan
 	}
 
 	tracebuf traceBufPtr
diff --git a/src/runtime/sema.go b/src/runtime/sema.go
index f94c1aa891..c076f2ed85 100644
--- a/src/runtime/sema.go
+++ b/src/runtime/sema.go
@@ -44,7 +44,7 @@ type semaRoot struct {
 }
 
 // Prime to not correlate with any user patterns.
-const semTabSize = 251
+const semTabSize = 251*_OS + noosSemTabSize
 
 var semtable [semTabSize]struct {
 	root semaRoot
diff --git a/src/runtime/signal_linux_arm.go b/src/runtime/signal_linux_armt.go
similarity index 98%
rename from src/runtime/signal_linux_arm.go
rename to src/runtime/signal_linux_armt.go
index 876b505917..41da819863 100644
--- a/src/runtime/signal_linux_arm.go
+++ b/src/runtime/signal_linux_armt.go
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build linux
+// +build arm thumb
+
 package runtime
 
 import (
diff --git a/src/runtime/signal_thumb.go b/src/runtime/signal_thumb.go
new file mode 100644
index 0000000000..41f354347d
--- /dev/null
+++ b/src/runtime/signal_thumb.go
@@ -0,0 +1,80 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build linux
+
+package runtime
+
+import "unsafe"
+
+func dumpregs(c *sigctxt) {
+	print("trap    ", hex(c.trap()), "\n")
+	print("error   ", hex(c.error()), "\n")
+	print("oldmask ", hex(c.oldmask()), "\n")
+	print("r0      ", hex(c.r0()), "\n")
+	print("r1      ", hex(c.r1()), "\n")
+	print("r2      ", hex(c.r2()), "\n")
+	print("r3      ", hex(c.r3()), "\n")
+	print("r4      ", hex(c.r4()), "\n")
+	print("r5      ", hex(c.r5()), "\n")
+	print("r6      ", hex(c.r6()), "\n")
+	print("r7      ", hex(c.r7()), "\n")
+	print("r8      ", hex(c.r8()), "\n")
+	print("r9      ", hex(c.r9()), "\n")
+	print("r10     ", hex(c.r10()), "\n")
+	print("fp      ", hex(c.fp()), "\n")
+	print("ip      ", hex(c.ip()), "\n")
+	print("sp      ", hex(c.sp()), "\n")
+	print("lr      ", hex(c.lr()), "\n")
+	print("pc      ", hex(c.pc()), "\n")
+	print("cpsr    ", hex(c.cpsr()), "\n")
+	print("fault   ", hex(c.fault()), "\n")
+}
+
+//go:nosplit
+//go:nowritebarrierrec
+func (c *sigctxt) sigpc() uintptr { return uintptr(c.pc() | 1) }
+
+func (c *sigctxt) sigsp() uintptr { return uintptr(c.sp()) }
+func (c *sigctxt) siglr() uintptr { return uintptr(c.lr()) }
+
+// preparePanic sets up the stack to look like a call to sigpanic.
+func (c *sigctxt) preparePanic(sig uint32, gp *g) {
+	// We arrange lr, and pc to pretend the panicking
+	// function calls sigpanic directly.
+	// Always save LR to stack so that panics in leaf
+	// functions are correctly handled. This smashes
+	// the stack frame but we're not going back there
+	// anyway.
+	sp := c.sp() - 4
+	c.set_sp(sp)
+	*(*uint32)(unsafe.Pointer(uintptr(sp))) = c.lr()
+
+	pc := gp.sigpc
+
+	if shouldPushSigpanic(gp, pc, uintptr(c.lr())) {
+		// Make it look the like faulting PC called sigpanic.
+		c.set_lr(uint32(pc))
+	}
+
+	// In case we are panicking from external C code
+	c.set_r10(uint32(uintptr(unsafe.Pointer(gp))))
+	c.set_pc(uint32(funcPC(sigpanic) &^ 1))
+}
+
+const pushCallSupported = false
+
+func (c *sigctxt) pushCall(targetPC, resumePC uintptr) {
+	// Push the LR to stack, as we'll clobber it in order to
+	// push the call. The function being pushed is responsible
+	// for restoring the LR and setting the SP back.
+	// This extra slot is known to gentraceback.
+	sp := c.sp() - 4
+	c.set_sp(sp)
+	*(*uint32)(unsafe.Pointer(uintptr(sp))) = c.lr()
+	// Set up PC and LR to pretend the function being signaled
+	// calls targetPC at the faulting PC.
+	c.set_lr(uint32(resumePC) | 1)
+	c.set_pc(uint32(targetPC) &^ 1)
+}
diff --git a/src/runtime/sizeclasses.go b/src/runtime/sizeclasses.go
index c5521ce1bd..f88cfad0eb 100644
--- a/src/runtime/sizeclasses.go
+++ b/src/runtime/sizeclasses.go
@@ -1,6 +1,8 @@
 // Code generated by mksizeclasses.go; DO NOT EDIT.
 //go:generate go run mksizeclasses.go
 
+// +build !noos
+
 package runtime
 
 // class  bytes/obj  bytes/span  objects  tail waste  max waste
diff --git a/src/runtime/sizeclasses_mcu.go b/src/runtime/sizeclasses_mcu.go
new file mode 100644
index 0000000000..790985d766
--- /dev/null
+++ b/src/runtime/sizeclasses_mcu.go
@@ -0,0 +1,49 @@
+// Code generated by mksizeclasses_mcu.go; DO NOT EDIT.
+//go:generate go run mksizeclasses_mcu.go
+
+// +build noos,thumb
+
+package runtime
+
+// class  bytes/obj  bytes/span  objects  tail waste  max waste
+//     1          8         256       32           0     87.50%
+//     2         16         256       16           0     43.75%
+//     3         32         256        8           0     46.88%
+//     4         48         256        5          16     35.55%
+//     5         64         256        4           0     23.44%
+//     6         80         256        3          16     23.83%
+//     7         96         512        5          32     20.90%
+//     8        128         256        2           0     24.22%
+//     9        144         768        5          48     16.02%
+//    10        160         512        3          32     15.04%
+//    11        192         768        4           0     16.15%
+//    12        208        1280        6          32      9.53%
+//    13        256         256        1           0     18.36%
+//    14        288        1280        4         128     19.69%
+//    15        320        1024        3          64     15.33%
+//    16        384         768        2           0     16.41%
+//    17        416        1280        3          32      9.77%
+//    18        512         512        1           0     18.55%
+
+const (
+	_MaxSmallSize   = 512
+	smallSizeDiv    = 8
+	smallSizeMax    = 256
+	largeSizeDiv    = 128
+	_NumSizeClasses = 19
+	_PageShift      = 8
+)
+
+var class_to_size = [_NumSizeClasses]uint16{0, 8, 16, 32, 48, 64, 80, 96, 128, 144, 160, 192, 208, 256, 288, 320, 384, 416, 512}
+var class_to_allocnpages = [_NumSizeClasses]uint8{0, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 3, 5, 1, 5, 4, 3, 5, 2}
+
+type divMagic struct {
+	shift    uint8
+	shift2   uint8
+	mul      uint16
+	baseMask uint16
+}
+
+var class_to_divmagic = [_NumSizeClasses]divMagic{{0, 0, 0, 0}, {3, 0, 1, 65528}, {4, 0, 1, 65520}, {5, 0, 1, 65504}, {4, 5, 11, 0}, {6, 0, 1, 65472}, {4, 6, 13, 0}, {5, 5, 11, 0}, {7, 0, 1, 65408}, {4, 8, 29, 0}, {5, 6, 13, 0}, {6, 5, 11, 0}, {4, 10, 79, 0}, {8, 0, 1, 65280}, {5, 8, 29, 0}, {6, 6, 13, 0}, {7, 3, 3, 0}, {5, 6, 5, 0}, {9, 0, 1, 65024}}
+var size_to_class8 = [smallSizeMax/smallSizeDiv + 1]uint8{0, 1, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10, 11, 11, 11, 11, 12, 12, 13, 13, 13, 13, 13, 13}
+var size_to_class128 = [(_MaxSmallSize-smallSizeMax)/largeSizeDiv + 1]uint8{13, 16, 18}
diff --git a/src/runtime/sizeclasses_mcu64.go b/src/runtime/sizeclasses_mcu64.go
new file mode 100644
index 0000000000..562da4934e
--- /dev/null
+++ b/src/runtime/sizeclasses_mcu64.go
@@ -0,0 +1,68 @@
+// Code generated by mksizeclasses_mcu64.go; DO NOT EDIT.
+//go:generate go run mksizeclasses_mcu64.go
+
+// +build noos,riscv64
+
+package runtime
+
+// class  bytes/obj  bytes/span  objects  tail waste  max waste
+//     1          8        2048      256           0     87.50%
+//     2         16        2048      128           0     43.75%
+//     3         32        2048       64           0     46.88%
+//     4         48        2048       42          32     32.32%
+//     5         64        2048       32           0     23.44%
+//     6         80        2048       25          48     20.65%
+//     7         96        2048       21          32     16.94%
+//     8        112        2048       18          32     14.75%
+//     9        128        2048       16           0     11.72%
+//    10        144        2048       14          32     11.82%
+//    11        160        2048       12         128     15.04%
+//    12        176        2048       11         112     13.53%
+//    13        192        2048       10         128     13.57%
+//    14        224        2048        9          32     15.19%
+//    15        256        2048        8           0     12.11%
+//    16        288        2048        7          32     12.16%
+//    17        320        2048        6         128     15.33%
+//    18        352        4096       11         224     13.79%
+//    19        384        2048        5         128     13.82%
+//    20        416        4096        9         352     15.41%
+//    21        512        2048        4           0     18.55%
+//    22        576        4096        7          64     12.33%
+//    23        640        2048        3         128     15.48%
+//    24        768        6144        8           0     16.54%
+//    25        768        4096        5         256      6.13%
+//    26        832        6144        7         320     12.39%
+//    27       1024        2048        2           0     18.65%
+//    28       1152        6144        5         384     16.59%
+//    29       1280        4096        3         256     15.55%
+//    30       1536        6144        4           0     16.60%
+//    31       1664       10240        6         256      9.94%
+//    32       2048        2048        1           0     18.70%
+//    33       2560       10240        4           0     19.96%
+//    34       2688        8192        3         128      6.21%
+//    35       3072        6144        2           0     12.47%
+//    36       3328       10240        3         256      9.97%
+//    37       4096        4096        1           0     18.73%
+
+const (
+	_MaxSmallSize   = 4096
+	smallSizeDiv    = 8
+	smallSizeMax    = 256
+	largeSizeDiv    = 128
+	_NumSizeClasses = 38
+	_PageShift      = 11
+)
+
+var class_to_size = [_NumSizeClasses]uint16{0, 8, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 224, 256, 288, 320, 352, 384, 416, 512, 576, 640, 768, 768, 832, 1024, 1152, 1280, 1536, 1664, 2048, 2560, 2688, 3072, 3328, 4096}
+var class_to_allocnpages = [_NumSizeClasses]uint8{0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 2, 3, 1, 3, 2, 3, 5, 1, 5, 4, 3, 5, 2}
+
+type divMagic struct {
+	shift    uint8
+	shift2   uint8
+	mul      uint16
+	baseMask uint16
+}
+
+var class_to_divmagic = [_NumSizeClasses]divMagic{{0, 0, 0, 0}, {3, 0, 1, 65528}, {4, 0, 1, 65520}, {5, 0, 1, 65504}, {4, 9, 171, 0}, {6, 0, 1, 65472}, {4, 9, 103, 0}, {5, 7, 43, 0}, {4, 10, 147, 0}, {7, 0, 1, 65408}, {4, 9, 57, 0}, {5, 9, 103, 0}, {4, 11, 187, 0}, {6, 7, 43, 0}, {5, 8, 37, 0}, {8, 0, 1, 65280}, {5, 9, 57, 0}, {6, 6, 13, 0}, {5, 11, 187, 0}, {7, 5, 11, 0}, {5, 10, 79, 0}, {9, 0, 1, 65024}, {6, 9, 57, 0}, {7, 6, 13, 0}, {8, 5, 11, 0}, {8, 5, 11, 0}, {6, 10, 79, 0}, {10, 0, 1, 64512}, {7, 8, 29, 0}, {8, 6, 13, 0}, {9, 5, 11, 0}, {7, 10, 79, 0}, {11, 0, 1, 63488}, {9, 6, 13, 0}, {7, 10, 49, 0}, {10, 3, 3, 0}, {8, 6, 5, 0}, {12, 0, 1, 61440}}
+var size_to_class8 = [smallSizeMax/smallSizeDiv + 1]uint8{0, 1, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15}
+var size_to_class128 = [(_MaxSmallSize-smallSizeMax)/largeSizeDiv + 1]uint8{15, 19, 21, 23, 24, 27, 27, 28, 29, 30, 30, 31, 32, 32, 32, 33, 33, 33, 33, 34, 35, 35, 35, 36, 36, 37, 37, 37, 37, 37, 37}
diff --git a/src/runtime/stack.go b/src/runtime/stack.go
index 7b9dce5393..fd20059593 100644
--- a/src/runtime/stack.go
+++ b/src/runtime/stack.go
@@ -66,10 +66,10 @@ const (
 	// to each stack below the usual guard area for OS-specific
 	// purposes like signal handling. Used on Windows, Plan 9,
 	// and iOS because they do not use a separate stack.
-	_StackSystem = sys.GoosWindows*512*sys.PtrSize + sys.GoosPlan9*512 + sys.GoosIos*sys.GoarchArm64*1024
+	_StackSystem = sys.GoosWindows*512*sys.PtrSize + sys.GoosPlan9*512 + sys.GoosIos*sys.GoarchArm64*1024 + noosStackSystem
 
 	// The minimum size of stack used by Go code
-	_StackMin = 2048
+	_StackMin = 2048*_OS + noosStackMin
 
 	// The minimum stack size to allocate.
 	// The hackery here rounds FixedStack0 up to a power of 2.
@@ -91,7 +91,7 @@ const (
 
 	// The stack guard is a pointer this many bytes above the
 	// bottom of the stack.
-	_StackGuard = 928*sys.StackGuardMultiplier + _StackSystem
+	_StackGuard = 928*sys.StackGuardMultiplier*_OS + noosStackGuard + _StackSystem
 
 	// After a stack split check the SP is allowed to be this
 	// many bytes below the stack guard. This saves an instruction
@@ -1142,7 +1142,7 @@ func shrinkstack(gp *g) {
 	// Check for self-shrinks while in a libcall. These may have
 	// pointers into the stack disguised as uintptrs, but these
 	// code paths should all be nosplit.
-	if gp == getg().m.curg && gp.m.libcallsp != 0 {
+	if !noos && gp == getg().m.curg && gp.m.libcallsp != 0 {
 		throw("shrinking stack in libcall")
 	}
 
diff --git a/src/runtime/stubs2.go b/src/runtime/stubs2.go
index 85088b3ab9..8f0f29d055 100644
--- a/src/runtime/stubs2.go
+++ b/src/runtime/stubs2.go
@@ -9,6 +9,7 @@
 // +build !plan9
 // +build !solaris
 // +build !windows
+// +build !noos
 
 package runtime
 
diff --git a/src/runtime/stubs_arm.go b/src/runtime/stubs_armt.go
similarity index 95%
rename from src/runtime/stubs_arm.go
rename to src/runtime/stubs_armt.go
index c13bf16de2..1181174589 100644
--- a/src/runtime/stubs_arm.go
+++ b/src/runtime/stubs_armt.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build arm thumb
+
 package runtime
 
 // Called from compiler-generated code; declared for go vet.
diff --git a/src/runtime/stubs_os.go b/src/runtime/stubs_os.go
new file mode 100644
index 0000000000..8bad021868
--- /dev/null
+++ b/src/runtime/stubs_os.go
@@ -0,0 +1,19 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package runtime
+
+func isr() bool {
+	return false
+}
+
+func sysReserveMaxArena() (addr, size uintptr) {
+	return 0, 0
+}
+
+func sysPersistentAlloc(size, align uintptr, sysStat *sysMemStat) *notInHeap {
+	return nil
+}
diff --git a/src/runtime/sys_arm.go b/src/runtime/sys_armt.go
similarity index 96%
rename from src/runtime/sys_arm.go
rename to src/runtime/sys_armt.go
index 730b9c918f..86118ab3e6 100644
--- a/src/runtime/sys_arm.go
+++ b/src/runtime/sys_armt.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build arm thumb
+
 package runtime
 
 import "unsafe"
diff --git a/src/runtime/sys_linux_thumb.s b/src/runtime/sys_linux_thumb.s
new file mode 100644
index 0000000000..910b8cb4ac
--- /dev/null
+++ b/src/runtime/sys_linux_thumb.s
@@ -0,0 +1,719 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+//
+// System calls and other sys.stuff for arm, Linux
+//
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+
+#define CLOCK_REALTIME 0
+#define CLOCK_MONOTONIC 1
+
+// for EABI, as we don't support OABI
+#define SYS_BASE 0x0
+
+#define SYS_exit (SYS_BASE + 1)
+#define SYS_read (SYS_BASE + 3)
+#define SYS_write (SYS_BASE + 4)
+#define SYS_open (SYS_BASE + 5)
+#define SYS_close (SYS_BASE + 6)
+#define SYS_getpid (SYS_BASE + 20)
+#define SYS_kill (SYS_BASE + 37)
+#define SYS_pipe (SYS_BASE + 42)
+#define SYS_clone (SYS_BASE + 120)
+#define SYS_rt_sigreturn (SYS_BASE + 173)
+#define SYS_rt_sigaction (SYS_BASE + 174)
+#define SYS_rt_sigprocmask (SYS_BASE + 175)
+#define SYS_sigaltstack (SYS_BASE + 186)
+#define SYS_mmap2 (SYS_BASE + 192)
+#define SYS_futex (SYS_BASE + 240)
+#define SYS_exit_group (SYS_BASE + 248)
+#define SYS_munmap (SYS_BASE + 91)
+#define SYS_madvise (SYS_BASE + 220)
+#define SYS_setitimer (SYS_BASE + 104)
+#define SYS_mincore (SYS_BASE + 219)
+#define SYS_gettid (SYS_BASE + 224)
+#define SYS_tgkill (SYS_BASE + 268)
+#define SYS_sched_yield (SYS_BASE + 158)
+#define SYS_nanosleep (SYS_BASE + 162)
+#define SYS_sched_getaffinity (SYS_BASE + 242)
+#define SYS_clock_gettime (SYS_BASE + 263)
+#define SYS_epoll_create (SYS_BASE + 250)
+#define SYS_epoll_ctl (SYS_BASE + 251)
+#define SYS_epoll_wait (SYS_BASE + 252)
+#define SYS_epoll_create1 (SYS_BASE + 357)
+#define SYS_pipe2 (SYS_BASE + 359)
+#define SYS_fcntl (SYS_BASE + 55)
+#define SYS_access (SYS_BASE + 33)
+#define SYS_connect (SYS_BASE + 283)
+#define SYS_socket (SYS_BASE + 281)
+#define SYS_brk (SYS_BASE + 45)
+
+#define ARM_BASE (SYS_BASE + 0x0f0000)
+
+TEXT runtime·open(SB),NOSPLIT,$0
+	MOVW     name+0(FP), R0
+	MOVW     mode+4(FP), R1
+	MOVW     perm+8(FP), R2
+	MOVW     $SYS_open, R7
+	SWI      $0
+	MOVW     $0xfffff001, R1
+	CMP      R1, R0
+	MOVW.HI  $-1, R0
+	MOVW     R0, ret+12(FP)
+	RET
+
+TEXT runtime·closefd(SB),NOSPLIT,$0
+	MOVW     fd+0(FP), R0
+	MOVW     $SYS_close, R7
+	SWI      $0
+	MOVW     $0xfffff001, R1
+	CMP      R1, R0
+	MOVW.HI  $-1, R0
+	MOVW     R0, ret+4(FP)
+	RET
+
+TEXT runtime·write1(SB),NOSPLIT,$0
+	MOVW  fd+0(FP), R0
+	MOVW  p+4(FP), R1
+	MOVW  n+8(FP), R2
+	MOVW  $SYS_write, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET
+
+TEXT runtime·read(SB),NOSPLIT,$0
+	MOVW  fd+0(FP), R0
+	MOVW  p+4(FP), R1
+	MOVW  n+8(FP), R2
+	MOVW  $SYS_read, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET
+
+// func pipe() (r, w int32, errno int32)
+TEXT runtime·pipe(SB),NOSPLIT,$0-12
+	MOVW  $r+0(FP), R0
+	MOVW  $SYS_pipe, R7
+	SWI   $0
+	MOVW  R0, errno+8(FP)
+	RET
+
+// func pipe2(flags int32) (r, w int32, errno int32)
+TEXT runtime·pipe2(SB),NOSPLIT,$0-16
+	MOVW  $r+4(FP), R0
+	MOVW  flags+0(FP), R1
+	MOVW  $SYS_pipe2, R7
+	SWI   $0
+	MOVW  R0, errno+12(FP)
+	RET
+
+TEXT runtime·exit(SB),NOSPLIT|NOFRAME,$0
+	MOVW  code+0(FP), R0
+	MOVW  $SYS_exit_group, R7
+	SWI   $0
+	MOVW  $1234, R0
+	MOVW  $1002, R1
+	MOVW  R0, (R1)  // fail hard
+
+TEXT exit1<>(SB),NOSPLIT|NOFRAME,$0
+	MOVW  code+0(FP), R0
+	MOVW  $SYS_exit, R7
+	SWI   $0
+	MOVW  $1234, R0
+	MOVW  $1003, R1
+	MOVW  R0, (R1)  // fail hard
+
+// func exitThread(wait *uint32)
+TEXT runtime·exitThread(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  wait+0(FP), R0
+	// We're done using the stack.
+	// Alas, there's no reliable way to make this write atomic
+	// without potentially using the stack. So it goes.
+	MOVW  $0, R1
+	MOVW  R1, (R0)
+	MOVW  $0, R0  // exit code
+	MOVW  $SYS_exit, R7
+	SWI   $0
+	MOVW  $1234, R0
+	MOVW  $1004, R1
+	MOVW  R0, (R1)  // fail hard
+	JMP   0(PC)
+
+TEXT runtime·gettid(SB),NOSPLIT,$0-4
+	MOVW  $SYS_gettid, R7
+	SWI   $0
+	MOVW  R0, ret+0(FP)
+	RET
+
+TEXT runtime·raise(SB),NOSPLIT|NOFRAME,$0
+	MOVW  $SYS_getpid, R7
+	SWI   $0
+	MOVW  R0, R4
+	MOVW  $SYS_gettid, R7
+	SWI   $0
+	MOVW  R0, R1         // arg 2 tid
+	MOVW  R4, R0         // arg 1 pid
+	MOVW  sig+0(FP), R2  // arg 3
+	MOVW  $SYS_tgkill, R7
+	SWI   $0
+	RET
+
+TEXT runtime·raiseproc(SB),NOSPLIT|NOFRAME,$0
+	MOVW  $SYS_getpid, R7
+	SWI   $0
+	// arg 1 tid already in R0 from getpid
+	MOVW  sig+0(FP), R1  // arg 2 - signal
+	MOVW  $SYS_kill, R7
+	SWI   $0
+	RET
+
+TEXT ·getpid(SB),NOSPLIT,$0-4
+	MOVW  $SYS_getpid, R7
+	SWI   $0
+	MOVW  R0, ret+0(FP)
+	RET
+
+TEXT ·tgkill(SB),NOSPLIT,$0-12
+	MOVW  tgid+0(FP), R0
+	MOVW  tid+4(FP), R1
+	MOVW  sig+8(FP), R2
+	MOVW  $SYS_tgkill, R7
+	SWI   $0
+	RET
+
+TEXT runtime·mmap(SB),NOSPLIT,$0
+	MOVW       addr+0(FP), R0
+	MOVW       n+4(FP), R1
+	MOVW       prot+8(FP), R2
+	MOVW       flags+12(FP), R3
+	MOVW       fd+16(FP), R4
+	MOVW       off+20(FP), R5
+	MOVW       $SYS_mmap2, R7
+	SWI        $0
+	MOVW       $0xfffff001, R6
+	MOVW       $0, R1
+	CMP        R6, R0
+	RSB.P.HI   $0, R0
+	MOVW.P.HI  R0, R1  // if error, put in R1
+	MOVW.HI    $0, R0
+	MOVW       R0, p+24(FP)
+	MOVW       R1, err+28(FP)
+	RET
+
+TEXT runtime·munmap(SB),NOSPLIT,$0
+	MOVW       addr+0(FP), R0
+	MOVW       n+4(FP), R1
+	MOVW       $SYS_munmap, R7
+	SWI        $0
+	MOVW       $0xfffff001, R6
+	CMP        R6, R0
+	MOVW.P.HI  $0, R8  // crash on syscall failure
+	MOVW.HI    R8, (R8)
+	RET
+
+TEXT runtime·madvise(SB),NOSPLIT,$0
+	MOVW  addr+0(FP), R0
+	MOVW  n+4(FP), R1
+	MOVW  flags+8(FP), R2
+	MOVW  $SYS_madvise, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET
+
+TEXT runtime·setitimer(SB),NOSPLIT,$0
+	MOVW  mode+0(FP), R0
+	MOVW  new+4(FP), R1
+	MOVW  old+8(FP), R2
+	MOVW  $SYS_setitimer, R7
+	SWI   $0
+	RET
+
+TEXT runtime·mincore(SB),NOSPLIT,$0
+	MOVW  addr+0(FP), R0
+	MOVW  n+4(FP), R1
+	MOVW  dst+8(FP), R2
+	MOVW  $SYS_mincore, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET
+
+TEXT runtime·walltime1(SB),NOSPLIT,$8-12
+	// We don't know how much stack space the VDSO code will need,
+	// so switch to g0.
+
+	// Save old SP. Use R13 instead of SP to avoid linker rewriting the offsets.
+	MOVW  R13, R4  // R4 is unchanged by C code.
+
+	MOVW  g_m(g), R5  // R5 is unchanged by C code.
+
+	// Set vdsoPC and vdsoSP for SIGPROF traceback.
+	// Save the old values on stack and restore them on exit,
+	// so this function is reentrant.
+	MOVW  m_vdsoPC(R5), R1
+	MOVW  m_vdsoSP(R5), R2
+	MOVW  R1, 4(R13)
+	MOVW  R2, 8(R13)
+
+	MOVW  LR, m_vdsoPC(R5)
+	MOVW  R13, m_vdsoSP(R5)
+
+	MOVW  m_curg(R5), R0
+
+	CMP   g, R0  // Only switch if on curg.
+	B.NE  noswitch
+
+	MOVW  m_g0(R5), R0
+	MOVW  (g_sched+gobuf_sp)(R0), R13  // Set SP to g0 stack
+
+noswitch:
+	SUB  $24, R13   // Space for results
+	BIC  $0x7, R13  // Align for C code
+
+	MOVW  $CLOCK_REALTIME, R0
+	MOVW  $8(R13), R1  // timespec
+	MOVW  runtime·vdsoClockgettimeSym(SB), R2
+	CMP   $0, R2
+	B.EQ  fallback
+
+	// Store g on gsignal's stack, so if we receive a signal
+	// during VDSO code we can find the g.
+	// If we don't have a signal stack, we won't receive signal,
+	// so don't bother saving g.
+	// When using cgo, we already saved g on TLS, also don't save
+	// g here.
+	// Also don't save g if we are already on the signal stack.
+	// We won't get a nested signal.
+	MOVB  runtime·iscgo(SB), R6
+	CMP   $0, R6
+	BNE   nosaveg
+	MOVW  m_gsignal(R5), R6  // g.m.gsignal
+	CMP   $0, R6
+	BEQ   nosaveg
+	CMP   g, R6
+	BEQ   nosaveg
+	MOVW  (g_stack+stack_lo)(R6), R6  // g.m.gsignal.stack.lo
+	MOVW  g, (R6)
+
+	BL  (R2)
+
+	MOVW  $0, R1
+	MOVW  R1, (R6)  // clear g slot, R6 is unchanged by C code
+
+	JMP  finish
+
+nosaveg:
+	BL   (R2)
+	JMP  finish
+
+fallback:
+	MOVW  $SYS_clock_gettime, R7
+	SWI   $0
+
+finish:
+	MOVW  8(R13), R0   // sec
+	MOVW  12(R13), R2  // nsec
+
+	MOVW  R4, R13  // Restore real SP
+	// Restore vdsoPC, vdsoSP
+	// We don't worry about being signaled between the two stores.
+	// If we are not in a signal handler, we'll restore vdsoSP to 0,
+	// and no one will care about vdsoPC. If we are in a signal handler,
+	// we cannot receive another signal.
+	MOVW  8(R13), R1
+	MOVW  R1, m_vdsoSP(R5)
+	MOVW  4(R13), R1
+	MOVW  R1, m_vdsoPC(R5)
+
+	MOVW  R0, sec_lo+0(FP)
+	MOVW  R1, sec_hi+4(FP)
+	MOVW  R2, nsec+8(FP)
+	RET
+
+// int64 nanotime1(void)
+TEXT runtime·nanotime1(SB),NOSPLIT,$8-8
+	// Switch to g0 stack. See comment above in runtime·walltime.
+
+	// Save old SP. Use R13 instead of SP to avoid linker rewriting the offsets.
+	MOVW  R13, R4  // R4 is unchanged by C code.
+
+	MOVW  g_m(g), R5  // R5 is unchanged by C code.
+
+	// Set vdsoPC and vdsoSP for SIGPROF traceback.
+	// Save the old values on stack and restore them on exit,
+	// so this function is reentrant.
+	MOVW  m_vdsoPC(R5), R1
+	MOVW  m_vdsoSP(R5), R2
+	MOVW  R1, 4(R13)
+	MOVW  R2, 8(R13)
+
+	MOVW  LR, m_vdsoPC(R5)
+	MOVW  R13, m_vdsoSP(R5)
+
+	MOVW  m_curg(R5), R0
+
+	CMP   g, R0  // Only switch if on curg.
+	B.NE  noswitch
+
+	MOVW  m_g0(R5), R0
+	MOVW  (g_sched+gobuf_sp)(R0), R13  // Set SP to g0 stack
+
+noswitch:
+	SUB  $24, R13   // Space for results
+	BIC  $0x7, R13  // Align for C code
+
+	MOVW  $CLOCK_MONOTONIC, R0
+	MOVW  $8(R13), R1  // timespec
+	MOVW  runtime·vdsoClockgettimeSym(SB), R2
+	CMP   $0, R2
+	B.EQ  fallback
+
+	// Store g on gsignal's stack, so if we receive a signal
+	// during VDSO code we can find the g.
+	// If we don't have a signal stack, we won't receive signal,
+	// so don't bother saving g.
+	// When using cgo, we already saved g on TLS, also don't save
+	// g here.
+	// Also don't save g if we are already on the signal stack.
+	// We won't get a nested signal.
+	MOVB  runtime·iscgo(SB), R6
+	CMP   $0, R6
+	BNE   nosaveg
+	MOVW  m_gsignal(R5), R6  // g.m.gsignal
+	CMP   $0, R6
+	BEQ   nosaveg
+	CMP   g, R6
+	BEQ   nosaveg
+	MOVW  (g_stack+stack_lo)(R6), R6  // g.m.gsignal.stack.lo
+	MOVW  g, (R6)
+
+	BL  (R2)
+
+	MOVW  $0, R1
+	MOVW  R1, (R6)  // clear g slot, R6 is unchanged by C code
+
+	JMP  finish
+
+nosaveg:
+	BL   (R2)
+	JMP  finish
+
+fallback:
+	MOVW  $SYS_clock_gettime, R7
+	SWI   $0
+
+finish:
+	MOVW  8(R13), R0   // sec
+	MOVW  12(R13), R2  // nsec
+
+	MOVW  R4, R13  // Restore real SP
+	// Restore vdsoPC, vdsoSP
+	// We don't worry about being signaled between the two stores.
+	// If we are not in a signal handler, we'll restore vdsoSP to 0,
+	// and no one will care about vdsoPC. If we are in a signal handler,
+	// we cannot receive another signal.
+	MOVW  8(R13), R4
+	MOVW  R4, m_vdsoSP(R5)
+	MOVW  4(R13), R4
+	MOVW  R4, m_vdsoPC(R5)
+
+	MOVW   $1000000000, R3
+	MULLU  R0, R3, (R1, R0)
+	ADD.S  R2, R0
+	ADC    R4, R1
+
+	MOVW  R0, ret_lo+0(FP)
+	MOVW  R1, ret_hi+4(FP)
+	RET
+
+// int32 futex(int32 *uaddr, int32 op, int32 val,
+//	struct timespec *timeout, int32 *uaddr2, int32 val2);
+TEXT runtime·futex(SB),NOSPLIT,$0
+	MOVW  addr+0(FP), R0
+	MOVW  op+4(FP), R1
+	MOVW  val+8(FP), R2
+	MOVW  ts+12(FP), R3
+	MOVW  addr2+16(FP), R4
+	MOVW  val3+20(FP), R5
+	MOVW  $SYS_futex, R7
+	SWI   $0
+	MOVW  R0, ret+24(FP)
+	RET
+
+// int32 clone(int32 flags, void *stack, M *mp, G *gp, void (*fn)(void));
+TEXT runtime·clone(SB),NOSPLIT,$0
+	MOVW  flags+0(FP), R0
+	MOVW  stk+4(FP), R1
+	MOVW  $0, R2  // parent tid ptr
+	MOVW  $0, R3  // tls_val
+	MOVW  $0, R4  // child tid ptr
+	MOVW  $0, R5
+
+	// Copy mp, gp, fn off parent stack for use by child.
+	MOVW  $-16(R1), R1
+	MOVW  mp+8(FP), R6
+	MOVW  R6, 0(R1)
+	MOVW  gp+12(FP), R6
+	MOVW  R6, 4(R1)
+	MOVW  fn+16(FP), R6
+	MOVW  R6, 8(R1)
+	MOVW  $1234, R6
+	MOVW  R6, 12(R1)
+
+	MOVW  $SYS_clone, R7
+	SWI   $0
+
+	// In parent, return.
+	CMP   $0, R0
+	BEQ   3(PC)
+	MOVW  R0, ret+20(FP)
+	RET
+
+	// Paranoia: check that SP is as we expect. Use R13 to avoid linker 'fixup'
+	NOP R13  // tell vet SP/R13 changed - stop checking offsets
+	MOVW  12(R13), R0
+	MOVW  $1234, R1
+	CMP   R0, R1
+	BEQ   2(PC)
+	BL    runtime·abort(SB)
+
+	MOVW  0(R13), R8  // m
+	MOVW  4(R13), R0  // g
+
+	CMP  $0, R8
+	BEQ  nog
+	CMP  $0, R0
+	BEQ  nog
+
+	MOVW  R0, g
+	MOVW  R8, g_m(g)
+
+	// paranoia; check they are not nil
+	MOVW  0(R8), R0
+	MOVW  0(g), R0
+
+	BL  runtime·emptyfunc(SB)  // fault if stack check is wrong
+
+	// Initialize m->procid to Linux tid
+	MOVW  $SYS_gettid, R7
+	SWI   $0
+	MOVW  g_m(g), R8
+	MOVW  R0, m_procid(R8)
+
+nog:
+	// Call fn
+	MOVW  8(R13), R0
+	MOVW  $16(R13), R13
+	BL    (R0)
+
+	// It shouldn't return. If it does, exit that thread.
+	SUB   $16, R13  // restore the stack pointer to avoid memory corruption
+	MOVW  $0, R0
+	MOVW  R0, 4(R13)
+	BL    exit1<>(SB)
+
+	MOVW  $1234, R0
+	MOVW  $1005, R1
+	MOVW  R0, (R1)
+
+TEXT runtime·sigaltstack(SB),NOSPLIT,$0
+	MOVW       new+0(FP), R0
+	MOVW       old+4(FP), R1
+	MOVW       $SYS_sigaltstack, R7
+	SWI        $0
+	MOVW       $0xfffff001, R6
+	CMP        R6, R0
+	MOVW.P.HI  $0, R8  // crash on syscall failure
+	MOVW.HI    R8, (R8)
+	RET
+
+TEXT runtime·sigfwd(SB),NOSPLIT,$0-16
+	MOVW  sig+4(FP), R0
+	MOVW  info+8(FP), R1
+	MOVW  ctx+12(FP), R2
+	MOVW  fn+0(FP), REGTMP
+	MOVW  R13, R4
+	SUB   $24, R13
+	BIC   $0x7, R13  // alignment for ELF ABI
+	BL    (REGTMP)
+	MOVW  R4, R13
+	RET
+
+TEXT runtime·sigtramp(SB),NOSPLIT,$0
+	// Reserve space for callee-save registers and arguments.
+	MOVM.DB.W  [R4-R11], (R13)
+	SUB        $16, R13
+
+	MOVW  R0, 4(R13)
+	MOVW  R1, 8(R13)
+	MOVW  R2, 12(R13)
+	MOVW  $runtime·sigtrampgo(SB), REGTMP
+	BL    (REGTMP)
+
+	// Restore callee-save registers.
+	ADD        $16, R13
+	MOVM.IA.W  (R13), [R4-R11]
+
+	RET
+
+TEXT runtime·cgoSigtramp(SB),NOSPLIT,$0
+	MOVW  $runtime·sigtramp(SB), REGTMP
+	B     (REGTMP)
+
+TEXT runtime·rtsigprocmask(SB),NOSPLIT,$0
+	MOVW  how+0(FP), R0
+	MOVW  new+4(FP), R1
+	MOVW  old+8(FP), R2
+	MOVW  size+12(FP), R3
+	MOVW  $SYS_rt_sigprocmask, R7
+	SWI   $0
+	RET
+
+TEXT runtime·rt_sigaction(SB),NOSPLIT,$0
+	MOVW  sig+0(FP), R0
+	MOVW  new+4(FP), R1
+	MOVW  old+8(FP), R2
+	MOVW  size+12(FP), R3
+	MOVW  $SYS_rt_sigaction, R7
+	SWI   $0
+	MOVW  R0, ret+16(FP)
+	RET
+
+TEXT runtime·usleep(SB),NOSPLIT,$12
+	MOVW  usec+0(FP), R0
+	CALL  runtime·usplitR0(SB)
+	MOVW  R0, 4(R13)
+	MOVW  $1000, R0  // usec to nsec
+	MUL   R0, R1
+	MOVW  R1, 8(R13)
+	MOVW  $4(R13), R0
+	MOVW  $0, R1
+	MOVW  $SYS_nanosleep, R7
+	SWI   $0
+	RET
+
+TEXT ·publicationBarrier(SB),NOSPLIT,$0
+	DMB  MB_ST
+	RET
+
+TEXT runtime·osyield(SB),NOSPLIT,$0
+	MOVW  $SYS_sched_yield, R7
+	SWI   $0
+	RET
+
+TEXT runtime·sched_getaffinity(SB),NOSPLIT,$0
+	MOVW  pid+0(FP), R0
+	MOVW  len+4(FP), R1
+	MOVW  buf+8(FP), R2
+	MOVW  $SYS_sched_getaffinity, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET
+
+// int32 runtime·epollcreate(int32 size)
+TEXT runtime·epollcreate(SB),NOSPLIT,$0
+	MOVW  size+0(FP), R0
+	MOVW  $SYS_epoll_create, R7
+	SWI   $0
+	MOVW  R0, ret+4(FP)
+	RET
+
+// int32 runtime·epollcreate1(int32 flags)
+TEXT runtime·epollcreate1(SB),NOSPLIT,$0
+	MOVW  flags+0(FP), R0
+	MOVW  $SYS_epoll_create1, R7
+	SWI   $0
+	MOVW  R0, ret+4(FP)
+	RET
+
+// func epollctl(epfd, op, fd int32, ev *epollEvent) int
+TEXT runtime·epollctl(SB),NOSPLIT,$0
+	MOVW  epfd+0(FP), R0
+	MOVW  op+4(FP), R1
+	MOVW  fd+8(FP), R2
+	MOVW  ev+12(FP), R3
+	MOVW  $SYS_epoll_ctl, R7
+	SWI   $0
+	MOVW  R0, ret+16(FP)
+	RET
+
+// int32 runtime·epollwait(int32 epfd, EpollEvent *ev, int32 nev, int32 timeout)
+TEXT runtime·epollwait(SB),NOSPLIT,$0
+	MOVW  epfd+0(FP), R0
+	MOVW  ev+4(FP), R1
+	MOVW  nev+8(FP), R2
+	MOVW  timeout+12(FP), R3
+	MOVW  $SYS_epoll_wait, R7
+	SWI   $0
+	MOVW  R0, ret+16(FP)
+	RET
+
+// void runtime·closeonexec(int32 fd)
+TEXT runtime·closeonexec(SB),NOSPLIT,$0
+	MOVW  fd+0(FP), R0  // fd
+	MOVW  $2, R1        // F_SETFD
+	MOVW  $1, R2        // FD_CLOEXEC
+	MOVW  $SYS_fcntl, R7
+	SWI   $0
+	RET
+
+// func runtime·setNonblock(fd int32)
+TEXT runtime·setNonblock(SB),NOSPLIT,$0-4
+	MOVW  fd+0(FP), R0  // fd
+	MOVW  $3, R1        // F_GETFL
+	MOVW  $0, R2
+	MOVW  $SYS_fcntl, R7
+	SWI   $0
+	ORR   $0x800, R0, R2  // O_NONBLOCK
+	MOVW  fd+0(FP), R0    // fd
+	MOVW  $4, R1          // F_SETFL
+	MOVW  $SYS_fcntl, R7
+	SWI   $0
+	RET
+
+// b __kuser_get_tls @ 0xffff0fe0
+TEXT runtime·read_tls_fallback(SB),NOSPLIT|NOFRAME,$0
+	MOVW  $0xffff0fe0, R0
+	B     (R0)
+
+TEXT runtime·access(SB),NOSPLIT,$0
+	MOVW  name+0(FP), R0
+	MOVW  mode+4(FP), R1
+	MOVW  $SYS_access, R7
+	SWI   $0
+	MOVW  R0, ret+8(FP)
+	RET
+
+TEXT runtime·connect(SB),NOSPLIT,$0
+	MOVW  fd+0(FP), R0
+	MOVW  addr+4(FP), R1
+	MOVW  len+8(FP), R2
+	MOVW  $SYS_connect, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET
+
+TEXT runtime·socket(SB),NOSPLIT,$0
+	MOVW  domain+0(FP), R0
+	MOVW  typ+4(FP), R1
+	MOVW  prot+8(FP), R2
+	MOVW  $SYS_socket, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET
+
+// func sbrk0() uintptr
+TEXT runtime·sbrk0(SB),NOSPLIT,$0-4
+	// Implemented as brk(NULL).
+	MOVW  $0, R0
+	MOVW  $SYS_brk, R7
+	SWI   $0
+	MOVW  R0, ret+0(FP)
+	RET
+
+TEXT runtime·sigreturn(SB),NOSPLIT,$0-0
+	RET
diff --git a/src/runtime/sys_noos_riscv64.s b/src/runtime/sys_noos_riscv64.s
new file mode 100644
index 0000000000..f3679b004e
--- /dev/null
+++ b/src/runtime/sys_noos_riscv64.s
@@ -0,0 +1,139 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+#include "syscall_noos.h"
+
+// if you add new syscall you must check sysMaxArgs in tasker_noos_riscv64.s
+
+// syscalls allowed for low priority interrupt handlers
+DATA runtime·syscalls+(SYS_nanotime*8)(SB)/8, $·sysnanotime(SB)
+DATA runtime·syscalls+(SYS_irqctl*8)(SB)/8, $·sysirqctl(SB)
+DATA runtime·syscalls+(SYS_setprivlevel*8)(SB)/8, $·syssetprivlevel(SB)
+DATA runtime·syscalls+(SYS_write*8)(SB)/8, $·syswrite(SB)
+
+// syscalls disallowed for low priority interrupt handlers
+DATA runtime·syscalls+(SYS_setsystim1*8)(SB)/8, $·syssetsystim1(SB)
+DATA runtime·syscalls+(SYS_setsyswriter1*8)(SB)/8, $·syssetsyswriter1(SB)
+DATA runtime·syscalls+(SYS_newosproc*8)(SB)/8, $·sysnewosproc(SB)
+DATA runtime·syscalls+(SYS_exitThread*8)(SB)/8, $·sysexitThread(SB)
+DATA runtime·syscalls+(SYS_futexsleep*8)(SB)/8, $·sysfutexsleep(SB)
+DATA runtime·syscalls+(SYS_futexwakeup*8)(SB)/8, $·sysfutexwakeup(SB)
+DATA runtime·syscalls+(SYS_osyield*8)(SB)/8, $·curcpuSchedule(SB)
+DATA runtime·syscalls+(SYS_nanosleep*8)(SB)/8, $·sysnanosleep(SB)
+
+GLOBL runtime·syscalls(SB), RODATA, $(SYS_NUM*8)
+
+// func nanotime() int64
+TEXT ·nanotime(SB),NOSPLIT|NOFRAME,$0-8
+	MOV  $SYS_nanotime, A3
+	MOV  $(0+8), A4
+	MOV  $8, A5
+	ECALL
+	RET
+
+// func cputicks() int64
+TEXT ·cputicks(SB),NOSPLIT|NOFRAME,$0-8
+	MOV  $SYS_nanotime, A3
+	MOV  $(0+8), A4
+	MOV  $8, A5
+	ECALL
+	RET
+
+// func irqctl(irq, ctl, ctxid int) (enabled, prio, errno int)
+TEXT ·irqctl(SB),NOSPLIT|NOFRAME,$0-48
+	MOV  $SYS_irqctl, A3
+	MOV  $(24+8), A4
+	MOV  $24, A5
+	ECALL
+	RET
+
+// func setprivlevel(newlevel int) (oldlevel, errno int)
+TEXT ·setprivlevel(SB),NOSPLIT|NOFRAME,$0-24
+	MOV  $SYS_setprivlevel, A3
+	MOV  $(8+8), A4
+	MOV  $16, A5
+	ECALL
+	RET
+
+// func write(fd uintptr, p unsafe.Pointer, n int32) int32
+TEXT ·write(SB),NOSPLIT|NOFRAME,$0-32
+	MOV  $SYS_write, A3
+	MOV  $(24+8), A4
+	MOV  $8, A5
+	ECALL
+	RET
+
+// func setsystim1()
+TEXT ·setsystim1(SB),NOSPLIT|NOFRAME,$0-0
+	MOV  $SYS_setsystim1, A3
+	MOV  $(0+8), A4
+	MOV  $0, A5
+	ECALL
+	RET
+
+// func setsyswriter1()
+TEXT ·setsyswriter1(SB),NOSPLIT|NOFRAME,$0-0
+	MOV  $SYS_setsyswriter1, A3
+	MOV  $(0+8), A4
+	MOV  $0, A5
+	ECALL
+	RET
+
+// func newosproc(mp *m)
+TEXT ·newosproc(SB),NOSPLIT|NOFRAME,$0-8
+	MOV  $SYS_newosproc, A3
+	MOV  $(8+8), A4
+	MOV  $0, A5
+	ECALL
+	RET
+
+// func exitThread(wait *uint32)
+TEXT ·exitThread(SB),NOSPLIT|NOFRAME,$0-8
+	MOV  $SYS_exitThread, A3
+	MOV  $(8+8), A4
+	MOV  $0, A5
+	ECALL
+	RET
+
+// func futexsleep(addr *uint32, val uint32, ns int64)
+TEXT ·futexsleep(SB),NOSPLIT|NOFRAME,$0-24
+	MOV  $SYS_futexsleep, A3
+	MOV  $(24+8), A4
+	MOV  $0, A5
+	ECALL
+	RET
+
+// func futexwakeup(addr *uint32, cnt uint32)
+TEXT ·futexwakeup(SB),NOSPLIT|NOFRAME,$0-16
+	MOV  $SYS_futexwakeup, A3
+	MOV  $(16+8), A4
+	MOV  $0, A5
+	ECALL
+	RET
+
+// func osyield()
+TEXT ·osyield(SB),NOSPLIT|NOFRAME,$0-0
+	MOV  $SYS_osyield, A3
+	MOV  $(0+8), A4
+	MOV  $0, A5
+	ECALL
+	RET
+
+// func nanosleep(ns int64)
+TEXT ·nanosleep(SB),NOSPLIT|NOFRAME,$0-8
+	MOV  $SYS_nanosleep, A3
+	MOV  $(8+8), A4
+	MOV  $0, A5
+	ECALL
+	RET
+
+// unsupported syscalls
+
+// func exit(r int32)
+TEXT ·exit(SB),NOSPLIT|NOFRAME,$0-8
+	EBREAK
+	JMP  -1(PC)
diff --git a/src/runtime/sys_noos_thumb.s b/src/runtime/sys_noos_thumb.s
new file mode 100644
index 0000000000..535d8d2b8b
--- /dev/null
+++ b/src/runtime/sys_noos_thumb.s
@@ -0,0 +1,140 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+#include "syscall_noos.h"
+
+// if you add new syscall you must check sysMaxArgs in tasker_noos_thumb.s
+
+// syscalls allowed for low priority interrupt handlers
+DATA runtime·syscalls+(SYS_nanotime*4)(SB)/4, $·sysnanotime(SB)
+DATA runtime·syscalls+(SYS_irqctl*4)(SB)/4, $·sysirqctl(SB)
+DATA runtime·syscalls+(SYS_setprivlevel*4)(SB)/4, $·syssetprivlevel(SB)
+DATA runtime·syscalls+(SYS_write*4)(SB)/4, $·syswrite(SB)
+
+// syscalls disallowed for low priority interrupt handlers
+DATA runtime·syscalls+(SYS_setsystim1*4)(SB)/4, $·syssetsystim1(SB)
+DATA runtime·syscalls+(SYS_setsyswriter1*4)(SB)/4, $·syssetsyswriter1(SB)
+DATA runtime·syscalls+(SYS_newosproc*4)(SB)/4, $·sysnewosproc(SB)
+DATA runtime·syscalls+(SYS_exitThread*4)(SB)/4, $·sysexitThread(SB)
+DATA runtime·syscalls+(SYS_futexsleep*4)(SB)/4, $·sysfutexsleep(SB)
+DATA runtime·syscalls+(SYS_futexwakeup*4)(SB)/4, $·sysfutexwakeup(SB)
+DATA runtime·syscalls+(SYS_osyield*4)(SB)/4, $·curcpuSchedule(SB)
+DATA runtime·syscalls+(SYS_nanosleep*4)(SB)/4, $·sysnanosleep(SB)
+
+GLOBL runtime·syscalls(SB), RODATA, $(SYS_NUM*4)
+
+// func nanotime() int64
+TEXT ·nanotime(SB),NOSPLIT|NOFRAME,$0-8
+	MOVW  $SYS_nanotime, R4
+	MOVW  $(0+4), R5
+	MOVW  $8, R6
+	SWI
+	RET
+
+// func irqctl(irq, ctl, ctxid int) (enabled, prio, errno int)
+TEXT ·irqctl(SB),NOSPLIT|NOFRAME,$0-24
+	MOVW  $SYS_irqctl, R4
+	MOVW  $(12+4), R5
+	MOVW  $12, R6
+	SWI
+	RET
+
+// func setprivlevel(newlevel int) (oldlevel, errno int)
+TEXT ·setprivlevel(SB),NOSPLIT|NOFRAME,$0-12
+	MOVW  $SYS_setprivlevel, R4
+	MOVW  $(4+4), R5
+	MOVW  $8, R6
+	SWI
+	RET
+
+// func write(fd uintptr, p unsafe.Pointer, n int32) int32
+TEXT ·write(SB),NOSPLIT|NOFRAME,$0-16
+	MOVW  $SYS_write, R4
+	MOVW  $(12+4), R5
+	MOVW  $4, R6
+	SWI
+	RET
+
+// func setsystim1()
+TEXT ·setsystim1(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $SYS_setsystim1, R4
+	MOVW  $(0+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+
+// func setsyswriter1()
+TEXT ·setsyswriter1(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $SYS_setsyswriter1, R4
+	MOVW  $(0+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+
+// func newosproc(mp *m)
+TEXT ·newosproc(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  $SYS_newosproc, R4
+	MOVW  $(4+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func exitThread(wait *uint32)
+TEXT ·exitThread(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  $SYS_exitThread, R4
+	MOVW  $(4+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func futexsleep(addr *uint32, val uint32, ns int64)
+TEXT ·futexsleep(SB),NOSPLIT|NOFRAME,$0-16
+	MOVW  $SYS_futexsleep, R4
+	MOVW  $(16+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func futexwakeup(addr *uint32, cnt uint32)
+TEXT ·futexwakeup(SB),NOSPLIT|NOFRAME,$0-8
+	MOVW  $SYS_futexwakeup, R4
+	MOVW  $(8+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func osyield()
+TEXT ·osyield(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $SYS_osyield, R4
+	MOVW  $(0+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func nanosleep(ns int64)
+TEXT ·nanosleep(SB),NOSPLIT|NOFRAME,$0-8
+	MOVW  $SYS_nanosleep, R4
+	MOVW  $(8+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// unsupported syscalls
+
+// func exit(r int32)
+TEXT ·exit(SB),NOSPLIT|NOFRAME,$0-4
+	BKPT
+	B   -1(PC)
+
+// utils
+
+// func publicationBarrier()
+TEXT ·publicationBarrier(SB),NOSPLIT|NOFRAME,$0-0
+	DMB  MB_ST
+	RET
diff --git a/src/runtime/syscall_noos.h b/src/runtime/syscall_noos.h
new file mode 100644
index 0000000000..9d367ea1bd
--- /dev/null
+++ b/src/runtime/syscall_noos.h
@@ -0,0 +1,26 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Some syscalls are alowed to be called by low priority interrupt handlers.
+//
+// Other syscalls, that can't run concurently with the scheduler, are available
+// only to threads.
+
+#define SYS_nanotime       0
+#define SYS_irqctl         1
+#define SYS_setprivlevel   2
+#define SYS_write          3
+
+#define SYS_LAST_FAST      3
+
+#define SYS_setsystim1     4
+#define SYS_setsyswriter1  5
+#define SYS_newosproc      6
+#define SYS_exitThread     7
+#define SYS_futexsleep     8
+#define SYS_futexwakeup    9
+#define SYS_osyield       10
+#define SYS_nanosleep     11
+
+#define SYS_NUM           12
diff --git a/src/runtime/tasker_noos.go b/src/runtime/tasker_noos.go
new file mode 100644
index 0000000000..b67a2a27c8
--- /dev/null
+++ b/src/runtime/tasker_noos.go
@@ -0,0 +1,520 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/atomic"
+	"unsafe"
+)
+
+// Tasker implements simple threads for noos target. It works directly on the m
+// type pointers obtained from the Go scheduler.
+//
+// Every CPU has two local lists of threads: the cpuctx.runnable queue and the
+// cpuctx.waitingt sorted list.
+//
+// The cpuctx.runnable queue contains runnable threads waiting for their
+// timeslots on the CPU.
+//
+// The cpuctx.waitingt list contains threads that sleep until some time in the
+// future, sorted by this time.
+//
+// Both lists are also accessed by other CPUs if they wakeup this CPU's thread
+// that sleeps on a futex. The other CPU can only remove from the
+// cpuctx.waitingt or add to the cpuctx.runnable.
+//
+// There is also tasker.waitingf, a global hash table of threads that sleep on
+// futexes.
+//
+// The sleeping thread can be found in the cpuctx.waitingt, in the
+// tasker.waitingf or in the both places.
+//
+// Tasker does not define new fields in m but reuses unused ones:
+//
+// - tls[0:4], ncgocall, thread: used by mq, msl, mcl types (see mq.go),
+// - tls[4:6] and some other unused fields are used by architecture-specific code
+//   (see: tasker_GOARCH.go),
+// - cgoCallersUse, caughtsig : used by tasker.
+//
+// Tasker relies on the following architecture-specific functions:
+//
+// curcpuSleep
+//
+// This functions is called to put the CPU to sleep. It is allowed it does
+// nothing.
+//
+// cpuctx.newwork
+//
+// This function is used to inform the another CPU that there is a new thread
+// added to its runnable queue. It should wake up the sleeping CPU or preempt
+// the currently running thread to run the scheduler. The thread preemption can
+// be set as delayed to allow a running thread to run for a minimum period of
+// time.
+//
+// curcpuSavectxCall, curcpuSavectxSched
+//
+// This function is called to save the remaining context, not saved at syscall
+// entry (eg. it can save FPU state).
+//
+// archnewm
+//
+// This function is called to create the inintial state of the new thread and
+// save it in provided m.
+//
+// curcpuSchedule
+//
+// Run scheduler immediately or at syscall exit. It's called only just before
+// syscall exit.
+//
+// The actual context switch is performed by architecture specific code at
+// curcpuRunScheduler exit. It should check the cpuctx.newexe variable and if
+// true switch the context to the new thread specified in cpuctx.exe.
+//
+// Tasker code does not use FPU so the architecture specific context switch
+// code can avoid saving/restoring FPU context if not need.
+
+func dummyNanotime() int64   { return 1 }
+func dummySetalarm(ns int64) {}
+
+var thetasker = tasker{
+	nanotime: dummyNanotime,
+	setalarm: dummySetalarm,
+	write:    defaultWrite,
+}
+
+const fbnum = 4 // number of futex hash table buckets, must be power of two
+
+type cpuctx struct {
+	gh       g               // for ISRs, must be the first field in this struct
+	t        *tasker         // points to thetasker
+	exe      muintptr        // m currently executed by CPU
+	newexe   bool            // for architecture-dependent code: exe changed
+	schedule bool            // for architecture-dependent code: run scheduler
+	runnable mq              // threads in runnable state
+	waitingt msl             // threads waiting until some time elapses
+	wakerq   [fbnum]notelist // futex wakeup request from interrupt handlers
+	mh       m               // for ISRs, mostly not written so works as cache line pad
+}
+
+// id returns CPU identifier. It must be a positive integer from 0 to the
+// number of CPUs/cores/harts.
+func (cpu *cpuctx) id() int { return int(cpu.gh.goid) }
+
+type tasker struct {
+	allcpu   []*cpuctx
+	waitingf [fbnum]mcl // threads waiting on futex
+	tidgen   uintptr
+
+	nanotime func() int64
+	setalarm func(ns int64)
+	write    func(fd int, p []byte) int
+	writemx  cpumtx
+
+	newnanotime func() int64               // see embedded/rtos.SetSystemTimer
+	newsetalarm func(ns int64)             // see embedded/rtos.SetSystemTimer
+	newwrite    func(fd int, p []byte) int // see embedded/rtos.SetSystemWriter
+}
+
+//go:nosplit
+func fhash(addr uintptr) int { return int(addr>>3) & (fbnum - 1) }
+
+//go:nosplit
+func (t *tasker) fbucketbyaddr(addr uintptr) *mcl {
+	return &t.waitingf[fhash(addr)]
+}
+
+// gh is the first field of the cpuctx so we can benefit from the getg which is
+// intrinsic function, often compiled to 0 or 1 instruction. Don't call in
+// thread mode (valid only in handler mode).
+//go:nosplit
+func curcpu() *cpuctx { return (*cpuctx)(unsafe.Pointer(getg())) }
+
+//go:nosplit
+func taskerSetrunnable(m *m) bool {
+	curcpu := curcpu()
+	allcpu := curcpu.t.allcpu
+	var (
+		bestcpu *cpuctx
+		bestn   int
+	)
+	p := m.nextp
+	if p != 0 {
+		goto byid
+	}
+	p = m.p
+	if p != 0 {
+		goto byid
+	}
+	p = m.oldp
+	if p != 0 {
+		goto byid
+	}
+	// naive search for the less loaded cpu
+	bestcpu = curcpu
+	bestn = bestcpu.runnable.atomicLen()
+	for _, cpu := range allcpu {
+		if n := cpu.runnable.atomicLen(); n < bestn {
+			bestcpu = cpu
+			bestn = n
+		}
+	}
+	goto end
+byid:
+	bestcpu = allcpu[int(p.ptr().id)%len(allcpu)]
+end:
+	bestcpu.runnable.lock()
+	n := bestcpu.runnable.n
+	bestcpu.runnable.push(m)
+	bestcpu.runnable.unlock()
+	if n != 0 {
+		return false
+	}
+	if bestcpu != curcpu {
+		bestcpu.newwork()
+		return false
+	}
+	return true
+}
+
+//go:nosplit
+func taskerFutexwakeup(fb *mcl, addr *uint32, cnt uint32) (schedule bool) {
+	for ; cnt != 0; cnt-- {
+		fb.lock()
+		m := fb.find(uintptr(unsafe.Pointer(addr)))
+		if m == nil {
+			fb.unlock()
+			break
+		}
+		owned := true
+		wt := mwt(m)
+		if wt != nil {
+			// this thread sleeps also in the cpuctx.waitingt
+			owned = atomic.Cas(mownedptr(m), 0, 1)
+		}
+		if owned {
+			fb.remove(m)
+		}
+		fb.unlock()
+		if owned {
+			if wt != nil {
+				wt.lock()
+				wt.remove(m)
+				wt.unlock()
+			}
+			schedule = schedule || taskerSetrunnable(m)
+		}
+	}
+	return
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func curcpuRunScheduler() {
+	curcpu := curcpu()
+	exe := curcpu.exe.ptr()
+	for {
+		// handle the wakeup requests from interrupt handlers
+		for i := range curcpu.wakerq {
+			n := curcpu.wakerq[i].removeall()
+			for n != nil {
+				next := n.release()
+				taskerFutexwakeup(&curcpu.t.waitingf[i], key32(&n.key), 1)
+				n = next
+			}
+		}
+
+		var nextschedt int64
+
+		// waking up the threads sleeping in the curcpu.waitingt
+		now := curcpu.t.nanotime()
+		wt := &curcpu.waitingt
+		for {
+			wt.lock()
+			m := wt.first()
+			if m == nil {
+				nextschedt = -1
+				break
+			}
+			nextschedt = mval(m)
+			if nextschedt > now {
+				break
+			}
+			owned := true
+			addr := mkey(m)
+			if addr != 0 {
+				// this thread sleeps also in the tasker.waitingf
+				owned = atomic.Cas(mownedptr(m), 0, 1)
+			}
+			if owned {
+				wt.remove(m)
+			}
+			wt.unlock()
+			if owned {
+				if addr != 0 {
+					fb := curcpu.t.fbucketbyaddr(addr)
+					fb.lock()
+					fb.remove(m)
+					fb.unlock()
+				}
+				taskerSetrunnable(m)
+			}
+		}
+		wt.unlock()
+
+		// schedule the next thread from the curcpu.runnable
+		curcpu.runnable.lock()
+		next := curcpu.runnable.pop()
+		if next != nil && exe != nil {
+			curcpuSavectxSched()
+			curcpu.runnable.push(exe)
+		}
+		n := curcpu.runnable.n
+		curcpu.runnable.unlock()
+
+		if n != 0 {
+			nextschedt = now + 2e6
+		}
+		curcpu.t.setalarm(nextschedt)
+
+		if next != nil {
+			curcpu.exe.set(next)
+			curcpu.newexe = true
+			return
+		}
+		if exe != nil {
+			return
+		}
+
+		// Nothing to execute. If this will be a work-stealing scheduler it will
+		// try to steal some work from other CPU here.
+		curcpuSleep()
+	}
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func rtos_notewakeup(n *notel) {
+	if !atomic.Cas(key32(&n.key), 0, 1) {
+		return
+	}
+	if !isr() {
+		futexwakeup(key32(&n.key), 1)
+		return
+	}
+	if n.acquire() {
+		curcpu().wakerq[fhash(uintptr(unsafe.Pointer(&n.key)))].insert(n)
+		curcpuWakeup()
+	}
+}
+
+// notelist
+
+// notel is a note that contains a link field to construct linked lists of notes
+type notel struct {
+	key  uintptr // must be the first field
+	link notelptr
+}
+
+//go:nosplit
+func (n *notel) acquire() bool {
+	return (&n.link).atomicCAS(0, 1)
+}
+
+//go:nosplit
+func (n *notel) release() *notel {
+	next := n.link
+	atomic.Storeuintptr((*uintptr)(&n.link), 0)
+	return next.ptr()
+}
+
+//go:nosplit
+func (n *notel) notelptr() notelptr { return notelptr(unsafe.Pointer(n)) }
+
+type notelptr uintptr
+
+//go:nosplit
+func (n notelptr) ptr() *notel { return (*notel)(unsafe.Pointer(n)) }
+
+//go:nosplit
+func (p *notelptr) atomicLoad() notelptr {
+	return notelptr(atomic.Loaduintptr((*uintptr)(p)))
+}
+
+//go:nosplit
+func (p *notelptr) atomicCAS(old, new notelptr) bool {
+	return atomic.Casuintptr((*uintptr)(p), uintptr(old), uintptr(new))
+}
+
+type notelist struct {
+	head notelptr
+}
+
+// insert inserts n at the beginning of l. You must acquire n before insert it.
+//go:nosplit
+func (l *notelist) insert(n *notel) {
+	if n.link != 1 {
+		for {
+			breakpoint()
+		}
+	}
+	for {
+		head := (&l.head).atomicLoad()
+		n.link = head
+		if (&l.head).atomicCAS(head, n.notelptr()) {
+			return
+		}
+	}
+}
+
+// removeall removes and returns the whole content of l.
+//go:nosplit
+func (l *notelist) removeall() *notel {
+	for {
+		head := (&l.head).atomicLoad()
+		if (&l.head).atomicCAS(head, 0) {
+			return head.ptr()
+		}
+	}
+}
+
+// syscall handlers
+
+//go:nowritebarrierrec
+//go:nosplit
+func syssetsystim1() {
+	t := curcpu().t
+	const n = unsafe.Sizeof(t.nanotime) / unsafe.Sizeof(uintptr(0))
+	// BUG: non-atomic writes
+	*(*[n]uintptr)(unsafe.Pointer(&t.nanotime)) = *(*[n]uintptr)(unsafe.Pointer(&t.newnanotime))
+	*(*[n]uintptr)(unsafe.Pointer(&t.setalarm)) = *(*[n]uintptr)(unsafe.Pointer(&t.newsetalarm))
+	curcpuSchedule() // ensure scheduler uses new timer: BUG(md): other CPUs?
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func syssetsyswriter1() {
+	t := curcpu().t
+	const n = unsafe.Sizeof(t.write) / unsafe.Sizeof(uintptr(0))
+	// BUG: non-atomic write
+	*(*[n]uintptr)(unsafe.Pointer(&t.write)) = *(*[n]uintptr)(unsafe.Pointer(&t.newwrite))
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysnanotime() int64 {
+	return curcpu().t.nanotime()
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysnewosproc(m *m) {
+	curcpu := curcpu()
+	m.procid = uint64(atomic.Xadduintptr(&curcpu.t.tidgen, 1))
+	archnewm(m)
+	if taskerSetrunnable(m) {
+		curcpuSchedule()
+	}
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysexitThread(wait *uint32) {
+	curcpu().exe = 0
+	*wait = 0
+	curcpuSchedule()
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysfutexsleep(addr *uint32, val uint32, ns int64) {
+	if uint64(ns) < 64 {
+		return // to short to sleep (64 ns selected arbitrary)
+	}
+	curcpu := curcpu()
+	m := curcpu.exe.ptr()
+	if ns >= 0 {
+		// pre-insert m into curcpu.waitingt, m is not visible for other CPUs
+		// until it is published in the thetasker.waitingf.
+		msetval(m, curcpu.t.nanotime()+ns)
+		msetowned(m, 0)
+		wt := &curcpu.waitingt
+		msetwt(m, wt)
+		wt.lock()
+		wt.insertbyval(m)
+		wt.unlock()
+	} else {
+		msetwt(m, nil)
+	}
+	fb := curcpu.t.fbucketbyaddr(uintptr(unsafe.Pointer(addr)))
+	fb.lock()
+	sleep := (*addr == val)
+	if sleep {
+		curcpuSavectxCall()
+		curcpu.exe = 0
+		msetkey(m, uintptr(unsafe.Pointer(addr)))
+		fb.push(m)
+	}
+	fb.unlock()
+	if sleep {
+		curcpuSchedule()
+	} else if ns >= 0 {
+		// revert the pre-insert
+		curcpu.waitingt.lock()
+		curcpu.waitingt.remove(m)
+		curcpu.waitingt.unlock()
+	}
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysfutexwakeup(addr *uint32, cnt uint32) {
+	fb := curcpu().t.fbucketbyaddr(uintptr(unsafe.Pointer(addr)))
+	if taskerFutexwakeup(fb, addr, cnt) {
+		curcpuSchedule()
+	}
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysnanosleep(ns int64) {
+	if uint64(ns) < 64 {
+		return // to short to sleep (64 ns selected arbitrary)
+	}
+	curcpuSavectxCall()
+	curcpu := curcpu()
+	m := curcpu.exe.ptr()
+	curcpu.exe = 0
+	msetkey(m, 0)
+	msetval(m, curcpu.t.nanotime()+ns)
+	wt := &curcpu.waitingt
+	wt.lock()
+	wt.insertbyval(m)
+	wt.unlock()
+	curcpuSchedule()
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func syswrite(fd uintptr, p unsafe.Pointer, n int32) int32 {
+	t := curcpu().t
+	t.writemx.lock()
+	n = int32(t.write(int(fd), (*[1 << 30]byte)(p)[:n]))
+	t.writemx.unlock()
+	return n
+}
+
+// m fields used
+
+//go:nosplit
+func mownedptr(m *m) *uint32 { return &m.cgoCallersUse }
+
+//go:nosplit
+func msetowned(m *m, owned uint32) { m.cgoCallersUse = owned }
+
+//go:nosplit
+func mwt(m *m) *msl { return (*msl)(unsafe.Pointer(m.caughtsig)) }
+
+//go:nosplit
+func msetwt(m *m, wt *msl) { m.caughtsig = guintptr(unsafe.Pointer(wt)) }
diff --git a/src/runtime/tasker_noos_riscv64.go b/src/runtime/tasker_noos_riscv64.go
new file mode 100644
index 0000000000..2309a12288
--- /dev/null
+++ b/src/runtime/tasker_noos_riscv64.go
@@ -0,0 +1,219 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"internal/cpu/riscv/clint"
+	"internal/cpu/riscv/plic"
+	"unsafe"
+)
+
+func cpuid() int
+func curcpuSavectxSched()
+func curcpuSleep()
+func curcpuWakeup()      {}
+func curcpuSavectxCall() {} // all registars saved on caller's stack
+
+//go:nosplit
+func (cpu *cpuctx) newwork() {
+	clint.CLINT().MSIP[cpu.id()].Store(1)
+}
+
+//go:nosplit
+func curcpuSchedule() { curcpu().schedule = true }
+
+const thrSmallCtx = 1 // context saved in mOS contains only LR, SP, g
+
+//go:nosplit
+func archnewm(m *m) {
+	m.tls[mstatus] = thrSmallCtx
+	m.tls[mepc] = funcPC(mstart)
+	m.x[numGPRS-3] = m.g0.stack.hi                 // SP
+	m.x[numGPRS-2] = uintptr(unsafe.Pointer(m.g0)) // g
+}
+
+const maxHarts = 2
+
+var (
+	harts  [maxHarts]cpuctx
+	pharts [maxHarts]*cpuctx
+)
+
+//go:nowritebarrierrec
+//go:nosplit
+func taskerinit() {
+	// only hart0 runs this function
+
+	uharts := (*[maxHarts]uintptr)(unsafe.Pointer(&pharts))
+	for i := range harts {
+		hart := &harts[i]
+		*(*uintptr)(unsafe.Pointer(&hart.t)) = uintptr(unsafe.Pointer(&thetasker))
+		uharts[i] = uintptr(unsafe.Pointer(hart))
+	}
+	allcpu := (*slice)(unsafe.Pointer(&thetasker.allcpu))
+	*(*uintptr)(unsafe.Pointer(&allcpu.array)) = uintptr(unsafe.Pointer(uharts))
+	allcpu.len = 1
+	allcpu.cap = maxHarts
+	curcpu().exe.set(getg().m)
+
+	// reset PLIC to known state
+	// BUG: tries to reset all possible interrupts and all possible contexts,
+	// can raise Store Access Fault exception
+	PLIC := plic.PLIC()
+	for ctxid := range PLIC.EN {
+		for i := range PLIC.EN[ctxid] {
+			PLIC.EN[ctxid][i].Store(0)
+		}
+		PLIC.TC[ctxid].THR.Store(0)
+	}
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func defaultWrite(fd int, p []byte) int { return len(p) }
+
+const (
+	// m.tls fields
+	mstatus = 4 // privilege level (MPP>>7), thrSmallCtx
+	mepc    = 5 // exception return address
+)
+
+const (
+	numGPRS = 31
+	numFPRS = 32 + 1 // include fcsr
+)
+
+type mOS struct {
+	x    [numGPRS]uintptr
+	fcsr uintptr
+	f    [numFPRS - 1]float64
+}
+
+func syssetprivlevel(newlevel int) (oldlevel, errno int)
+
+var plicmx cpumtx
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysirqctl(irq, ctl, ctxid int) (enabled, prio, errno int) {
+	PLIC := plic.PLIC()
+	if uint(irq) > uint(len(PLIC.PRIO)) {
+		errno = 4 // rtos.ErrBadIntNumber
+		return
+	}
+	if uint(ctxid) > uint(len(PLIC.EN)) {
+		errno = 6 // rtos.ErrBadIntCtx
+	}
+	// rtos package ensures valid ctl
+	if ctl >= 0 {
+		PLIC.PRIO[irq].Store(uint32(ctl))
+	}
+	rn, bn := irq>>5, irq&31
+	switch {
+	case ctl >= -1:
+		plicmx.lock()
+		PLIC.EN[ctxid][rn].SetBit(bn)
+		plicmx.unlock()
+	case ctl == -2:
+		plicmx.lock()
+		PLIC.EN[ctxid][rn].ClearBit(bn)
+		plicmx.unlock()
+	default:
+		enabled = int(PLIC.EN[ctxid][rn].Load()) >> bn & 1
+		prio = int(PLIC.PRIO[irq].Load())
+	}
+	return
+}
+
+var exceptionNames = [...]string{
+	0:  "instruction address misaligned",
+	1:  "instruction access fault",
+	2:  "illegal instruction",
+	3:  "breakpoint",
+	4:  "load address misaligned",
+	5:  "load access fault",
+	6:  "store/AMO address misaligned",
+	7:  "store/AMO access fault",
+	8:  "environment call from U-mode",
+	9:  "environment call from S-mode",
+	10: "reserved",
+	11: "environment call from M-mode",
+	12: "instruction page fault",
+	13: "load page fault",
+	14: "reserved",
+	15: "store/AMO page fault",
+}
+
+func printReg(name string, u uintptr, end string) {
+	const dig = "0123456789abcdef"
+	var buf [19]byte
+	buf[0] = ' '
+	buf[1] = '0'
+	buf[2] = 'x'
+	for i := 0; i < 16; i++ {
+		buf[18-i] = dig[u&15]
+		u >>= 4
+	}
+	print(name, string(buf[:]), end)
+}
+
+// keep lr, a0, mstatus, mepc, mie and rs order in sync with asm_riscv64.h
+//go:nosplit
+func fatalException(rs [numGPRS - 4]uintptr, mcause int, sp, lr, a0, mstatus, mepc, mie uintptr) {
+	mode := "handler"
+	gp := getg()
+	g := gp
+	if mepc&1 != 0 {
+		mode = "thread"
+		g = gp.sched.g.ptr()
+		sp = gp.sched.sp
+		mepc &^= 1
+	}
+	print("\n\nfatal exception in ", mode, " mode at ", hex(mepc), ": ")
+	if mcause < len(exceptionNames) {
+		print(exceptionNames[mcause])
+	} else {
+		print("reserved (", mcause, ")")
+	}
+	print("\n\n")
+
+	/*
+		print("ZERO                  \t")
+		printReg("LR ", lr, "\n")
+		printReg("SP ", sp, "\t")
+		printReg("GP ", rs[0], "\n")
+		printReg("g  ", uintptr(unsafe.Pointer(g)), "\t")
+		printReg("T0 ", rs[1], "\n")
+		printReg("T1 ", rs[2], "\t")
+		printReg("T2 ", rs[3], "\n")
+		printReg("S0 ", rs[4], "\t")
+		printReg("S1 ", rs[5], "\n")
+		printReg("A0 ", a0, "\t")
+		printReg("A1 ", rs[6], "\n")
+		printReg("A2 ", rs[7], "\t")
+		printReg("A3 ", rs[8], "\n")
+		printReg("A4 ", rs[9], "\t")
+		printReg("A5 ", rs[10], "\n")
+		printReg("A6 ", rs[11], "\t")
+		printReg("A7 ", rs[12], "\n")
+		printReg("S2 ", rs[13], "\t")
+		printReg("S3 ", rs[14], "\n")
+		printReg("S4 ", rs[15], "\t")
+		printReg("S5 ", rs[16], "\n")
+		printReg("S6 ", rs[17], "\t")
+		printReg("S7 ", rs[18], "\n")
+		printReg("S8 ", rs[19], "\t")
+		printReg("S9 ", rs[20], "\n")
+		printReg("S10", rs[21], "\t")
+		printReg("S11", rs[22], "\n")
+		printReg("T3 ", rs[23], "\t")
+		printReg("T4 ", rs[24], "\n")
+		printReg("T5 ", rs[25], "\t")
+		printReg("tmp", rs[26], "\n\n")
+	*/
+
+	traceback1(mepc, sp, lr, g, _TraceTrap)
+	crash()
+}
diff --git a/src/runtime/tasker_noos_riscv64.s b/src/runtime/tasker_noos_riscv64.s
new file mode 100644
index 0000000000..04e05d20a4
--- /dev/null
+++ b/src/runtime/tasker_noos_riscv64.s
@@ -0,0 +1,535 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+#include "syscall_noos.h"
+#include "asm_riscv64.h"
+
+
+// func cpuid() int
+TEXT ·cpuid(SB),NOSPLIT|NOFRAME,$0
+	CSRR  (mhartid, s0)
+	MOV   S0, ret+0(FP)
+	RET
+
+#define userPriv 0
+#define supePriv 1
+#define resePriv 2
+#define machPriv 3
+
+#define softInt 0
+#define timeInt 4
+#define exteInt 8
+
+DATA runtime·interruptHandlers+(softInt+userPriv)*8(SB)/8, $·defaultInterruptHandler(SB)
+DATA runtime·interruptHandlers+(softInt+supePriv)*8(SB)/8, $·defaultInterruptHandler(SB)
+DATA runtime·interruptHandlers+(softInt+resePriv)*8(SB)/8, $·defaultInterruptHandler(SB)
+DATA runtime·interruptHandlers+(softInt+machPriv)*8(SB)/8, $·enterScheduler(SB)
+DATA runtime·interruptHandlers+(timeInt+userPriv)*8(SB)/8, $·defaultInterruptHandler(SB)
+DATA runtime·interruptHandlers+(timeInt+supePriv)*8(SB)/8, $·defaultInterruptHandler(SB)
+DATA runtime·interruptHandlers+(timeInt+resePriv)*8(SB)/8, $·defaultInterruptHandler(SB)
+DATA runtime·interruptHandlers+(timeInt+machPriv)*8(SB)/8, $·enterScheduler(SB)
+DATA runtime·interruptHandlers+(exteInt+userPriv)*8(SB)/8, $·defaultInterruptHandler(SB)
+DATA runtime·interruptHandlers+(exteInt+supePriv)*8(SB)/8, $·externalInterruptHandler(SB)
+DATA runtime·interruptHandlers+(exteInt+resePriv)*8(SB)/8, $·defaultInterruptHandler(SB)
+DATA runtime·interruptHandlers+(exteInt+machPriv)*8(SB)/8, $·externalInterruptHandler(SB)
+#define interruptHandlersSize ((1+exteInt+machPriv)*8)
+GLOBL runtime·interruptHandlers(SB), RODATA, $interruptHandlersSize
+
+DATA runtime·exceptionHandlers+(0*8)(SB)/8, $·defaultExceptionHandler(SB)
+DATA runtime·exceptionHandlers+(1*8)(SB)/8, $·defaultExceptionHandler(SB)
+DATA runtime·exceptionHandlers+(2*8)(SB)/8, $·defaultExceptionHandler(SB)
+DATA runtime·exceptionHandlers+(3*8)(SB)/8, $·defaultExceptionHandler(SB)
+DATA runtime·exceptionHandlers+(4*8)(SB)/8, $·defaultExceptionHandler(SB)
+DATA runtime·exceptionHandlers+(5*8)(SB)/8, $·defaultExceptionHandler(SB)
+DATA runtime·exceptionHandlers+(6*8)(SB)/8, $·defaultExceptionHandler(SB)
+DATA runtime·exceptionHandlers+(7*8)(SB)/8, $·defaultExceptionHandler(SB)
+DATA runtime·exceptionHandlers+(8*8)(SB)/8, $·environmentCallHandler(SB)
+DATA runtime·exceptionHandlers+(9*8)(SB)/8, $·environmentCallHandler(SB)
+DATA runtime·exceptionHandlers+(10*8)(SB)/8, $·environmentCallHandler(SB)
+DATA runtime·exceptionHandlers+(11*8)(SB)/8, $·environmentCallHandler(SB)
+#define exceptionHandlersSize (12*8)
+GLOBL runtime·exceptionHandlers(SB), RODATA, $exceptionHandlersSize
+
+
+#define sysMaxArgs (48+8)
+#define envCallFrameSize (sysMaxArgs+3*8)
+
+
+// The RISC-V Instruction Set Manual Volume II: Privileged Architecture defines
+// the following increasing interrupt priority order:
+//
+// UTI, USI, UEI, STI, SSI, SEI, MTI, MSI, MEI
+//
+// That's a hardware priority order conclusive in case of multiple simultaneous
+// pending interrupts provided that all are enabled in the mie register.
+//
+// The trapHandler supports nested interrupts and implements different
+// software order using mie register to mask lower priority interrupts:
+//
+// MTI, MSI - timer and software interrupts have the same, lowest priority,
+//            both used to enter or wakeup the scheduler,
+//
+// SEI, MEI - external interrupts have higher priority than MTI and MSI, they
+//            can preempt and wakeup the scheduler, MEI has higher priority
+//            than SEI.
+//
+// We don't support supervisor or user mode interrupts. The platform-specific
+// interrupts with id >= 16 (local interrupts) are probably supported (with
+// higher priority than MEI) but not tested.
+TEXT runtime·trapHandler(SB),NOSPLIT|NOFRAME,$0
+	// At this point the interrupts are globaly disabled (mstatus.MIE=0).
+	// We want to enable higher priority interrupts as soon as possible.
+	// Be carefult to don't clobber T6 (TMP) and A3-A5 (syscall args).
+
+	// mscratch contains &cpuctx if trap from thread mode, 0 for nested trap
+	CSRRW  (a0, mscratch, a0)  // swap A0 with cpuctx in mscratch
+
+	// setup g and SP for handler mode, save thread ones to the cpuctx.gh.sched
+	BEQ  ZERO, A0, nestedTrap
+	MOV  X2, (g_sched+gobuf_sp)(A0)
+	MOV  g, (g_sched+gobuf_g)(A0)
+	MOV  (g_stack+stack_hi)(A0), X2
+	MOV  A0, g
+nestedTrap:
+
+	// save trap context, free another register (LR)
+	ADD     $-trapCtxSize, X2
+	MOV     LR, _LR(X2)
+	SLTU    A0, ZERO, LR       // calculate fromThread flag
+	CSRRWI  (0, mscratch, a0)  // set mscratch=0
+	MOV     A0, _A0(X2)        // save original A0 content
+	CSRR    (mstatus, a0)
+	OR      $(1<<MPIEn), A0  // fix RISC-V <1.10 behavior if trap from user mode
+	MOV     A0, _mstatus(X2)
+	CSRR    (mepc, a0)
+	OR      LR, A0
+	MOV     A0, _mepc(X2)
+	// mie will be saved below
+
+	// mask same or lower priority interrupts (always mask MSI and MTI)
+	CSRR  (mcause, lr)
+	SRA   $63, LR, A0
+	AND   A0, LR  // interrupt: LR=mcause, exception: LR=0
+	MOV   $~1, A0
+	SLL   LR, A0      // only 6 lower bits of LR are used as shift amount
+	AND   $~0xFF, A0  // always mask software and timer interrupts
+	CSRR  (mie, lr)
+	AND   LR, A0
+	CSRW  (a0, mie)
+
+	// enable interrupts
+	CSRR   (mcause, a0)  // read mcause before enable interrupts
+	CSRSI  ((1<<MIEn), mstatus)
+
+	MOV  LR, _mie(X2)
+
+	// jump to the exception/interrupt handler passing mcause*8 in A0
+	BGE  A0, ZERO, handleException
+handleInterrupt:
+	SLL  $3, A0
+	MOV  $interruptHandlersSize, LR
+	BGE  A0, LR, unsupported
+	MOV  $·interruptHandlers(SB), LR
+	ADD  A0, LR
+	MOV  (LR), LR
+	JMP  (LR)
+handleException:
+	SLL  $3, A0
+	MOV  $exceptionHandlersSize, LR
+	BGE  A0, LR, unsupported
+	MOV  $·exceptionHandlers(SB), LR
+	ADD  A0, LR
+	MOV  (LR), LR
+	JMP  (LR)
+
+unsupported:
+	EBREAK
+	JMP  -1(PC)
+
+
+// enterScheduler is caled by timer interrupt or environment call trap
+TEXT runtime·enterScheduler(SB),NOSPLIT|NOFRAME,$0
+
+	// if cpuctx.schedule then context saved by environmentCallHandler
+	MOVB  (cpuctx_schedule)(g), A0
+	BNE   ZERO, A0, contextSaved
+
+	MOV        (cpuctx_exe)(g), A0
+	SAVE_GPRS  (A0, m_mOS)  // save most of GPRs
+
+	// save the remaining registers: LR, SP, g, status, mepc
+	MOV  _LR(X2), A1
+	MOV  _A0(X2), A2
+	MOV  _mstatus(X2), S0
+	SRL  $7, S0  // MPP field is in a very unfortunate place
+	AND  $(3<<(MPPn-7)), S0
+	MOV  _mepc(X2), S1
+	MOV  (g_sched+gobuf_sp)(g), A3
+	MOV  (g_sched+gobuf_g)(g), A4
+	MOV  A1, (m_mOS+(const_numGPRS-4)*8)(A0)  // LR
+	MOV  A3, (m_mOS+(const_numGPRS-3)*8)(A0)  // SP
+	MOV  A4, (m_mOS+(const_numGPRS-2)*8)(A0)  // g
+	MOV  A2, (m_mOS+(const_numGPRS-1)*8)(A0)  // A0
+	MOV  S0, (m_tls+const_mstatus*8)(A0)
+	MOV  S1, (m_tls+const_mepc*8)(A0)
+
+contextSaved:
+	MOVB  ZERO, (cpuctx_schedule)(g)
+
+	// clear MSI and MTI
+	MOV   $msip, A0
+	CSRR  (mhartid, s0)
+	SLL   $2, S0
+	ADD   S0, A0
+	MOVW  ZERO, (A0)
+	MOV   $mtimecmp, A0
+	SLL   $1, S0
+	ADD   S0, A0
+	MOV   $-1, S0
+	MOV   S0, (A0)
+
+	FENCE  // ensure clearing happens before checking nanotime and futexes
+
+	// enter scheduler
+	CALL  ·curcpuRunScheduler(SB)
+
+	MOV  (cpuctx_exe)(g), A0  // load cpuctx.exe
+
+	// check context size
+	MOV  (m_tls+const_mstatus*8)(A0), S0
+	AND  $const_thrSmallCtx, S0
+	BNE  ZERO, S0, smallCtx
+	// no need to restore FPRs if exe didn't changed
+	MOVB          (cpuctx_newexe)(g), A1
+	BEQ           ZERO, A1, 2(PC)
+	RESTORE_FPRS  (A0, m_mOS+const_numGPRS*8)  // clobbers TMP
+	RESTORE_GPRS  (A0, m_mOS)                  // restore most of GPRs
+smallCtx:
+	MOVB  ZERO, (cpuctx_newexe)(g)  // clear cpuctx.newexe
+	SCW   (zero, zero, x2)          // invalidate possible dangling LR.x instruction by SC to the free word on top of the stack
+
+	// scheduler always returns to the thread mode
+
+	// restore mstatus
+	MOV   _mstatus(X2), LR
+	SRL   $7, LR
+	AND   $~(3<<(MPPn-7)), LR  // clear MPP field
+	SLL   $7, LR
+	CSRW  (lr, mstatus)  // disables interrupts
+
+	// restore remaining CSRs
+	CSRW  (G, mscratch)
+	MOV   (m_tls+const_mstatus*8)(A0), g  // load thread status
+	AND   $(3<<(MPPn-7)), g
+	SLL   $7, g
+	CSRS  (G, mstatus)  // set priority field
+	MOV   (m_tls+const_mepc*8)(A0), g
+	CSRW  (G, mepc)
+	MOV   _mie(X2), g
+	CSRW  (G, mie)
+
+	// restore remaining GPRs
+	MOV  (m_mOS+(const_numGPRS-4)*8)(A0), LR
+	MOV  (m_mOS+(const_numGPRS-3)*8)(A0), X2
+	MOV  (m_mOS+(const_numGPRS-2)*8)(A0), g
+	MOV  (m_mOS+(const_numGPRS-1)*8)(A0), A0
+
+	MRET
+
+
+TEXT runtime·externalInterruptHandler(SB),NOSPLIT|NOFRAME,$0
+	ADD        $-(const_numGPRS-4+const_numFPRS+3)*8, X2
+	SAVE_GPRS  (X2, 3*8)
+	SAVE_FPRS  (X2, (const_numGPRS-4+3)*8)
+
+	// BUG: the following code assumes two (M,S) PLIC contexts per hart
+	// ctxid = mhartid*2 + (11-mcause)/2
+	SRL   $3, A0  // mcause
+	CSRR  (mhartid, a1)
+	SLL   $2, A1
+	SUB   A0, A1   // mhartid*4 - mcause
+	SLL   $11, A1  // 0x1000*mhartid*2 - 0x1000*mcause/2
+	ADD   $(PLIC_TC+11*0x1000/2), A1
+	MOV   A1, (X2)
+	MOV   $1, S1
+	SLL   A0, S1  // 1<<mcause
+	MOVW  S1, 12(X2)
+	MOVW  (A1), A3    // PLIC.TC[ctxid].THR
+	MOVW  A3, 16(X2)  // save current priority threshold
+
+loop:
+	MOVW  4(A1), S0  // claim
+	BEQ   ZERO, S0, done
+	MOVW  S0, 8(X2)
+
+	// allow nested interrupts
+	MOV   $PLIC_BASE, A0
+	SLL   $2, S0, A2
+	ADD   A2, A0
+	MOVW  (A0), A2
+	MOVW  A2, (A1)  // PLIC.TC[ctxid].THR = PLIC.PRIO[claim]
+	FENCE
+	CSRS  (s1, mie)
+
+	// get interrupt vector
+	MOV  $runtime·vectors(SB), A0
+	MOV  (A0), S1
+	BGE  S0, S1, noHandler
+	SLL  $3, S0
+	ADD  S0, A0
+	MOV  (A0), A0
+
+	CALL  A0  // call user handler (IRQn_Handler)
+
+	MOV   (X2), A1
+	MOV   8(X2), S0
+	SRL   $32, S0, S1
+	CSRC  (s1, mie)  // disallow nested interrupts
+	FENCE
+	MOVW  S0, 4(A1)  // complete
+
+	MOVW  16(X2), A3      // saved priority threshold
+	BEQ   ZERO, A3, loop  // nested handler can handle only one interrupt
+
+done:
+	MOVW  A3, (A1)  // restore priority threshold
+
+	RESTORE_FPRS  (X2, (const_numGPRS-4+3)*8)
+	RESTORE_GPRS  (X2, 3*8)
+	ADD           $(const_numGPRS-4+const_numFPRS+3)*8, X2
+
+	MOV  _LR(X2), LR  // restore LR
+
+	// restore CSRs
+	MOV   _mstatus(X2), A0
+	CSRW  (a0, mstatus)  // disables interrupts
+	MOV   _mie(X2), A0
+	CSRW  (a0, mie)
+	MOV   _mepc(X2), A0
+	CSRW  (a0, mepc)
+	AND   $1, A0
+	BEQ   ZERO, A0, returnToHandler
+
+	// return to thread
+	MOV   _A0(X2), A0                // restore A0
+	MOV   (g_sched+gobuf_sp)(g), X2  // restore thread SP
+	CSRW  (G, mscratch)              // cpuctx to mscratch
+	MOV   (g_sched+gobuf_g)(g), g    // restore thread g
+	MRET
+
+returnToHandler:
+	MOV  _A0(X2), A0  // restore A0
+	ADD  $trapCtxSize, X2
+	MRET
+
+noHandler:
+	JMP  ·unhandledExternalInterrupt(SB)
+
+
+// System call is like oridnary function call so all registers are
+// caller save (Go ABI0). The tiny wrapper over ECALL instruction add
+// additional parameters in A3-A5 registers:
+//
+// A3: syscall number
+// A4: argument data size on the stack (+8 for caller return address)
+// A5: return data size on the stack
+TEXT runtime·environmentCallHandler(SB),NOSPLIT|NOFRAME,$0
+
+	// check the syscall number
+	MOV   $SYS_NUM, A0
+	BGEU  A3, A0, badSyscall
+	MOV   $SYS_LAST_FAST, S0
+
+	// determine the caller stack
+	MOV  _mepc(X2), S1
+	AND  $1, S1, A0  // fromThread flag
+	BEQ  ZERO, A0, currentStack
+
+	// saved stack (called from thread)
+	MOV   (g_sched+gobuf_sp)(g), A0
+	BGEU  S0, A3, continue  // fast syscall
+	// save thread context (small): LR, SP, g, thrSmallCtx+prio, mepc
+	MOV  (g_sched+gobuf_g)(g), A1
+	MOV  _LR(X2), A2
+	MOV  (cpuctx_exe)(g), S0
+	MOV  A2, (m_mOS+(const_numGPRS-4)*8)(S0)  // LR
+	MOV  A0, (m_mOS+(const_numGPRS-3)*8)(S0)  // SP
+	MOV  A1, (m_mOS+(const_numGPRS-2)*8)(S0)  // g
+	MOV  _mstatus(X2), A1
+	SRL  $7, A1  // MPP field is in a very unfortunate place
+	AND  $(3<<(MPPn-7)), A1
+	OR   $const_thrSmallCtx, A1
+	MOV  A1, (m_tls+const_mstatus*8)(S0)
+	ADD  $4, S1  // mepc points back to ECALL, adjust it
+	MOV  S1, (m_tls+const_mepc*8)(S0)
+	JMP  continue
+
+currentStack: // called from handler
+	BLTU  S0, A3, slowSyscallFromHandler  // handlers can use fast syscalls only
+	ADD   $trapCtxSize, X2, A0
+
+continue:
+	// make a space on the stack for arguments + 3 registers
+	ADD  $-envCallFrameSize, X2
+
+	// copy arguments from the caller's stack
+	MOV   $·duffcopy+2048(SB), A2
+	SLL   $1, A4
+	SUB   A4, A2
+	MOV   X2, A1
+	CALL  A2
+
+	// save data needed to copy the return values back to the caller's stack
+	MOV  A0, (sysMaxArgs+0*8)(X2)
+	MOV  A1, (sysMaxArgs+1*8)(X2)
+	MOV  A5, (sysMaxArgs+2*8)(X2)
+
+	// call the service routine
+	MOV   $·syscalls(SB), A0
+	SLL   $3, A3
+	ADD   A3, A0
+	MOV   (A0), A0
+	CALL  A0
+
+	// copy the return values back to the caller's stack
+	MOV   (sysMaxArgs+2*8)(X2), A4
+	BEQ   ZERO, A4, nothingToCopy
+	MOV   (sysMaxArgs+0*8)(X2), A1
+	MOV   (sysMaxArgs+1*8)(X2), A0
+	MOV   $·duffcopy+2048(SB), A2
+	SLL   $1, A4
+	SUB   A4, A2
+	CALL  A2
+nothingToCopy:
+	ADD  $envCallFrameSize, X2
+
+	// run the scheduler if the syscall wants it
+	MOVB  cpuctx_schedule(g), S0
+	BEQ   ZERO, S0, 2(PC)
+	JMP   ·enterScheduler(SB)
+
+	// pop everything from the stack
+	MOV  _LR(X2), LR
+	MOV  _mstatus(X2), A0
+	MOV  _mepc(X2), A1
+	ADD  $4, A1  // mepc points back to ECALL, adjust it
+	MOV  _mie(X2), A2
+	ADD  $trapCtxSize, X2
+
+	// disable interrupts and restore trap context
+	CSRW  (a0, mstatus)
+	CSRW  (a1, mepc)
+	CSRW  (a2, mie)
+
+	AND  $1, A1  // fromThread flag
+	BEQ  ZERO, A1, fromHandler
+
+	// restore thread g and SP
+	MOV   (g_sched+gobuf_sp)(g), X2
+	CSRW  (G, mscratch)
+	MOV   (g_sched+gobuf_g)(g), g
+fromHandler:
+
+	MRET
+
+badSyscall:
+	EBREAK  // bad syscall number
+	JMP     -1(PC)
+
+slowSyscallFromHandler:
+	EBREAK  // syscall not allowed in handler mode
+	JMP     -1(PC)
+
+
+TEXT runtime·defaultInterruptHandler(SB),NOSPLIT|NOFRAME,$0
+	EBREAK
+	JMP  -1(PC)
+
+
+TEXT runtime·defaultExceptionHandler(SB),NOSPLIT|NOFRAME,$0
+	ADD  $-16, X2
+	SRA  $3, A0
+	MOV  A0, (X2)
+	ADD  $16+trapCtxSize, X2, A0
+	MOV  A0, 8(X2)
+
+	ADD        $-(const_numGPRS-4+1)*8, X2
+	SAVE_GPRS  (X2, 8)
+	MOV        ZERO, (X2)
+
+	JMP  ·fatalException(SB)
+
+
+TEXT runtime·unhandledExternalInterrupt(SB),NOSPLIT|NOFRAME,$0
+	EBREAK
+	JMP  -1(PC)
+
+
+// curcpuSavectxSched saves floating-point registers to m.mOS
+TEXT ·curcpuSavectxSched(SB),NOSPLIT|NOFRAME,$0
+	MOV        (cpuctx_exe)(g), A0
+	SAVE_FPRS  (A0, m_mOS+const_numGPRS*8)
+	RET
+
+
+//#define WFI CSRR(mie, s1); CSRR(mip, a0); AND S1, A0; BEQ ZERO, A0, -2(PC)
+
+
+// func curcpuSleep()
+TEXT ·curcpuSleep(SB),NOSPLIT|NOFRAME,$0-0
+
+	// We want MSI, MTI to wake up the hart from WFI so they must be enabled in
+	// the mie register. Unfortunately, it means that we need to disable
+	// interrupts globally which adds a few cycles to the interrupt latency.
+	MOV    $(MSI+MTI), S0
+	CSRCI  ((1<<MIEn), mstatus)
+	CSRS   (s0, mie)
+	WFI
+	CSRC   (s0, mie)
+	CSRSI  ((1<<MIEn), mstatus)
+
+	// clear MSI, MTI
+	MOV   $msip, A0
+	CSRR  (mhartid, s0)
+	SLL   $2, S0
+	ADD   S0, A0
+	MOVW  ZERO, (A0)
+	MOV   $mtimecmp, A0
+	SLL   $1, S0
+	ADD   S0, A0
+	MOV   $-1, S0
+	MOV   S0, (A0)
+
+	RET
+
+
+// func syssetprivlevel(newlevel int) (oldlevel, errno int)
+TEXT ·syssetprivlevel(SB),NOSPLIT|NOFRAME,$0-24
+	MOV  newlevel+0(FP), A0
+	MOV  (envCallFrameSize+_mstatus)(X2), S0
+	SRL  $MPPn, S0, S1
+	AND  $3, S1
+	MOV  $3, A1
+	SUB  S1, A1, S1
+	MOV  S1, oldlevel+8(FP)
+
+	BLTU  A1, A0, badPrivLevel
+	SUB   A0, A1, S1
+	SLL   $MPPn, S1
+	SLL   $MPPn, A1
+	XOR   $-1, A1
+	AND   A1, S0
+	OR    S1, S0
+	MOV   S0, (envCallFrameSize+_mstatus)(X2)
+	MOV   ZERO, errno+16(FP)
+	RET
+badPrivLevel:
+	MOV  $0, S0
+	BLT  A0, ZERO, 2(PC)
+	MOV  $2, S0  // rtos.ErrBadPrivLevel
+	MOV  S0, errno+16(FP)
+	RET
diff --git a/src/runtime/tasker_noos_thumb.go b/src/runtime/tasker_noos_thumb.go
new file mode 100644
index 0000000000..0b7ff560f8
--- /dev/null
+++ b/src/runtime/tasker_noos_thumb.go
@@ -0,0 +1,284 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"embedded/mmio"
+	"internal/cpu/cortexm"
+	"internal/cpu/cortexm/debug/itm"
+	"internal/cpu/cortexm/mpu"
+	"internal/cpu/cortexm/nvic"
+	"internal/cpu/cortexm/scb"
+	"internal/cpu/cortexm/scid"
+	"unsafe"
+)
+
+// for now noos/thumb supports only single CPU
+
+func sev()
+func curcpuSleep()
+func curcpuSavectxSched()
+func curcpuSavectxCall() {} // all registars saved on caller's stack
+
+//go:nosplit
+func cpuid() int {
+	// for now only single CPU is supported (see also identcurcpu, osinit)
+	return 0
+}
+
+//go:nosplit
+func curcpuWakeup() { sev() } // see ARM Errata 563915, STM32F10xx Errata 1.1.2
+
+//go:nosplit
+func (cpu *cpuctx) newwork() {
+	scb.SCB().ICSR.Store(scb.PENDSVSET)
+	sev()
+}
+
+//go:nosplit
+func curcpuSchedule() {
+	// Can't run the scheduler directly in the system call because the SVCall
+	// has higher priority than some interrupts. If there is no any thread to
+	// run the scheduler will sleep in a loop (there is no idle threads) at
+	// SVCall priority level thereby blocking the lower priority interrupts.
+	//
+	// Instead, we trigger PendSV (which has priority lower that any other
+	// interrupt) to run the scheduler and rely on the exception tail-chaining
+	// to don't execute any instruction after SWI until the scheduler does its
+	// job.
+	//
+	// Caution! You can't rely on tail-chaining in case of debuging.
+	curcpu().schedule = true
+	scb.SCB().ICSR.Store(scb.PENDSVSET)
+}
+
+// ARMv7-M requires at least 4 byte stack alignment so there are two bits
+// in saved stack pointer that can be used by tasker. It uses it for:
+const (
+	thrPrivLevel = 1 << 0 // thread privilege level
+	thrSmallCtx  = 1 << 1 // context saved in m contains only g (R10) register
+)
+
+// archnewm setups m's stack
+//go:nosplit
+func archnewm(m *m) {
+	sp := m.g0.stack.hi - unsafe.Sizeof(cortexm.StackFrame{})
+	sf := (*cortexm.StackFrame)(unsafe.Pointer(sp))
+	sf.PSR = cortexm.T
+	sf.PC = funcPC(mstart)
+	m.tls[msp] = sp | thrSmallCtx // small ctx
+	m.tls[mer] = cortexm.ExcReturnBase | cortexm.ExcReturnNoFPU |
+		cortexm.ExcReturnPSP
+	m.libcall.fn = uintptr(unsafe.Pointer(m.g0))
+}
+
+var (
+	cpu0  cpuctx
+	pcpu0 = &cpu0
+)
+
+//go:nowritebarrierrec
+//go:nosplit
+func taskerinit() {
+	*(*uintptr)(unsafe.Pointer(&cpu0.t)) = uintptr(unsafe.Pointer(&thetasker))
+	cpu0.exe.set(getg().m)
+	allcpu := (*slice)(unsafe.Pointer(&thetasker.allcpu))
+	*(*uintptr)(unsafe.Pointer(&allcpu.array)) = uintptr(unsafe.Pointer(&pcpu0))
+	allcpu.len = 1
+	allcpu.cap = 1
+
+	// setup exception priority levels
+
+	SCB := scb.SCB()
+
+	// enable fault handlers
+	SCB.SHCSR.SetBits(scb.MEMFAULTENA | scb.BUSFAULTENA | scb.USGFAULTENA)
+
+	// division by zero will cause the UsageFault
+	SCB.CCR.SetBits(scb.DIV_0_TRP)
+
+	// set PendSV and SVCall priorities according to description in rtos package
+	SCB.SHPR2.StoreBits(scb.PRI_SVCall, (4<<5)<<scb.PRI_SVCalln)
+	SCB.SHPR3.StoreBits(scb.PRI_PendSV, 255<<scb.PRI_PendSVn)
+
+	// All other exceptions/interrupts by default have the highest priority.
+
+	// use MPU if available to catch nil pointer dereferences (need 4 regions)
+	if _, d, _ := mpu.Type(); d >= 4 {
+		// Bellow there is the MPU configuration that more or less corresponds
+		// to the default CPU behavior, without MPU enabled.
+		//
+		// All regions starts at address 0x00000000. The SIZE and SRD
+		// (sub-region disabled) fields are used to set the region address
+		// ranges.
+		//
+		// The first 64 bytes of the code region that corresponds to the first
+		// 16 exception vectors are set inaccessible to catch nil pointer
+		// dereferences. The code region is declared read/write because some
+		// MCUs use normal memory access to program Flash.
+		//
+		// Tha RAM region is configured as shareable (usually shared with DMA).
+		// Shareable regions are by default not cacheable. If you enable L1
+		// cache in Cortex-M7 set the acc.SIWT bit so the RAM will be cacheable
+		// in write-through mode. WT mode degrades performance (not as much as
+		// you may think) but allows to avoid cache maintenance operations which
+		// are problematic in case of Cortex-M7.
+		var (
+			noacc  = mpu.A____
+			code   = mpu.Arwrw | mpu.C
+			ram    = mpu.Arwrw | mpu.TEX(1) | mpu.C | mpu.B | mpu.S
+			periph = mpu.Arwrw | mpu.B | mpu.S | mpu.XN
+		)
+		mpu.SetRegion(0x00000000|mpu.VALID|0, mpu.ENA|mpu.SIZE(29)|code)
+		mpu.SetRegion(0x00000000|mpu.VALID|1, mpu.ENA|mpu.SIZE(6)|noacc)
+		mpu.SetRegion(
+			0x00000000|mpu.VALID|2,
+			mpu.ENA|mpu.SIZE(32)|mpu.SRD(0b10011011)|periph,
+		)
+		mpu.SetRegion(
+			0x00000000|mpu.VALID|3,
+			mpu.ENA|mpu.SIZE(32)|mpu.SRD(0b11100101)|ram,
+		)
+		mmio.MB() // ensure any previous memory access is done before enable MPU
+		mpu.Set(mpu.ENABLE | mpu.PRIVDEFENA)
+	}
+
+	// ensure everything is set before any subsequent memory access
+	mmio.MB()
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func defaultWrite(fd int, p []byte) int {
+	ITM := itm.ITM()
+	port := &ITM.STIM[fd]
+	portena := mmio.UM32{&ITM.TER[fd>>5].U32, 1 << (fd & 31)}
+	for i := 0; i < len(p); {
+		for port.LoadBit(0) == 0 {
+			if portena.Load() == 0 || ITM.TCR.LoadBits(itm.ITMENA) == 0 {
+				return len(p) // do not block on disabled port/ITM
+			}
+		}
+		switch m := len(p) - i; {
+		case m >= 4:
+			port.U32.Store(uint32(p[i]) + uint32(p[i+1])<<8 |
+				uint32(p[i+2])<<16 | uint32(p[i+3])<<24)
+			i += 4
+		case m >= 2:
+			port16 := (*mmio.U16)(unsafe.Pointer(&port.U32))
+			port16.Store(uint16(p[i]) | uint16(p[i+1])<<8)
+			i += 2
+		default:
+			port8 := (*mmio.U8)(unsafe.Pointer(&port.U32))
+			port8.Store(p[i])
+			i++
+		}
+	}
+	return len(p)
+}
+
+// syscalls not used by runtime
+
+//go:nowritebarrierrec
+//go:nosplit
+func syssetprivlevel(newlevel int) (oldlevel, errno int) {
+	// this code requires thrPrivLevel == 1
+	const check byte = (thrPrivLevel - 1) * 256
+
+	ctrl := cpucontrol()
+	oldlevel = int(ctrl | 1)
+	if uint(newlevel) <= 1 {
+		setcpucontrol(ctrl&^1 | uint32(newlevel))
+	} else if newlevel > 0 {
+		errno = 2 // rtos.ErrBadPrivLevel
+	}
+	return
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysirqenabled(irq int) (enabled, errno int) {
+	if uint(irq) >= irqNum() {
+		return 0, 4 // rtos.ErrBadIRQNumber
+	}
+	return int(nvic.NVIC().ISER[irq>>5].Load()) >> uint(irq&31), 0
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysirqctl(irq, ctl, ctxid int) (enabled, prio, errno int) {
+	if uint(irq) >= irqNum() {
+		errno = 4 // rtos.ErrBadIntNumber
+		return
+	}
+	if uint(ctxid) > 0 {
+		errno = 6 // rtos.ErrBadIntCtx
+	}
+	NVIC := nvic.NVIC()
+	// rtos package ensures valid ctl
+	if ctl >= 0 {
+		NVIC.IPR[irq].Store(nvic.IPR(255 - ctl))
+	}
+	rn, bn := irq>>5, uint(irq&31)
+	switch {
+	case ctl >= -1:
+		NVIC.ISER[rn].Store(1 << bn)
+	case ctl == -2:
+		NVIC.ICER[rn].Store(1 << bn)
+	default:
+		enabled = int(NVIC.ISER[irq>>5].Load()) >> bn & 1
+		prio = 255 - int(NVIC.IPR[irq].Load())
+	}
+	return
+}
+
+// utils
+
+func cpucontrol() uint32
+func setcpucontrol(ctrl uint32)
+
+//go:nosplit
+func irqNum() uint {
+	n := uint(scid.SCID().ICTR.LoadBits(scid.INTLINESNUM)+1) * 32
+	if n > 496 {
+		n = 496
+	}
+	return n
+}
+
+// m.tls fields
+
+const msp = 4
+const mer = 5
+
+// Use libcall, libcallpc, libcallsp, libcallg, syscall, vdsoSP, vdsoPC and mOS
+// to save the second part of thread context. We do not save it on the gorutine
+// stack to avoid waste of memory (need to increasing stack guard for any
+// gorutine stack and there are much more gorutines than threads).
+//
+// We realy do not want to add another 96 bytes to the m so we take the trouble
+// to use these unused fields for our needs. The following constant declarations
+// are compile-time tests to ensure the fields were not changed or splitted.
+const (
+	_mLibcallAlign       = -(unsafe.Offsetof(m{}.libcall) & 3)
+	_mLibcallSize        = int8((unsafe.Sizeof(m{}.libcall) - 6*4) * 129)
+	_mLibcallFn          = int8(unsafe.Offsetof(m{}.libcall.fn) * 129)
+	_mLibcallLibcallpc   = int8((unsafe.Offsetof(m{}.libcallpc) - unsafe.Offsetof(m{}.libcall) - 6*4) * 129)
+	_mLibcallpcSize      = int8((unsafe.Sizeof(m{}.libcallpc) - 4) * 129)
+	_mLibcallpcLibcallsp = int8((unsafe.Offsetof(m{}.libcallsp) - unsafe.Offsetof(m{}.libcallpc) - 4) * 129)
+	_mLibcallspSize      = int8((unsafe.Sizeof(m{}.libcallsp) - 4) * 129)
+	_mLibcallspLibcallg  = int8((unsafe.Offsetof(m{}.libcallg) - unsafe.Offsetof(m{}.libcallsp) - 4) * 129)
+	_mLibcallgSize       = int8((unsafe.Sizeof(m{}.libcallg) - 4) * 129)
+	_mLibcallgSyscall    = int8((unsafe.Offsetof(m{}.syscall) - unsafe.Offsetof(m{}.libcallg) - 4) * 129)
+	_mSyscallSize        = int8((unsafe.Sizeof(m{}.syscall) - 6*4) * 129)
+	_mSyscallVdsosp      = int8((unsafe.Offsetof(m{}.vdsoSP) - unsafe.Offsetof(m{}.syscall) - 6*4) * 129)
+	_mVdsospSize         = int8((unsafe.Sizeof(m{}.vdsoSP) - 4) * 129)
+	_mVdsospVdsopc       = int8((unsafe.Offsetof(m{}.vdsoPC) - unsafe.Offsetof(m{}.vdsoSP) - 4) * 129)
+	_mVdsopcSize         = int8((unsafe.Sizeof(m{}.vdsoPC) - 4) * 129)
+	_mVdsopcMos          = int8((unsafe.Offsetof(m{}.mOS) - unsafe.Offsetof(m{}.vdsoPC) - 4) * 129)
+	_mSize               = int8((unsafe.Offsetof(m{}.mOS) - unsafe.Offsetof(m{}.libcall) + unsafe.Sizeof(m{}.mOS) - 24*4) * 129)
+)
+
+type mOS [7]uint32
diff --git a/src/runtime/tasker_noos_thumb.s b/src/runtime/tasker_noos_thumb.s
new file mode 100644
index 0000000000..2892cdca68
--- /dev/null
+++ b/src/runtime/tasker_noos_thumb.s
@@ -0,0 +1,331 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+#include "syscall_noos.h"
+
+// This code uses ADD and ORR instructions when wants to set bits from 0 to 1.  // Mixing these two ways to do the same thing may seem seemingly inconsistent
+// but it is not. The shorter encoding is prefered. If both gives the same
+// length the ORR instruction is used because of its less energy per instruction
+// factor (see: https://www.ics.forth.gr/carv/greenvm/files/tr450.pdf).
+
+// TODO: Use ICSR.RETTOBASE to avoid manual stacking of previously stacked
+// registers.
+
+#define ICSR_ADDR 0xE000ED04
+#define ICSR_PENDSVCLR (1<<27)
+
+// identcurcpu indetifies the current CPU and returns a pointer to its cpuctx in
+// R0. It can clobber R0-R4,LR registers (other registers must be preserved).
+TEXT runtime·identcurcpu(SB),NOSPLIT|NOFRAME,$0-0
+	// for now only single CPU is supported (see also cpuid, osinit)
+	MOVW  $·cpu0(SB), R0
+	RET
+
+// func sev()
+TEXT ·sev(SB),NOSPLIT|NOFRAME,$0-0
+	SEV
+	RET
+
+// func cpucontrol() uint32
+TEXT ·cpucontrol(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  CONTROL, R0
+	MOVW  R0, ret+0(FP)
+	RET
+
+// func setcpucontrol(ctrl uint32)
+TEXT ·setcpucontrol(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  ctrl+0(FP), R0
+	MOVW  R0, CONTROL
+	RET
+
+// func curcpuSleep()
+TEXT ·curcpuSleep(SB),NOSPLIT|NOFRAME,$0-0
+	DSB  // flush CPU write buffers before sleep
+	WFE
+	// still in pendsvHandler so clear PendSV to avoid unnecessary reentry
+	MOVW  $ICSR_ADDR, R0
+	MOVW  $ICSR_PENDSVCLR, R1
+	MOVW  R1, (R0)
+	DMB   // ensure clearing happens before reading something for what we woke
+	RET
+
+// Exception handlers
+
+TEXT runtime·faultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	// At this point a lot of things can be broken so don't touch
+	// stack nor memory. Do only few things that helps debuging.
+	TST   $4, LR
+	BNE   3(PC)
+	MOVW  MSP, R1
+	B     2(PC)
+	MOVW  PSP, R1
+	MOVW  IPSR, R0
+
+	// Now R0 and R1 contain useful information.
+
+	// R0 contains exception number:
+	// 3: HardFault  - see HFSR: x/xw 0xE000ED2C
+	// 4: MemManage  - see MMSR: x/xb 0xE000ED28, MMAR: x/xw 0xE000ED34
+	// 5: BusFault   - see BFSR: x/xb 0xE000ED29, BFAR: x/xw 0xE000ED38
+	// 6: UsageFault - see UFSR: x/xh 0xE000ED2A
+
+	// R1 should contain pointer to the exception stack frame:
+	// (R1) -> [R0, R1, R2, R3, IP, LR, PC, PSR]
+	// If R1 points to the valid memory examine:
+	// 1. Where PC points.
+	// 2. Thumb bit in PSR
+	// 3. IPSR in PSR
+
+	// To print stack frame in gdb use:
+	//   x/8xw $r1
+
+	BKPT
+	B   -1(PC)
+
+TEXT runtime·reservedHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   ·faultHandler(SB)
+
+TEXT runtime·nmiHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   ·faultHandler(SB)
+
+TEXT runtime·hardfaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   ·faultHandler(SB)
+
+TEXT runtime·memmanageHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   ·faultHandler(SB)
+
+TEXT runtime·busfaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   ·faultHandler(SB)
+
+TEXT runtime·usagefaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   ·faultHandler(SB)
+
+TEXT runtime·securefaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   ·faultHandler(SB)
+
+TEXT runtime·debugmonHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   ·faultHandler(SB)
+
+
+#define sysMaxArgs (24+4) // max. size of argumants and return values + LR
+
+
+// svcallHandler handles synhronous SVCall exception generated by SWI (SVC)
+// instruction. The tiny wrappers over SWI instruction add three additional
+// parameters in registers:
+//
+// R4 - syscall number,
+// R5 - argument data size on the stack (+4 for frame-pointer),
+// R6 - return data size on the stack.
+//
+// The R0-R3,R12,LR,PC,PSR registers have been pushed on the stack at exception
+// entry. If FP registers have been used the place for D0-D7,FPSCR have been
+// also reserved on the stack (lazy stacking). The content of R0-R3,R12
+// registers can be broken by late-arriving higher-priority exception so using
+// R4-R6 avoids reading from the stack.
+//
+// The ABI0 does not follow the AAPCS: almost all GP registers and all FP
+// registers are caller saved. Therefore other (asynchronous) excepion handlers
+// need to save and restore not stacked GP registers and FP ones (if extended
+// frame was used). The SVCall is synchronous exception so this handler is
+// called almost like normal function and does not have to save any registers
+// except g (R10) and LR (EXC_RETURN).
+TEXT runtime·svcallHandler(SB),NOSPLIT|NOFRAME,$0-0
+	// check the syscall number
+	CMP  $SYS_NUM, R4
+	BGE  badSyscall
+
+	// stacked SP to R7
+	TST      $(1<<2), LR
+	MOVW.EQ  MSP, R7
+	MOVW.NE  PSP, R7
+	MOVW     R7, R1
+
+	// check does paddnig was added to the frame (stacked xPSR bit 9)
+	MOVW    (7*4)(R7), R0
+	TST     $(1<<9), R0
+	ADD.NE  $4, R1
+
+	// check does the extended frame is used
+	TST     $(1<<4), LR
+	ADD.NE  $(8*4), R1
+	ADD.EQ  $(26*4), R1
+
+	// push g, LR on the stack
+	MOVM.DB.W  [g,LR], (R13)
+
+	// make space on the stack for arguments + 3 registers
+	SUB  $(sysMaxArgs+3*4), R13
+
+	// copy arguments from the caller's stack
+	MOVW  $·duffcopy+1024(SB), R0
+	MOVW  R13, R2
+	SUB   R5<<1, R0
+	MOVW  LR, R5  // save EXC_RETURN before call
+	BL    (R0)
+
+	// save data needed to copy the return values back to the caller's stack
+	ADD        $sysMaxArgs, R13, R0
+	MOVM.IA.W  [R1,R2,R6], (R0)
+
+	// current CPU context to R0
+	BL  ·identcurcpu(SB)
+
+	// check for fast syscall (unfortunately it lets through the calls by
+	// higher priority exceptions that are disallowed to use syscalls at all)
+	CMP  $SYS_LAST_FAST, R4
+	BLS  fast
+
+	// check for syscall from interrupt handler
+	CMP  R0, g
+	BEQ  badHandlerCall  // syscall not allowed in handler mode
+
+	// save thread context (small): SP+CONTROL[nPRIV], EXC_RETURN, g
+	MOVW  (cpuctx_exe)(R0), R3
+	MOVW  CONTROL, R2
+	AND   $const_thrPrivLevel, R2
+	ORR   R2, R7
+	ADD   $const_thrSmallCtx, R7  // set thrSmallCtx (only g saved)
+	MOVW  R7, (m_tls+const_msp*4)(R3)
+	MOVW  R5, (m_tls+const_mer*4)(R3)
+	MOVW  g, (m_libcall)(R3)
+
+fast:
+	// call the service routine
+	MOVW  R0, g
+	MOVW  $·syscalls(SB), R0
+	MOVW  (R0)(R4*4), R0
+	BL    (R0)
+
+	// copy the return values back to the caller's stack
+	MOVW  (sysMaxArgs+2*4)(R13), R5
+	CBZ   R5, 6(PC)  // check if is something to copy
+	MOVW  (sysMaxArgs+0*4)(R13), R2
+	MOVW  (sysMaxArgs+1*4)(R13), R1
+	MOVW  $·duffcopy+1024(SB), R0
+	SUB   R5<<1, R0
+	BL    (R0)
+
+	// wind up the stack and return from syscall
+	ADD        $(sysMaxArgs+3*4), R13
+	MOVM.IA.W  (R13), [g,R15]
+
+badSyscall:
+	BKPT
+	B   -1(PC)
+
+badHandlerCall:
+	BKPT
+	B   -1(PC)
+
+
+// pendsvHandler handles asynhronous PendSV exceptions generated by the system
+// timer routine or system calls, to schedule/wakeup next thread.
+TEXT runtime·pendsvHandler(SB),NOSPLIT|NOFRAME,$0-0
+	// load cpuctx
+	MOVW  LR, R12
+	BL    ·identcurcpu(SB)  // current CPU context to R0
+
+	// if cpuctx.schedule then context saved by syscall
+	MOVBU  (cpuctx_schedule)(R0), R3
+	CBNZ   R3, contextSaved
+
+	// save not stacked registers (R4-R11), SP, CONTROL[nPRIV], EXC_RETURN
+	MOVW     (cpuctx_exe)(R0), R3
+	TST      $(1<<2), R12
+	MOVW.EQ  MSP, R1
+	MOVW.NE  PSP, R1
+	MOVW     CONTROL, R2
+	AND      $const_thrPrivLevel, R2
+	ORR      R2, R1
+	MOVW     R1, (m_tls+const_msp*4)(R3)
+	MOVW     R12, (m_tls+const_mer*4)(R3)
+	ADD      $m_libcall, R3
+	MOVM.IA  [R4-R11], (R3)
+
+contextSaved:
+	MOVW  $0, R3
+	MOVB  R3, (cpuctx_schedule)(R0)
+
+	// clear PendSV if set again to avoid unnecessary reentry to this handler
+	MOVW  $ICSR_ADDR, R1
+	MOVW  $ICSR_PENDSVCLR, R2
+	MOVW  R2, (R1)
+	DMB   // ensure clearing happens before checking nanotime and futexes
+
+	// enter scheduler
+	MOVW  R0, g
+	BL    ·curcpuRunScheduler(SB)
+
+	// load SP+CONTROL[nPRIV], EXC_RETURN from new/old context pointed by exe
+	MOVW  (cpuctx_exe)(g), R3
+	MOVW  (m_tls+const_msp*4)(R3), R0
+	MOVW  (m_tls+const_mer*4)(R3), R1
+
+	// check does the context changed
+	MOVBU  (cpuctx_newexe)(g), R2
+	CBNZ   R2, newexe
+
+	// fast path if exe did not changed (cpuctx.newexe == false)
+	TST      $const_thrSmallCtx, R0
+	MOVW.NE  (m_libcall)(R3), g
+	B.NE     (R1)
+	ADD      $m_libcall, R3
+	MOVM.IA  (R3), [R4-R11]
+	B        (R1)
+
+newexe:
+	// clear cpuctx.newexe
+	MOVW  $0, R2
+	MOVB  R2, (cpuctx_newexe)(g)
+
+	// restore privilege level
+	MOVW  CONTROL, R2
+	BIC   $const_thrPrivLevel, R2
+	AND   $const_thrPrivLevel, R0, R4
+	ORR   R4, R2
+	MOVW  R2, CONTROL
+
+	// restore PSP or MSP
+	BIC      $(const_thrPrivLevel+const_thrSmallCtx), R0, R2
+	TST      $(1<<2), R1
+	MOVW.EQ  R2, MSP
+	MOVW.NE  R2, PSP
+
+	// fast path in case of small context (only g saved in libcall)
+	TST      $const_thrSmallCtx, R0
+	MOVW.NE  (m_libcall)(R3), g
+	B.NE     (R1)
+
+	// restore registers saved in m.libcall
+	ADD        $m_libcall, R3
+	MOVM.IA.W  (R3), [R4-R11]
+	TST        $0x10, R1
+	BNE        3(PC)
+	HWORD      $0xEC93  // VLDM R3
+	HWORD      $0x8B10  // [D8-D15]
+	B          (R1)
+
+TEXT runtime·curcpuSavectxSched(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  (cpuctx_exe)(g), R0
+
+	MOVW  (m_tls+const_mer*4)(R0), R1
+	TST   $0x10, R1
+	RET.NE
+
+	ADD   $(m_libcall+8*4), R0
+	MOVW  CONTROL, R1
+	CPSID
+	HWORD  $0xEC80      // VSTM R0
+	HWORD  $0x8B10      // [D8-D15]
+	MOVW   R1, CONTROL  // to avoid stacking again by higher priority handler
+	CPSIE
+	RET
+
+TEXT runtime·unhandledException(SB),NOSPLIT|NOFRAME,$0-0
+	BKPT
+	B   -1(PC)
diff --git a/src/runtime/tasker_test.go b/src/runtime/tasker_test.go
new file mode 100644
index 0000000000..71d237e4cb
--- /dev/null
+++ b/src/runtime/tasker_test.go
@@ -0,0 +1,18 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime_test
+
+import (
+	"runtime"
+	"testing"
+)
+
+// Unity tests that can be run on the host machine (not on the target MCU)
+
+func TestMQ(t *testing.T) {
+	if s := runtime.MQTest(); s != "" {
+		t.Error(s)
+	}
+}
diff --git a/src/runtime/textflag.h b/src/runtime/textflag.h
index daca36d948..512eac25ac 100644
--- a/src/runtime/textflag.h
+++ b/src/runtime/textflag.h
@@ -35,3 +35,5 @@
 // Function is the top of the call stack. Call stack unwinders should stop
 // at this function.
 #define TOPFRAME 2048
+// Generate interrupt handler prologue / epilogue.
+#define ISR 4096
diff --git a/src/runtime/time_fake.go b/src/runtime/time_fake.go
index c64d2994a9..b9ccf637f5 100644
--- a/src/runtime/time_fake.go
+++ b/src/runtime/time_fake.go
@@ -4,6 +4,7 @@
 
 // +build faketime
 // +build !windows
+// +build !noos
 
 // Faketime isn't currently supported on Windows. This would require:
 //
diff --git a/src/runtime/time_nofake.go b/src/runtime/time_nofake.go
index 1912a94e87..f64703054c 100644
--- a/src/runtime/time_nofake.go
+++ b/src/runtime/time_nofake.go
@@ -3,6 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build !faketime
+// +build !noos
 
 package runtime
 
diff --git a/src/runtime/timestub.go b/src/runtime/timestub.go
index 459bf8e543..9282e168c4 100644
--- a/src/runtime/timestub.go
+++ b/src/runtime/timestub.go
@@ -6,6 +6,7 @@
 // indirectly, in terms of walltime and nanotime assembly.
 
 // +build !windows
+// +build !noos
 
 package runtime
 
diff --git a/src/runtime/timestub2.go b/src/runtime/timestub2.go
index 68777ee4a9..a04a735aa2 100644
--- a/src/runtime/timestub2.go
+++ b/src/runtime/timestub2.go
@@ -8,6 +8,7 @@
 // +build !openbsd
 // +build !solaris
 // +build !windows
+// +build !noos
 
 package runtime
 
diff --git a/src/runtime/tls_noos_riscv64.s b/src/runtime/tls_noos_riscv64.s
new file mode 100644
index 0000000000..a64e501ca9
--- /dev/null
+++ b/src/runtime/tls_noos_riscv64.s
@@ -0,0 +1,11 @@
+// Copyright 2021 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT runtime·save_g(SB),NOSPLIT|NOFRAME,$0-0
+	RET
+
+TEXT runtime·load_g(SB),NOSPLIT|NOFRAME,$0-0
+	RET
diff --git a/src/runtime/tls_riscv64.s b/src/runtime/tls_riscv64.s
index 22b550b761..5985243a7a 100644
--- a/src/runtime/tls_riscv64.s
+++ b/src/runtime/tls_riscv64.s
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build !noos
+
 #include "go_asm.h"
 #include "go_tls.h"
 #include "funcdata.h"
diff --git a/src/runtime/trace.go b/src/runtime/trace.go
index bcd0b9d56c..1a3fd0f42f 100644
--- a/src/runtime/trace.go
+++ b/src/runtime/trace.go
@@ -10,6 +10,8 @@
 // trace is captured for most events.
 // See https://golang.org/s/go15trace for more info.
 
+// +build !noos
+
 package runtime
 
 import (
diff --git a/src/runtime/trace_noos.go b/src/runtime/trace_noos.go
new file mode 100644
index 0000000000..2c6d5e0f20
--- /dev/null
+++ b/src/runtime/trace_noos.go
@@ -0,0 +1,96 @@
+package runtime
+
+// Event types in the trace, args are given in square brackets.
+const (
+	traceEvNone              = 0  // unused
+	traceEvBatch             = 1  // start of per-P batch of events [pid, timestamp]
+	traceEvFrequency         = 2  // contains tracer timer frequency [frequency (ticks per second)]
+	traceEvStack             = 3  // stack [stack id, number of PCs, array of {PC, func string ID, file string ID, line}]
+	traceEvGomaxprocs        = 4  // current value of GOMAXPROCS [timestamp, GOMAXPROCS, stack id]
+	traceEvProcStart         = 5  // start of P [timestamp, thread id]
+	traceEvProcStop          = 6  // stop of P [timestamp]
+	traceEvGCStart           = 7  // GC start [timestamp, seq, stack id]
+	traceEvGCDone            = 8  // GC done [timestamp]
+	traceEvGCSTWStart        = 9  // GC STW start [timestamp, kind]
+	traceEvGCSTWDone         = 10 // GC STW done [timestamp]
+	traceEvGCSweepStart      = 11 // GC sweep start [timestamp, stack id]
+	traceEvGCSweepDone       = 12 // GC sweep done [timestamp, swept, reclaimed]
+	traceEvGoCreate          = 13 // goroutine creation [timestamp, new goroutine id, new stack id, stack id]
+	traceEvGoStart           = 14 // goroutine starts running [timestamp, goroutine id, seq]
+	traceEvGoEnd             = 15 // goroutine ends [timestamp]
+	traceEvGoStop            = 16 // goroutine stops (like in select{}) [timestamp, stack]
+	traceEvGoSched           = 17 // goroutine calls Gosched [timestamp, stack]
+	traceEvGoPreempt         = 18 // goroutine is preempted [timestamp, stack]
+	traceEvGoSleep           = 19 // goroutine calls Sleep [timestamp, stack]
+	traceEvGoBlock           = 20 // goroutine blocks [timestamp, stack]
+	traceEvGoUnblock         = 21 // goroutine is unblocked [timestamp, goroutine id, seq, stack]
+	traceEvGoBlockSend       = 22 // goroutine blocks on chan send [timestamp, stack]
+	traceEvGoBlockRecv       = 23 // goroutine blocks on chan recv [timestamp, stack]
+	traceEvGoBlockSelect     = 24 // goroutine blocks on select [timestamp, stack]
+	traceEvGoBlockSync       = 25 // goroutine blocks on Mutex/RWMutex [timestamp, stack]
+	traceEvGoBlockCond       = 26 // goroutine blocks on Cond [timestamp, stack]
+	traceEvGoBlockNet        = 27 // goroutine blocks on network [timestamp, stack]
+	traceEvGoSysCall         = 28 // syscall enter [timestamp, stack]
+	traceEvGoSysExit         = 29 // syscall exit [timestamp, goroutine id, seq, real timestamp]
+	traceEvGoSysBlock        = 30 // syscall blocks [timestamp]
+	traceEvGoWaiting         = 31 // denotes that goroutine is blocked when tracing starts [timestamp, goroutine id]
+	traceEvGoInSyscall       = 32 // denotes that goroutine is in syscall when tracing starts [timestamp, goroutine id]
+	traceEvHeapAlloc         = 33 // memstats.heap_live change [timestamp, heap_alloc]
+	traceEvNextGC            = 34 // memstats.next_gc change [timestamp, next_gc]
+	traceEvTimerGoroutine    = 35 // denotes timer goroutine [timer goroutine id]
+	traceEvFutileWakeup      = 36 // denotes that the previous wakeup of this goroutine was futile [timestamp]
+	traceEvString            = 37 // string dictionary entry [ID, length, string]
+	traceEvGoStartLocal      = 38 // goroutine starts running on the same P as the last event [timestamp, goroutine id]
+	traceEvGoUnblockLocal    = 39 // goroutine is unblocked on the same P as the last event [timestamp, goroutine id, stack]
+	traceEvGoSysExitLocal    = 40 // syscall exit on the same P as the last event [timestamp, goroutine id, real timestamp]
+	traceEvGoStartLabel      = 41 // goroutine starts running with label [timestamp, goroutine id, seq, label string id]
+	traceEvGoBlockGC         = 42 // goroutine blocks on GC assist [timestamp, stack]
+	traceEvGCMarkAssistStart = 43 // GC mark assist start [timestamp, stack]
+	traceEvGCMarkAssistDone  = 44 // GC mark assist done [timestamp]
+	traceEvUserTaskCreate    = 45 // trace.NewContext [timestamp, internal task id, internal parent task id, stack, name string]
+	traceEvUserTaskEnd       = 46 // end of a task [timestamp, internal task id, stack]
+	traceEvUserRegion        = 47 // trace.WithRegion [timestamp, internal task id, mode(0:start, 1:end), stack, name string]
+	traceEvUserLog           = 48 // trace.Log [timestamp, internal task id, key string id, stack, value string]
+	traceEvCount             = 49
+	// Byte is used but only 6 bits are available for event type.
+	// The remaining 2 bits are used to specify the number of arguments.
+	// That means, the max event type value is 63.
+)
+
+type traceBufPtr uintptr
+
+var trace struct {
+	lock        mutex
+	stringsLock mutex
+	bufLock     mutex
+	stackTab    struct{ lock mutex }
+	enabled     bool
+	shutdown    bool
+}
+
+func traceGCSweepStart()                  {}
+func traceGCSweepDone()                   {}
+func traceGCMarkAssistStart()             {}
+func traceGCMarkAssistDone()              {}
+func traceHeapAlloc()                     {}
+func traceGoUnpark(gp *g, skip int)       {}
+func traceNextGC()                        {}
+func traceGCStart()                       {}
+func traceGCDone()                        {}
+func traceGCSTWStart(kind int)            {}
+func traceGCSTWDone()                     {}
+func traceGCSweepSpan(bytesSwept uintptr) {}
+func traceProcStart()                     {}
+func traceProcStop(pp *p)                 {}
+func traceProcFree(pp *p)                 {}
+func traceGoCreate(newg *g, pc uintptr)   {}
+func traceGoStart()                       {}
+func traceGoSched()                       {}
+func traceGoPreempt()                     {}
+func traceGoEnd()                         {}
+func traceGoPark(traceEv byte, skip int)  {}
+func traceGomaxprocs(procs int32)         {}
+func traceGoSysCall()                     {}
+func traceGoSysBlock(pp *p)               {}
+func traceGoSysExit(ts int64)             {}
+func traceReader() *g                     { return nil }
diff --git a/src/runtime/traceback.go b/src/runtime/traceback.go
index 2601cd697f..9f60de7758 100644
--- a/src/runtime/traceback.go
+++ b/src/runtime/traceback.go
@@ -148,6 +148,9 @@ func gentraceback(pc0, sp0, lr0 uintptr, gp *g, skip int, pcbuf *uintptr, max in
 			frame.sp += sys.RegSize
 		}
 	}
+	if GOARCH == "thumb" {
+		frame.pc |= 1
+	}
 
 	f := findfunc(frame.pc)
 	if !f.valid() {
@@ -611,7 +614,11 @@ func getArgInfo(frame *stkframe, f funcInfo, needArgMap bool, ctxt *funcval) (ar
 				// in the return values.
 				retValid = *(*bool)(unsafe.Pointer(arg0 + 3*sys.PtrSize))
 			}
-			if mv.fn != f.entry {
+			fn := mv.fn
+			if GOARCH == "thumb" {
+				fn |= 1
+			}
+			if fn != f.entry {
 				print("runtime: confused by ", funcname(f), "\n")
 				throw("reflect mismatch")
 			}
@@ -695,7 +702,7 @@ func traceback(pc, sp, lr uintptr, gp *g) {
 // If gp.m.libcall{g,pc,sp} information is available, it uses that information in preference to
 // the pc/sp/lr passed in.
 func tracebacktrap(pc, sp, lr uintptr, gp *g) {
-	if gp.m.libcallsp != 0 {
+	if !noos && gp.m.libcallsp != 0 {
 		// We're in C code somewhere, traceback from the saved position.
 		traceback1(gp.m.libcallpc, gp.m.libcallsp, 0, gp.m.libcallg.ptr(), 0)
 		return
diff --git a/src/runtime/vdso_elf32.go b/src/runtime/vdso_elf32.go
index 2720f33eed..49fa841549 100644
--- a/src/runtime/vdso_elf32.go
+++ b/src/runtime/vdso_elf32.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build 386 arm
+// +build 386 arm thumb
 
 package runtime
 
diff --git a/src/runtime/vdso_in_none.go b/src/runtime/vdso_in_none.go
index 7f4019c0d6..0f780b120d 100644
--- a/src/runtime/vdso_in_none.go
+++ b/src/runtime/vdso_in_none.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build linux,!386,!amd64,!arm,!arm64,!mips64,!mips64le,!ppc64,!ppc64le !linux
+// +build linux,!386,!amd64,!arm,!arm64,!mips64,!mips64le,!ppc64,!ppc64le,!thumb !linux
 
 package runtime
 
diff --git a/src/runtime/vdso_linux.go b/src/runtime/vdso_linux.go
index 6e2942498d..22f79c4377 100644
--- a/src/runtime/vdso_linux.go
+++ b/src/runtime/vdso_linux.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build 386 amd64 arm arm64 mips64 mips64le ppc64 ppc64le
+// +build 386 amd64 arm arm64 mips64 mips64le ppc64 ppc64le thumb
 
 package runtime
 
diff --git a/src/runtime/vdso_linux_arm.go b/src/runtime/vdso_linux_armt.go
similarity index 94%
rename from src/runtime/vdso_linux_arm.go
rename to src/runtime/vdso_linux_armt.go
index ac3bdcf043..99ae532e15 100644
--- a/src/runtime/vdso_linux_arm.go
+++ b/src/runtime/vdso_linux_armt.go
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build linux
+// +build arm thumb
+
 package runtime
 
 const (
diff --git a/src/runtime/vlop_thumb.s b/src/runtime/vlop_thumb.s
new file mode 100644
index 0000000000..6fe75a7342
--- /dev/null
+++ b/src/runtime/vlop_thumb.s
@@ -0,0 +1,35 @@
+// Inferno's libkern/vlop-arm.s
+// https://bitbucket.org/inferno-os/inferno-os/src/master/libkern/vlop-arm.s
+//
+//         Copyright © 1994-1999 Lucent Technologies Inc. All rights reserved.
+//         Revisions Copyright © 2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com).  All rights reserved.
+//         Portions Copyright 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+#include "textflag.h"
+
+// _mul64by32 and _div64by32 not implemented on thumb
+TEXT runtime·_mul64by32(SB), NOSPLIT, $0
+	MOVW	$0, R0
+	MOVW	(R0), R1 // crash
+
+TEXT runtime·_div64by32(SB), NOSPLIT, $0
+	MOVW	$0, R0
+	MOVW	(R0), R1 // crash
diff --git a/src/runtime/vlop_thumb_test.go b/src/runtime/vlop_thumb_test.go
new file mode 100644
index 0000000000..015126adb5
--- /dev/null
+++ b/src/runtime/vlop_thumb_test.go
@@ -0,0 +1,128 @@
+// Copyright 2012 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime_test
+
+import (
+	"runtime"
+	"testing"
+)
+
+// arm soft division benchmarks adapted from
+// https://ridiculousfish.com/files/division_benchmarks.tar.gz
+
+const numeratorsSize = 1 << 21
+
+var numerators = randomNumerators()
+
+type randstate struct {
+	hi, lo uint32
+}
+
+func (r *randstate) rand() uint32 {
+	r.hi = r.hi<<16 + r.hi>>16
+	r.hi += r.lo
+	r.lo += r.hi
+	return r.hi
+}
+
+func randomNumerators() []uint32 {
+	numerators := make([]uint32, numeratorsSize)
+	random := &randstate{2147483563, 2147483563 ^ 0x49616E42}
+	for i := range numerators {
+		numerators[i] = random.rand()
+	}
+	return numerators
+}
+
+func bmUint32Div(divisor uint32, b *testing.B) {
+	var sum uint32
+	for i := 0; i < b.N; i++ {
+		sum += numerators[i&(numeratorsSize-1)] / divisor
+	}
+}
+
+func BenchmarkUint32Div7(b *testing.B)         { bmUint32Div(7, b) }
+func BenchmarkUint32Div37(b *testing.B)        { bmUint32Div(37, b) }
+func BenchmarkUint32Div123(b *testing.B)       { bmUint32Div(123, b) }
+func BenchmarkUint32Div763(b *testing.B)       { bmUint32Div(763, b) }
+func BenchmarkUint32Div1247(b *testing.B)      { bmUint32Div(1247, b) }
+func BenchmarkUint32Div9305(b *testing.B)      { bmUint32Div(9305, b) }
+func BenchmarkUint32Div13307(b *testing.B)     { bmUint32Div(13307, b) }
+func BenchmarkUint32Div52513(b *testing.B)     { bmUint32Div(52513, b) }
+func BenchmarkUint32Div60978747(b *testing.B)  { bmUint32Div(60978747, b) }
+func BenchmarkUint32Div106956295(b *testing.B) { bmUint32Div(106956295, b) }
+
+func bmUint32Mod(divisor uint32, b *testing.B) {
+	var sum uint32
+	for i := 0; i < b.N; i++ {
+		sum += numerators[i&(numeratorsSize-1)] % divisor
+	}
+}
+
+func BenchmarkUint32Mod7(b *testing.B)         { bmUint32Mod(7, b) }
+func BenchmarkUint32Mod37(b *testing.B)        { bmUint32Mod(37, b) }
+func BenchmarkUint32Mod123(b *testing.B)       { bmUint32Mod(123, b) }
+func BenchmarkUint32Mod763(b *testing.B)       { bmUint32Mod(763, b) }
+func BenchmarkUint32Mod1247(b *testing.B)      { bmUint32Mod(1247, b) }
+func BenchmarkUint32Mod9305(b *testing.B)      { bmUint32Mod(9305, b) }
+func BenchmarkUint32Mod13307(b *testing.B)     { bmUint32Mod(13307, b) }
+func BenchmarkUint32Mod52513(b *testing.B)     { bmUint32Mod(52513, b) }
+func BenchmarkUint32Mod60978747(b *testing.B)  { bmUint32Mod(60978747, b) }
+func BenchmarkUint32Mod106956295(b *testing.B) { bmUint32Mod(106956295, b) }
+
+func TestUsplit(t *testing.T) {
+	var den uint32 = 1000000
+	for _, x := range []uint32{0, 1, 999999, 1000000, 1010101, 0xFFFFFFFF} {
+		q1, r1 := runtime.Usplit(x)
+		q2, r2 := x/den, x%den
+		if q1 != q2 || r1 != r2 {
+			t.Errorf("%d/1e6, %d%%1e6 = %d, %d, want %d, %d", x, x, q1, r1, q2, r2)
+		}
+	}
+}
+
+//go:noinline
+func armFloatWrite(a *[129]float64) {
+	// This used to miscompile on arm5.
+	// The offset is too big to fit in a load.
+	// So the code does:
+	//   ldr     r0, [sp, #8]
+	//   bl      6f690 <_sfloat>
+	//   ldr     fp, [pc, #32]   ; (address of 128.0)
+	//   vldr    d0, [fp]
+	//   ldr     fp, [pc, #28]   ; (1024)
+	//   add     fp, fp, r0
+	//   vstr    d0, [fp]
+	// The software floating-point emulator gives up on the add.
+	// This causes the store to not work.
+	// See issue 15440.
+	a[128] = 128.0
+}
+func TestArmFloatBigOffsetWrite(t *testing.T) {
+	var a [129]float64
+	for i := 0; i < 128; i++ {
+		a[i] = float64(i)
+	}
+	armFloatWrite(&a)
+	for i, x := range a {
+		if x != float64(i) {
+			t.Errorf("bad entry %d:%f\n", i, x)
+		}
+	}
+}
+
+//go:noinline
+func armFloatRead(a *[129]float64) float64 {
+	return a[128]
+}
+func TestArmFloatBigOffsetRead(t *testing.T) {
+	var a [129]float64
+	for i := 0; i < 129; i++ {
+		a[i] = float64(i)
+	}
+	if x := armFloatRead(&a); x != 128.0 {
+		t.Errorf("bad value %f\n", x)
+	}
+}
diff --git a/src/runtime/vlrt.go b/src/runtime/vlrt.go
index 996c0611fd..fa0351c902 100644
--- a/src/runtime/vlrt.go
+++ b/src/runtime/vlrt.go
@@ -23,7 +23,7 @@
 // OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 // THE SOFTWARE.
 
-// +build arm 386 mips mipsle
+// +build arm thumb 386 mips mipsle
 
 package runtime
 
@@ -193,7 +193,7 @@ func _div64by32(a uint64, b uint32, r *uint32) (q uint32)
 
 //go:nosplit
 func dodiv(n, d uint64) (q, r uint64) {
-	if GOARCH == "arm" {
+	if GOARCH == "arm" || GOARCH == "thumb" {
 		// arm doesn't have a division instruction, so
 		// slowdodiv is the best that we can do.
 		return slowdodiv(n, d)
diff --git a/src/strconv/decimal.go b/src/strconv/decimal.go
index b58001888e..aa2ac5e960 100644
--- a/src/strconv/decimal.go
+++ b/src/strconv/decimal.go
@@ -175,7 +175,7 @@ type leftCheat struct {
 	cutoff string // minus one digit if original < a.
 }
 
-var leftcheats = []leftCheat{
+var leftcheats = [...]leftCheat{
 	// Leading digits of 1/2^i = 5^i.
 	// 5^23 is not an exact 64-bit floating point number,
 	// so have to use bc for the math.
diff --git a/src/strconv/isprint.go b/src/strconv/isprint.go
index 994a8e423c..c31e2d6aeb 100644
--- a/src/strconv/isprint.go
+++ b/src/strconv/isprint.go
@@ -8,7 +8,7 @@ package strconv
 
 // (434+132+95)*2 + (468)*4 = 3194 bytes
 
-var isPrint16 = []uint16{
+var isPrint16 = [...]uint16{
 	0x0020, 0x007e,
 	0x00a1, 0x0377,
 	0x037a, 0x037f,
@@ -228,7 +228,7 @@ var isPrint16 = []uint16{
 	0xfffc, 0xfffd,
 }
 
-var isNotPrint16 = []uint16{
+var isNotPrint16 = [...]uint16{
 	0x00ad,
 	0x038b,
 	0x038d,
@@ -363,7 +363,7 @@ var isNotPrint16 = []uint16{
 	0xffe7,
 }
 
-var isPrint32 = []uint32{
+var isPrint32 = [...]uint32{
 	0x010000, 0x01004d,
 	0x010050, 0x01005d,
 	0x010080, 0x0100fa,
@@ -600,7 +600,7 @@ var isPrint32 = []uint32{
 	0x0e0100, 0x0e01ef,
 }
 
-var isNotPrint32 = []uint16{ // add 0x10000 to each entry
+var isNotPrint32 = [...]uint16{ // add 0x10000 to each entry
 	0x000c,
 	0x0027,
 	0x003b,
@@ -699,7 +699,7 @@ var isNotPrint32 = []uint16{ // add 0x10000 to each entry
 }
 
 // isGraphic lists the graphic runes not matched by IsPrint.
-var isGraphic = []uint16{
+var isGraphic = [...]uint16{
 	0x00a0,
 	0x1680,
 	0x2000,
diff --git a/src/strconv/makeisprint.go b/src/strconv/makeisprint.go
index 0e6e90a6c6..be6980e544 100644
--- a/src/strconv/makeisprint.go
+++ b/src/strconv/makeisprint.go
@@ -148,25 +148,25 @@ func main() {
 		(len(range16)+len(except16)+len(except32))*2+
 			(len(range32))*4)
 
-	fmt.Fprintf(&buf, "var isPrint16 = []uint16{\n")
+	fmt.Fprintf(&buf, "var isPrint16 = [...]uint16{\n")
 	for i := 0; i < len(range16); i += 2 {
 		fmt.Fprintf(&buf, "\t%#04x, %#04x,\n", range16[i], range16[i+1])
 	}
 	fmt.Fprintf(&buf, "}\n\n")
 
-	fmt.Fprintf(&buf, "var isNotPrint16 = []uint16{\n")
+	fmt.Fprintf(&buf, "var isNotPrint16 = [...]uint16{\n")
 	for _, r := range except16 {
 		fmt.Fprintf(&buf, "\t%#04x,\n", r)
 	}
 	fmt.Fprintf(&buf, "}\n\n")
 
-	fmt.Fprintf(&buf, "var isPrint32 = []uint32{\n")
+	fmt.Fprintf(&buf, "var isPrint32 = [...]uint32{\n")
 	for i := 0; i < len(range32); i += 2 {
 		fmt.Fprintf(&buf, "\t%#06x, %#06x,\n", range32[i], range32[i+1])
 	}
 	fmt.Fprintf(&buf, "}\n\n")
 
-	fmt.Fprintf(&buf, "var isNotPrint32 = []uint16{ // add 0x10000 to each entry\n")
+	fmt.Fprintf(&buf, "var isNotPrint32 = [...]uint16{ // add 0x10000 to each entry\n")
 	for _, r := range except32 {
 		if r >= 0x20000 {
 			log.Fatalf("%U too big for isNotPrint32\n", r)
@@ -177,7 +177,7 @@ func main() {
 
 	// The list of graphic but not "printable" runes is short. Just make one easy table.
 	fmt.Fprintf(&buf, "// isGraphic lists the graphic runes not matched by IsPrint.\n")
-	fmt.Fprintf(&buf, "var isGraphic = []uint16{\n")
+	fmt.Fprintf(&buf, "var isGraphic = [...]uint16{\n")
 	for r := rune(0); r <= unicode.MaxRune; r++ {
 		if unicode.IsPrint(r) != unicode.IsGraphic(r) {
 			// Sanity check.
diff --git a/src/strconv/quote.go b/src/strconv/quote.go
index bcbdbc514d..6063c006ac 100644
--- a/src/strconv/quote.go
+++ b/src/strconv/quote.go
@@ -501,7 +501,7 @@ func IsPrint(r rune) bool {
 	// If we find x in a range, make sure x is not in isNotPrint list.
 
 	if 0 <= r && r < 1<<16 {
-		rr, isPrint, isNotPrint := uint16(r), isPrint16, isNotPrint16
+		rr, isPrint, isNotPrint := uint16(r), isPrint16[:], isNotPrint16[:]
 		i := bsearch16(isPrint, rr)
 		if i >= len(isPrint) || rr < isPrint[i&^1] || isPrint[i|1] < rr {
 			return false
@@ -510,7 +510,7 @@ func IsPrint(r rune) bool {
 		return j >= len(isNotPrint) || isNotPrint[j] != rr
 	}
 
-	rr, isPrint, isNotPrint := uint32(r), isPrint32, isNotPrint32
+	rr, isPrint, isNotPrint := uint32(r), isPrint32[:], isNotPrint32[:]
 	i := bsearch32(isPrint, rr)
 	if i >= len(isPrint) || rr < isPrint[i&^1] || isPrint[i|1] < rr {
 		return false
@@ -542,6 +542,6 @@ func isInGraphicList(r rune) bool {
 		return false
 	}
 	rr := uint16(r)
-	i := bsearch16(isGraphic, rr)
+	i := bsearch16(isGraphic[:], rr)
 	return i < len(isGraphic) && rr == isGraphic[i]
 }
diff --git a/src/syscall/asm_linux_thumb.s b/src/syscall/asm_linux_thumb.s
new file mode 100644
index 0000000000..458e9cce79
--- /dev/null
+++ b/src/syscall/asm_linux_thumb.s
@@ -0,0 +1,167 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "funcdata.h"
+
+//
+// System calls for arm, Linux
+//
+
+// func Syscall(syscall uintptr, a1, a2, a3 uintptr) (r1, r2, err uintptr);
+TEXT ·Syscall(SB),NOSPLIT,$0-28
+	BL	runtime·entersyscall(SB)
+	MOVW	trap+0(FP), R7
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	MOVW	$0, R3
+	MOVW	$0, R4
+	MOVW	$0, R5
+	SWI	$0
+	MOVW	$0xfffff001, R1
+	CMP	R1, R0
+	BLS	ok
+	MOVW	$-1, R1
+	MOVW	R1, r1+16(FP)
+	MOVW	$0, R2
+	MOVW	R2, r2+20(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+24(FP)
+	BL	runtime·exitsyscall(SB)
+	RET
+ok:
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	MOVW	R0, err+24(FP)
+	BL	runtime·exitsyscall(SB)
+	RET
+
+// func Syscall6(trap uintptr, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr);
+// Actually Syscall5 but the rest of the code expects it to be named Syscall6.
+TEXT ·Syscall6(SB),NOSPLIT,$0-40
+	BL	runtime·entersyscall(SB)
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	MOVW	a4+16(FP), R3
+	MOVW	a5+20(FP), R4
+	MOVW	a6+24(FP), R5
+	SWI	$0
+	MOVW	$0xfffff001, R6
+	CMP	R6, R0
+	BLS	ok6
+	MOVW	$-1, R1
+	MOVW	R1, r1+28(FP)
+	MOVW	$0, R2
+	MOVW	R2, r2+32(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+36(FP)
+	BL	runtime·exitsyscall(SB)
+	RET
+ok6:
+	MOVW	R0, r1+28(FP)
+	MOVW	R1, r2+32(FP)
+	MOVW	$0, R0
+	MOVW	R0, err+36(FP)
+	BL	runtime·exitsyscall(SB)
+	RET
+
+// func RawSyscall6(trap uintptr, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr);
+// Actually RawSyscall5 but the rest of the code expects it to be named RawSyscall6.
+TEXT	·RawSyscall6(SB),NOSPLIT,$0-40
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	MOVW	a4+16(FP), R3
+	MOVW	a5+20(FP), R4
+	MOVW	a6+24(FP), R5
+	SWI	$0
+	MOVW	$0xfffff001, R6
+	CMP	R6, R0
+	BLS	ok2
+	MOVW	$-1, R1
+	MOVW	R1, r1+28(FP)
+	MOVW	$0, R2
+	MOVW	R2, r2+32(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+36(FP)
+	RET
+ok2:
+	MOVW	R0, r1+28(FP)
+	MOVW	R1, r2+32(FP)
+	MOVW	$0, R0
+	MOVW	R0, err+36(FP)
+	RET
+
+#define SYS__LLSEEK 140  /* from zsysnum_linux_arm.go */
+// func seek(fd int, offset int64, whence int) (newoffset int64, errno int)
+// Implemented in assembly to avoid allocation when
+// taking the address of the return value newoffset.
+// Underlying system call is
+//	llseek(int fd, int offhi, int offlo, int64 *result, int whence)
+TEXT ·seek(SB),NOSPLIT,$0-28
+	BL	runtime·entersyscall(SB)
+	MOVW	$SYS__LLSEEK, R7	// syscall entry
+	MOVW	fd+0(FP), R0
+	MOVW	offset_hi+8(FP), R1
+	MOVW	offset_lo+4(FP), R2
+	MOVW	$newoffset_lo+16(FP), R3
+	MOVW	whence+12(FP), R4
+	SWI	$0
+	MOVW	$0xfffff001, R6
+	CMP	R6, R0
+	BLS	okseek
+	MOVW	$0, R1
+	MOVW	R1, newoffset_lo+16(FP)
+	MOVW	R1, newoffset_hi+20(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+24(FP)
+	BL	runtime·exitsyscall(SB)
+	RET
+okseek:
+	// system call filled in newoffset already
+	MOVW	$0, R0
+	MOVW	R0, err+24(FP)
+	BL	runtime·exitsyscall(SB)
+	RET
+
+// func RawSyscall(trap uintptr, a1, a2, a3 uintptr) (r1, r2, err uintptr);
+TEXT ·RawSyscall(SB),NOSPLIT,$0-28
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	SWI	$0
+	MOVW	$0xfffff001, R1
+	CMP	R1, R0
+	BLS	ok1
+	MOVW	$-1, R1
+	MOVW	R1, r1+16(FP)
+	MOVW	$0, R2
+	MOVW	R2, r2+20(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+24(FP)
+	RET
+ok1:
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	MOVW	R0, err+24(FP)
+	RET
+
+// func rawSyscallNoError(trap uintptr, a1, a2, a3 uintptr) (r1, r2 uintptr);
+TEXT ·rawSyscallNoError(SB),NOSPLIT,$0-24
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	SWI	$0
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	RET
diff --git a/src/syscall/endian_little.go b/src/syscall/endian_little.go
index 43b081d640..96861a3632 100644
--- a/src/syscall/endian_little.go
+++ b/src/syscall/endian_little.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 //
-// +build 386 amd64 arm arm64 ppc64le mips64le mipsle riscv64 wasm
+// +build 386 amd64 arm arm64 ppc64le mips64le mipsle riscv64 wasm thumb
 
 package syscall
 
diff --git a/src/syscall/flock_linux_32bit.go b/src/syscall/flock_linux_32bit.go
index e11aed6ed1..315c7e8a42 100644
--- a/src/syscall/flock_linux_32bit.go
+++ b/src/syscall/flock_linux_32bit.go
@@ -5,7 +5,7 @@
 // If you change the build tags here, see
 // internal/syscall/unix/fcntl_linux_32bit.go.
 
-// +build linux,386 linux,arm linux,mips linux,mipsle
+// +build linux,386 linux,arm linux,mips linux,mipsle linux,thumb
 
 package syscall
 
diff --git a/src/syscall/mkall.sh b/src/syscall/mkall.sh
index d1e28efa8c..c18c0bd6de 100755
--- a/src/syscall/mkall.sh
+++ b/src/syscall/mkall.sh
@@ -181,7 +181,7 @@ linux_amd64)
 	mksysnum="./mksysnum_linux.pl $unistd_h"
 	mktypes="GOARCH=$GOARCH go tool cgo -godefs"
 	;;
-linux_arm)
+linux_arm|linux_thumb)
 	mkerrors="$mkerrors"
 	mksyscall="./mksyscall.pl -l32 -arm"
 	mksysnum="curl -s 'http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/plain/arch/arm/include/uapi/asm/unistd.h' | ./mksysnum_linux.pl -"
diff --git a/src/syscall/setuidgid_32_linux.go b/src/syscall/setuidgid_32_linux.go
index b0b7f61d22..9ce79e1e6e 100644
--- a/src/syscall/setuidgid_32_linux.go
+++ b/src/syscall/setuidgid_32_linux.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build 386 arm
+// +build 386 arm thumb
 
 package syscall
 
diff --git a/src/syscall/setuidgid_linux.go b/src/syscall/setuidgid_linux.go
index 38c83c92f9..357be51468 100644
--- a/src/syscall/setuidgid_linux.go
+++ b/src/syscall/setuidgid_linux.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build !386,!arm
+// +build !386,!arm,!thumb
 
 package syscall
 
diff --git a/src/syscall/syscall_dup2_linux.go b/src/syscall/syscall_dup2_linux.go
index f03a923112..6a18debd1a 100644
--- a/src/syscall/syscall_dup2_linux.go
+++ b/src/syscall/syscall_dup2_linux.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build !android
-// +build 386 amd64 arm mips mipsle mips64 mips64le ppc64 ppc64le s390x
+// +build 386 amd64 arm mips mipsle mips64 mips64le ppc64 ppc64le s390x thumb
 
 package syscall
 
diff --git a/src/syscall/syscall_linux_thumb.go b/src/syscall/syscall_linux_thumb.go
new file mode 100644
index 0000000000..ae32067388
--- /dev/null
+++ b/src/syscall/syscall_linux_thumb.go
@@ -0,0 +1,235 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package syscall
+
+import "unsafe"
+
+// archHonorsR2 captures the fact that r2 is honored by the
+// runtime.GOARCH.  Syscall conventions are generally r1, r2, err :=
+// syscall(trap, ...).  Not all architectures define r2 in their
+// ABI. See "man syscall". [EABI assumed.]
+const archHonorsR2 = true
+
+const _SYS_setgroups = SYS_SETGROUPS32
+
+func setTimespec(sec, nsec int64) Timespec {
+	return Timespec{Sec: int32(sec), Nsec: int32(nsec)}
+}
+
+func setTimeval(sec, usec int64) Timeval {
+	return Timeval{Sec: int32(sec), Usec: int32(usec)}
+}
+
+func Pipe(p []int) (err error) {
+	if len(p) != 2 {
+		return EINVAL
+	}
+	var pp [2]_C_int
+	err = pipe2(&pp, 0)
+	p[0] = int(pp[0])
+	p[1] = int(pp[1])
+	return
+}
+
+//sysnb pipe2(p *[2]_C_int, flags int) (err error)
+
+func Pipe2(p []int, flags int) (err error) {
+	if len(p) != 2 {
+		return EINVAL
+	}
+	var pp [2]_C_int
+	err = pipe2(&pp, flags)
+	p[0] = int(pp[0])
+	p[1] = int(pp[1])
+	return
+}
+
+// Underlying system call writes to newoffset via pointer.
+// Implemented in assembly to avoid allocation.
+func seek(fd int, offset int64, whence int) (newoffset int64, err Errno)
+
+func Seek(fd int, offset int64, whence int) (newoffset int64, err error) {
+	newoffset, errno := seek(fd, offset, whence)
+	if errno != 0 {
+		return 0, errno
+	}
+	return newoffset, nil
+}
+
+//sys	accept(s int, rsa *RawSockaddrAny, addrlen *_Socklen) (fd int, err error)
+//sys	accept4(s int, rsa *RawSockaddrAny, addrlen *_Socklen, flags int) (fd int, err error)
+//sys	bind(s int, addr unsafe.Pointer, addrlen _Socklen) (err error)
+//sys	connect(s int, addr unsafe.Pointer, addrlen _Socklen) (err error)
+//sysnb	getgroups(n int, list *_Gid_t) (nn int, err error) = SYS_GETGROUPS32
+//sys	getsockopt(s int, level int, name int, val unsafe.Pointer, vallen *_Socklen) (err error)
+//sys	setsockopt(s int, level int, name int, val unsafe.Pointer, vallen uintptr) (err error)
+//sysnb	socket(domain int, typ int, proto int) (fd int, err error)
+//sysnb	getpeername(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error)
+//sysnb	getsockname(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error)
+//sys	recvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error)
+//sys	sendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error)
+//sysnb	socketpair(domain int, typ int, flags int, fd *[2]int32) (err error)
+//sys	recvmsg(s int, msg *Msghdr, flags int) (n int, err error)
+//sys	sendmsg(s int, msg *Msghdr, flags int) (n int, err error)
+
+// 64-bit file system and 32-bit uid calls
+// (16-bit uid calls are not always supported in newer kernels)
+//sys	Dup2(oldfd int, newfd int) (err error)
+//sysnb	EpollCreate(size int) (fd int, err error)
+//sys	Fchown(fd int, uid int, gid int) (err error) = SYS_FCHOWN32
+//sys	Fstat(fd int, stat *Stat_t) (err error) = SYS_FSTAT64
+//sys	fstatat(dirfd int, path string, stat *Stat_t, flags int) (err error) = SYS_FSTATAT64
+//sysnb	Getegid() (egid int) = SYS_GETEGID32
+//sysnb	Geteuid() (euid int) = SYS_GETEUID32
+//sysnb	Getgid() (gid int) = SYS_GETGID32
+//sysnb	Getuid() (uid int) = SYS_GETUID32
+//sysnb	InotifyInit() (fd int, err error)
+//sys	Listen(s int, n int) (err error)
+//sys	Pause() (err error)
+//sys	Renameat(olddirfd int, oldpath string, newdirfd int, newpath string) (err error)
+//sys	sendfile(outfd int, infd int, offset *int64, count int) (written int, err error) = SYS_SENDFILE64
+//sys	Select(nfd int, r *FdSet, w *FdSet, e *FdSet, timeout *Timeval) (n int, err error) = SYS__NEWSELECT
+//sys	Setfsgid(gid int) (err error) = SYS_SETFSGID32
+//sys	Setfsuid(uid int) (err error) = SYS_SETFSUID32
+//sys	Shutdown(fd int, how int) (err error)
+//sys	Splice(rfd int, roff *int64, wfd int, woff *int64, len int, flags int) (n int, err error)
+//sys	Ustat(dev int, ubuf *Ustat_t) (err error)
+
+//sys	futimesat(dirfd int, path string, times *[2]Timeval) (err error)
+//sysnb	Gettimeofday(tv *Timeval) (err error)
+//sysnb	Time(t *Time_t) (tt Time_t, err error)
+//sys	Utime(path string, buf *Utimbuf) (err error)
+//sys	utimes(path string, times *[2]Timeval) (err error)
+
+//sys   Pread(fd int, p []byte, offset int64) (n int, err error) = SYS_PREAD64
+//sys   Pwrite(fd int, p []byte, offset int64) (n int, err error) = SYS_PWRITE64
+//sys	Truncate(path string, length int64) (err error) = SYS_TRUNCATE64
+//sys	Ftruncate(fd int, length int64) (err error) = SYS_FTRUNCATE64
+
+//sys	mmap2(addr uintptr, length uintptr, prot int, flags int, fd int, pageOffset uintptr) (xaddr uintptr, err error)
+//sys	EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error)
+
+func Stat(path string, stat *Stat_t) (err error) {
+	return fstatat(_AT_FDCWD, path, stat, 0)
+}
+
+func Lchown(path string, uid int, gid int) (err error) {
+	return Fchownat(_AT_FDCWD, path, uid, gid, _AT_SYMLINK_NOFOLLOW)
+}
+
+func Lstat(path string, stat *Stat_t) (err error) {
+	return fstatat(_AT_FDCWD, path, stat, _AT_SYMLINK_NOFOLLOW)
+}
+
+func Fstatfs(fd int, buf *Statfs_t) (err error) {
+	_, _, e := Syscall(SYS_FSTATFS64, uintptr(fd), unsafe.Sizeof(*buf), uintptr(unsafe.Pointer(buf)))
+	if e != 0 {
+		err = e
+	}
+	return
+}
+
+func Statfs(path string, buf *Statfs_t) (err error) {
+	pathp, err := BytePtrFromString(path)
+	if err != nil {
+		return err
+	}
+	_, _, e := Syscall(SYS_STATFS64, uintptr(unsafe.Pointer(pathp)), unsafe.Sizeof(*buf), uintptr(unsafe.Pointer(buf)))
+	if e != 0 {
+		err = e
+	}
+	return
+}
+
+func mmap(addr uintptr, length uintptr, prot int, flags int, fd int, offset int64) (xaddr uintptr, err error) {
+	page := uintptr(offset / 4096)
+	if offset != int64(page)*4096 {
+		return 0, EINVAL
+	}
+	return mmap2(addr, length, prot, flags, fd, page)
+}
+
+type rlimit32 struct {
+	Cur uint32
+	Max uint32
+}
+
+//sysnb getrlimit(resource int, rlim *rlimit32) (err error) = SYS_GETRLIMIT
+
+const rlimInf32 = ^uint32(0)
+const rlimInf64 = ^uint64(0)
+
+func Getrlimit(resource int, rlim *Rlimit) (err error) {
+	err = prlimit(0, resource, nil, rlim)
+	if err != ENOSYS {
+		return err
+	}
+
+	rl := rlimit32{}
+	err = getrlimit(resource, &rl)
+	if err != nil {
+		return
+	}
+
+	if rl.Cur == rlimInf32 {
+		rlim.Cur = rlimInf64
+	} else {
+		rlim.Cur = uint64(rl.Cur)
+	}
+
+	if rl.Max == rlimInf32 {
+		rlim.Max = rlimInf64
+	} else {
+		rlim.Max = uint64(rl.Max)
+	}
+	return
+}
+
+//sysnb setrlimit(resource int, rlim *rlimit32) (err error) = SYS_SETRLIMIT
+
+func Setrlimit(resource int, rlim *Rlimit) (err error) {
+	err = prlimit(0, resource, rlim, nil)
+	if err != ENOSYS {
+		return err
+	}
+
+	rl := rlimit32{}
+	if rlim.Cur == rlimInf64 {
+		rl.Cur = rlimInf32
+	} else if rlim.Cur < uint64(rlimInf32) {
+		rl.Cur = uint32(rlim.Cur)
+	} else {
+		return EINVAL
+	}
+	if rlim.Max == rlimInf64 {
+		rl.Max = rlimInf32
+	} else if rlim.Max < uint64(rlimInf32) {
+		rl.Max = uint32(rlim.Max)
+	} else {
+		return EINVAL
+	}
+
+	return setrlimit(resource, &rl)
+}
+
+func (r *PtraceRegs) PC() uint64 { return uint64(r.Uregs[15]) }
+
+func (r *PtraceRegs) SetPC(pc uint64) { r.Uregs[15] = uint32(pc) }
+
+func (iov *Iovec) SetLen(length int) {
+	iov.Len = uint32(length)
+}
+
+func (msghdr *Msghdr) SetControllen(length int) {
+	msghdr.Controllen = uint32(length)
+}
+
+func (cmsg *Cmsghdr) SetLen(length int) {
+	cmsg.Len = uint32(length)
+}
+
+func rawVforkSyscall(trap, a1 uintptr) (r1 uintptr, err Errno) {
+	panic("not implemented")
+}
diff --git a/src/syscall/syscall_noos.go b/src/syscall/syscall_noos.go
new file mode 100644
index 0000000000..6ee09c7e6b
--- /dev/null
+++ b/src/syscall/syscall_noos.go
@@ -0,0 +1,200 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package syscall
+
+import (
+	"internal/oserror"
+	"sync"
+)
+
+var (
+	Stdin  = 0
+	Stdout = 1
+	Stderr = 2
+)
+
+type Errno uintptr
+
+const (
+	ENOTSUP Errno = iota
+	EINVAL
+	ENOENT
+	EACCES
+	EPERM
+	EEXIST
+	ENOTEMPTY
+	EISDIR
+	ENOTDIR
+	ENAMETOOLONG
+	EBUSY
+	EMFILE
+	ENOSPC
+	EBADF
+	ECANCELED
+	EINTR // for os package only, never returned
+)
+
+var errors = [...]string{
+	ENOTSUP:      "operation not supported",
+	EINVAL:       "invalid argument",
+	ENOENT:       "no such file or directory",
+	EACCES:       "permission denied",
+	EPERM:        "operation not permitted",
+	EEXIST:       "file exists",
+	ENOTEMPTY:    "directory not empty",
+	EISDIR:       "is a directory",
+	ENOTDIR:      "not a directory",
+	ENAMETOOLONG: "file name too long",
+	EBUSY:        "device or resource busy",
+	EMFILE:       "too many open files",
+	ENOSPC:       "no space left on device",
+	EBADF:        "bad file descriptor",
+	ECANCELED:    "operation canceled",
+	EINTR:        "interrupt",
+}
+
+func (e Errno) Is(target error) bool {
+	switch target {
+	case oserror.ErrPermission:
+		return e == EACCES || e == EPERM
+	case oserror.ErrExist:
+		return e == EEXIST || e == ENOTEMPTY
+	case oserror.ErrNotExist:
+		return e == ENOENT
+	}
+	return false
+}
+
+func (e Errno) Error() string {
+	if 0 <= int(e) && int(e) < len(errors) {
+		s := errors[e]
+		if s != "" {
+			return s
+		}
+	}
+	return "errno " + itoa(int(e))
+}
+
+type Timespec struct {
+	Sec  int64
+	Nsec int64
+}
+
+type Timeval struct {
+	Sec  int32
+	Usec int32
+}
+
+var env = struct {
+	mx   sync.RWMutex
+	dict map[string]string
+}{dict: map[string]string{
+	"HOME": "/tmp",
+}}
+
+func Setenv(key, value string) error {
+	env.mx.Lock()
+	env.dict[key] = value
+	env.mx.Unlock()
+	return nil
+}
+
+func Getenv(key string) (value string, found bool) {
+	env.mx.RLock()
+	val, ok := env.dict[key]
+	env.mx.RUnlock()
+	return val, ok
+}
+
+func Unsetenv(key string) error {
+	env.mx.Lock()
+	delete(env.dict, key)
+	env.mx.Unlock()
+	return nil
+}
+
+func Clearenv() {
+	env.mx.Lock()
+	for key := range env.dict {
+		delete(env.dict, key)
+	}
+	env.mx.Unlock()
+}
+
+func Environ() []string {
+	env.mx.RLock()
+	a := make([]string, 0, len(env.dict))
+	for key, val := range env.dict {
+		a = append(a, key+"="+val)
+	}
+	env.mx.RUnlock()
+	return a
+}
+
+type SysProcAttr struct{}
+
+const (
+	O_RDONLY = 0x0
+	O_WRONLY = 0x1
+	O_RDWR   = 0x2
+	O_CREAT  = 0x40
+	O_EXCL   = 0x80
+	O_TRUNC  = 0x200
+	O_APPEND = 0x400
+	O_SYNC   = 0x101000
+)
+
+type Qid struct {
+	Path uint64 // the file server's unique identification for the file
+}
+
+type Dir struct {
+	// system-modified data
+	Type uint16 // server type
+	Dev  uint32 // server subtype
+
+	// file data
+	Qid Qid // unique id from server
+}
+
+func Getpid() int {
+	return 1
+}
+
+func Getppid() int {
+	return 1
+}
+
+func Mkdir(path string, mode uint32) (err error)
+
+func Chdir(path string) error {
+	return ENOTSUP
+}
+
+const ImplementsGetwd = true
+
+func Getwd() (wd string, err error) {
+	return "", ENOTSUP
+}
+
+func Getuid() int {
+	return -1
+}
+
+func Geteuid() int {
+	return -1
+}
+
+func Getgid() int {
+	return -1
+}
+
+func Getegid() int {
+	return -1
+}
+
+func Getgroups() (gids []int, err error) {
+	return nil, ENOTSUP
+}
diff --git a/src/syscall/zerrors_linux_thumb.go b/src/syscall/zerrors_linux_thumb.go
new file mode 100644
index 0000000000..e7fddf95d1
--- /dev/null
+++ b/src/syscall/zerrors_linux_thumb.go
@@ -0,0 +1,1560 @@
+// mkerrors.sh
+// Code generated by the command above; DO NOT EDIT.
+
+// Created by cgo -godefs - DO NOT EDIT
+// cgo -godefs -- _const.go
+
+// +build thumb,linux
+
+package syscall
+
+const (
+	AF_ALG                           = 0x26
+	AF_APPLETALK                     = 0x5
+	AF_ASH                           = 0x12
+	AF_ATMPVC                        = 0x8
+	AF_ATMSVC                        = 0x14
+	AF_AX25                          = 0x3
+	AF_BLUETOOTH                     = 0x1f
+	AF_BRIDGE                        = 0x7
+	AF_CAIF                          = 0x25
+	AF_CAN                           = 0x1d
+	AF_DECnet                        = 0xc
+	AF_ECONET                        = 0x13
+	AF_FILE                          = 0x1
+	AF_IEEE802154                    = 0x24
+	AF_INET                          = 0x2
+	AF_INET6                         = 0xa
+	AF_IPX                           = 0x4
+	AF_IRDA                          = 0x17
+	AF_ISDN                          = 0x22
+	AF_IUCV                          = 0x20
+	AF_KEY                           = 0xf
+	AF_LLC                           = 0x1a
+	AF_LOCAL                         = 0x1
+	AF_MAX                           = 0x27
+	AF_NETBEUI                       = 0xd
+	AF_NETLINK                       = 0x10
+	AF_NETROM                        = 0x6
+	AF_PACKET                        = 0x11
+	AF_PHONET                        = 0x23
+	AF_PPPOX                         = 0x18
+	AF_RDS                           = 0x15
+	AF_ROSE                          = 0xb
+	AF_ROUTE                         = 0x10
+	AF_RXRPC                         = 0x21
+	AF_SECURITY                      = 0xe
+	AF_SNA                           = 0x16
+	AF_TIPC                          = 0x1e
+	AF_UNIX                          = 0x1
+	AF_UNSPEC                        = 0x0
+	AF_WANPIPE                       = 0x19
+	AF_X25                           = 0x9
+	ARPHRD_ADAPT                     = 0x108
+	ARPHRD_APPLETLK                  = 0x8
+	ARPHRD_ARCNET                    = 0x7
+	ARPHRD_ASH                       = 0x30d
+	ARPHRD_ATM                       = 0x13
+	ARPHRD_AX25                      = 0x3
+	ARPHRD_BIF                       = 0x307
+	ARPHRD_CHAOS                     = 0x5
+	ARPHRD_CISCO                     = 0x201
+	ARPHRD_CSLIP                     = 0x101
+	ARPHRD_CSLIP6                    = 0x103
+	ARPHRD_DDCMP                     = 0x205
+	ARPHRD_DLCI                      = 0xf
+	ARPHRD_ECONET                    = 0x30e
+	ARPHRD_EETHER                    = 0x2
+	ARPHRD_ETHER                     = 0x1
+	ARPHRD_EUI64                     = 0x1b
+	ARPHRD_FCAL                      = 0x311
+	ARPHRD_FCFABRIC                  = 0x313
+	ARPHRD_FCPL                      = 0x312
+	ARPHRD_FCPP                      = 0x310
+	ARPHRD_FDDI                      = 0x306
+	ARPHRD_FRAD                      = 0x302
+	ARPHRD_HDLC                      = 0x201
+	ARPHRD_HIPPI                     = 0x30c
+	ARPHRD_HWX25                     = 0x110
+	ARPHRD_IEEE1394                  = 0x18
+	ARPHRD_IEEE802                   = 0x6
+	ARPHRD_IEEE80211                 = 0x321
+	ARPHRD_IEEE80211_PRISM           = 0x322
+	ARPHRD_IEEE80211_RADIOTAP        = 0x323
+	ARPHRD_IEEE802154                = 0x324
+	ARPHRD_IEEE802154_PHY            = 0x325
+	ARPHRD_IEEE802_TR                = 0x320
+	ARPHRD_INFINIBAND                = 0x20
+	ARPHRD_IPDDP                     = 0x309
+	ARPHRD_IPGRE                     = 0x30a
+	ARPHRD_IRDA                      = 0x30f
+	ARPHRD_LAPB                      = 0x204
+	ARPHRD_LOCALTLK                  = 0x305
+	ARPHRD_LOOPBACK                  = 0x304
+	ARPHRD_METRICOM                  = 0x17
+	ARPHRD_NETROM                    = 0x0
+	ARPHRD_NONE                      = 0xfffe
+	ARPHRD_PIMREG                    = 0x30b
+	ARPHRD_PPP                       = 0x200
+	ARPHRD_PRONET                    = 0x4
+	ARPHRD_RAWHDLC                   = 0x206
+	ARPHRD_ROSE                      = 0x10e
+	ARPHRD_RSRVD                     = 0x104
+	ARPHRD_SIT                       = 0x308
+	ARPHRD_SKIP                      = 0x303
+	ARPHRD_SLIP                      = 0x100
+	ARPHRD_SLIP6                     = 0x102
+	ARPHRD_TUNNEL                    = 0x300
+	ARPHRD_TUNNEL6                   = 0x301
+	ARPHRD_VOID                      = 0xffff
+	ARPHRD_X25                       = 0x10f
+	BPF_A                            = 0x10
+	BPF_ABS                          = 0x20
+	BPF_ADD                          = 0x0
+	BPF_ALU                          = 0x4
+	BPF_AND                          = 0x50
+	BPF_B                            = 0x10
+	BPF_DIV                          = 0x30
+	BPF_H                            = 0x8
+	BPF_IMM                          = 0x0
+	BPF_IND                          = 0x40
+	BPF_JA                           = 0x0
+	BPF_JEQ                          = 0x10
+	BPF_JGE                          = 0x30
+	BPF_JGT                          = 0x20
+	BPF_JMP                          = 0x5
+	BPF_JSET                         = 0x40
+	BPF_K                            = 0x0
+	BPF_LD                           = 0x0
+	BPF_LDX                          = 0x1
+	BPF_LEN                          = 0x80
+	BPF_LSH                          = 0x60
+	BPF_MAJOR_VERSION                = 0x1
+	BPF_MAXINSNS                     = 0x1000
+	BPF_MEM                          = 0x60
+	BPF_MEMWORDS                     = 0x10
+	BPF_MINOR_VERSION                = 0x1
+	BPF_MISC                         = 0x7
+	BPF_MSH                          = 0xa0
+	BPF_MUL                          = 0x20
+	BPF_NEG                          = 0x80
+	BPF_OR                           = 0x40
+	BPF_RET                          = 0x6
+	BPF_RSH                          = 0x70
+	BPF_ST                           = 0x2
+	BPF_STX                          = 0x3
+	BPF_SUB                          = 0x10
+	BPF_TAX                          = 0x0
+	BPF_TXA                          = 0x80
+	BPF_W                            = 0x0
+	BPF_X                            = 0x8
+	CLONE_CHILD_CLEARTID             = 0x200000
+	CLONE_CHILD_SETTID               = 0x1000000
+	CLONE_DETACHED                   = 0x400000
+	CLONE_FILES                      = 0x400
+	CLONE_FS                         = 0x200
+	CLONE_IO                         = 0x80000000
+	CLONE_NEWIPC                     = 0x8000000
+	CLONE_NEWNET                     = 0x40000000
+	CLONE_NEWNS                      = 0x20000
+	CLONE_NEWPID                     = 0x20000000
+	CLONE_NEWUSER                    = 0x10000000
+	CLONE_NEWUTS                     = 0x4000000
+	CLONE_PARENT                     = 0x8000
+	CLONE_PARENT_SETTID              = 0x100000
+	CLONE_PTRACE                     = 0x2000
+	CLONE_SETTLS                     = 0x80000
+	CLONE_SIGHAND                    = 0x800
+	CLONE_SYSVSEM                    = 0x40000
+	CLONE_THREAD                     = 0x10000
+	CLONE_UNTRACED                   = 0x800000
+	CLONE_VFORK                      = 0x4000
+	CLONE_VM                         = 0x100
+	DT_BLK                           = 0x6
+	DT_CHR                           = 0x2
+	DT_DIR                           = 0x4
+	DT_FIFO                          = 0x1
+	DT_LNK                           = 0xa
+	DT_REG                           = 0x8
+	DT_SOCK                          = 0xc
+	DT_UNKNOWN                       = 0x0
+	DT_WHT                           = 0xe
+	ELF_NGREG                        = 0x12
+	ELF_PRARGSZ                      = 0x50
+	EPOLLERR                         = 0x8
+	EPOLLET                          = -0x80000000
+	EPOLLHUP                         = 0x10
+	EPOLLIN                          = 0x1
+	EPOLLMSG                         = 0x400
+	EPOLLONESHOT                     = 0x40000000
+	EPOLLOUT                         = 0x4
+	EPOLLPRI                         = 0x2
+	EPOLLRDBAND                      = 0x80
+	EPOLLRDHUP                       = 0x2000
+	EPOLLRDNORM                      = 0x40
+	EPOLLWRBAND                      = 0x200
+	EPOLLWRNORM                      = 0x100
+	EPOLL_CLOEXEC                    = 0x80000
+	EPOLL_CTL_ADD                    = 0x1
+	EPOLL_CTL_DEL                    = 0x2
+	EPOLL_CTL_MOD                    = 0x3
+	EPOLL_NONBLOCK                   = 0x800
+	ETH_P_1588                       = 0x88f7
+	ETH_P_8021Q                      = 0x8100
+	ETH_P_802_2                      = 0x4
+	ETH_P_802_3                      = 0x1
+	ETH_P_AARP                       = 0x80f3
+	ETH_P_ALL                        = 0x3
+	ETH_P_AOE                        = 0x88a2
+	ETH_P_ARCNET                     = 0x1a
+	ETH_P_ARP                        = 0x806
+	ETH_P_ATALK                      = 0x809b
+	ETH_P_ATMFATE                    = 0x8884
+	ETH_P_ATMMPOA                    = 0x884c
+	ETH_P_AX25                       = 0x2
+	ETH_P_BPQ                        = 0x8ff
+	ETH_P_CAIF                       = 0xf7
+	ETH_P_CAN                        = 0xc
+	ETH_P_CONTROL                    = 0x16
+	ETH_P_CUST                       = 0x6006
+	ETH_P_DDCMP                      = 0x6
+	ETH_P_DEC                        = 0x6000
+	ETH_P_DIAG                       = 0x6005
+	ETH_P_DNA_DL                     = 0x6001
+	ETH_P_DNA_RC                     = 0x6002
+	ETH_P_DNA_RT                     = 0x6003
+	ETH_P_DSA                        = 0x1b
+	ETH_P_ECONET                     = 0x18
+	ETH_P_EDSA                       = 0xdada
+	ETH_P_FCOE                       = 0x8906
+	ETH_P_FIP                        = 0x8914
+	ETH_P_HDLC                       = 0x19
+	ETH_P_IEEE802154                 = 0xf6
+	ETH_P_IEEEPUP                    = 0xa00
+	ETH_P_IEEEPUPAT                  = 0xa01
+	ETH_P_IP                         = 0x800
+	ETH_P_IPV6                       = 0x86dd
+	ETH_P_IPX                        = 0x8137
+	ETH_P_IRDA                       = 0x17
+	ETH_P_LAT                        = 0x6004
+	ETH_P_LINK_CTL                   = 0x886c
+	ETH_P_LOCALTALK                  = 0x9
+	ETH_P_LOOP                       = 0x60
+	ETH_P_MOBITEX                    = 0x15
+	ETH_P_MPLS_MC                    = 0x8848
+	ETH_P_MPLS_UC                    = 0x8847
+	ETH_P_PAE                        = 0x888e
+	ETH_P_PAUSE                      = 0x8808
+	ETH_P_PHONET                     = 0xf5
+	ETH_P_PPPTALK                    = 0x10
+	ETH_P_PPP_DISC                   = 0x8863
+	ETH_P_PPP_MP                     = 0x8
+	ETH_P_PPP_SES                    = 0x8864
+	ETH_P_PUP                        = 0x200
+	ETH_P_PUPAT                      = 0x201
+	ETH_P_RARP                       = 0x8035
+	ETH_P_SCA                        = 0x6007
+	ETH_P_SLOW                       = 0x8809
+	ETH_P_SNAP                       = 0x5
+	ETH_P_TEB                        = 0x6558
+	ETH_P_TIPC                       = 0x88ca
+	ETH_P_TRAILER                    = 0x1c
+	ETH_P_TR_802_2                   = 0x11
+	ETH_P_WAN_PPP                    = 0x7
+	ETH_P_WCCP                       = 0x883e
+	ETH_P_X25                        = 0x805
+	FD_CLOEXEC                       = 0x1
+	FD_SETSIZE                       = 0x400
+	F_DUPFD                          = 0x0
+	F_DUPFD_CLOEXEC                  = 0x406
+	F_EXLCK                          = 0x4
+	F_GETFD                          = 0x1
+	F_GETFL                          = 0x3
+	F_GETLEASE                       = 0x401
+	F_GETLK                          = 0xc
+	F_GETLK64                        = 0xc
+	F_GETOWN                         = 0x9
+	F_GETOWN_EX                      = 0x10
+	F_GETPIPE_SZ                     = 0x408
+	F_GETSIG                         = 0xb
+	F_LOCK                           = 0x1
+	F_NOTIFY                         = 0x402
+	F_OK                             = 0x0
+	F_RDLCK                          = 0x0
+	F_SETFD                          = 0x2
+	F_SETFL                          = 0x4
+	F_SETLEASE                       = 0x400
+	F_SETLK                          = 0xd
+	F_SETLK64                        = 0xd
+	F_SETLKW                         = 0xe
+	F_SETLKW64                       = 0xe
+	F_SETOWN                         = 0x8
+	F_SETOWN_EX                      = 0xf
+	F_SETPIPE_SZ                     = 0x407
+	F_SETSIG                         = 0xa
+	F_SHLCK                          = 0x8
+	F_TEST                           = 0x3
+	F_TLOCK                          = 0x2
+	F_ULOCK                          = 0x0
+	F_UNLCK                          = 0x2
+	F_WRLCK                          = 0x1
+	ICMPV6_FILTER                    = 0x1
+	IFA_F_DADFAILED                  = 0x8
+	IFA_F_DEPRECATED                 = 0x20
+	IFA_F_HOMEADDRESS                = 0x10
+	IFA_F_NODAD                      = 0x2
+	IFA_F_OPTIMISTIC                 = 0x4
+	IFA_F_PERMANENT                  = 0x80
+	IFA_F_SECONDARY                  = 0x1
+	IFA_F_TEMPORARY                  = 0x1
+	IFA_F_TENTATIVE                  = 0x40
+	IFA_MAX                          = 0x7
+	IFF_ALLMULTI                     = 0x200
+	IFF_AUTOMEDIA                    = 0x4000
+	IFF_BROADCAST                    = 0x2
+	IFF_DEBUG                        = 0x4
+	IFF_DYNAMIC                      = 0x8000
+	IFF_LOOPBACK                     = 0x8
+	IFF_MASTER                       = 0x400
+	IFF_MULTICAST                    = 0x1000
+	IFF_NOARP                        = 0x80
+	IFF_NOTRAILERS                   = 0x20
+	IFF_NO_PI                        = 0x1000
+	IFF_ONE_QUEUE                    = 0x2000
+	IFF_POINTOPOINT                  = 0x10
+	IFF_PORTSEL                      = 0x2000
+	IFF_PROMISC                      = 0x100
+	IFF_RUNNING                      = 0x40
+	IFF_SLAVE                        = 0x800
+	IFF_TAP                          = 0x2
+	IFF_TUN                          = 0x1
+	IFF_TUN_EXCL                     = 0x8000
+	IFF_UP                           = 0x1
+	IFF_VNET_HDR                     = 0x4000
+	IFNAMSIZ                         = 0x10
+	IN_ACCESS                        = 0x1
+	IN_ALL_EVENTS                    = 0xfff
+	IN_ATTRIB                        = 0x4
+	IN_CLASSA_HOST                   = 0xffffff
+	IN_CLASSA_MAX                    = 0x80
+	IN_CLASSA_NET                    = 0xff000000
+	IN_CLASSA_NSHIFT                 = 0x18
+	IN_CLASSB_HOST                   = 0xffff
+	IN_CLASSB_MAX                    = 0x10000
+	IN_CLASSB_NET                    = 0xffff0000
+	IN_CLASSB_NSHIFT                 = 0x10
+	IN_CLASSC_HOST                   = 0xff
+	IN_CLASSC_NET                    = 0xffffff00
+	IN_CLASSC_NSHIFT                 = 0x8
+	IN_CLOEXEC                       = 0x80000
+	IN_CLOSE                         = 0x18
+	IN_CLOSE_NOWRITE                 = 0x10
+	IN_CLOSE_WRITE                   = 0x8
+	IN_CREATE                        = 0x100
+	IN_DELETE                        = 0x200
+	IN_DELETE_SELF                   = 0x400
+	IN_DONT_FOLLOW                   = 0x2000000
+	IN_EXCL_UNLINK                   = 0x4000000
+	IN_IGNORED                       = 0x8000
+	IN_ISDIR                         = 0x40000000
+	IN_LOOPBACKNET                   = 0x7f
+	IN_MASK_ADD                      = 0x20000000
+	IN_MODIFY                        = 0x2
+	IN_MOVE                          = 0xc0
+	IN_MOVED_FROM                    = 0x40
+	IN_MOVED_TO                      = 0x80
+	IN_MOVE_SELF                     = 0x800
+	IN_NONBLOCK                      = 0x800
+	IN_ONESHOT                       = 0x80000000
+	IN_ONLYDIR                       = 0x1000000
+	IN_OPEN                          = 0x20
+	IN_Q_OVERFLOW                    = 0x4000
+	IN_UNMOUNT                       = 0x2000
+	IPPROTO_AH                       = 0x33
+	IPPROTO_COMP                     = 0x6c
+	IPPROTO_DCCP                     = 0x21
+	IPPROTO_DSTOPTS                  = 0x3c
+	IPPROTO_EGP                      = 0x8
+	IPPROTO_ENCAP                    = 0x62
+	IPPROTO_ESP                      = 0x32
+	IPPROTO_FRAGMENT                 = 0x2c
+	IPPROTO_GRE                      = 0x2f
+	IPPROTO_HOPOPTS                  = 0x0
+	IPPROTO_ICMP                     = 0x1
+	IPPROTO_ICMPV6                   = 0x3a
+	IPPROTO_IDP                      = 0x16
+	IPPROTO_IGMP                     = 0x2
+	IPPROTO_IP                       = 0x0
+	IPPROTO_IPIP                     = 0x4
+	IPPROTO_IPV6                     = 0x29
+	IPPROTO_MTP                      = 0x5c
+	IPPROTO_NONE                     = 0x3b
+	IPPROTO_PIM                      = 0x67
+	IPPROTO_PUP                      = 0xc
+	IPPROTO_RAW                      = 0xff
+	IPPROTO_ROUTING                  = 0x2b
+	IPPROTO_RSVP                     = 0x2e
+	IPPROTO_SCTP                     = 0x84
+	IPPROTO_TCP                      = 0x6
+	IPPROTO_TP                       = 0x1d
+	IPPROTO_UDP                      = 0x11
+	IPPROTO_UDPLITE                  = 0x88
+	IPV6_2292DSTOPTS                 = 0x4
+	IPV6_2292HOPLIMIT                = 0x8
+	IPV6_2292HOPOPTS                 = 0x3
+	IPV6_2292PKTINFO                 = 0x2
+	IPV6_2292PKTOPTIONS              = 0x6
+	IPV6_2292RTHDR                   = 0x5
+	IPV6_ADDRFORM                    = 0x1
+	IPV6_ADD_MEMBERSHIP              = 0x14
+	IPV6_AUTHHDR                     = 0xa
+	IPV6_CHECKSUM                    = 0x7
+	IPV6_DROP_MEMBERSHIP             = 0x15
+	IPV6_DSTOPTS                     = 0x3b
+	IPV6_HOPLIMIT                    = 0x34
+	IPV6_HOPOPTS                     = 0x36
+	IPV6_IPSEC_POLICY                = 0x22
+	IPV6_JOIN_ANYCAST                = 0x1b
+	IPV6_JOIN_GROUP                  = 0x14
+	IPV6_LEAVE_ANYCAST               = 0x1c
+	IPV6_LEAVE_GROUP                 = 0x15
+	IPV6_MTU                         = 0x18
+	IPV6_MTU_DISCOVER                = 0x17
+	IPV6_MULTICAST_HOPS              = 0x12
+	IPV6_MULTICAST_IF                = 0x11
+	IPV6_MULTICAST_LOOP              = 0x13
+	IPV6_NEXTHOP                     = 0x9
+	IPV6_PKTINFO                     = 0x32
+	IPV6_PMTUDISC_DO                 = 0x2
+	IPV6_PMTUDISC_DONT               = 0x0
+	IPV6_PMTUDISC_PROBE              = 0x3
+	IPV6_PMTUDISC_WANT               = 0x1
+	IPV6_RECVDSTOPTS                 = 0x3a
+	IPV6_RECVERR                     = 0x19
+	IPV6_RECVHOPLIMIT                = 0x33
+	IPV6_RECVHOPOPTS                 = 0x35
+	IPV6_RECVPKTINFO                 = 0x31
+	IPV6_RECVRTHDR                   = 0x38
+	IPV6_RECVTCLASS                  = 0x42
+	IPV6_ROUTER_ALERT                = 0x16
+	IPV6_RTHDR                       = 0x39
+	IPV6_RTHDRDSTOPTS                = 0x37
+	IPV6_RTHDR_LOOSE                 = 0x0
+	IPV6_RTHDR_STRICT                = 0x1
+	IPV6_RTHDR_TYPE_0                = 0x0
+	IPV6_RXDSTOPTS                   = 0x3b
+	IPV6_RXHOPOPTS                   = 0x36
+	IPV6_TCLASS                      = 0x43
+	IPV6_UNICAST_HOPS                = 0x10
+	IPV6_V6ONLY                      = 0x1a
+	IPV6_XFRM_POLICY                 = 0x23
+	IP_ADD_MEMBERSHIP                = 0x23
+	IP_ADD_SOURCE_MEMBERSHIP         = 0x27
+	IP_BLOCK_SOURCE                  = 0x26
+	IP_DEFAULT_MULTICAST_LOOP        = 0x1
+	IP_DEFAULT_MULTICAST_TTL         = 0x1
+	IP_DF                            = 0x4000
+	IP_DROP_MEMBERSHIP               = 0x24
+	IP_DROP_SOURCE_MEMBERSHIP        = 0x28
+	IP_FREEBIND                      = 0xf
+	IP_HDRINCL                       = 0x3
+	IP_IPSEC_POLICY                  = 0x10
+	IP_MAXPACKET                     = 0xffff
+	IP_MAX_MEMBERSHIPS               = 0x14
+	IP_MF                            = 0x2000
+	IP_MINTTL                        = 0x15
+	IP_MSFILTER                      = 0x29
+	IP_MSS                           = 0x240
+	IP_MTU                           = 0xe
+	IP_MTU_DISCOVER                  = 0xa
+	IP_MULTICAST_IF                  = 0x20
+	IP_MULTICAST_LOOP                = 0x22
+	IP_MULTICAST_TTL                 = 0x21
+	IP_OFFMASK                       = 0x1fff
+	IP_OPTIONS                       = 0x4
+	IP_ORIGDSTADDR                   = 0x14
+	IP_PASSSEC                       = 0x12
+	IP_PKTINFO                       = 0x8
+	IP_PKTOPTIONS                    = 0x9
+	IP_PMTUDISC                      = 0xa
+	IP_PMTUDISC_DO                   = 0x2
+	IP_PMTUDISC_DONT                 = 0x0
+	IP_PMTUDISC_PROBE                = 0x3
+	IP_PMTUDISC_WANT                 = 0x1
+	IP_RECVERR                       = 0xb
+	IP_RECVOPTS                      = 0x6
+	IP_RECVORIGDSTADDR               = 0x14
+	IP_RECVRETOPTS                   = 0x7
+	IP_RECVTOS                       = 0xd
+	IP_RECVTTL                       = 0xc
+	IP_RETOPTS                       = 0x7
+	IP_RF                            = 0x8000
+	IP_ROUTER_ALERT                  = 0x5
+	IP_TOS                           = 0x1
+	IP_TRANSPARENT                   = 0x13
+	IP_TTL                           = 0x2
+	IP_UNBLOCK_SOURCE                = 0x25
+	IP_XFRM_POLICY                   = 0x11
+	LINUX_REBOOT_CMD_CAD_OFF         = 0x0
+	LINUX_REBOOT_CMD_CAD_ON          = 0x89abcdef
+	LINUX_REBOOT_CMD_HALT            = 0xcdef0123
+	LINUX_REBOOT_CMD_KEXEC           = 0x45584543
+	LINUX_REBOOT_CMD_POWER_OFF       = 0x4321fedc
+	LINUX_REBOOT_CMD_RESTART         = 0x1234567
+	LINUX_REBOOT_CMD_RESTART2        = 0xa1b2c3d4
+	LINUX_REBOOT_CMD_SW_SUSPEND      = 0xd000fce2
+	LINUX_REBOOT_MAGIC1              = 0xfee1dead
+	LINUX_REBOOT_MAGIC2              = 0x28121969
+	LOCK_EX                          = 0x2
+	LOCK_NB                          = 0x4
+	LOCK_SH                          = 0x1
+	LOCK_UN                          = 0x8
+	MADV_DOFORK                      = 0xb
+	MADV_DONTFORK                    = 0xa
+	MADV_DONTNEED                    = 0x4
+	MADV_HUGEPAGE                    = 0xe
+	MADV_HWPOISON                    = 0x64
+	MADV_MERGEABLE                   = 0xc
+	MADV_NOHUGEPAGE                  = 0xf
+	MADV_NORMAL                      = 0x0
+	MADV_RANDOM                      = 0x1
+	MADV_REMOVE                      = 0x9
+	MADV_SEQUENTIAL                  = 0x2
+	MADV_UNMERGEABLE                 = 0xd
+	MADV_WILLNEED                    = 0x3
+	MAP_ANON                         = 0x20
+	MAP_ANONYMOUS                    = 0x20
+	MAP_DENYWRITE                    = 0x800
+	MAP_EXECUTABLE                   = 0x1000
+	MAP_FILE                         = 0x0
+	MAP_FIXED                        = 0x10
+	MAP_GROWSDOWN                    = 0x100
+	MAP_LOCKED                       = 0x2000
+	MAP_NONBLOCK                     = 0x10000
+	MAP_NORESERVE                    = 0x4000
+	MAP_POPULATE                     = 0x8000
+	MAP_PRIVATE                      = 0x2
+	MAP_SHARED                       = 0x1
+	MAP_TYPE                         = 0xf
+	MCL_CURRENT                      = 0x1
+	MCL_FUTURE                       = 0x2
+	MNT_DETACH                       = 0x2
+	MNT_EXPIRE                       = 0x4
+	MNT_FORCE                        = 0x1
+	MSG_CMSG_CLOEXEC                 = 0x40000000
+	MSG_CONFIRM                      = 0x800
+	MSG_CTRUNC                       = 0x8
+	MSG_DONTROUTE                    = 0x4
+	MSG_DONTWAIT                     = 0x40
+	MSG_EOR                          = 0x80
+	MSG_ERRQUEUE                     = 0x2000
+	MSG_FASTOPEN                     = 0x20000000
+	MSG_FIN                          = 0x200
+	MSG_MORE                         = 0x8000
+	MSG_NOSIGNAL                     = 0x4000
+	MSG_OOB                          = 0x1
+	MSG_PEEK                         = 0x2
+	MSG_PROXY                        = 0x10
+	MSG_RST                          = 0x1000
+	MSG_SYN                          = 0x400
+	MSG_TRUNC                        = 0x20
+	MSG_TRYHARD                      = 0x4
+	MSG_WAITALL                      = 0x100
+	MSG_WAITFORONE                   = 0x10000
+	MS_ACTIVE                        = 0x40000000
+	MS_ASYNC                         = 0x1
+	MS_BIND                          = 0x1000
+	MS_DIRSYNC                       = 0x80
+	MS_INVALIDATE                    = 0x2
+	MS_I_VERSION                     = 0x800000
+	MS_KERNMOUNT                     = 0x400000
+	MS_MANDLOCK                      = 0x40
+	MS_MGC_MSK                       = 0xffff0000
+	MS_MGC_VAL                       = 0xc0ed0000
+	MS_MOVE                          = 0x2000
+	MS_NOATIME                       = 0x400
+	MS_NODEV                         = 0x4
+	MS_NODIRATIME                    = 0x800
+	MS_NOEXEC                        = 0x8
+	MS_NOSUID                        = 0x2
+	MS_NOUSER                        = -0x80000000
+	MS_POSIXACL                      = 0x10000
+	MS_PRIVATE                       = 0x40000
+	MS_RDONLY                        = 0x1
+	MS_REC                           = 0x4000
+	MS_RELATIME                      = 0x200000
+	MS_REMOUNT                       = 0x20
+	MS_RMT_MASK                      = 0x800051
+	MS_SHARED                        = 0x100000
+	MS_SILENT                        = 0x8000
+	MS_SLAVE                         = 0x80000
+	MS_STRICTATIME                   = 0x1000000
+	MS_SYNC                          = 0x4
+	MS_SYNCHRONOUS                   = 0x10
+	MS_UNBINDABLE                    = 0x20000
+	NAME_MAX                         = 0xff
+	NETLINK_ADD_MEMBERSHIP           = 0x1
+	NETLINK_AUDIT                    = 0x9
+	NETLINK_BROADCAST_ERROR          = 0x4
+	NETLINK_CONNECTOR                = 0xb
+	NETLINK_DNRTMSG                  = 0xe
+	NETLINK_DROP_MEMBERSHIP          = 0x2
+	NETLINK_ECRYPTFS                 = 0x13
+	NETLINK_FIB_LOOKUP               = 0xa
+	NETLINK_FIREWALL                 = 0x3
+	NETLINK_GENERIC                  = 0x10
+	NETLINK_INET_DIAG                = 0x4
+	NETLINK_IP6_FW                   = 0xd
+	NETLINK_ISCSI                    = 0x8
+	NETLINK_KOBJECT_UEVENT           = 0xf
+	NETLINK_NETFILTER                = 0xc
+	NETLINK_NFLOG                    = 0x5
+	NETLINK_NO_ENOBUFS               = 0x5
+	NETLINK_PKTINFO                  = 0x3
+	NETLINK_RDMA                     = 0x14
+	NETLINK_ROUTE                    = 0x0
+	NETLINK_SCSITRANSPORT            = 0x12
+	NETLINK_SELINUX                  = 0x7
+	NETLINK_UNUSED                   = 0x1
+	NETLINK_USERSOCK                 = 0x2
+	NETLINK_XFRM                     = 0x6
+	NLA_ALIGNTO                      = 0x4
+	NLA_F_NESTED                     = 0x8000
+	NLA_F_NET_BYTEORDER              = 0x4000
+	NLA_HDRLEN                       = 0x4
+	NLMSG_ALIGNTO                    = 0x4
+	NLMSG_DONE                       = 0x3
+	NLMSG_ERROR                      = 0x2
+	NLMSG_HDRLEN                     = 0x10
+	NLMSG_MIN_TYPE                   = 0x10
+	NLMSG_NOOP                       = 0x1
+	NLMSG_OVERRUN                    = 0x4
+	NLM_F_ACK                        = 0x4
+	NLM_F_APPEND                     = 0x800
+	NLM_F_ATOMIC                     = 0x400
+	NLM_F_CREATE                     = 0x400
+	NLM_F_DUMP                       = 0x300
+	NLM_F_ECHO                       = 0x8
+	NLM_F_EXCL                       = 0x200
+	NLM_F_MATCH                      = 0x200
+	NLM_F_MULTI                      = 0x2
+	NLM_F_REPLACE                    = 0x100
+	NLM_F_REQUEST                    = 0x1
+	NLM_F_ROOT                       = 0x100
+	O_ACCMODE                        = 0x3
+	O_APPEND                         = 0x400
+	O_ASYNC                          = 0x2000
+	O_CLOEXEC                        = 0x80000
+	O_CREAT                          = 0x40
+	O_DIRECT                         = 0x10000
+	O_DIRECTORY                      = 0x4000
+	O_DSYNC                          = 0x1000
+	O_EXCL                           = 0x80
+	O_FSYNC                          = 0x1000
+	O_LARGEFILE                      = 0x20000
+	O_NDELAY                         = 0x800
+	O_NOATIME                        = 0x40000
+	O_NOCTTY                         = 0x100
+	O_NOFOLLOW                       = 0x8000
+	O_NONBLOCK                       = 0x800
+	O_RDONLY                         = 0x0
+	O_RDWR                           = 0x2
+	O_RSYNC                          = 0x1000
+	O_SYNC                           = 0x1000
+	O_TRUNC                          = 0x200
+	O_WRONLY                         = 0x1
+	PACKET_ADD_MEMBERSHIP            = 0x1
+	PACKET_BROADCAST                 = 0x1
+	PACKET_DROP_MEMBERSHIP           = 0x2
+	PACKET_FASTROUTE                 = 0x6
+	PACKET_HOST                      = 0x0
+	PACKET_LOOPBACK                  = 0x5
+	PACKET_MR_ALLMULTI               = 0x2
+	PACKET_MR_MULTICAST              = 0x0
+	PACKET_MR_PROMISC                = 0x1
+	PACKET_MULTICAST                 = 0x2
+	PACKET_OTHERHOST                 = 0x3
+	PACKET_OUTGOING                  = 0x4
+	PACKET_RECV_OUTPUT               = 0x3
+	PACKET_RX_RING                   = 0x5
+	PACKET_STATISTICS                = 0x6
+	PRIO_PGRP                        = 0x1
+	PRIO_PROCESS                     = 0x0
+	PRIO_USER                        = 0x2
+	PROT_EXEC                        = 0x4
+	PROT_GROWSDOWN                   = 0x1000000
+	PROT_GROWSUP                     = 0x2000000
+	PROT_NONE                        = 0x0
+	PROT_READ                        = 0x1
+	PROT_WRITE                       = 0x2
+	PR_CAPBSET_DROP                  = 0x18
+	PR_CAPBSET_READ                  = 0x17
+	PR_CLEAR_SECCOMP_FILTER          = 0x25
+	PR_ENDIAN_BIG                    = 0x0
+	PR_ENDIAN_LITTLE                 = 0x1
+	PR_ENDIAN_PPC_LITTLE             = 0x2
+	PR_FPEMU_NOPRINT                 = 0x1
+	PR_FPEMU_SIGFPE                  = 0x2
+	PR_FP_EXC_ASYNC                  = 0x2
+	PR_FP_EXC_DISABLED               = 0x0
+	PR_FP_EXC_DIV                    = 0x10000
+	PR_FP_EXC_INV                    = 0x100000
+	PR_FP_EXC_NONRECOV               = 0x1
+	PR_FP_EXC_OVF                    = 0x20000
+	PR_FP_EXC_PRECISE                = 0x3
+	PR_FP_EXC_RES                    = 0x80000
+	PR_FP_EXC_SW_ENABLE              = 0x80
+	PR_FP_EXC_UND                    = 0x40000
+	PR_GET_DUMPABLE                  = 0x3
+	PR_GET_ENDIAN                    = 0x13
+	PR_GET_FPEMU                     = 0x9
+	PR_GET_FPEXC                     = 0xb
+	PR_GET_KEEPCAPS                  = 0x7
+	PR_GET_NAME                      = 0x10
+	PR_GET_PDEATHSIG                 = 0x2
+	PR_GET_SECCOMP                   = 0x15
+	PR_GET_SECCOMP_FILTER            = 0x23
+	PR_GET_SECUREBITS                = 0x1b
+	PR_GET_TIMERSLACK                = 0x1e
+	PR_GET_TIMING                    = 0xd
+	PR_GET_TSC                       = 0x19
+	PR_GET_UNALIGN                   = 0x5
+	PR_MCE_KILL                      = 0x21
+	PR_MCE_KILL_CLEAR                = 0x0
+	PR_MCE_KILL_DEFAULT              = 0x2
+	PR_MCE_KILL_EARLY                = 0x1
+	PR_MCE_KILL_GET                  = 0x22
+	PR_MCE_KILL_LATE                 = 0x0
+	PR_MCE_KILL_SET                  = 0x1
+	PR_SECCOMP_FILTER_EVENT          = 0x1
+	PR_SECCOMP_FILTER_SYSCALL        = 0x0
+	PR_SET_DUMPABLE                  = 0x4
+	PR_SET_ENDIAN                    = 0x14
+	PR_SET_FPEMU                     = 0xa
+	PR_SET_FPEXC                     = 0xc
+	PR_SET_KEEPCAPS                  = 0x8
+	PR_SET_NAME                      = 0xf
+	PR_SET_PDEATHSIG                 = 0x1
+	PR_SET_PTRACER                   = 0x59616d61
+	PR_SET_SECCOMP                   = 0x16
+	PR_SET_SECCOMP_FILTER            = 0x24
+	PR_SET_SECUREBITS                = 0x1c
+	PR_SET_TIMERSLACK                = 0x1d
+	PR_SET_TIMING                    = 0xe
+	PR_SET_TSC                       = 0x1a
+	PR_SET_UNALIGN                   = 0x6
+	PR_TASK_PERF_EVENTS_DISABLE      = 0x1f
+	PR_TASK_PERF_EVENTS_ENABLE       = 0x20
+	PR_TIMING_STATISTICAL            = 0x0
+	PR_TIMING_TIMESTAMP              = 0x1
+	PR_TSC_ENABLE                    = 0x1
+	PR_TSC_SIGSEGV                   = 0x2
+	PR_UNALIGN_NOPRINT               = 0x1
+	PR_UNALIGN_SIGBUS                = 0x2
+	PTRACE_ATTACH                    = 0x10
+	PTRACE_CONT                      = 0x7
+	PTRACE_DETACH                    = 0x11
+	PTRACE_EVENT_CLONE               = 0x3
+	PTRACE_EVENT_EXEC                = 0x4
+	PTRACE_EVENT_EXIT                = 0x6
+	PTRACE_EVENT_FORK                = 0x1
+	PTRACE_EVENT_VFORK               = 0x2
+	PTRACE_EVENT_VFORK_DONE          = 0x5
+	PTRACE_GETCRUNCHREGS             = 0x19
+	PTRACE_GETEVENTMSG               = 0x4201
+	PTRACE_GETFPREGS                 = 0xe
+	PTRACE_GETHBPREGS                = 0x1d
+	PTRACE_GETREGS                   = 0xc
+	PTRACE_GETREGSET                 = 0x4204
+	PTRACE_GETSIGINFO                = 0x4202
+	PTRACE_GETVFPREGS                = 0x1b
+	PTRACE_GETWMMXREGS               = 0x12
+	PTRACE_GET_THREAD_AREA           = 0x16
+	PTRACE_KILL                      = 0x8
+	PTRACE_OLDSETOPTIONS             = 0x15
+	PTRACE_O_MASK                    = 0x7f
+	PTRACE_O_TRACECLONE              = 0x8
+	PTRACE_O_TRACEEXEC               = 0x10
+	PTRACE_O_TRACEEXIT               = 0x40
+	PTRACE_O_TRACEFORK               = 0x2
+	PTRACE_O_TRACESYSGOOD            = 0x1
+	PTRACE_O_TRACEVFORK              = 0x4
+	PTRACE_O_TRACEVFORKDONE          = 0x20
+	PTRACE_PEEKDATA                  = 0x2
+	PTRACE_PEEKTEXT                  = 0x1
+	PTRACE_PEEKUSR                   = 0x3
+	PTRACE_POKEDATA                  = 0x5
+	PTRACE_POKETEXT                  = 0x4
+	PTRACE_POKEUSR                   = 0x6
+	PTRACE_SETCRUNCHREGS             = 0x1a
+	PTRACE_SETFPREGS                 = 0xf
+	PTRACE_SETHBPREGS                = 0x1e
+	PTRACE_SETOPTIONS                = 0x4200
+	PTRACE_SETREGS                   = 0xd
+	PTRACE_SETREGSET                 = 0x4205
+	PTRACE_SETSIGINFO                = 0x4203
+	PTRACE_SETVFPREGS                = 0x1c
+	PTRACE_SETWMMXREGS               = 0x13
+	PTRACE_SET_SYSCALL               = 0x17
+	PTRACE_SINGLESTEP                = 0x9
+	PTRACE_SYSCALL                   = 0x18
+	PTRACE_TRACEME                   = 0x0
+	PT_DATA_ADDR                     = 0x10004
+	PT_TEXT_ADDR                     = 0x10000
+	PT_TEXT_END_ADDR                 = 0x10008
+	RLIMIT_AS                        = 0x9
+	RLIMIT_CORE                      = 0x4
+	RLIMIT_CPU                       = 0x0
+	RLIMIT_DATA                      = 0x2
+	RLIMIT_FSIZE                     = 0x1
+	RLIMIT_NOFILE                    = 0x7
+	RLIMIT_STACK                     = 0x3
+	RLIM_INFINITY                    = -0x1
+	RTAX_ADVMSS                      = 0x8
+	RTAX_CWND                        = 0x7
+	RTAX_FEATURES                    = 0xc
+	RTAX_FEATURE_ALLFRAG             = 0x8
+	RTAX_FEATURE_ECN                 = 0x1
+	RTAX_FEATURE_SACK                = 0x2
+	RTAX_FEATURE_TIMESTAMP           = 0x4
+	RTAX_HOPLIMIT                    = 0xa
+	RTAX_INITCWND                    = 0xb
+	RTAX_INITRWND                    = 0xe
+	RTAX_LOCK                        = 0x1
+	RTAX_MAX                         = 0xe
+	RTAX_MTU                         = 0x2
+	RTAX_REORDERING                  = 0x9
+	RTAX_RTO_MIN                     = 0xd
+	RTAX_RTT                         = 0x4
+	RTAX_RTTVAR                      = 0x5
+	RTAX_SSTHRESH                    = 0x6
+	RTAX_UNSPEC                      = 0x0
+	RTAX_WINDOW                      = 0x3
+	RTA_ALIGNTO                      = 0x4
+	RTA_MAX                          = 0x10
+	RTCF_DIRECTSRC                   = 0x4000000
+	RTCF_DOREDIRECT                  = 0x1000000
+	RTCF_LOG                         = 0x2000000
+	RTCF_MASQ                        = 0x400000
+	RTCF_NAT                         = 0x800000
+	RTCF_VALVE                       = 0x200000
+	RTF_ADDRCLASSMASK                = 0xf8000000
+	RTF_ADDRCONF                     = 0x40000
+	RTF_ALLONLINK                    = 0x20000
+	RTF_BROADCAST                    = 0x10000000
+	RTF_CACHE                        = 0x1000000
+	RTF_DEFAULT                      = 0x10000
+	RTF_DYNAMIC                      = 0x10
+	RTF_FLOW                         = 0x2000000
+	RTF_GATEWAY                      = 0x2
+	RTF_HOST                         = 0x4
+	RTF_INTERFACE                    = 0x40000000
+	RTF_IRTT                         = 0x100
+	RTF_LINKRT                       = 0x100000
+	RTF_LOCAL                        = 0x80000000
+	RTF_MODIFIED                     = 0x20
+	RTF_MSS                          = 0x40
+	RTF_MTU                          = 0x40
+	RTF_MULTICAST                    = 0x20000000
+	RTF_NAT                          = 0x8000000
+	RTF_NOFORWARD                    = 0x1000
+	RTF_NONEXTHOP                    = 0x200000
+	RTF_NOPMTUDISC                   = 0x4000
+	RTF_POLICY                       = 0x4000000
+	RTF_REINSTATE                    = 0x8
+	RTF_REJECT                       = 0x200
+	RTF_STATIC                       = 0x400
+	RTF_THROW                        = 0x2000
+	RTF_UP                           = 0x1
+	RTF_WINDOW                       = 0x80
+	RTF_XRESOLVE                     = 0x800
+	RTM_BASE                         = 0x10
+	RTM_DELACTION                    = 0x31
+	RTM_DELADDR                      = 0x15
+	RTM_DELADDRLABEL                 = 0x49
+	RTM_DELLINK                      = 0x11
+	RTM_DELNEIGH                     = 0x1d
+	RTM_DELQDISC                     = 0x25
+	RTM_DELROUTE                     = 0x19
+	RTM_DELRULE                      = 0x21
+	RTM_DELTCLASS                    = 0x29
+	RTM_DELTFILTER                   = 0x2d
+	RTM_F_CLONED                     = 0x200
+	RTM_F_EQUALIZE                   = 0x400
+	RTM_F_NOTIFY                     = 0x100
+	RTM_F_PREFIX                     = 0x800
+	RTM_GETACTION                    = 0x32
+	RTM_GETADDR                      = 0x16
+	RTM_GETADDRLABEL                 = 0x4a
+	RTM_GETANYCAST                   = 0x3e
+	RTM_GETDCB                       = 0x4e
+	RTM_GETLINK                      = 0x12
+	RTM_GETMULTICAST                 = 0x3a
+	RTM_GETNEIGH                     = 0x1e
+	RTM_GETNEIGHTBL                  = 0x42
+	RTM_GETQDISC                     = 0x26
+	RTM_GETROUTE                     = 0x1a
+	RTM_GETRULE                      = 0x22
+	RTM_GETTCLASS                    = 0x2a
+	RTM_GETTFILTER                   = 0x2e
+	RTM_MAX                          = 0x4f
+	RTM_NEWACTION                    = 0x30
+	RTM_NEWADDR                      = 0x14
+	RTM_NEWADDRLABEL                 = 0x48
+	RTM_NEWLINK                      = 0x10
+	RTM_NEWNDUSEROPT                 = 0x44
+	RTM_NEWNEIGH                     = 0x1c
+	RTM_NEWNEIGHTBL                  = 0x40
+	RTM_NEWPREFIX                    = 0x34
+	RTM_NEWQDISC                     = 0x24
+	RTM_NEWROUTE                     = 0x18
+	RTM_NEWRULE                      = 0x20
+	RTM_NEWTCLASS                    = 0x28
+	RTM_NEWTFILTER                   = 0x2c
+	RTM_NR_FAMILIES                  = 0x10
+	RTM_NR_MSGTYPES                  = 0x40
+	RTM_SETDCB                       = 0x4f
+	RTM_SETLINK                      = 0x13
+	RTM_SETNEIGHTBL                  = 0x43
+	RTNH_ALIGNTO                     = 0x4
+	RTNH_F_DEAD                      = 0x1
+	RTNH_F_ONLINK                    = 0x4
+	RTNH_F_PERVASIVE                 = 0x2
+	RTN_MAX                          = 0xb
+	RTPROT_BIRD                      = 0xc
+	RTPROT_BOOT                      = 0x3
+	RTPROT_DHCP                      = 0x10
+	RTPROT_DNROUTED                  = 0xd
+	RTPROT_GATED                     = 0x8
+	RTPROT_KERNEL                    = 0x2
+	RTPROT_MRT                       = 0xa
+	RTPROT_NTK                       = 0xf
+	RTPROT_RA                        = 0x9
+	RTPROT_REDIRECT                  = 0x1
+	RTPROT_STATIC                    = 0x4
+	RTPROT_UNSPEC                    = 0x0
+	RTPROT_XORP                      = 0xe
+	RTPROT_ZEBRA                     = 0xb
+	RT_CLASS_DEFAULT                 = 0xfd
+	RT_CLASS_LOCAL                   = 0xff
+	RT_CLASS_MAIN                    = 0xfe
+	RT_CLASS_MAX                     = 0xff
+	RT_CLASS_UNSPEC                  = 0x0
+	RUSAGE_CHILDREN                  = -0x1
+	RUSAGE_SELF                      = 0x0
+	RUSAGE_THREAD                    = 0x1
+	SCM_CREDENTIALS                  = 0x2
+	SCM_RIGHTS                       = 0x1
+	SCM_TIMESTAMP                    = 0x1d
+	SCM_TIMESTAMPING                 = 0x25
+	SCM_TIMESTAMPNS                  = 0x23
+	SHUT_RD                          = 0x0
+	SHUT_RDWR                        = 0x2
+	SHUT_WR                          = 0x1
+	SIOCADDDLCI                      = 0x8980
+	SIOCADDMULTI                     = 0x8931
+	SIOCADDRT                        = 0x890b
+	SIOCATMARK                       = 0x8905
+	SIOCDARP                         = 0x8953
+	SIOCDELDLCI                      = 0x8981
+	SIOCDELMULTI                     = 0x8932
+	SIOCDELRT                        = 0x890c
+	SIOCDEVPRIVATE                   = 0x89f0
+	SIOCDIFADDR                      = 0x8936
+	SIOCDRARP                        = 0x8960
+	SIOCGARP                         = 0x8954
+	SIOCGIFADDR                      = 0x8915
+	SIOCGIFBR                        = 0x8940
+	SIOCGIFBRDADDR                   = 0x8919
+	SIOCGIFCONF                      = 0x8912
+	SIOCGIFCOUNT                     = 0x8938
+	SIOCGIFDSTADDR                   = 0x8917
+	SIOCGIFENCAP                     = 0x8925
+	SIOCGIFFLAGS                     = 0x8913
+	SIOCGIFHWADDR                    = 0x8927
+	SIOCGIFINDEX                     = 0x8933
+	SIOCGIFMAP                       = 0x8970
+	SIOCGIFMEM                       = 0x891f
+	SIOCGIFMETRIC                    = 0x891d
+	SIOCGIFMTU                       = 0x8921
+	SIOCGIFNAME                      = 0x8910
+	SIOCGIFNETMASK                   = 0x891b
+	SIOCGIFPFLAGS                    = 0x8935
+	SIOCGIFSLAVE                     = 0x8929
+	SIOCGIFTXQLEN                    = 0x8942
+	SIOCGPGRP                        = 0x8904
+	SIOCGRARP                        = 0x8961
+	SIOCGSTAMP                       = 0x8906
+	SIOCGSTAMPNS                     = 0x8907
+	SIOCPROTOPRIVATE                 = 0x89e0
+	SIOCRTMSG                        = 0x890d
+	SIOCSARP                         = 0x8955
+	SIOCSIFADDR                      = 0x8916
+	SIOCSIFBR                        = 0x8941
+	SIOCSIFBRDADDR                   = 0x891a
+	SIOCSIFDSTADDR                   = 0x8918
+	SIOCSIFENCAP                     = 0x8926
+	SIOCSIFFLAGS                     = 0x8914
+	SIOCSIFHWADDR                    = 0x8924
+	SIOCSIFHWBROADCAST               = 0x8937
+	SIOCSIFLINK                      = 0x8911
+	SIOCSIFMAP                       = 0x8971
+	SIOCSIFMEM                       = 0x8920
+	SIOCSIFMETRIC                    = 0x891e
+	SIOCSIFMTU                       = 0x8922
+	SIOCSIFNAME                      = 0x8923
+	SIOCSIFNETMASK                   = 0x891c
+	SIOCSIFPFLAGS                    = 0x8934
+	SIOCSIFSLAVE                     = 0x8930
+	SIOCSIFTXQLEN                    = 0x8943
+	SIOCSPGRP                        = 0x8902
+	SIOCSRARP                        = 0x8962
+	SOCK_CLOEXEC                     = 0x80000
+	SOCK_DCCP                        = 0x6
+	SOCK_DGRAM                       = 0x2
+	SOCK_NONBLOCK                    = 0x800
+	SOCK_PACKET                      = 0xa
+	SOCK_RAW                         = 0x3
+	SOCK_RDM                         = 0x4
+	SOCK_SEQPACKET                   = 0x5
+	SOCK_STREAM                      = 0x1
+	SOL_AAL                          = 0x109
+	SOL_ATM                          = 0x108
+	SOL_DECNET                       = 0x105
+	SOL_ICMPV6                       = 0x3a
+	SOL_IP                           = 0x0
+	SOL_IPV6                         = 0x29
+	SOL_IRDA                         = 0x10a
+	SOL_PACKET                       = 0x107
+	SOL_RAW                          = 0xff
+	SOL_SOCKET                       = 0x1
+	SOL_TCP                          = 0x6
+	SOL_X25                          = 0x106
+	SOMAXCONN                        = 0x80
+	SO_ACCEPTCONN                    = 0x1e
+	SO_ATTACH_FILTER                 = 0x1a
+	SO_BINDTODEVICE                  = 0x19
+	SO_BROADCAST                     = 0x6
+	SO_BSDCOMPAT                     = 0xe
+	SO_DEBUG                         = 0x1
+	SO_DETACH_FILTER                 = 0x1b
+	SO_DOMAIN                        = 0x27
+	SO_DONTROUTE                     = 0x5
+	SO_ERROR                         = 0x4
+	SO_KEEPALIVE                     = 0x9
+	SO_LINGER                        = 0xd
+	SO_MARK                          = 0x24
+	SO_NO_CHECK                      = 0xb
+	SO_OOBINLINE                     = 0xa
+	SO_PASSCRED                      = 0x10
+	SO_PASSSEC                       = 0x22
+	SO_PEERCRED                      = 0x11
+	SO_PEERNAME                      = 0x1c
+	SO_PEERSEC                       = 0x1f
+	SO_PRIORITY                      = 0xc
+	SO_PROTOCOL                      = 0x26
+	SO_RCVBUF                        = 0x8
+	SO_RCVBUFFORCE                   = 0x21
+	SO_RCVLOWAT                      = 0x12
+	SO_RCVTIMEO                      = 0x14
+	SO_REUSEADDR                     = 0x2
+	SO_RXQ_OVFL                      = 0x28
+	SO_SECURITY_AUTHENTICATION       = 0x16
+	SO_SECURITY_ENCRYPTION_NETWORK   = 0x18
+	SO_SECURITY_ENCRYPTION_TRANSPORT = 0x17
+	SO_SNDBUF                        = 0x7
+	SO_SNDBUFFORCE                   = 0x20
+	SO_SNDLOWAT                      = 0x13
+	SO_SNDTIMEO                      = 0x15
+	SO_TIMESTAMP                     = 0x1d
+	SO_TIMESTAMPING                  = 0x25
+	SO_TIMESTAMPNS                   = 0x23
+	SO_TYPE                          = 0x3
+	S_BLKSIZE                        = 0x200
+	S_IEXEC                          = 0x40
+	S_IFBLK                          = 0x6000
+	S_IFCHR                          = 0x2000
+	S_IFDIR                          = 0x4000
+	S_IFIFO                          = 0x1000
+	S_IFLNK                          = 0xa000
+	S_IFMT                           = 0xf000
+	S_IFREG                          = 0x8000
+	S_IFSOCK                         = 0xc000
+	S_IREAD                          = 0x100
+	S_IRGRP                          = 0x20
+	S_IROTH                          = 0x4
+	S_IRUSR                          = 0x100
+	S_IRWXG                          = 0x38
+	S_IRWXO                          = 0x7
+	S_IRWXU                          = 0x1c0
+	S_ISGID                          = 0x400
+	S_ISUID                          = 0x800
+	S_ISVTX                          = 0x200
+	S_IWGRP                          = 0x10
+	S_IWOTH                          = 0x2
+	S_IWRITE                         = 0x80
+	S_IWUSR                          = 0x80
+	S_IXGRP                          = 0x8
+	S_IXOTH                          = 0x1
+	S_IXUSR                          = 0x40
+	TCIFLUSH                         = 0x0
+	TCIOFLUSH                        = 0x2
+	TCOFLUSH                         = 0x1
+	TCP_CONGESTION                   = 0xd
+	TCP_CORK                         = 0x3
+	TCP_DEFER_ACCEPT                 = 0x9
+	TCP_INFO                         = 0xb
+	TCP_KEEPCNT                      = 0x6
+	TCP_KEEPIDLE                     = 0x4
+	TCP_KEEPINTVL                    = 0x5
+	TCP_LINGER2                      = 0x8
+	TCP_MAXSEG                       = 0x2
+	TCP_MAXWIN                       = 0xffff
+	TCP_MAX_WINSHIFT                 = 0xe
+	TCP_MD5SIG                       = 0xe
+	TCP_MD5SIG_MAXKEYLEN             = 0x50
+	TCP_MSS                          = 0x200
+	TCP_NODELAY                      = 0x1
+	TCP_QUICKACK                     = 0xc
+	TCP_SYNCNT                       = 0x7
+	TCP_WINDOW_CLAMP                 = 0xa
+	TIOCCBRK                         = 0x5428
+	TIOCCONS                         = 0x541d
+	TIOCEXCL                         = 0x540c
+	TIOCGDEV                         = 0x80045432
+	TIOCGETD                         = 0x5424
+	TIOCGICOUNT                      = 0x545d
+	TIOCGLCKTRMIOS                   = 0x5456
+	TIOCGPGRP                        = 0x540f
+	TIOCGPTN                         = 0x80045430
+	TIOCGRS485                       = 0x542e
+	TIOCGSERIAL                      = 0x541e
+	TIOCGSID                         = 0x5429
+	TIOCGSOFTCAR                     = 0x5419
+	TIOCGWINSZ                       = 0x5413
+	TIOCINQ                          = 0x541b
+	TIOCLINUX                        = 0x541c
+	TIOCMBIC                         = 0x5417
+	TIOCMBIS                         = 0x5416
+	TIOCMGET                         = 0x5415
+	TIOCMIWAIT                       = 0x545c
+	TIOCMSET                         = 0x5418
+	TIOCM_CAR                        = 0x40
+	TIOCM_CD                         = 0x40
+	TIOCM_CTS                        = 0x20
+	TIOCM_DSR                        = 0x100
+	TIOCM_DTR                        = 0x2
+	TIOCM_LE                         = 0x1
+	TIOCM_RI                         = 0x80
+	TIOCM_RNG                        = 0x80
+	TIOCM_RTS                        = 0x4
+	TIOCM_SR                         = 0x10
+	TIOCM_ST                         = 0x8
+	TIOCNOTTY                        = 0x5422
+	TIOCNXCL                         = 0x540d
+	TIOCOUTQ                         = 0x5411
+	TIOCPKT                          = 0x5420
+	TIOCPKT_DATA                     = 0x0
+	TIOCPKT_DOSTOP                   = 0x20
+	TIOCPKT_FLUSHREAD                = 0x1
+	TIOCPKT_FLUSHWRITE               = 0x2
+	TIOCPKT_IOCTL                    = 0x40
+	TIOCPKT_NOSTOP                   = 0x10
+	TIOCPKT_START                    = 0x8
+	TIOCPKT_STOP                     = 0x4
+	TIOCSBRK                         = 0x5427
+	TIOCSCTTY                        = 0x540e
+	TIOCSERCONFIG                    = 0x5453
+	TIOCSERGETLSR                    = 0x5459
+	TIOCSERGETMULTI                  = 0x545a
+	TIOCSERGSTRUCT                   = 0x5458
+	TIOCSERGWILD                     = 0x5454
+	TIOCSERSETMULTI                  = 0x545b
+	TIOCSERSWILD                     = 0x5455
+	TIOCSER_TEMT                     = 0x1
+	TIOCSETD                         = 0x5423
+	TIOCSIG                          = 0x40045436
+	TIOCSLCKTRMIOS                   = 0x5457
+	TIOCSPGRP                        = 0x5410
+	TIOCSPTLCK                       = 0x40045431
+	TIOCSRS485                       = 0x542f
+	TIOCSSERIAL                      = 0x541f
+	TIOCSSOFTCAR                     = 0x541a
+	TIOCSTI                          = 0x5412
+	TIOCSWINSZ                       = 0x5414
+	TIOCVHANGUP                      = 0x5437
+	TUNATTACHFILTER                  = 0x400854d5
+	TUNDETACHFILTER                  = 0x400854d6
+	TUNGETFEATURES                   = 0x800454cf
+	TUNGETIFF                        = 0x800454d2
+	TUNGETSNDBUF                     = 0x800454d3
+	TUNGETVNETHDRSZ                  = 0x800454d7
+	TUNSETDEBUG                      = 0x400454c9
+	TUNSETGROUP                      = 0x400454ce
+	TUNSETIFF                        = 0x400454ca
+	TUNSETLINK                       = 0x400454cd
+	TUNSETNOCSUM                     = 0x400454c8
+	TUNSETOFFLOAD                    = 0x400454d0
+	TUNSETOWNER                      = 0x400454cc
+	TUNSETPERSIST                    = 0x400454cb
+	TUNSETSNDBUF                     = 0x400454d4
+	TUNSETTXFILTER                   = 0x400454d1
+	TUNSETVNETHDRSZ                  = 0x400454d8
+	WALL                             = 0x40000000
+	WCLONE                           = 0x80000000
+	WCONTINUED                       = 0x8
+	WEXITED                          = 0x4
+	WNOHANG                          = 0x1
+	WNOTHREAD                        = 0x20000000
+	WNOWAIT                          = 0x1000000
+	WORDSIZE                         = 0x20
+	WSTOPPED                         = 0x2
+	WUNTRACED                        = 0x2
+)
+
+// Errors
+const (
+	E2BIG           = Errno(0x7)
+	EACCES          = Errno(0xd)
+	EADDRINUSE      = Errno(0x62)
+	EADDRNOTAVAIL   = Errno(0x63)
+	EADV            = Errno(0x44)
+	EAFNOSUPPORT    = Errno(0x61)
+	EAGAIN          = Errno(0xb)
+	EALREADY        = Errno(0x72)
+	EBADE           = Errno(0x34)
+	EBADF           = Errno(0x9)
+	EBADFD          = Errno(0x4d)
+	EBADMSG         = Errno(0x4a)
+	EBADR           = Errno(0x35)
+	EBADRQC         = Errno(0x38)
+	EBADSLT         = Errno(0x39)
+	EBFONT          = Errno(0x3b)
+	EBUSY           = Errno(0x10)
+	ECANCELED       = Errno(0x7d)
+	ECHILD          = Errno(0xa)
+	ECHRNG          = Errno(0x2c)
+	ECOMM           = Errno(0x46)
+	ECONNABORTED    = Errno(0x67)
+	ECONNREFUSED    = Errno(0x6f)
+	ECONNRESET      = Errno(0x68)
+	EDEADLK         = Errno(0x23)
+	EDEADLOCK       = Errno(0x23)
+	EDESTADDRREQ    = Errno(0x59)
+	EDOM            = Errno(0x21)
+	EDOTDOT         = Errno(0x49)
+	EDQUOT          = Errno(0x7a)
+	EEXIST          = Errno(0x11)
+	EFAULT          = Errno(0xe)
+	EFBIG           = Errno(0x1b)
+	EHOSTDOWN       = Errno(0x70)
+	EHOSTUNREACH    = Errno(0x71)
+	EHWPOISON       = Errno(0x85)
+	EIDRM           = Errno(0x2b)
+	EILSEQ          = Errno(0x54)
+	EINPROGRESS     = Errno(0x73)
+	EINTR           = Errno(0x4)
+	EINVAL          = Errno(0x16)
+	EIO             = Errno(0x5)
+	EISCONN         = Errno(0x6a)
+	EISDIR          = Errno(0x15)
+	EISNAM          = Errno(0x78)
+	EKEYEXPIRED     = Errno(0x7f)
+	EKEYREJECTED    = Errno(0x81)
+	EKEYREVOKED     = Errno(0x80)
+	EL2HLT          = Errno(0x33)
+	EL2NSYNC        = Errno(0x2d)
+	EL3HLT          = Errno(0x2e)
+	EL3RST          = Errno(0x2f)
+	ELIBACC         = Errno(0x4f)
+	ELIBBAD         = Errno(0x50)
+	ELIBEXEC        = Errno(0x53)
+	ELIBMAX         = Errno(0x52)
+	ELIBSCN         = Errno(0x51)
+	ELNRNG          = Errno(0x30)
+	ELOOP           = Errno(0x28)
+	EMEDIUMTYPE     = Errno(0x7c)
+	EMFILE          = Errno(0x18)
+	EMLINK          = Errno(0x1f)
+	EMSGSIZE        = Errno(0x5a)
+	EMULTIHOP       = Errno(0x48)
+	ENAMETOOLONG    = Errno(0x24)
+	ENAVAIL         = Errno(0x77)
+	ENETDOWN        = Errno(0x64)
+	ENETRESET       = Errno(0x66)
+	ENETUNREACH     = Errno(0x65)
+	ENFILE          = Errno(0x17)
+	ENOANO          = Errno(0x37)
+	ENOBUFS         = Errno(0x69)
+	ENOCSI          = Errno(0x32)
+	ENODATA         = Errno(0x3d)
+	ENODEV          = Errno(0x13)
+	ENOENT          = Errno(0x2)
+	ENOEXEC         = Errno(0x8)
+	ENOKEY          = Errno(0x7e)
+	ENOLCK          = Errno(0x25)
+	ENOLINK         = Errno(0x43)
+	ENOMEDIUM       = Errno(0x7b)
+	ENOMEM          = Errno(0xc)
+	ENOMSG          = Errno(0x2a)
+	ENONET          = Errno(0x40)
+	ENOPKG          = Errno(0x41)
+	ENOPROTOOPT     = Errno(0x5c)
+	ENOSPC          = Errno(0x1c)
+	ENOSR           = Errno(0x3f)
+	ENOSTR          = Errno(0x3c)
+	ENOSYS          = Errno(0x26)
+	ENOTBLK         = Errno(0xf)
+	ENOTCONN        = Errno(0x6b)
+	ENOTDIR         = Errno(0x14)
+	ENOTEMPTY       = Errno(0x27)
+	ENOTNAM         = Errno(0x76)
+	ENOTRECOVERABLE = Errno(0x83)
+	ENOTSOCK        = Errno(0x58)
+	ENOTSUP         = Errno(0x5f)
+	ENOTTY          = Errno(0x19)
+	ENOTUNIQ        = Errno(0x4c)
+	ENXIO           = Errno(0x6)
+	EOPNOTSUPP      = Errno(0x5f)
+	EOVERFLOW       = Errno(0x4b)
+	EOWNERDEAD      = Errno(0x82)
+	EPERM           = Errno(0x1)
+	EPFNOSUPPORT    = Errno(0x60)
+	EPIPE           = Errno(0x20)
+	EPROTO          = Errno(0x47)
+	EPROTONOSUPPORT = Errno(0x5d)
+	EPROTOTYPE      = Errno(0x5b)
+	ERANGE          = Errno(0x22)
+	EREMCHG         = Errno(0x4e)
+	EREMOTE         = Errno(0x42)
+	EREMOTEIO       = Errno(0x79)
+	ERESTART        = Errno(0x55)
+	ERFKILL         = Errno(0x84)
+	EROFS           = Errno(0x1e)
+	ESHUTDOWN       = Errno(0x6c)
+	ESOCKTNOSUPPORT = Errno(0x5e)
+	ESPIPE          = Errno(0x1d)
+	ESRCH           = Errno(0x3)
+	ESRMNT          = Errno(0x45)
+	ESTALE          = Errno(0x74)
+	ESTRPIPE        = Errno(0x56)
+	ETIME           = Errno(0x3e)
+	ETIMEDOUT       = Errno(0x6e)
+	ETOOMANYREFS    = Errno(0x6d)
+	ETXTBSY         = Errno(0x1a)
+	EUCLEAN         = Errno(0x75)
+	EUNATCH         = Errno(0x31)
+	EUSERS          = Errno(0x57)
+	EWOULDBLOCK     = Errno(0xb)
+	EXDEV           = Errno(0x12)
+	EXFULL          = Errno(0x36)
+)
+
+// Signals
+const (
+	SIGABRT   = Signal(0x6)
+	SIGALRM   = Signal(0xe)
+	SIGBUS    = Signal(0x7)
+	SIGCHLD   = Signal(0x11)
+	SIGCLD    = Signal(0x11)
+	SIGCONT   = Signal(0x12)
+	SIGFPE    = Signal(0x8)
+	SIGHUP    = Signal(0x1)
+	SIGILL    = Signal(0x4)
+	SIGINT    = Signal(0x2)
+	SIGIO     = Signal(0x1d)
+	SIGIOT    = Signal(0x6)
+	SIGKILL   = Signal(0x9)
+	SIGPIPE   = Signal(0xd)
+	SIGPOLL   = Signal(0x1d)
+	SIGPROF   = Signal(0x1b)
+	SIGPWR    = Signal(0x1e)
+	SIGQUIT   = Signal(0x3)
+	SIGSEGV   = Signal(0xb)
+	SIGSTKFLT = Signal(0x10)
+	SIGSTOP   = Signal(0x13)
+	SIGSYS    = Signal(0x1f)
+	SIGTERM   = Signal(0xf)
+	SIGTRAP   = Signal(0x5)
+	SIGTSTP   = Signal(0x14)
+	SIGTTIN   = Signal(0x15)
+	SIGTTOU   = Signal(0x16)
+	SIGUNUSED = Signal(0x1f)
+	SIGURG    = Signal(0x17)
+	SIGUSR1   = Signal(0xa)
+	SIGUSR2   = Signal(0xc)
+	SIGVTALRM = Signal(0x1a)
+	SIGWINCH  = Signal(0x1c)
+	SIGXCPU   = Signal(0x18)
+	SIGXFSZ   = Signal(0x19)
+)
+
+// Error table
+var errors = [...]string{
+	1:   "operation not permitted",
+	2:   "no such file or directory",
+	3:   "no such process",
+	4:   "interrupted system call",
+	5:   "input/output error",
+	6:   "no such device or address",
+	7:   "argument list too long",
+	8:   "exec format error",
+	9:   "bad file descriptor",
+	10:  "no child processes",
+	11:  "resource temporarily unavailable",
+	12:  "cannot allocate memory",
+	13:  "permission denied",
+	14:  "bad address",
+	15:  "block device required",
+	16:  "device or resource busy",
+	17:  "file exists",
+	18:  "invalid cross-device link",
+	19:  "no such device",
+	20:  "not a directory",
+	21:  "is a directory",
+	22:  "invalid argument",
+	23:  "too many open files in system",
+	24:  "too many open files",
+	25:  "inappropriate ioctl for device",
+	26:  "text file busy",
+	27:  "file too large",
+	28:  "no space left on device",
+	29:  "illegal seek",
+	30:  "read-only file system",
+	31:  "too many links",
+	32:  "broken pipe",
+	33:  "numerical argument out of domain",
+	34:  "numerical result out of range",
+	35:  "resource deadlock avoided",
+	36:  "file name too long",
+	37:  "no locks available",
+	38:  "function not implemented",
+	39:  "directory not empty",
+	40:  "too many levels of symbolic links",
+	42:  "no message of desired type",
+	43:  "identifier removed",
+	44:  "channel number out of range",
+	45:  "level 2 not synchronized",
+	46:  "level 3 halted",
+	47:  "level 3 reset",
+	48:  "link number out of range",
+	49:  "protocol driver not attached",
+	50:  "no CSI structure available",
+	51:  "level 2 halted",
+	52:  "invalid exchange",
+	53:  "invalid request descriptor",
+	54:  "exchange full",
+	55:  "no anode",
+	56:  "invalid request code",
+	57:  "invalid slot",
+	59:  "bad font file format",
+	60:  "device not a stream",
+	61:  "no data available",
+	62:  "timer expired",
+	63:  "out of streams resources",
+	64:  "machine is not on the network",
+	65:  "package not installed",
+	66:  "object is remote",
+	67:  "link has been severed",
+	68:  "advertise error",
+	69:  "srmount error",
+	70:  "communication error on send",
+	71:  "protocol error",
+	72:  "multihop attempted",
+	73:  "RFS specific error",
+	74:  "bad message",
+	75:  "value too large for defined data type",
+	76:  "name not unique on network",
+	77:  "file descriptor in bad state",
+	78:  "remote address changed",
+	79:  "can not access a needed shared library",
+	80:  "accessing a corrupted shared library",
+	81:  ".lib section in a.out corrupted",
+	82:  "attempting to link in too many shared libraries",
+	83:  "cannot exec a shared library directly",
+	84:  "invalid or incomplete multibyte or wide character",
+	85:  "interrupted system call should be restarted",
+	86:  "streams pipe error",
+	87:  "too many users",
+	88:  "socket operation on non-socket",
+	89:  "destination address required",
+	90:  "message too long",
+	91:  "protocol wrong type for socket",
+	92:  "protocol not available",
+	93:  "protocol not supported",
+	94:  "socket type not supported",
+	95:  "operation not supported",
+	96:  "protocol family not supported",
+	97:  "address family not supported by protocol",
+	98:  "address already in use",
+	99:  "cannot assign requested address",
+	100: "network is down",
+	101: "network is unreachable",
+	102: "network dropped connection on reset",
+	103: "software caused connection abort",
+	104: "connection reset by peer",
+	105: "no buffer space available",
+	106: "transport endpoint is already connected",
+	107: "transport endpoint is not connected",
+	108: "cannot send after transport endpoint shutdown",
+	109: "too many references: cannot splice",
+	110: "connection timed out",
+	111: "connection refused",
+	112: "host is down",
+	113: "no route to host",
+	114: "operation already in progress",
+	115: "operation now in progress",
+	116: "stale NFS file handle",
+	117: "structure needs cleaning",
+	118: "not a XENIX named type file",
+	119: "no XENIX semaphores available",
+	120: "is a named type file",
+	121: "remote I/O error",
+	122: "disk quota exceeded",
+	123: "no medium found",
+	124: "wrong medium type",
+	125: "operation canceled",
+	126: "required key not available",
+	127: "key has expired",
+	128: "key has been revoked",
+	129: "key was rejected by service",
+	130: "owner died",
+	131: "state not recoverable",
+	132: "operation not possible due to RF-kill",
+	133: "unknown error 133",
+}
+
+// Signal table
+var signals = [...]string{
+	1:  "hangup",
+	2:  "interrupt",
+	3:  "quit",
+	4:  "illegal instruction",
+	5:  "trace/breakpoint trap",
+	6:  "aborted",
+	7:  "bus error",
+	8:  "floating point exception",
+	9:  "killed",
+	10: "user defined signal 1",
+	11: "segmentation fault",
+	12: "user defined signal 2",
+	13: "broken pipe",
+	14: "alarm clock",
+	15: "terminated",
+	16: "stack fault",
+	17: "child exited",
+	18: "continued",
+	19: "stopped (signal)",
+	20: "stopped",
+	21: "stopped (tty input)",
+	22: "stopped (tty output)",
+	23: "urgent I/O condition",
+	24: "CPU time limit exceeded",
+	25: "file size limit exceeded",
+	26: "virtual timer expired",
+	27: "profiling timer expired",
+	28: "window changed",
+	29: "I/O possible",
+	30: "power failure",
+	31: "bad system call",
+}
diff --git a/src/syscall/zsyscall_linux_thumb.go b/src/syscall/zsyscall_linux_thumb.go
new file mode 100644
index 0000000000..e9b32b710c
--- /dev/null
+++ b/src/syscall/zsyscall_linux_thumb.go
@@ -0,0 +1,1620 @@
+// mksyscall.pl -l32 -arm -tags linux,arm syscall_linux.go syscall_linux_arm.go
+// Code generated by the command above; DO NOT EDIT.
+
+// +build linux,thumb
+
+package syscall
+
+import "unsafe"
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func faccessat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FACCESSAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fchmodat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FCHMODAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func linkat(olddirfd int, oldpath string, newdirfd int, newpath string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_LINKAT, uintptr(olddirfd), uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func openat(dirfd int, path string, flags int, mode uint32) (fd int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall6(SYS_OPENAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(flags), uintptr(mode), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func readlinkat(dirfd int, path string, buf []byte) (n int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(buf) > 0 {
+		_p1 = unsafe.Pointer(&buf[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_READLINKAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(buf)), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func symlinkat(oldpath string, newdirfd int, newpath string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_SYMLINKAT, uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func unlinkat(dirfd int, path string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UNLINKAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(flags))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func utimensat(dirfd int, path string, times *[2]Timespec, flag int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_UTIMENSAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)), uintptr(flag), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getcwd(buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_GETCWD, uintptr(_p0), uintptr(len(buf)), 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func wait4(pid int, wstatus *_C_int, options int, rusage *Rusage) (wpid int, err error) {
+	r0, _, e1 := Syscall6(SYS_WAIT4, uintptr(pid), uintptr(unsafe.Pointer(wstatus)), uintptr(options), uintptr(unsafe.Pointer(rusage)), 0, 0)
+	wpid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func ptrace(request int, pid int, addr uintptr, data uintptr) (err error) {
+	_, _, e1 := Syscall6(SYS_PTRACE, uintptr(request), uintptr(pid), uintptr(addr), uintptr(data), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func reboot(magic1 uint, magic2 uint, cmd int, arg string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(arg)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_REBOOT, uintptr(magic1), uintptr(magic2), uintptr(cmd), uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func mount(source string, target string, fstype string, flags uintptr, data *byte) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(source)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(target)
+	if err != nil {
+		return
+	}
+	var _p2 *byte
+	_p2, err = BytePtrFromString(fstype)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_MOUNT, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(unsafe.Pointer(_p2)), uintptr(flags), uintptr(unsafe.Pointer(data)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Acct(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_ACCT, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Adjtimex(buf *Timex) (state int, err error) {
+	r0, _, e1 := Syscall(SYS_ADJTIMEX, uintptr(unsafe.Pointer(buf)), 0, 0)
+	state = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Chdir(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_CHDIR, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Chroot(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_CHROOT, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Close(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_CLOSE, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup(oldfd int) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_DUP, uintptr(oldfd), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup3(oldfd int, newfd int, flags int) (err error) {
+	_, _, e1 := Syscall(SYS_DUP3, uintptr(oldfd), uintptr(newfd), uintptr(flags))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCreate1(flag int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_EPOLL_CREATE1, uintptr(flag), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCtl(epfd int, op int, fd int, event *EpollEvent) (err error) {
+	_, _, e1 := RawSyscall6(SYS_EPOLL_CTL, uintptr(epfd), uintptr(op), uintptr(fd), uintptr(unsafe.Pointer(event)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fallocate(fd int, mode uint32, off int64, len int64) (err error) {
+	_, _, e1 := Syscall6(SYS_FALLOCATE, uintptr(fd), uintptr(mode), uintptr(off), uintptr(off>>32), uintptr(len), uintptr(len>>32))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchdir(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FCHDIR, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchmod(fd int, mode uint32) (err error) {
+	_, _, e1 := Syscall(SYS_FCHMOD, uintptr(fd), uintptr(mode), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchownat(dirfd int, path string, uid int, gid int, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_FCHOWNAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(uid), uintptr(gid), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fcntl(fd int, cmd int, arg int) (val int, err error) {
+	r0, _, e1 := Syscall(SYS_FCNTL, uintptr(fd), uintptr(cmd), uintptr(arg))
+	val = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fdatasync(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FDATASYNC, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Flock(fd int, how int) (err error) {
+	_, _, e1 := Syscall(SYS_FLOCK, uintptr(fd), uintptr(how), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fsync(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FSYNC, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getdents(fd int, buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_GETDENTS64, uintptr(fd), uintptr(_p0), uintptr(len(buf)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpgid(pid int) (pgid int, err error) {
+	r0, _, e1 := RawSyscall(SYS_GETPGID, uintptr(pid), 0, 0)
+	pgid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpid() (pid int) {
+	r0, _ := rawSyscallNoError(SYS_GETPID, 0, 0, 0)
+	pid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getppid() (ppid int) {
+	r0, _ := rawSyscallNoError(SYS_GETPPID, 0, 0, 0)
+	ppid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpriority(which int, who int) (prio int, err error) {
+	r0, _, e1 := Syscall(SYS_GETPRIORITY, uintptr(which), uintptr(who), 0)
+	prio = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getrusage(who int, rusage *Rusage) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETRUSAGE, uintptr(who), uintptr(unsafe.Pointer(rusage)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Gettid() (tid int) {
+	r0, _ := rawSyscallNoError(SYS_GETTID, 0, 0, 0)
+	tid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getxattr(path string, attr string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(dest) > 0 {
+		_p2 = unsafe.Pointer(&dest[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_GETXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(dest)), 0, 0)
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyAddWatch(fd int, pathname string, mask uint32) (watchdesc int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(pathname)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall(SYS_INOTIFY_ADD_WATCH, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(mask))
+	watchdesc = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyInit1(flags int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_INIT1, uintptr(flags), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyRmWatch(fd int, watchdesc uint32) (success int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_RM_WATCH, uintptr(fd), uintptr(watchdesc), 0)
+	success = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Kill(pid int, sig Signal) (err error) {
+	_, _, e1 := RawSyscall(SYS_KILL, uintptr(pid), uintptr(sig), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Klogctl(typ int, buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_SYSLOG, uintptr(typ), uintptr(_p0), uintptr(len(buf)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Listxattr(path string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(dest) > 0 {
+		_p1 = unsafe.Pointer(&dest[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_LISTXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(dest)))
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mkdirat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_MKDIRAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mknodat(dirfd int, path string, mode uint32, dev int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_MKNODAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode), uintptr(dev), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Nanosleep(time *Timespec, leftover *Timespec) (err error) {
+	_, _, e1 := Syscall(SYS_NANOSLEEP, uintptr(unsafe.Pointer(time)), uintptr(unsafe.Pointer(leftover)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func PivotRoot(newroot string, putold string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(newroot)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(putold)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_PIVOT_ROOT, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func prlimit(pid int, resource int, newlimit *Rlimit, old *Rlimit) (err error) {
+	_, _, e1 := RawSyscall6(SYS_PRLIMIT64, uintptr(pid), uintptr(resource), uintptr(unsafe.Pointer(newlimit)), uintptr(unsafe.Pointer(old)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func read(fd int, p []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_READ, uintptr(fd), uintptr(_p0), uintptr(len(p)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Removexattr(path string, attr string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_REMOVEXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setdomainname(p []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_SETDOMAINNAME, uintptr(_p0), uintptr(len(p)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sethostname(p []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_SETHOSTNAME, uintptr(_p0), uintptr(len(p)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setpgid(pid int, pgid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETPGID, uintptr(pid), uintptr(pgid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setsid() (pid int, err error) {
+	r0, _, e1 := RawSyscall(SYS_SETSID, 0, 0, 0)
+	pid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Settimeofday(tv *Timeval) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETTIMEOFDAY, uintptr(unsafe.Pointer(tv)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setpriority(which int, who int, prio int) (err error) {
+	_, _, e1 := Syscall(SYS_SETPRIORITY, uintptr(which), uintptr(who), uintptr(prio))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setxattr(path string, attr string, data []byte, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(data) > 0 {
+		_p2 = unsafe.Pointer(&data[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_SETXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(data)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sync() {
+	Syscall(SYS_SYNC, 0, 0, 0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sysinfo(info *Sysinfo_t) (err error) {
+	_, _, e1 := RawSyscall(SYS_SYSINFO, uintptr(unsafe.Pointer(info)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Tee(rfd int, wfd int, len int, flags int) (n int64, err error) {
+	r0, r1, e1 := Syscall6(SYS_TEE, uintptr(rfd), uintptr(wfd), uintptr(len), uintptr(flags), 0, 0)
+	n = int64(int64(r1)<<32 | int64(r0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Tgkill(tgid int, tid int, sig Signal) (err error) {
+	_, _, e1 := RawSyscall(SYS_TGKILL, uintptr(tgid), uintptr(tid), uintptr(sig))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Times(tms *Tms) (ticks uintptr, err error) {
+	r0, _, e1 := RawSyscall(SYS_TIMES, uintptr(unsafe.Pointer(tms)), 0, 0)
+	ticks = uintptr(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Umask(mask int) (oldmask int) {
+	r0, _ := rawSyscallNoError(SYS_UMASK, uintptr(mask), 0, 0)
+	oldmask = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Uname(buf *Utsname) (err error) {
+	_, _, e1 := RawSyscall(SYS_UNAME, uintptr(unsafe.Pointer(buf)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Unmount(target string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(target)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UMOUNT2, uintptr(unsafe.Pointer(_p0)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Unshare(flags int) (err error) {
+	_, _, e1 := Syscall(SYS_UNSHARE, uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func write(fd int, p []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_WRITE, uintptr(fd), uintptr(_p0), uintptr(len(p)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func exitThread(code int) (err error) {
+	_, _, e1 := Syscall(SYS_EXIT, uintptr(code), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func readlen(fd int, p *byte, np int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_READ, uintptr(fd), uintptr(unsafe.Pointer(p)), uintptr(np))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func writelen(fd int, p *byte, np int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_WRITE, uintptr(fd), uintptr(unsafe.Pointer(p)), uintptr(np))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func munmap(addr uintptr, length uintptr) (err error) {
+	_, _, e1 := Syscall(SYS_MUNMAP, uintptr(addr), uintptr(length), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Madvise(b []byte, advice int) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MADVISE, uintptr(_p0), uintptr(len(b)), uintptr(advice))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mprotect(b []byte, prot int) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MPROTECT, uintptr(_p0), uintptr(len(b)), uintptr(prot))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mlock(b []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MLOCK, uintptr(_p0), uintptr(len(b)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Munlock(b []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MUNLOCK, uintptr(_p0), uintptr(len(b)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mlockall(flags int) (err error) {
+	_, _, e1 := Syscall(SYS_MLOCKALL, uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Munlockall() (err error) {
+	_, _, e1 := Syscall(SYS_MUNLOCKALL, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func pipe(p *[2]_C_int) (err error) {
+	_, _, e1 := RawSyscall(SYS_PIPE, uintptr(unsafe.Pointer(p)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func pipe2(p *[2]_C_int, flags int) (err error) {
+	_, _, e1 := RawSyscall(SYS_PIPE2, uintptr(unsafe.Pointer(p)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func accept(s int, rsa *RawSockaddrAny, addrlen *_Socklen) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_ACCEPT, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func accept4(s int, rsa *RawSockaddrAny, addrlen *_Socklen, flags int) (fd int, err error) {
+	r0, _, e1 := Syscall6(SYS_ACCEPT4, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), uintptr(flags), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func bind(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := Syscall(SYS_BIND, uintptr(s), uintptr(addr), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func connect(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := Syscall(SYS_CONNECT, uintptr(s), uintptr(addr), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getgroups(n int, list *_Gid_t) (nn int, err error) {
+	r0, _, e1 := RawSyscall(SYS_GETGROUPS32, uintptr(n), uintptr(unsafe.Pointer(list)), 0)
+	nn = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getsockopt(s int, level int, name int, val unsafe.Pointer, vallen *_Socklen) (err error) {
+	_, _, e1 := Syscall6(SYS_GETSOCKOPT, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(unsafe.Pointer(vallen)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setsockopt(s int, level int, name int, val unsafe.Pointer, vallen uintptr) (err error) {
+	_, _, e1 := Syscall6(SYS_SETSOCKOPT, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(vallen), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func socket(domain int, typ int, proto int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_SOCKET, uintptr(domain), uintptr(typ), uintptr(proto))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getpeername(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETPEERNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getsockname(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETSOCKNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func recvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_RECVFROM, uintptr(fd), uintptr(_p0), uintptr(len(p)), uintptr(flags), uintptr(unsafe.Pointer(from)), uintptr(unsafe.Pointer(fromlen)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_SENDTO, uintptr(s), uintptr(_p0), uintptr(len(buf)), uintptr(flags), uintptr(to), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func socketpair(domain int, typ int, flags int, fd *[2]int32) (err error) {
+	_, _, e1 := RawSyscall6(SYS_SOCKETPAIR, uintptr(domain), uintptr(typ), uintptr(flags), uintptr(unsafe.Pointer(fd)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func recvmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_RECVMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_SENDMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup2(oldfd int, newfd int) (err error) {
+	_, _, e1 := Syscall(SYS_DUP2, uintptr(oldfd), uintptr(newfd), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCreate(size int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_EPOLL_CREATE, uintptr(size), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchown(fd int, uid int, gid int) (err error) {
+	_, _, e1 := Syscall(SYS_FCHOWN32, uintptr(fd), uintptr(uid), uintptr(gid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fstat(fd int, stat *Stat_t) (err error) {
+	_, _, e1 := Syscall(SYS_FSTAT64, uintptr(fd), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fstatat(dirfd int, path string, stat *Stat_t, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_FSTATAT64, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getegid() (egid int) {
+	r0, _ := rawSyscallNoError(SYS_GETEGID32, 0, 0, 0)
+	egid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Geteuid() (euid int) {
+	r0, _ := rawSyscallNoError(SYS_GETEUID32, 0, 0, 0)
+	euid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getgid() (gid int) {
+	r0, _ := rawSyscallNoError(SYS_GETGID32, 0, 0, 0)
+	gid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getuid() (uid int) {
+	r0, _ := rawSyscallNoError(SYS_GETUID32, 0, 0, 0)
+	uid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyInit() (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_INIT, 0, 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Listen(s int, n int) (err error) {
+	_, _, e1 := Syscall(SYS_LISTEN, uintptr(s), uintptr(n), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pause() (err error) {
+	_, _, e1 := Syscall(SYS_PAUSE, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Renameat(olddirfd int, oldpath string, newdirfd int, newpath string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_RENAMEAT, uintptr(olddirfd), uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendfile(outfd int, infd int, offset *int64, count int) (written int, err error) {
+	r0, _, e1 := Syscall6(SYS_SENDFILE64, uintptr(outfd), uintptr(infd), uintptr(unsafe.Pointer(offset)), uintptr(count), 0, 0)
+	written = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Select(nfd int, r *FdSet, w *FdSet, e *FdSet, timeout *Timeval) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS__NEWSELECT, uintptr(nfd), uintptr(unsafe.Pointer(r)), uintptr(unsafe.Pointer(w)), uintptr(unsafe.Pointer(e)), uintptr(unsafe.Pointer(timeout)), 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setfsgid(gid int) (err error) {
+	_, _, e1 := Syscall(SYS_SETFSGID32, uintptr(gid), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setfsuid(uid int) (err error) {
+	_, _, e1 := Syscall(SYS_SETFSUID32, uintptr(uid), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Shutdown(fd int, how int) (err error) {
+	_, _, e1 := Syscall(SYS_SHUTDOWN, uintptr(fd), uintptr(how), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Splice(rfd int, roff *int64, wfd int, woff *int64, len int, flags int) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS_SPLICE, uintptr(rfd), uintptr(unsafe.Pointer(roff)), uintptr(wfd), uintptr(unsafe.Pointer(woff)), uintptr(len), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Ustat(dev int, ubuf *Ustat_t) (err error) {
+	_, _, e1 := Syscall(SYS_USTAT, uintptr(dev), uintptr(unsafe.Pointer(ubuf)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func futimesat(dirfd int, path string, times *[2]Timeval) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FUTIMESAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Gettimeofday(tv *Timeval) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETTIMEOFDAY, uintptr(unsafe.Pointer(tv)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Time(t *Time_t) (tt Time_t, err error) {
+	r0, _, e1 := RawSyscall(SYS_TIME, uintptr(unsafe.Pointer(t)), 0, 0)
+	tt = Time_t(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Utime(path string, buf *Utimbuf) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UTIME, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(buf)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func utimes(path string, times *[2]Timeval) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UTIMES, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pread(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_PREAD64, uintptr(fd), uintptr(_p0), uintptr(len(p)), 0, uintptr(offset), uintptr(offset>>32))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pwrite(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_PWRITE64, uintptr(fd), uintptr(_p0), uintptr(len(p)), 0, uintptr(offset), uintptr(offset>>32))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Truncate(path string, length int64) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_TRUNCATE64, uintptr(unsafe.Pointer(_p0)), 0, uintptr(length), uintptr(length>>32), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Ftruncate(fd int, length int64) (err error) {
+	_, _, e1 := Syscall6(SYS_FTRUNCATE64, uintptr(fd), 0, uintptr(length), uintptr(length>>32), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func mmap2(addr uintptr, length uintptr, prot int, flags int, fd int, pageOffset uintptr) (xaddr uintptr, err error) {
+	r0, _, e1 := Syscall6(SYS_MMAP2, uintptr(addr), uintptr(length), uintptr(prot), uintptr(flags), uintptr(fd), uintptr(pageOffset))
+	xaddr = uintptr(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(events) > 0 {
+		_p0 = unsafe.Pointer(&events[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_EPOLL_WAIT, uintptr(epfd), uintptr(_p0), uintptr(len(events)), uintptr(msec), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getrlimit(resource int, rlim *rlimit32) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETRLIMIT, uintptr(resource), uintptr(unsafe.Pointer(rlim)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setrlimit(resource int, rlim *rlimit32) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRLIMIT, uintptr(resource), uintptr(unsafe.Pointer(rlim)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
diff --git a/src/syscall/zsysnum_linux_thumb.go b/src/syscall/zsysnum_linux_thumb.go
new file mode 100644
index 0000000000..7335db6847
--- /dev/null
+++ b/src/syscall/zsysnum_linux_thumb.go
@@ -0,0 +1,356 @@
+// mksysnum_linux.pl
+// Code generated by the command above; DO NOT EDIT.
+
+// +build thumb,linux
+
+package syscall
+
+const (
+	SYS_OABI_SYSCALL_BASE      = 0
+	SYS_SYSCALL_BASE           = 0
+	SYS_RESTART_SYSCALL        = 0
+	SYS_EXIT                   = 1
+	SYS_FORK                   = 2
+	SYS_READ                   = 3
+	SYS_WRITE                  = 4
+	SYS_OPEN                   = 5
+	SYS_CLOSE                  = 6
+	SYS_CREAT                  = 8
+	SYS_LINK                   = 9
+	SYS_UNLINK                 = 10
+	SYS_EXECVE                 = 11
+	SYS_CHDIR                  = 12
+	SYS_TIME                   = 13
+	SYS_MKNOD                  = 14
+	SYS_CHMOD                  = 15
+	SYS_LCHOWN                 = 16
+	SYS_LSEEK                  = 19
+	SYS_GETPID                 = 20
+	SYS_MOUNT                  = 21
+	SYS_UMOUNT                 = 22
+	SYS_SETUID                 = 23
+	SYS_GETUID                 = 24
+	SYS_STIME                  = 25
+	SYS_PTRACE                 = 26
+	SYS_ALARM                  = 27
+	SYS_PAUSE                  = 29
+	SYS_UTIME                  = 30
+	SYS_ACCESS                 = 33
+	SYS_NICE                   = 34
+	SYS_SYNC                   = 36
+	SYS_KILL                   = 37
+	SYS_RENAME                 = 38
+	SYS_MKDIR                  = 39
+	SYS_RMDIR                  = 40
+	SYS_DUP                    = 41
+	SYS_PIPE                   = 42
+	SYS_TIMES                  = 43
+	SYS_BRK                    = 45
+	SYS_SETGID                 = 46
+	SYS_GETGID                 = 47
+	SYS_GETEUID                = 49
+	SYS_GETEGID                = 50
+	SYS_ACCT                   = 51
+	SYS_UMOUNT2                = 52
+	SYS_IOCTL                  = 54
+	SYS_FCNTL                  = 55
+	SYS_SETPGID                = 57
+	SYS_UMASK                  = 60
+	SYS_CHROOT                 = 61
+	SYS_USTAT                  = 62
+	SYS_DUP2                   = 63
+	SYS_GETPPID                = 64
+	SYS_GETPGRP                = 65
+	SYS_SETSID                 = 66
+	SYS_SIGACTION              = 67
+	SYS_SETREUID               = 70
+	SYS_SETREGID               = 71
+	SYS_SIGSUSPEND             = 72
+	SYS_SIGPENDING             = 73
+	SYS_SETHOSTNAME            = 74
+	SYS_SETRLIMIT              = 75
+	SYS_GETRLIMIT              = 76
+	SYS_GETRUSAGE              = 77
+	SYS_GETTIMEOFDAY           = 78
+	SYS_SETTIMEOFDAY           = 79
+	SYS_GETGROUPS              = 80
+	SYS_SETGROUPS              = 81
+	SYS_SELECT                 = 82
+	SYS_SYMLINK                = 83
+	SYS_READLINK               = 85
+	SYS_USELIB                 = 86
+	SYS_SWAPON                 = 87
+	SYS_REBOOT                 = 88
+	SYS_READDIR                = 89
+	SYS_MMAP                   = 90
+	SYS_MUNMAP                 = 91
+	SYS_TRUNCATE               = 92
+	SYS_FTRUNCATE              = 93
+	SYS_FCHMOD                 = 94
+	SYS_FCHOWN                 = 95
+	SYS_GETPRIORITY            = 96
+	SYS_SETPRIORITY            = 97
+	SYS_STATFS                 = 99
+	SYS_FSTATFS                = 100
+	SYS_SOCKETCALL             = 102
+	SYS_SYSLOG                 = 103
+	SYS_SETITIMER              = 104
+	SYS_GETITIMER              = 105
+	SYS_STAT                   = 106
+	SYS_LSTAT                  = 107
+	SYS_FSTAT                  = 108
+	SYS_VHANGUP                = 111
+	SYS_SYSCALL                = 113
+	SYS_WAIT4                  = 114
+	SYS_SWAPOFF                = 115
+	SYS_SYSINFO                = 116
+	SYS_IPC                    = 117
+	SYS_FSYNC                  = 118
+	SYS_SIGRETURN              = 119
+	SYS_CLONE                  = 120
+	SYS_SETDOMAINNAME          = 121
+	SYS_UNAME                  = 122
+	SYS_ADJTIMEX               = 124
+	SYS_MPROTECT               = 125
+	SYS_SIGPROCMASK            = 126
+	SYS_INIT_MODULE            = 128
+	SYS_DELETE_MODULE          = 129
+	SYS_QUOTACTL               = 131
+	SYS_GETPGID                = 132
+	SYS_FCHDIR                 = 133
+	SYS_BDFLUSH                = 134
+	SYS_SYSFS                  = 135
+	SYS_PERSONALITY            = 136
+	SYS_SETFSUID               = 138
+	SYS_SETFSGID               = 139
+	SYS__LLSEEK                = 140
+	SYS_GETDENTS               = 141
+	SYS__NEWSELECT             = 142
+	SYS_FLOCK                  = 143
+	SYS_MSYNC                  = 144
+	SYS_READV                  = 145
+	SYS_WRITEV                 = 146
+	SYS_GETSID                 = 147
+	SYS_FDATASYNC              = 148
+	SYS__SYSCTL                = 149
+	SYS_MLOCK                  = 150
+	SYS_MUNLOCK                = 151
+	SYS_MLOCKALL               = 152
+	SYS_MUNLOCKALL             = 153
+	SYS_SCHED_SETPARAM         = 154
+	SYS_SCHED_GETPARAM         = 155
+	SYS_SCHED_SETSCHEDULER     = 156
+	SYS_SCHED_GETSCHEDULER     = 157
+	SYS_SCHED_YIELD            = 158
+	SYS_SCHED_GET_PRIORITY_MAX = 159
+	SYS_SCHED_GET_PRIORITY_MIN = 160
+	SYS_SCHED_RR_GET_INTERVAL  = 161
+	SYS_NANOSLEEP              = 162
+	SYS_MREMAP                 = 163
+	SYS_SETRESUID              = 164
+	SYS_GETRESUID              = 165
+	SYS_POLL                   = 168
+	SYS_NFSSERVCTL             = 169
+	SYS_SETRESGID              = 170
+	SYS_GETRESGID              = 171
+	SYS_PRCTL                  = 172
+	SYS_RT_SIGRETURN           = 173
+	SYS_RT_SIGACTION           = 174
+	SYS_RT_SIGPROCMASK         = 175
+	SYS_RT_SIGPENDING          = 176
+	SYS_RT_SIGTIMEDWAIT        = 177
+	SYS_RT_SIGQUEUEINFO        = 178
+	SYS_RT_SIGSUSPEND          = 179
+	SYS_PREAD64                = 180
+	SYS_PWRITE64               = 181
+	SYS_CHOWN                  = 182
+	SYS_GETCWD                 = 183
+	SYS_CAPGET                 = 184
+	SYS_CAPSET                 = 185
+	SYS_SIGALTSTACK            = 186
+	SYS_SENDFILE               = 187
+	SYS_VFORK                  = 190
+	SYS_UGETRLIMIT             = 191
+	SYS_MMAP2                  = 192
+	SYS_TRUNCATE64             = 193
+	SYS_FTRUNCATE64            = 194
+	SYS_STAT64                 = 195
+	SYS_LSTAT64                = 196
+	SYS_FSTAT64                = 197
+	SYS_LCHOWN32               = 198
+	SYS_GETUID32               = 199
+	SYS_GETGID32               = 200
+	SYS_GETEUID32              = 201
+	SYS_GETEGID32              = 202
+	SYS_SETREUID32             = 203
+	SYS_SETREGID32             = 204
+	SYS_GETGROUPS32            = 205
+	SYS_SETGROUPS32            = 206
+	SYS_FCHOWN32               = 207
+	SYS_SETRESUID32            = 208
+	SYS_GETRESUID32            = 209
+	SYS_SETRESGID32            = 210
+	SYS_GETRESGID32            = 211
+	SYS_CHOWN32                = 212
+	SYS_SETUID32               = 213
+	SYS_SETGID32               = 214
+	SYS_SETFSUID32             = 215
+	SYS_SETFSGID32             = 216
+	SYS_GETDENTS64             = 217
+	SYS_PIVOT_ROOT             = 218
+	SYS_MINCORE                = 219
+	SYS_MADVISE                = 220
+	SYS_FCNTL64                = 221
+	SYS_GETTID                 = 224
+	SYS_READAHEAD              = 225
+	SYS_SETXATTR               = 226
+	SYS_LSETXATTR              = 227
+	SYS_FSETXATTR              = 228
+	SYS_GETXATTR               = 229
+	SYS_LGETXATTR              = 230
+	SYS_FGETXATTR              = 231
+	SYS_LISTXATTR              = 232
+	SYS_LLISTXATTR             = 233
+	SYS_FLISTXATTR             = 234
+	SYS_REMOVEXATTR            = 235
+	SYS_LREMOVEXATTR           = 236
+	SYS_FREMOVEXATTR           = 237
+	SYS_TKILL                  = 238
+	SYS_SENDFILE64             = 239
+	SYS_FUTEX                  = 240
+	SYS_SCHED_SETAFFINITY      = 241
+	SYS_SCHED_GETAFFINITY      = 242
+	SYS_IO_SETUP               = 243
+	SYS_IO_DESTROY             = 244
+	SYS_IO_GETEVENTS           = 245
+	SYS_IO_SUBMIT              = 246
+	SYS_IO_CANCEL              = 247
+	SYS_EXIT_GROUP             = 248
+	SYS_LOOKUP_DCOOKIE         = 249
+	SYS_EPOLL_CREATE           = 250
+	SYS_EPOLL_CTL              = 251
+	SYS_EPOLL_WAIT             = 252
+	SYS_REMAP_FILE_PAGES       = 253
+	SYS_SET_TID_ADDRESS        = 256
+	SYS_TIMER_CREATE           = 257
+	SYS_TIMER_SETTIME          = 258
+	SYS_TIMER_GETTIME          = 259
+	SYS_TIMER_GETOVERRUN       = 260
+	SYS_TIMER_DELETE           = 261
+	SYS_CLOCK_SETTIME          = 262
+	SYS_CLOCK_GETTIME          = 263
+	SYS_CLOCK_GETRES           = 264
+	SYS_CLOCK_NANOSLEEP        = 265
+	SYS_STATFS64               = 266
+	SYS_FSTATFS64              = 267
+	SYS_TGKILL                 = 268
+	SYS_UTIMES                 = 269
+	SYS_ARM_FADVISE64_64       = 270
+	SYS_PCICONFIG_IOBASE       = 271
+	SYS_PCICONFIG_READ         = 272
+	SYS_PCICONFIG_WRITE        = 273
+	SYS_MQ_OPEN                = 274
+	SYS_MQ_UNLINK              = 275
+	SYS_MQ_TIMEDSEND           = 276
+	SYS_MQ_TIMEDRECEIVE        = 277
+	SYS_MQ_NOTIFY              = 278
+	SYS_MQ_GETSETATTR          = 279
+	SYS_WAITID                 = 280
+	SYS_SOCKET                 = 281
+	SYS_BIND                   = 282
+	SYS_CONNECT                = 283
+	SYS_LISTEN                 = 284
+	SYS_ACCEPT                 = 285
+	SYS_GETSOCKNAME            = 286
+	SYS_GETPEERNAME            = 287
+	SYS_SOCKETPAIR             = 288
+	SYS_SEND                   = 289
+	SYS_SENDTO                 = 290
+	SYS_RECV                   = 291
+	SYS_RECVFROM               = 292
+	SYS_SHUTDOWN               = 293
+	SYS_SETSOCKOPT             = 294
+	SYS_GETSOCKOPT             = 295
+	SYS_SENDMSG                = 296
+	SYS_RECVMSG                = 297
+	SYS_SEMOP                  = 298
+	SYS_SEMGET                 = 299
+	SYS_SEMCTL                 = 300
+	SYS_MSGSND                 = 301
+	SYS_MSGRCV                 = 302
+	SYS_MSGGET                 = 303
+	SYS_MSGCTL                 = 304
+	SYS_SHMAT                  = 305
+	SYS_SHMDT                  = 306
+	SYS_SHMGET                 = 307
+	SYS_SHMCTL                 = 308
+	SYS_ADD_KEY                = 309
+	SYS_REQUEST_KEY            = 310
+	SYS_KEYCTL                 = 311
+	SYS_SEMTIMEDOP             = 312
+	SYS_VSERVER                = 313
+	SYS_IOPRIO_SET             = 314
+	SYS_IOPRIO_GET             = 315
+	SYS_INOTIFY_INIT           = 316
+	SYS_INOTIFY_ADD_WATCH      = 317
+	SYS_INOTIFY_RM_WATCH       = 318
+	SYS_MBIND                  = 319
+	SYS_GET_MEMPOLICY          = 320
+	SYS_SET_MEMPOLICY          = 321
+	SYS_OPENAT                 = 322
+	SYS_MKDIRAT                = 323
+	SYS_MKNODAT                = 324
+	SYS_FCHOWNAT               = 325
+	SYS_FUTIMESAT              = 326
+	SYS_FSTATAT64              = 327
+	SYS_UNLINKAT               = 328
+	SYS_RENAMEAT               = 329
+	SYS_LINKAT                 = 330
+	SYS_SYMLINKAT              = 331
+	SYS_READLINKAT             = 332
+	SYS_FCHMODAT               = 333
+	SYS_FACCESSAT              = 334
+	SYS_PSELECT6               = 335
+	SYS_PPOLL                  = 336
+	SYS_UNSHARE                = 337
+	SYS_SET_ROBUST_LIST        = 338
+	SYS_GET_ROBUST_LIST        = 339
+	SYS_SPLICE                 = 340
+	SYS_ARM_SYNC_FILE_RANGE    = 341
+	SYS_TEE                    = 342
+	SYS_VMSPLICE               = 343
+	SYS_MOVE_PAGES             = 344
+	SYS_GETCPU                 = 345
+	SYS_EPOLL_PWAIT            = 346
+	SYS_KEXEC_LOAD             = 347
+	SYS_UTIMENSAT              = 348
+	SYS_SIGNALFD               = 349
+	SYS_TIMERFD_CREATE         = 350
+	SYS_EVENTFD                = 351
+	SYS_FALLOCATE              = 352
+	SYS_TIMERFD_SETTIME        = 353
+	SYS_TIMERFD_GETTIME        = 354
+	SYS_SIGNALFD4              = 355
+	SYS_EVENTFD2               = 356
+	SYS_EPOLL_CREATE1          = 357
+	SYS_DUP3                   = 358
+	SYS_PIPE2                  = 359
+	SYS_INOTIFY_INIT1          = 360
+	SYS_PREADV                 = 361
+	SYS_PWRITEV                = 362
+	SYS_RT_TGSIGQUEUEINFO      = 363
+	SYS_PERF_EVENT_OPEN        = 364
+	SYS_RECVMMSG               = 365
+	SYS_ACCEPT4                = 366
+	SYS_FANOTIFY_INIT          = 367
+	SYS_FANOTIFY_MARK          = 368
+	SYS_PRLIMIT64              = 369
+	SYS_NAME_TO_HANDLE_AT      = 370
+	SYS_OPEN_BY_HANDLE_AT      = 371
+	SYS_CLOCK_ADJTIME          = 372
+	SYS_SYNCFS                 = 373
+	SYS_SENDMMSG               = 374
+	SYS_SETNS                  = 375
+	SYS_PROCESS_VM_READV       = 376
+	SYS_PROCESS_VM_WRITEV      = 377
+)
diff --git a/src/syscall/ztypes_linux_thumb.go b/src/syscall/ztypes_linux_thumb.go
new file mode 100644
index 0000000000..7f5bc66cf4
--- /dev/null
+++ b/src/syscall/ztypes_linux_thumb.go
@@ -0,0 +1,689 @@
+// Created by cgo -godefs - DO NOT EDIT
+// cgo -godefs types_linux.go
+
+// +build thumb,linux
+
+package syscall
+
+const (
+	sizeofPtr      = 0x4
+	sizeofShort    = 0x2
+	sizeofInt      = 0x4
+	sizeofLong     = 0x4
+	sizeofLongLong = 0x8
+	PathMax        = 0x1000
+)
+
+type (
+	_C_short     int16
+	_C_int       int32
+	_C_long      int32
+	_C_long_long int64
+)
+
+type Timespec struct {
+	Sec  int32
+	Nsec int32
+}
+
+type Timeval struct {
+	Sec  int32
+	Usec int32
+}
+
+type Timex struct {
+	Modes     uint32
+	Offset    int32
+	Freq      int32
+	Maxerror  int32
+	Esterror  int32
+	Status    int32
+	Constant  int32
+	Precision int32
+	Tolerance int32
+	Time      Timeval
+	Tick      int32
+	Ppsfreq   int32
+	Jitter    int32
+	Shift     int32
+	Stabil    int32
+	Jitcnt    int32
+	Calcnt    int32
+	Errcnt    int32
+	Stbcnt    int32
+	Tai       int32
+	Pad_cgo_0 [44]byte
+}
+
+type Time_t int32
+
+type Tms struct {
+	Utime  int32
+	Stime  int32
+	Cutime int32
+	Cstime int32
+}
+
+type Utimbuf struct {
+	Actime  int32
+	Modtime int32
+}
+
+type Rusage struct {
+	Utime    Timeval
+	Stime    Timeval
+	Maxrss   int32
+	Ixrss    int32
+	Idrss    int32
+	Isrss    int32
+	Minflt   int32
+	Majflt   int32
+	Nswap    int32
+	Inblock  int32
+	Oublock  int32
+	Msgsnd   int32
+	Msgrcv   int32
+	Nsignals int32
+	Nvcsw    int32
+	Nivcsw   int32
+}
+
+type Rlimit struct {
+	Cur uint64
+	Max uint64
+}
+
+type _Gid_t uint32
+
+type Stat_t struct {
+	Dev       uint64
+	X__pad1   uint16
+	Pad_cgo_0 [2]byte
+	X__st_ino uint32
+	Mode      uint32
+	Nlink     uint32
+	Uid       uint32
+	Gid       uint32
+	Rdev      uint64
+	X__pad2   uint16
+	Pad_cgo_1 [6]byte
+	Size      int64
+	Blksize   int32
+	Pad_cgo_2 [4]byte
+	Blocks    int64
+	Atim      Timespec
+	Mtim      Timespec
+	Ctim      Timespec
+	Ino       uint64
+}
+
+type Statfs_t struct {
+	Type      int32
+	Bsize     int32
+	Blocks    uint64
+	Bfree     uint64
+	Bavail    uint64
+	Files     uint64
+	Ffree     uint64
+	Fsid      Fsid
+	Namelen   int32
+	Frsize    int32
+	Flags     int32
+	Spare     [4]int32
+	Pad_cgo_0 [4]byte
+}
+
+type Dirent struct {
+	Ino       uint64
+	Off       int64
+	Reclen    uint16
+	Type      uint8
+	Name      [256]uint8
+	Pad_cgo_0 [5]byte
+}
+
+type Fsid struct {
+	X__val [2]int32
+}
+
+type Flock_t struct {
+	Type      int16
+	Whence    int16
+	Pad_cgo_0 [4]byte
+	Start     int64
+	Len       int64
+	Pid       int32
+	Pad_cgo_1 [4]byte
+}
+
+type RawSockaddrInet4 struct {
+	Family uint16
+	Port   uint16
+	Addr   [4]byte /* in_addr */
+	Zero   [8]uint8
+}
+
+type RawSockaddrInet6 struct {
+	Family   uint16
+	Port     uint16
+	Flowinfo uint32
+	Addr     [16]byte /* in6_addr */
+	Scope_id uint32
+}
+
+type RawSockaddrUnix struct {
+	Family uint16
+	Path   [108]int8
+}
+
+type RawSockaddrLinklayer struct {
+	Family   uint16
+	Protocol uint16
+	Ifindex  int32
+	Hatype   uint16
+	Pkttype  uint8
+	Halen    uint8
+	Addr     [8]uint8
+}
+
+type RawSockaddrNetlink struct {
+	Family uint16
+	Pad    uint16
+	Pid    uint32
+	Groups uint32
+}
+
+type RawSockaddr struct {
+	Family uint16
+	Data   [14]uint8
+}
+
+type RawSockaddrAny struct {
+	Addr RawSockaddr
+	Pad  [96]uint8
+}
+
+type _Socklen uint32
+
+type Linger struct {
+	Onoff  int32
+	Linger int32
+}
+
+type Iovec struct {
+	Base *byte
+	Len  uint32
+}
+
+type IPMreq struct {
+	Multiaddr [4]byte /* in_addr */
+	Interface [4]byte /* in_addr */
+}
+
+type IPMreqn struct {
+	Multiaddr [4]byte /* in_addr */
+	Address   [4]byte /* in_addr */
+	Ifindex   int32
+}
+
+type IPv6Mreq struct {
+	Multiaddr [16]byte /* in6_addr */
+	Interface uint32
+}
+
+type Msghdr struct {
+	Name       *byte
+	Namelen    uint32
+	Iov        *Iovec
+	Iovlen     uint32
+	Control    *byte
+	Controllen uint32
+	Flags      int32
+}
+
+type Cmsghdr struct {
+	Len   uint32
+	Level int32
+	Type  int32
+}
+
+type Inet4Pktinfo struct {
+	Ifindex  int32
+	Spec_dst [4]byte /* in_addr */
+	Addr     [4]byte /* in_addr */
+}
+
+type Inet6Pktinfo struct {
+	Addr    [16]byte /* in6_addr */
+	Ifindex uint32
+}
+
+type IPv6MTUInfo struct {
+	Addr RawSockaddrInet6
+	Mtu  uint32
+}
+
+type ICMPv6Filter struct {
+	Data [8]uint32
+}
+
+type Ucred struct {
+	Pid int32
+	Uid uint32
+	Gid uint32
+}
+
+type TCPInfo struct {
+	State          uint8
+	Ca_state       uint8
+	Retransmits    uint8
+	Probes         uint8
+	Backoff        uint8
+	Options        uint8
+	Pad_cgo_0      [2]byte
+	Rto            uint32
+	Ato            uint32
+	Snd_mss        uint32
+	Rcv_mss        uint32
+	Unacked        uint32
+	Sacked         uint32
+	Lost           uint32
+	Retrans        uint32
+	Fackets        uint32
+	Last_data_sent uint32
+	Last_ack_sent  uint32
+	Last_data_recv uint32
+	Last_ack_recv  uint32
+	Pmtu           uint32
+	Rcv_ssthresh   uint32
+	Rtt            uint32
+	Rttvar         uint32
+	Snd_ssthresh   uint32
+	Snd_cwnd       uint32
+	Advmss         uint32
+	Reordering     uint32
+	Rcv_rtt        uint32
+	Rcv_space      uint32
+	Total_retrans  uint32
+}
+
+const (
+	SizeofSockaddrInet4     = 0x10
+	SizeofSockaddrInet6     = 0x1c
+	SizeofSockaddrAny       = 0x70
+	SizeofSockaddrUnix      = 0x6e
+	SizeofSockaddrLinklayer = 0x14
+	SizeofSockaddrNetlink   = 0xc
+	SizeofLinger            = 0x8
+	SizeofIPMreq            = 0x8
+	SizeofIPMreqn           = 0xc
+	SizeofIPv6Mreq          = 0x14
+	SizeofMsghdr            = 0x1c
+	SizeofCmsghdr           = 0xc
+	SizeofInet4Pktinfo      = 0xc
+	SizeofInet6Pktinfo      = 0x14
+	SizeofIPv6MTUInfo       = 0x20
+	SizeofICMPv6Filter      = 0x20
+	SizeofUcred             = 0xc
+	SizeofTCPInfo           = 0x68
+)
+
+const (
+	IFA_UNSPEC          = 0x0
+	IFA_ADDRESS         = 0x1
+	IFA_LOCAL           = 0x2
+	IFA_LABEL           = 0x3
+	IFA_BROADCAST       = 0x4
+	IFA_ANYCAST         = 0x5
+	IFA_CACHEINFO       = 0x6
+	IFA_MULTICAST       = 0x7
+	IFLA_UNSPEC         = 0x0
+	IFLA_ADDRESS        = 0x1
+	IFLA_BROADCAST      = 0x2
+	IFLA_IFNAME         = 0x3
+	IFLA_MTU            = 0x4
+	IFLA_LINK           = 0x5
+	IFLA_QDISC          = 0x6
+	IFLA_STATS          = 0x7
+	IFLA_COST           = 0x8
+	IFLA_PRIORITY       = 0x9
+	IFLA_MASTER         = 0xa
+	IFLA_WIRELESS       = 0xb
+	IFLA_PROTINFO       = 0xc
+	IFLA_TXQLEN         = 0xd
+	IFLA_MAP            = 0xe
+	IFLA_WEIGHT         = 0xf
+	IFLA_OPERSTATE      = 0x10
+	IFLA_LINKMODE       = 0x11
+	IFLA_LINKINFO       = 0x12
+	IFLA_NET_NS_PID     = 0x13
+	IFLA_IFALIAS        = 0x14
+	IFLA_MAX            = 0x1d
+	RT_SCOPE_UNIVERSE   = 0x0
+	RT_SCOPE_SITE       = 0xc8
+	RT_SCOPE_LINK       = 0xfd
+	RT_SCOPE_HOST       = 0xfe
+	RT_SCOPE_NOWHERE    = 0xff
+	RT_TABLE_UNSPEC     = 0x0
+	RT_TABLE_COMPAT     = 0xfc
+	RT_TABLE_DEFAULT    = 0xfd
+	RT_TABLE_MAIN       = 0xfe
+	RT_TABLE_LOCAL      = 0xff
+	RT_TABLE_MAX        = 0xffffffff
+	RTA_UNSPEC          = 0x0
+	RTA_DST             = 0x1
+	RTA_SRC             = 0x2
+	RTA_IIF             = 0x3
+	RTA_OIF             = 0x4
+	RTA_GATEWAY         = 0x5
+	RTA_PRIORITY        = 0x6
+	RTA_PREFSRC         = 0x7
+	RTA_METRICS         = 0x8
+	RTA_MULTIPATH       = 0x9
+	RTA_FLOW            = 0xb
+	RTA_CACHEINFO       = 0xc
+	RTA_TABLE           = 0xf
+	RTN_UNSPEC          = 0x0
+	RTN_UNICAST         = 0x1
+	RTN_LOCAL           = 0x2
+	RTN_BROADCAST       = 0x3
+	RTN_ANYCAST         = 0x4
+	RTN_MULTICAST       = 0x5
+	RTN_BLACKHOLE       = 0x6
+	RTN_UNREACHABLE     = 0x7
+	RTN_PROHIBIT        = 0x8
+	RTN_THROW           = 0x9
+	RTN_NAT             = 0xa
+	RTN_XRESOLVE        = 0xb
+	RTNLGRP_NONE        = 0x0
+	RTNLGRP_LINK        = 0x1
+	RTNLGRP_NOTIFY      = 0x2
+	RTNLGRP_NEIGH       = 0x3
+	RTNLGRP_TC          = 0x4
+	RTNLGRP_IPV4_IFADDR = 0x5
+	RTNLGRP_IPV4_MROUTE = 0x6
+	RTNLGRP_IPV4_ROUTE  = 0x7
+	RTNLGRP_IPV4_RULE   = 0x8
+	RTNLGRP_IPV6_IFADDR = 0x9
+	RTNLGRP_IPV6_MROUTE = 0xa
+	RTNLGRP_IPV6_ROUTE  = 0xb
+	RTNLGRP_IPV6_IFINFO = 0xc
+	RTNLGRP_IPV6_PREFIX = 0x12
+	RTNLGRP_IPV6_RULE   = 0x13
+	RTNLGRP_ND_USEROPT  = 0x14
+	SizeofNlMsghdr      = 0x10
+	SizeofNlMsgerr      = 0x14
+	SizeofRtGenmsg      = 0x1
+	SizeofNlAttr        = 0x4
+	SizeofRtAttr        = 0x4
+	SizeofIfInfomsg     = 0x10
+	SizeofIfAddrmsg     = 0x8
+	SizeofRtMsg         = 0xc
+	SizeofRtNexthop     = 0x8
+)
+
+type NlMsghdr struct {
+	Len   uint32
+	Type  uint16
+	Flags uint16
+	Seq   uint32
+	Pid   uint32
+}
+
+type NlMsgerr struct {
+	Error int32
+	Msg   NlMsghdr
+}
+
+type RtGenmsg struct {
+	Family uint8
+}
+
+type NlAttr struct {
+	Len  uint16
+	Type uint16
+}
+
+type RtAttr struct {
+	Len  uint16
+	Type uint16
+}
+
+type IfInfomsg struct {
+	Family     uint8
+	X__ifi_pad uint8
+	Type       uint16
+	Index      int32
+	Flags      uint32
+	Change     uint32
+}
+
+type IfAddrmsg struct {
+	Family    uint8
+	Prefixlen uint8
+	Flags     uint8
+	Scope     uint8
+	Index     uint32
+}
+
+type RtMsg struct {
+	Family   uint8
+	Dst_len  uint8
+	Src_len  uint8
+	Tos      uint8
+	Table    uint8
+	Protocol uint8
+	Scope    uint8
+	Type     uint8
+	Flags    uint32
+}
+
+type RtNexthop struct {
+	Len     uint16
+	Flags   uint8
+	Hops    uint8
+	Ifindex int32
+}
+
+const (
+	SizeofSockFilter = 0x8
+	SizeofSockFprog  = 0x8
+)
+
+type SockFilter struct {
+	Code uint16
+	Jt   uint8
+	Jf   uint8
+	K    uint32
+}
+
+type SockFprog struct {
+	Len       uint16
+	Pad_cgo_0 [2]byte
+	Filter    *SockFilter
+}
+
+type InotifyEvent struct {
+	Wd     int32
+	Mask   uint32
+	Cookie uint32
+	Len    uint32
+	Name   [0]uint8
+}
+
+const SizeofInotifyEvent = 0x10
+
+type PtraceRegs struct {
+	Uregs [18]uint32
+}
+
+type FdSet struct {
+	Bits [32]int32
+}
+
+type Sysinfo_t struct {
+	Uptime    int32
+	Loads     [3]uint32
+	Totalram  uint32
+	Freeram   uint32
+	Sharedram uint32
+	Bufferram uint32
+	Totalswap uint32
+	Freeswap  uint32
+	Procs     uint16
+	Pad       uint16
+	Totalhigh uint32
+	Freehigh  uint32
+	Unit      uint32
+	X_f       [8]uint8
+}
+
+type Utsname struct {
+	Sysname    [65]uint8
+	Nodename   [65]uint8
+	Release    [65]uint8
+	Version    [65]uint8
+	Machine    [65]uint8
+	Domainname [65]uint8
+}
+
+type Ustat_t struct {
+	Tfree  int32
+	Tinode uint32
+	Fname  [6]uint8
+	Fpack  [6]uint8
+}
+
+type EpollEvent struct {
+	Events uint32
+	PadFd  int32
+	Fd     int32
+	Pad    int32
+}
+
+const (
+	_AT_FDCWD            = -0x64
+	_AT_REMOVEDIR        = 0x200
+	_AT_SYMLINK_NOFOLLOW = 0x100
+	_AT_EACCESS          = 0x200
+)
+
+type pollFd struct {
+	Fd      int32
+	Events  int16
+	Revents int16
+}
+
+type Termios struct {
+	Iflag     uint32
+	Oflag     uint32
+	Cflag     uint32
+	Lflag     uint32
+	Line      uint8
+	Cc        [32]uint8
+	Pad_cgo_0 [3]byte
+	Ispeed    uint32
+	Ospeed    uint32
+}
+
+const (
+	VINTR    = 0x0
+	VQUIT    = 0x1
+	VERASE   = 0x2
+	VKILL    = 0x3
+	VEOF     = 0x4
+	VTIME    = 0x5
+	VMIN     = 0x6
+	VSWTC    = 0x7
+	VSTART   = 0x8
+	VSTOP    = 0x9
+	VSUSP    = 0xa
+	VEOL     = 0xb
+	VREPRINT = 0xc
+	VDISCARD = 0xd
+	VWERASE  = 0xe
+	VLNEXT   = 0xf
+	VEOL2    = 0x10
+	IGNBRK   = 0x1
+	BRKINT   = 0x2
+	IGNPAR   = 0x4
+	PARMRK   = 0x8
+	INPCK    = 0x10
+	ISTRIP   = 0x20
+	INLCR    = 0x40
+	IGNCR    = 0x80
+	ICRNL    = 0x100
+	IUCLC    = 0x200
+	IXON     = 0x400
+	IXANY    = 0x800
+	IXOFF    = 0x1000
+	IMAXBEL  = 0x2000
+	IUTF8    = 0x4000
+	OPOST    = 0x1
+	OLCUC    = 0x2
+	ONLCR    = 0x4
+	OCRNL    = 0x8
+	ONOCR    = 0x10
+	ONLRET   = 0x20
+	OFILL    = 0x40
+	OFDEL    = 0x80
+	B0       = 0x0
+	B50      = 0x1
+	B75      = 0x2
+	B110     = 0x3
+	B134     = 0x4
+	B150     = 0x5
+	B200     = 0x6
+	B300     = 0x7
+	B600     = 0x8
+	B1200    = 0x9
+	B1800    = 0xa
+	B2400    = 0xb
+	B4800    = 0xc
+	B9600    = 0xd
+	B19200   = 0xe
+	B38400   = 0xf
+	CSIZE    = 0x30
+	CS5      = 0x0
+	CS6      = 0x10
+	CS7      = 0x20
+	CS8      = 0x30
+	CSTOPB   = 0x40
+	CREAD    = 0x80
+	PARENB   = 0x100
+	PARODD   = 0x200
+	HUPCL    = 0x400
+	CLOCAL   = 0x800
+	B57600   = 0x1001
+	B115200  = 0x1002
+	B230400  = 0x1003
+	B460800  = 0x1004
+	B500000  = 0x1005
+	B576000  = 0x1006
+	B921600  = 0x1007
+	B1000000 = 0x1008
+	B1152000 = 0x1009
+	B1500000 = 0x100a
+	B2000000 = 0x100b
+	B2500000 = 0x100c
+	B3000000 = 0x100d
+	B3500000 = 0x100e
+	B4000000 = 0x100f
+	ISIG     = 0x1
+	ICANON   = 0x2
+	XCASE    = 0x4
+	ECHO     = 0x8
+	ECHOE    = 0x10
+	ECHOK    = 0x20
+	ECHONL   = 0x40
+	NOFLSH   = 0x80
+	TOSTOP   = 0x100
+	ECHOCTL  = 0x200
+	ECHOPRT  = 0x400
+	ECHOKE   = 0x800
+	FLUSHO   = 0x1000
+	PENDIN   = 0x4000
+	IEXTEN   = 0x8000
+	TCGETS   = 0x5401
+	TCSETS   = 0x5402
+)
diff --git a/src/time/format.go b/src/time/format.go
index f11fb7ed30..13cfda0802 100644
--- a/src/time/format.go
+++ b/src/time/format.go
@@ -283,7 +283,7 @@ func nextStdChunk(layout string) (prefix string, std int, suffix string) {
 	return layout, 0, ""
 }
 
-var longDayNames = []string{
+var longDayNames = [...]string{
 	"Sunday",
 	"Monday",
 	"Tuesday",
@@ -293,7 +293,7 @@ var longDayNames = []string{
 	"Saturday",
 }
 
-var shortDayNames = []string{
+var shortDayNames = [...]string{
 	"Sun",
 	"Mon",
 	"Tue",
@@ -303,7 +303,7 @@ var shortDayNames = []string{
 	"Sat",
 }
 
-var shortMonthNames = []string{
+var shortMonthNames = [...]string{
 	"Jan",
 	"Feb",
 	"Mar",
@@ -318,7 +318,7 @@ var shortMonthNames = []string{
 	"Dec",
 }
 
-var longMonthNames = []string{
+var longMonthNames = [...]string{
 	"January",
 	"February",
 	"March",
@@ -886,10 +886,10 @@ func parse(layout, value string, defaultLocation, local *Location) (Time, error)
 			p, value = value[0:4], value[4:]
 			year, err = atoi(p)
 		case stdMonth:
-			month, value, err = lookup(shortMonthNames, value)
+			month, value, err = lookup(shortMonthNames[:], value)
 			month++
 		case stdLongMonth:
-			month, value, err = lookup(longMonthNames, value)
+			month, value, err = lookup(longMonthNames[:], value)
 			month++
 		case stdNumMonth, stdZeroMonth:
 			month, value, err = getnum(value, std == stdZeroMonth)
@@ -898,9 +898,9 @@ func parse(layout, value string, defaultLocation, local *Location) (Time, error)
 			}
 		case stdWeekDay:
 			// Ignore weekday except for error checking.
-			_, value, err = lookup(shortDayNames, value)
+			_, value, err = lookup(shortDayNames[:], value)
 		case stdLongWeekDay:
-			_, value, err = lookup(longDayNames, value)
+			_, value, err = lookup(longDayNames[:], value)
 		case stdDay, stdUnderDay, stdZeroDay:
 			if std == stdUnderDay && len(value) > 0 && value[0] == ' ' {
 				value = value[1:]
diff --git a/src/time/sys_noos.go b/src/time/sys_noos.go
new file mode 100644
index 0000000000..835eb24e0a
--- /dev/null
+++ b/src/time/sys_noos.go
@@ -0,0 +1,42 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package time
+
+import "syscall"
+
+func open(name string) (uintptr, error) {
+	return 0, syscall.ENOENT
+}
+
+func closefd(fd uintptr) {
+}
+
+func read(fd uintptr, buf []byte) (int, error) {
+	return 0, syscall.EINVAL
+}
+
+func preadn(fd uintptr, buf []byte, off int) error {
+	return syscall.EINVAL
+}
+
+func move(sec int64, nsec int32) (sec0 int64, nsec0 int32)
+
+// Set sets the wall clock in a way that the old time will become the new time.
+// In fact it moves the wall clock by new - old amount of time. The monotonic
+// clock is unaffected. Set returns the new wall time that corresponds to the
+// the zero monotonic time.
+func Set(old, new Time) Time {
+	sec := new.sec() - old.sec()
+	nsec := new.nsec() - old.nsec()
+	if nsec < 0 {
+		nsec += 1e9
+		sec--
+	} else if nsec >= 1e9 {
+		nsec -= 1e9
+		sec++
+	}
+	sec0, nsec0 := move(sec, nsec)
+	return Unix(sec0, int64(nsec0))
+}
diff --git a/src/time/zoneinfo_noos.go b/src/time/zoneinfo_noos.go
new file mode 100644
index 0000000000..a80d5933dd
--- /dev/null
+++ b/src/time/zoneinfo_noos.go
@@ -0,0 +1,11 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package time
+
+var zoneSources = []string{}
+
+func initLocal() {
+	localLoc = *UTC
+}
diff --git a/src/unicode/tables.go b/src/unicode/tables.go
index a9b23bfacd..888cdc7633 100644
--- a/src/unicode/tables.go
+++ b/src/unicode/tables.go
@@ -7638,7 +7638,7 @@ var asciiFold = [MaxASCII + 1]uint16{
 	0x007F,
 }
 
-var caseOrbit = []foldPair{
+var caseOrbit = [...]foldPair{
 	{0x004B, 0x006B},
 	{0x0053, 0x0073},
 	{0x006B, 0x212A},
diff --git a/src/vendor/golang.org/x/sys/cpu/cpu_arm.go b/src/vendor/golang.org/x/sys/cpu/cpu_armt.go
similarity index 99%
rename from src/vendor/golang.org/x/sys/cpu/cpu_arm.go
rename to src/vendor/golang.org/x/sys/cpu/cpu_armt.go
index 301b752e9c..ca25b0ed56 100644
--- a/src/vendor/golang.org/x/sys/cpu/cpu_arm.go
+++ b/src/vendor/golang.org/x/sys/cpu/cpu_armt.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build arm thumb
+
 package cpu
 
 const cacheLineSize = 32
diff --git a/src/vendor/golang.org/x/sys/cpu/cpu_linux_arm.go b/src/vendor/golang.org/x/sys/cpu/cpu_linux_armt.go
similarity index 97%
rename from src/vendor/golang.org/x/sys/cpu/cpu_linux_arm.go
rename to src/vendor/golang.org/x/sys/cpu/cpu_linux_armt.go
index 2057006dce..c43403d181 100644
--- a/src/vendor/golang.org/x/sys/cpu/cpu_linux_arm.go
+++ b/src/vendor/golang.org/x/sys/cpu/cpu_linux_armt.go
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build linux
+// +build arm thumb
+
 package cpu
 
 func doinit() {
diff --git a/src/vendor/golang.org/x/sys/cpu/cpu_linux_noinit.go b/src/vendor/golang.org/x/sys/cpu/cpu_linux_noinit.go
index 42b5d33cb6..691e8813b1 100644
--- a/src/vendor/golang.org/x/sys/cpu/cpu_linux_noinit.go
+++ b/src/vendor/golang.org/x/sys/cpu/cpu_linux_noinit.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build linux,!arm,!arm64,!mips64,!mips64le,!ppc64,!ppc64le,!s390x
+// +build linux,!arm,!arm64,!mips64,!mips64le,!ppc64,!ppc64le,!s390x,!thumb
 
 package cpu
 
diff --git a/test/codegen/noextend.go b/test/codegen/noextend.go
index 424fd2008d..cbdc1e2c7a 100644
--- a/test/codegen/noextend.go
+++ b/test/codegen/noextend.go
@@ -23,10 +23,12 @@ func set16(x8 int8, u8 uint8, y8 int8, z8 uint8) {
 	// Truncate not needed, load does sign/zero extend
 	// ppc64:-"MOVB\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVB\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVB\tR\\d+,\\sR\\d+"
 	sval16[0] = int16(x8)
 
 	// ppc64:-"MOVBZ\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVBZ\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVBU\tR\\d+,\\sR\\d+"
 	val16[0] = uint16(u8)
 
 	// AND not needed due to size
@@ -36,24 +38,29 @@ func set16(x8 int8, u8 uint8, y8 int8, z8 uint8) {
 
 	// ppc64:-"ANDCC"
 	// ppc64le:-"ANDCC"
+	// thumb:-"AND"
 	val16[1] = 255 & uint16(u8+z8)
 
 }
 func shiftidx(x8 int8, u8 uint8, x16 int16, u16 uint16, x32 int32, u32 uint32) {
 	// ppc64:-"MOVB\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVB\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVB\tR\\d+,\\sR\\d+"
 	sval16[0] = int16(val16[x8>>1])
 
 	// ppc64:-"MOVBZ\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVBZ\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVBU\tR\\d+,\\sR\\d+"
 	val16[0] = uint16(sval16[u8>>2])
 
 	// ppc64:-"MOVH\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVH\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVH\tR\\d+,\\sR\\d+"
 	sval16[1] = int16(val16[x16>>1])
 
 	// ppc64:-"MOVHZ\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVHZ\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVHU\tR\\d+,\\sR\\d+"
 	val16[1] = uint16(sval16[u16>>2])
 
 }
@@ -63,10 +70,12 @@ func setnox(x8 int8, u8 uint8, y8 int8, z8 uint8, x16 int16, u16 uint16, x32 int
 
 	// ppc64:-"MOVB\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVB\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVB\tR\\d+,\\sR\\d+"
 	sval16[0] = int16(x8)
 
 	// ppc64:-"MOVBZ\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVBZ\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVBU\tR\\d+,\\sR\\d+"
 	val16[0] = uint16(u8)
 
 	// AND not needed due to size
@@ -76,22 +85,27 @@ func setnox(x8 int8, u8 uint8, y8 int8, z8 uint8, x16 int16, u16 uint16, x32 int
 
 	// ppc64:-"ANDCC"
 	// ppc64le:-"ANDCC"
+	// thumb:-"AND"
 	val16[1] = 255 & uint16(u8+z8)
 
 	// ppc64:-"MOVB\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVB\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVB\tR\\d+,\\sR\\d+"
 	sval32[0] = int32(x8)
 
 	// ppc64:-"MOVH\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVH\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVH\tR\\d+,\\sR\\d+"
 	sval32[1] = int32(x16)
 
 	//ppc64:-"MOVBZ\tR\\d+,\\sR\\d+"
 	//ppc64le:-"MOVBZ\tR\\d+,\\sR\\d+"
+	//thumb:-"MOVBU\tR\\d+,\\sR\\d+"
 	val32[0] = uint32(u8)
 
 	// ppc64:-"MOVHZ\tR\\d+,\\sR\\d+"
 	// ppc64le:-"MOVHZ\tR\\d+,\\sR\\d+"
+	// thumb:-"MOVHU\tR\\d+,\\sR\\d+"
 	val32[1] = uint32(u16)
 
 	// ppc64:-"MOVB\tR\\d+,\\sR\\d+"
diff --git a/test/fixedbugs/issue10607.go b/test/fixedbugs/issue10607.go
index 448a37dcac..b3f7fc340f 100644
--- a/test/fixedbugs/issue10607.go
+++ b/test/fixedbugs/issue10607.go
@@ -1,4 +1,4 @@
-// +build linux,!ppc64,!riscv64,gc
+// +build linux,!ppc64,!riscv64,!thumb,gc
 // run
 
 // Copyright 2015 The Go Authors. All rights reserved.
diff --git a/test/fixedbugs/issue11656.go b/test/fixedbugs/issue11656.go
index 85fe720b30..abd6028e1a 100644
--- a/test/fixedbugs/issue11656.go
+++ b/test/fixedbugs/issue11656.go
@@ -63,6 +63,10 @@ func f(n int) {
 	case "arm":
 		binary.LittleEndian.PutUint32(ill[0:4], 0xe3a00000) // MOVW $0, R0
 		binary.LittleEndian.PutUint32(ill[4:8], 0xe5800000) // MOVW R0, (R0)
+	case "thumb":
+		binary.LittleEndian.PutUint16(ill, 0x2000) // MOVW $0, R0
+		binary.LittleEndian.PutUint16(ill, 0x6000) // MOVW R0, (R0)
+		f.x = 1
 	case "arm64":
 		binary.LittleEndian.PutUint32(ill, 0xf90003ff) // MOVD ZR, (ZR)
 	case "ppc64":
@@ -82,7 +86,7 @@ func f(n int) {
 		// Just leave it as 0 and hope for the best.
 	}
 
-	f.x = uintptr(unsafe.Pointer(&ill[0]))
+	f.x += uintptr(unsafe.Pointer(&ill[0]))
 	p := &f
 	fn := *(*func())(unsafe.Pointer(&p))
 	fn()
diff --git a/test/fixedbugs/issue22200b.go b/test/fixedbugs/issue22200b.go
index ce20923334..e471414008 100644
--- a/test/fixedbugs/issue22200b.go
+++ b/test/fixedbugs/issue22200b.go
@@ -4,7 +4,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !386,!amd64p32,!arm,!mips,!mipsle
+// +build !386,!amd64p32,!arm,!thumb,!mips,!mipsle
 
 package p
 
diff --git a/test/fixedbugs/issue6036.go b/test/fixedbugs/issue6036.go
index 8ebef5a447..33e3b8c479 100644
--- a/test/fixedbugs/issue6036.go
+++ b/test/fixedbugs/issue6036.go
@@ -1,4 +1,4 @@
-// +build !386,!arm,!mips,!mipsle,!amd64p32
+// +build !386,!arm,!thumb,!mips,!mipsle,!amd64p32
 // compile
 
 // Copyright 2013 The Go Authors. All rights reserved.
diff --git a/test/inline_sync.go b/test/inline_sync.go
index 30b436af41..ebf5ec0068 100644
--- a/test/inline_sync.go
+++ b/test/inline_sync.go
@@ -1,4 +1,4 @@
-// +build !nacl,!386,!wasm,!arm,!gcflags_noopt
+// +build !nacl,!386,!wasm,!arm,!gcflags_noopt,!thumb
 // errorcheck -0 -m
 
 // Copyright 2019 The Go Authors. All rights reserved.
diff --git a/test/nosplit.go b/test/nosplit.go
index faa7b8c2d8..eb58281d00 100644
--- a/test/nosplit.go
+++ b/test/nosplit.go
@@ -275,7 +275,7 @@ TestCases:
 		case "ppc64", "ppc64le":
 			ptrSize = 8
 			fmt.Fprintf(&buf, "#define REGISTER (CTR)\n")
-		case "arm":
+		case "arm", "thumb":
 			fmt.Fprintf(&buf, "#define REGISTER (R0)\n")
 		case "arm64":
 			ptrSize = 8
diff --git a/test/run.go b/test/run.go
index 624f2236a1..6d080ab7a7 100644
--- a/test/run.go
+++ b/test/run.go
@@ -1501,6 +1501,7 @@ var (
 		"386":     {"GO386", "sse2", "softfloat"},
 		"amd64":   {},
 		"arm":     {"GOARM", "5", "6", "7"},
+		"thumb":   {},
 		"arm64":   {},
 		"mips":    {"GOMIPS", "hardfloat", "softfloat"},
 		"mips64":  {"GOMIPS64", "hardfloat", "softfloat"},
