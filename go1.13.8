diff --git a/api/go1embedded.txt b/api/go1embedded.txt
new file mode 100644
index 0000000000..65d16d97f5
--- /dev/null
+++ b/api/go1embedded.txt
@@ -0,0 +1,93 @@
+pkg embedded/mmio, func MB()
+pkg embedded/mmio, method (*U16) Addr() uintptr
+pkg embedded/mmio, method (*U16) LoadBit(int) int
+pkg embedded/mmio, method (*U16) LoadBits(uint16) uint16
+pkg embedded/mmio, method (*U16) ClearBit(int)
+pkg embedded/mmio, method (*U16) ClearBits(uint16)
+pkg embedded/mmio, method (*U16) Load() uint16
+pkg embedded/mmio, method (*U16) SetBit(int)
+pkg embedded/mmio, method (*U16) SetBits(uint16)
+pkg embedded/mmio, method (*U16) Store(uint16)
+pkg embedded/mmio, method (*U16) StoreBit(int, int)
+pkg embedded/mmio, method (*U16) StoreBits(uint16, uint16)
+pkg embedded/mmio, method (*U32) Addr() uintptr
+pkg embedded/mmio, method (*U32) LoadBit(int) int
+pkg embedded/mmio, method (*U32) LoadBits(uint32) uint32
+pkg embedded/mmio, method (*U32) ClearBit(int)
+pkg embedded/mmio, method (*U32) ClearBits(uint32)
+pkg embedded/mmio, method (*U32) Load() uint32
+pkg embedded/mmio, method (*U32) SetBit(int)
+pkg embedded/mmio, method (*U32) SetBits(uint32)
+pkg embedded/mmio, method (*U32) Store(uint32)
+pkg embedded/mmio, method (*U32) StoreBit(int, int)
+pkg embedded/mmio, method (*U32) StoreBits(uint32, uint32)
+pkg embedded/mmio, method (*U8) Addr() uintptr
+pkg embedded/mmio, method (*U8) LoadBit(int) int
+pkg embedded/mmio, method (*U8) LoadBits(uint8) uint8
+pkg embedded/mmio, method (*U8) ClearBit(int)
+pkg embedded/mmio, method (*U8) ClearBits(uint8)
+pkg embedded/mmio, method (*U8) Load() uint8
+pkg embedded/mmio, method (*U8) SetBit(int)
+pkg embedded/mmio, method (*U8) SetBits(uint8)
+pkg embedded/mmio, method (*U8) Store(uint8)
+pkg embedded/mmio, method (*U8) StoreBit(int, int)
+pkg embedded/mmio, method (*U8) StoreBits(uint8, uint8)
+pkg embedded/mmio, method (UM16) Clear()
+pkg embedded/mmio, method (UM16) Load() uint16
+pkg embedded/mmio, method (UM16) Set()
+pkg embedded/mmio, method (UM16) Store(uint16)
+pkg embedded/mmio, method (UM32) Clear()
+pkg embedded/mmio, method (UM32) Load() uint32
+pkg embedded/mmio, method (UM32) Set()
+pkg embedded/mmio, method (UM32) Store(uint32)
+pkg embedded/mmio, method (UM8) Clear()
+pkg embedded/mmio, method (UM8) Load() uint8
+pkg embedded/mmio, method (UM8) Set()
+pkg embedded/mmio, method (UM8) Store(uint8)
+pkg embedded/mmio, type U16 struct
+pkg embedded/mmio, type U32 struct
+pkg embedded/mmio, type U8 struct
+pkg embedded/mmio, type UM16 struct
+pkg embedded/mmio, type UM16 struct, Mask uint16
+pkg embedded/mmio, type UM16 struct, R *U16
+pkg embedded/mmio, type UM32 struct
+pkg embedded/mmio, type UM32 struct, Mask uint32
+pkg embedded/mmio, type UM32 struct, R *U32
+pkg embedded/mmio, type UM8 struct
+pkg embedded/mmio, type UM8 struct, Mask uint8
+pkg embedded/mmio, type UM8 struct, R *U8
+pkg embedded/rtos, const IntPrioCurrent = -1
+pkg embedded/rtos, const IntPrioCurrent ideal-int
+pkg embedded/rtos, const IntPrioHigh = 0
+pkg embedded/rtos, const IntPrioHigh ideal-int
+pkg embedded/rtos, const IntPrioHighest = 0
+pkg embedded/rtos, const IntPrioHighest ideal-int
+pkg embedded/rtos, const IntPrioLow = 0
+pkg embedded/rtos, const IntPrioLow ideal-int
+pkg embedded/rtos, const IntPrioLowest = 0
+pkg embedded/rtos, const IntPrioLowest ideal-int
+pkg embedded/rtos, const IntPrioMid = 0
+pkg embedded/rtos, const IntPrioMid ideal-int
+pkg embedded/rtos, const IntPrioSysCall = 0
+pkg embedded/rtos, const IntPrioSysCall ideal-int
+pkg embedded/rtos, const IntPrioSysTimer = 0
+pkg embedded/rtos, const IntPrioSysTimer ideal-int
+pkg embedded/rtos, func Nanotime() time.Duration
+pkg embedded/rtos, func SetPrivLevel(int) (int, error)
+pkg embedded/rtos, func SetSystemTimer(func() int64, func(int64) bool) error
+pkg embedded/rtos, method (*Error) Error() string
+pkg embedded/rtos, method (*Note) Clear()
+pkg embedded/rtos, method (*Note) Sleep(time.Duration) bool
+pkg embedded/rtos, method (*Note) Wakeup()
+pkg embedded/rtos, method (IRQ) Disable() error
+pkg embedded/rtos, method (IRQ) Enable(int) error
+pkg embedded/rtos, method (IRQ) Status() (bool, int, error)
+pkg embedded/rtos, type Error struct
+pkg embedded/rtos, type IRQ int
+pkg embedded/rtos, type Note struct
+pkg embedded/rtos, var ErrBadIntNumber *Error
+pkg embedded/rtos, var ErrBadIntPrio *Error
+pkg embedded/rtos, var ErrBadPrivLevel *Error
+pkg embedded/rtos, var ErrInsufPrivLevel *Error
+pkg embedded/rtos, var ErrNotSuppoted *Error
+pkg embedded/rtos, var ErrUknown *Error
diff --git a/src/cmd/asm/internal/arch/arch.go b/src/cmd/asm/internal/arch/arch.go
index eaa5cb8958..21eb12953c 100644
--- a/src/cmd/asm/internal/arch/arch.go
+++ b/src/cmd/asm/internal/arch/arch.go
@@ -12,6 +12,7 @@ import (
 	"cmd/internal/obj/mips"
 	"cmd/internal/obj/ppc64"
 	"cmd/internal/obj/s390x"
+	"cmd/internal/obj/thumb"
 	"cmd/internal/obj/wasm"
 	"cmd/internal/obj/x86"
 	"fmt"
@@ -89,6 +90,8 @@ func Set(GOARCH string) *Arch {
 		a := archS390x()
 		a.LinkArch = &s390x.Links390x
 		return a
+	case "thumb":
+		return archThumb()
 	case "wasm":
 		return archWasm()
 	}
@@ -589,6 +592,82 @@ func archS390x() *Arch {
 	}
 }
 
+func archThumb() *Arch {
+	register := make(map[string]int16)
+	// Create maps for easy lookup of instruction names etc.
+	// Note that there is no list of names as there is for x86.
+	for i := thumb.REG_R0; i < thumb.REG_FPSCR; i++ {
+		register[obj.Rconv(i)] = int16(i)
+	}
+	// Avoid unintentionally clobbering g using R10.
+	delete(register, "R10")
+	register["g"] = thumb.REG_R10
+	for i := 0; i < 16; i++ {
+		register[fmt.Sprintf("C%d", i)] = int16(i)
+	}
+
+	// Pseudo-registers.
+	register["SB"] = RSB
+	register["FP"] = RFP
+	register["PC"] = RPC
+	register["SP"] = RSP
+	registerPrefix := map[string]bool{
+		"F": true,
+		"R": true,
+	}
+
+	// special operands for DMB/DSB/ISB instructions
+	register["MB_SY"] = thumb.REG_MB_SY
+	register["MB_ST"] = thumb.REG_MB_ST
+	register["MB_ISH"] = thumb.REG_MB_ISH
+	register["MB_ISHST"] = thumb.REG_MB_ISHST
+	register["MB_NSH"] = thumb.REG_MB_NSH
+	register["MB_NSHST"] = thumb.REG_MB_NSHST
+	register["MB_OSH"] = thumb.REG_MB_OSH
+	register["MB_OSHST"] = thumb.REG_MB_OSHST
+
+	// secial operands for IT instuction
+	register["EQ"] = thumb.REG_EQ
+	register["NE"] = thumb.REG_NE
+	register["HS"] = thumb.REG_HS
+	register["CS"] = thumb.REG_HS
+	register["LO"] = thumb.REG_LO
+	register["CC"] = thumb.REG_LO
+	register["MI"] = thumb.REG_MI
+	register["PL"] = thumb.REG_PL
+	register["VS"] = thumb.REG_VS
+	register["VC"] = thumb.REG_VC
+	register["HI"] = thumb.REG_HI
+	register["LS"] = thumb.REG_LS
+	register["GE"] = thumb.REG_GE
+	register["LT"] = thumb.REG_LT
+	register["GT"] = thumb.REG_GT
+	register["LE"] = thumb.REG_LE
+	register["AL"] = thumb.REG_AL
+
+	instructions := make(map[string]obj.As)
+	for i, s := range obj.Anames {
+		instructions[s] = obj.As(i)
+	}
+	for i, s := range thumb.Anames {
+		if obj.As(i) >= obj.A_ARCHSPECIFIC {
+			instructions[s] = obj.As(i) + obj.ABaseThumb
+		}
+	}
+	// Annoying aliases.
+	instructions["B"] = obj.AJMP
+	instructions["BL"] = obj.ACALL
+
+	return &Arch{
+		LinkArch:       &thumb.Link,
+		Instructions:   instructions,
+		Register:       register,
+		RegisterPrefix: registerPrefix,
+		RegisterNumber: thumbRegisterNumber,
+		IsJump:         jumpThumb,
+	}
+}
+
 func archWasm() *Arch {
 	instructions := make(map[string]obj.As)
 	for i, s := range obj.Anames {
diff --git a/src/cmd/asm/internal/arch/thumb.go b/src/cmd/asm/internal/arch/thumb.go
new file mode 100644
index 0000000000..22a2024c0d
--- /dev/null
+++ b/src/cmd/asm/internal/arch/thumb.go
@@ -0,0 +1,207 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// This file encapsulates some of the odd characteristics of the Thumb
+// instruction set, to minimize its interaction with the core of the
+// assembler.
+
+package arch
+
+import (
+	"strings"
+
+	"cmd/internal/obj"
+	"cmd/internal/obj/thumb"
+)
+
+var thumbLS = map[string]uint8{
+	"U":  thumb.C_UBIT,
+	"S":  thumb.C_SBIT,
+	"W":  thumb.C_WBIT,
+	"P":  thumb.C_PBIT,
+	"PW": thumb.C_WBIT | thumb.C_PBIT,
+	"WP": thumb.C_WBIT | thumb.C_PBIT,
+}
+
+var thumbSCOND = map[string]uint8{
+	"EQ":  thumb.C_SCOND_EQ,
+	"NE":  thumb.C_SCOND_NE,
+	"CS":  thumb.C_SCOND_HS,
+	"HS":  thumb.C_SCOND_HS,
+	"CC":  thumb.C_SCOND_LO,
+	"LO":  thumb.C_SCOND_LO,
+	"MI":  thumb.C_SCOND_MI,
+	"PL":  thumb.C_SCOND_PL,
+	"VS":  thumb.C_SCOND_VS,
+	"VC":  thumb.C_SCOND_VC,
+	"HI":  thumb.C_SCOND_HI,
+	"LS":  thumb.C_SCOND_LS,
+	"GE":  thumb.C_SCOND_GE,
+	"LT":  thumb.C_SCOND_LT,
+	"GT":  thumb.C_SCOND_GT,
+	"LE":  thumb.C_SCOND_LE,
+	"AL":  thumb.C_SCOND_NONE,
+	"U":   thumb.C_UBIT,
+	"S":   thumb.C_SBIT,
+	"W":   thumb.C_WBIT,
+	"P":   thumb.C_PBIT,
+	"PW":  thumb.C_WBIT | thumb.C_PBIT,
+	"WP":  thumb.C_WBIT | thumb.C_PBIT,
+	"F":   thumb.C_FBIT,
+	"IAW": thumb.C_WBIT | thumb.C_UBIT,
+	"DBW": thumb.C_WBIT | thumb.C_PBIT,
+	"IA":  thumb.C_UBIT,
+	"DB":  thumb.C_PBIT,
+}
+
+var thumbJump = map[string]bool{
+	"B":    true,
+	"BL":   true,
+	"BEQ":  true,
+	"BNE":  true,
+	"BCS":  true,
+	"BHS":  true,
+	"BCC":  true,
+	"BLO":  true,
+	"BMI":  true,
+	"BPL":  true,
+	"BVS":  true,
+	"BVC":  true,
+	"BHI":  true,
+	"BLS":  true,
+	"BGE":  true,
+	"BLT":  true,
+	"BGT":  true,
+	"BLE":  true,
+	"CBZ":  true,
+	"CBNZ": true,
+	"CALL": true,
+	"JMP":  true,
+}
+
+func jumpThumb(word string) bool {
+	return thumbJump[word]
+}
+
+// IsThumbCMP reports whether the op (as defined by an thumb.A* constant) is
+// one of the comparison instructions that require special handling.
+func IsThumbCMP(op obj.As) bool {
+	switch op {
+	case thumb.ACMN, thumb.ACMP, thumb.ATEQ, thumb.ATST, thumb.ATBB, thumb.ATBH:
+		return true
+	}
+	return false
+}
+
+// IsThumbSTREX reports whether the op (as defined by an thumb.A* constant) is
+// one of the STREX-like instructions that require special handling.
+func IsThumbSTREX(op obj.As) bool {
+	switch op {
+	case thumb.ASTREX, thumb.ASTREXB, thumb.ASTREXH:
+		return true
+	}
+	return false
+}
+
+// IsThumbBFX reports whether the op (as defined by an thumb.A* constant) is one the
+// BFX-like instructions which are in the form of "op $width, $LSB, (Reg,) Reg".
+func IsThumbBFX(op obj.As) bool {
+	switch op {
+	case thumb.ABFX, thumb.ABFXU, thumb.ABFC, thumb.ABFI:
+		return true
+	}
+	return false
+}
+
+// IsThumbFloatCmp reports whether the op is a floating comparison instruction.
+func IsThumbFloatCmp(op obj.As) bool {
+	switch op {
+	case thumb.ACMPF, thumb.ACMPD:
+		return true
+	}
+	return false
+}
+
+// IsThumbMULA reports whether the op (as defined by an thumb.A* constant) is
+// MULA, MULS, MMULA, MMULS, MULABB, MULAWB or MULAWT, the 4-operand instructions.
+func IsThumbMULA(op obj.As) bool {
+	switch op {
+	case thumb.AMULA, thumb.AMULS, thumb.AMMULA, thumb.AMMULS, thumb.AMULABB, thumb.AMULAWB, thumb.AMULAWT:
+		return true
+	}
+	return false
+}
+
+var thumbBcode = []obj.As{
+	thumb.ABEQ,
+	thumb.ABNE,
+	thumb.ABCS,
+	thumb.ABCC,
+	thumb.ABMI,
+	thumb.ABPL,
+	thumb.ABVS,
+	thumb.ABVC,
+	thumb.ABHI,
+	thumb.ABLS,
+	thumb.ABGE,
+	thumb.ABLT,
+	thumb.ABGT,
+	thumb.ABLE,
+	thumb.AB,
+	obj.ANOP,
+}
+
+// ThumbConditionCodes handles the special condition code situation for the Thumb.
+// It returns a boolean to indicate success; failure means cond was unrecognized.
+func ThumbConditionCodes(prog *obj.Prog, cond string) bool {
+	if cond == "" {
+		return true
+	}
+	bits, ok := ParseThumbCondition(cond)
+	if ok {
+		prog.Scond = bits
+	}
+	return ok
+}
+
+// ParseThumbCondition parses the conditions attached to an Thumb instruction.
+// The input is a single string consisting of period-separated condition
+// codes, such as ".P.W". An initial period is ignored.
+func ParseThumbCondition(cond string) (uint8, bool) {
+	return parseThumbCondition(cond, thumbLS, thumbSCOND)
+}
+
+func parseThumbCondition(cond string, ls, scond map[string]uint8) (uint8, bool) {
+	cond = strings.TrimPrefix(cond, ".")
+	if cond == "" {
+		return thumb.C_SCOND_NONE, true
+	}
+	names := strings.Split(cond, ".")
+	bits := uint8(0)
+	for _, name := range names {
+		if b, present := ls[name]; present {
+			bits |= b
+			continue
+		}
+		if b, present := scond[name]; present {
+			bits = (bits &^ thumb.C_SCOND) | b
+			continue
+		}
+		return 0, false
+	}
+	return bits, true
+}
+
+func thumbRegisterNumber(name string, n int16) (int16, bool) {
+	if n < 0 || 15 < n {
+		return 0, false
+	}
+	switch name {
+	case "R":
+		return thumb.REG_R0 + n, true
+	case "F":
+		return thumb.REG_F0 + n, true
+	}
+	return 0, false
+}
diff --git a/src/cmd/asm/internal/asm/asm.go b/src/cmd/asm/internal/asm/asm.go
index d83cfb2284..6adddd2c1d 100644
--- a/src/cmd/asm/internal/asm/asm.go
+++ b/src/cmd/asm/internal/asm/asm.go
@@ -34,6 +34,12 @@ func (p *Parser) append(prog *obj.Prog, cond string, doLabel bool) {
 				return
 			}
 
+		case sys.Thumb:
+			if !arch.ThumbConditionCodes(prog, cond) {
+				p.errorf("unrecognized condition code .%q", cond)
+				return
+			}
+
 		case sys.ARM64:
 			if !arch.ARM64Suffix(prog, cond) {
 				p.errorf("unrecognized suffix .%q", cond)
@@ -564,6 +570,18 @@ func (p *Parser) asmInstruction(op obj.As, cond string, a []obj.Addr) {
 				prog.Reg = p.getRegister(prog, op, &a[1])
 				break
 			}
+		} else if p.arch.Family == sys.Thumb {
+			if arch.IsThumbCMP(op) {
+				prog.From = a[0]
+				prog.Reg = p.getRegister(prog, op, &a[1])
+				break
+			}
+			// Strange special cases.
+			if arch.IsThumbFloatCmp(op) {
+				prog.From = a[0]
+				prog.Reg = p.getRegister(prog, op, &a[1])
+				break
+			}
 		} else if p.arch.Family == sys.ARM64 && arch.IsARM64CMP(op) {
 			prog.From = a[0]
 			prog.Reg = p.getRegister(prog, op, &a[1])
@@ -606,6 +624,29 @@ func (p *Parser) asmInstruction(op obj.As, cond string, a []obj.Addr) {
 			prog.From = a[0]
 			prog.Reg = p.getRegister(prog, op, &a[1])
 			prog.To = a[2]
+		case sys.Thumb:
+			// Special cases.
+			if arch.IsThumbSTREX(op) {
+				/*
+					STREX x, (y), z
+						from=(y) reg=x to=z
+				*/
+				prog.From = a[1]
+				prog.Reg = p.getRegister(prog, op, &a[0])
+				prog.To = a[2]
+				break
+			}
+			if arch.IsThumbBFX(op) {
+				// a[0] and a[1] must be constants, a[2] must be a register
+				prog.From = a[0]
+				prog.SetFrom3(a[1])
+				prog.To = a[2]
+				break
+			}
+			// Otherwise the 2nd operand (a[1]) must be a register.
+			prog.From = a[0]
+			prog.Reg = p.getRegister(prog, op, &a[1])
+			prog.To = a[2]
 		case sys.AMD64:
 			prog.From = a[0]
 			prog.SetFrom3(a[1])
@@ -700,6 +741,29 @@ func (p *Parser) asmInstruction(op obj.As, cond string, a []obj.Addr) {
 				break
 			}
 		}
+		if p.arch.Family == sys.Thumb {
+			if arch.IsThumbBFX(op) {
+				// a[0] and a[1] must be constants, a[2] and a[3] must be registers
+				prog.From = a[0]
+				prog.SetFrom3(a[1])
+				prog.Reg = p.getRegister(prog, op, &a[2])
+				prog.To = a[3]
+				break
+			}
+			if arch.IsThumbMULA(op) {
+				// All must be registers.
+				p.getRegister(prog, op, &a[0])
+				r1 := p.getRegister(prog, op, &a[1])
+				r2 := p.getRegister(prog, op, &a[2])
+				p.getRegister(prog, op, &a[3])
+				prog.From = a[0]
+				prog.To = a[3]
+				prog.To.Type = obj.TYPE_REGREG2
+				prog.To.Offset = int64(r2)
+				prog.Reg = r1
+				break
+			}
+		}
 		if p.arch.Family == sys.AMD64 {
 			prog.From = a[0]
 			prog.RestArgs = []obj.Addr{a[1], a[2]}
diff --git a/src/cmd/asm/internal/asm/parse.go b/src/cmd/asm/internal/asm/parse.go
index 17d40ee415..f431f49cde 100644
--- a/src/cmd/asm/internal/asm/parse.go
+++ b/src/cmd/asm/internal/asm/parse.go
@@ -181,7 +181,7 @@ next:
 		for {
 			tok = p.lex.Next()
 			if len(operands) == 0 && len(items) == 0 {
-				if p.arch.InFamily(sys.ARM, sys.ARM64, sys.AMD64, sys.I386) && tok == '.' {
+				if p.arch.InFamily(sys.ARM, sys.Thumb, sys.ARM64, sys.AMD64, sys.I386) && tok == '.' {
 					// Suffixes: ARM conditionals or x86 modifiers.
 					tok = p.lex.Next()
 					str := p.lex.Text()
@@ -505,7 +505,7 @@ func (p *Parser) atStartOfRegister(name string) bool {
 // We have consumed the register or R prefix.
 func (p *Parser) atRegisterShift() bool {
 	// ARM only.
-	if !p.arch.InFamily(sys.ARM, sys.ARM64) {
+	if !p.arch.InFamily(sys.ARM, sys.Thumb, sys.ARM64) {
 		return false
 	}
 	// R1<<...
@@ -577,7 +577,7 @@ func (p *Parser) register(name string, prefix rune) (r1, r2 int16, scale int8, o
 		// Check the architectures match the syntax.
 		switch p.next().ScanToken {
 		case ',':
-			if !p.arch.InFamily(sys.ARM, sys.ARM64) {
+			if !p.arch.InFamily(sys.ARM, sys.Thumb, sys.ARM64) {
 				p.errorf("(register,register) not supported on this architecture")
 				return
 			}
@@ -865,7 +865,7 @@ func (p *Parser) registerIndirect(a *obj.Addr, prefix rune) {
 	a.Reg = r1
 	if r2 != 0 {
 		// TODO: Consistency in the encoding would be nice here.
-		if p.arch.InFamily(sys.ARM, sys.ARM64) {
+		if p.arch.InFamily(sys.ARM, sys.Thumb, sys.ARM64) {
 			// Special form
 			// ARM: destination register pair (R1, R2).
 			// ARM64: register pair (R1, R2) for LDP/STP.
@@ -963,7 +963,7 @@ func (p *Parser) registerListARM(a *obj.Addr) {
 	var bits uint16
 	var arrangement int64
 	switch p.arch.Family {
-	case sys.ARM:
+	case sys.ARM, sys.Thumb:
 		maxReg = 16
 	case sys.ARM64:
 		maxReg = 32
@@ -1011,7 +1011,7 @@ ListLoop:
 			}
 			regCnt++
 			nextReg = (nextReg + 1) % 32
-		case sys.ARM:
+		case sys.ARM, sys.Thumb:
 			// Parse the upper and lower bounds.
 			lo := p.registerNumber(tok.String())
 			hi := lo
@@ -1039,7 +1039,7 @@ ListLoop:
 	}
 	a.Type = obj.TYPE_REGLIST
 	switch p.arch.Family {
-	case sys.ARM:
+	case sys.ARM, sys.Thumb:
 		a.Offset = int64(bits)
 	case sys.ARM64:
 		offset, err := arch.ARM64RegisterListOffset(firstReg, regCnt, arrangement)
@@ -1087,7 +1087,7 @@ func (p *Parser) registerListX86(a *obj.Addr) {
 
 // register number is ARM-specific. It returns the number of the specified register.
 func (p *Parser) registerNumber(name string) uint16 {
-	if p.arch.Family == sys.ARM && name == "g" {
+	if (p.arch.Family == sys.ARM || p.arch.Family == sys.Thumb) && name == "g" {
 		return 10
 	}
 	if name[0] != 'R' {
diff --git a/src/cmd/asm/internal/asm/testdata/thumb.s b/src/cmd/asm/internal/asm/testdata/thumb.s
new file mode 100644
index 0000000000..2947e04a57
--- /dev/null
+++ b/src/cmd/asm/internal/asm/testdata/thumb.s
@@ -0,0 +1,49 @@
+#include "../../../../../runtime/textflag.h"
+
+TEXT foo(SB), DUPOK|NOSPLIT, $0
+
+// Load/store
+	MOVW  0x28(R15), R7  // 4f0a
+	MOVW  R1, (R2)(R7)   // 51d1
+	MOVW  R8, (R9)(R7)   // f849 8007
+	MOVW  R1, (R7)       // 6039
+	MOVW  g, (R7)        // f8c7 a000
+	MOVW  (R2)(R7), R1   // 59d1
+	MOVW  (R9)(R7), R8   // f859 8007
+	MOVW  (R7), R1       // 6839
+	MOVW  (R7), R10      // f8d7 a000
+
+	MOVB   (R7), R3  // f997 3000
+	MOVH   (R7), R3  // f9b7 3000
+	MOVBU  (R7), R3  // 783b
+	MOVHU  (R7), R3  // 883b
+	MOVW   (R7), R3  // 683b
+	MOVB   R3, (R7)  // 703b
+	MOVH   R3, (R7)  // 803b
+	MOVW   R3, (R7)  // 603b
+
+// FP load/store
+	MOVF  0x10(R4), F0   // ed94 0a10
+	MOVF  0x20(R4), F1   // ed94 1a20
+	MOVF  0x30(R4), F2   // ed94 2a30
+	MOVD  -0x10(R4), F0  // ed14 0b10
+	MOVD  -0x20(R4), F1  // ed14 1b20
+	MOVD  -0x30(R4), F2  // ed14 2b30
+	MOVF  F0, -0x10(R4)  // ed04 0a10
+	MOVF  F1, -0x20(R4)  // ed04 1a20
+	MOVF  F2, -0x30(R4)  // ed04 2a30
+	MOVD  F0, 0x10(R4)   // ed84 0b10
+	MOVD  F1, 0x20(R4)   // ed84 1b20
+	MOVD  F2, 0x30(R4)   // ed84 2b30
+
+// FP data processing
+	SQRTF  F3, F4  // eeb1 4ac3
+	SQRTD  F5, F6  // eeb1 6bc5
+
+// System registers
+	MOVW  R1, IAPSR  // f381 8801
+	MOVW  APSR, R3   // f3ef 8300
+
+// Branch
+	B   (R14)  // 4770
+
diff --git a/src/cmd/cgo/gcc.go b/src/cmd/cgo/gcc.go
index 01b86adadb..c1b2d376f4 100644
--- a/src/cmd/cgo/gcc.go
+++ b/src/cmd/cgo/gcc.go
@@ -1548,6 +1548,8 @@ func (p *Package) gccMachine() []string {
 		return []string{"-mabi=64"}
 	case "mips", "mipsle":
 		return []string{"-mabi=32"}
+	case "thumb":
+		return []string{"-mthumb"}
 	}
 	return nil
 }
diff --git a/src/cmd/cgo/main.go b/src/cmd/cgo/main.go
index 5a7bb3f87b..c6e2d22346 100644
--- a/src/cmd/cgo/main.go
+++ b/src/cmd/cgo/main.go
@@ -182,6 +182,7 @@ var ptrSizeMap = map[string]int64{
 	"s390":     4,
 	"s390x":    8,
 	"sparc64":  8,
+	"thumb":    4,
 }
 
 var intSizeMap = map[string]int64{
@@ -199,6 +200,7 @@ var intSizeMap = map[string]int64{
 	"s390":     4,
 	"s390x":    8,
 	"sparc64":  8,
+	"thumb":    4,
 }
 
 var cPrefix string
diff --git a/src/cmd/compile/internal/gc/gsubr.go b/src/cmd/compile/internal/gc/gsubr.go
index 51c0fffc9e..f68b4314ab 100644
--- a/src/cmd/compile/internal/gc/gsubr.go
+++ b/src/cmd/compile/internal/gc/gsubr.go
@@ -271,6 +271,9 @@ func (f *Func) initLSym(hasBody bool) {
 	if f.ReflectMethod() {
 		flag |= obj.REFLECTMETHOD
 	}
+	if f.Pragma&Interrupthandler != 0 {
+		flag |= obj.ISR
+	}
 
 	// Clumsy but important.
 	// See test/recover.go for test cases and src/reflect/value.go
diff --git a/src/cmd/compile/internal/gc/lex.go b/src/cmd/compile/internal/gc/lex.go
index 557f98604d..4e102f0d89 100644
--- a/src/cmd/compile/internal/gc/lex.go
+++ b/src/cmd/compile/internal/gc/lex.go
@@ -44,6 +44,7 @@ const (
 	Nowritebarrier     // emit compiler error instead of write barrier
 	Nowritebarrierrec  // error on write barrier in this or recursive callees
 	Yeswritebarrierrec // cancels Nowritebarrierrec in this function and callees
+	Interrupthandler   // generate interrupt handler prologue / epilogue
 
 	// Runtime-only type pragmas
 	NotInHeap // values of this type must not be heap allocated
@@ -88,6 +89,8 @@ func pragmaValue(verb string) syntax.Pragma {
 		return UintptrEscapes
 	case "go:notinheap":
 		return NotInHeap
+	case "go:interrupthandler":
+		return Interrupthandler
 	}
 	return 0
 }
diff --git a/src/cmd/compile/internal/gc/main.go b/src/cmd/compile/internal/gc/main.go
index 2a1fd8e4fa..8fad96d34a 100644
--- a/src/cmd/compile/internal/gc/main.go
+++ b/src/cmd/compile/internal/gc/main.go
@@ -649,7 +649,7 @@ func Main(archInit func(*Arch)) {
 	// checking. This must happen before transformclosure.
 	// We'll do the final check after write barriers are
 	// inserted.
-	if compiling_runtime {
+	if compiling_runtime || Ctxt.Headtype == objabi.Hnoos {
 		nowritebarrierrecCheck = newNowritebarrierrecChecker()
 	}
 
diff --git a/src/cmd/compile/internal/gc/noder.go b/src/cmd/compile/internal/gc/noder.go
index 6aca89cad9..5d8b055240 100644
--- a/src/cmd/compile/internal/gc/noder.go
+++ b/src/cmd/compile/internal/gc/noder.go
@@ -1534,6 +1534,9 @@ func (p *noder) pragma(pos syntax.Pos, text string) syntax.Pragma {
 		if !compiling_runtime && prag&runtimePragmas != 0 {
 			p.error(syntax.Error{Pos: pos, Msg: fmt.Sprintf("//%s only allowed in runtime", verb)})
 		}
+		if prag == Interrupthandler {
+			prag |= Nosplit | Nowritebarrierrec | Nowritebarrier
+		}
 		if prag == 0 && !allowedStdPragmas[verb] && compiling_std {
 			p.error(syntax.Error{Pos: pos, Msg: fmt.Sprintf("//%s is not allowed in the standard library", verb)})
 		}
diff --git a/src/cmd/compile/internal/gc/range.go b/src/cmd/compile/internal/gc/range.go
index 4d354f23cf..b81fae779b 100644
--- a/src/cmd/compile/internal/gc/range.go
+++ b/src/cmd/compile/internal/gc/range.go
@@ -139,7 +139,7 @@ func cheapComputableIndex(width int64) bool {
 	// but the architecture supports it.
 	case sys.PPC64, sys.S390X:
 		return width == 1
-	case sys.AMD64, sys.I386, sys.ARM64, sys.ARM:
+	case sys.AMD64, sys.I386, sys.ARM64, sys.ARM, sys.Thumb:
 		switch width {
 		case 1, 2, 4, 8:
 			return true
diff --git a/src/cmd/compile/internal/gc/ssa.go b/src/cmd/compile/internal/gc/ssa.go
index 5509e3d182..d1c5099a81 100644
--- a/src/cmd/compile/internal/gc/ssa.go
+++ b/src/cmd/compile/internal/gc/ssa.go
@@ -3016,6 +3016,7 @@ func init() {
 			},
 			all...)
 	}
+
 	addF("runtime/internal/math", "MulUintptr",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			if s.config.PtrSize == 4 {
@@ -3054,22 +3055,22 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz32, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64)
 	addF("runtime/internal/sys", "Ctz64",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz64, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64)
 	addF("runtime/internal/sys", "Bswap32",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBswap32, types.Types[TUINT32], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.Thumb, sys.S390X)
 	addF("runtime/internal/sys", "Bswap64",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBswap64, types.Types[TUINT64], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.Thumb, sys.S390X)
 
 	/******** runtime/internal/atomic ********/
 	addF("runtime/internal/atomic", "Load",
@@ -3267,7 +3268,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpSqrt, types.Types[TFLOAT64], args[0])
 		},
-		sys.I386, sys.AMD64, sys.ARM, sys.ARM64, sys.MIPS, sys.MIPS64, sys.PPC64, sys.S390X, sys.Wasm)
+		sys.I386, sys.AMD64, sys.ARM, sys.Thumb, sys.ARM64, sys.MIPS, sys.MIPS64, sys.PPC64, sys.S390X, sys.Wasm)
 	addF("math", "Trunc",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpTrunc, types.Types[TFLOAT64], args[0])
@@ -3352,12 +3353,12 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz64, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
 	addF("math/bits", "TrailingZeros32",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz32, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
 	addF("math/bits", "TrailingZeros16",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			x := s.newValue1(ssa.OpZeroExt16to32, types.Types[TUINT32], args[0])
@@ -3370,7 +3371,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz16, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM, sys.ARM64, sys.Wasm)
+		sys.AMD64, sys.ARM, sys.Thumb, sys.ARM64, sys.Wasm)
 	addF("math/bits", "TrailingZeros16",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			x := s.newValue1(ssa.OpZeroExt16to64, types.Types[TUINT64], args[0])
@@ -3391,7 +3392,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpCtz8, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM, sys.ARM64, sys.Wasm)
+		sys.AMD64, sys.ARM, sys.Thumb, sys.ARM64, sys.Wasm)
 	addF("math/bits", "TrailingZeros8",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			x := s.newValue1(ssa.OpZeroExt8to64, types.Types[TUINT64], args[0])
@@ -3408,7 +3409,7 @@ func init() {
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
 	addF("math/bits", "Len32",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBitLen32, types.Types[TINT], args[0])
@@ -3422,7 +3423,7 @@ func init() {
 			x := s.newValue1(ssa.OpZeroExt32to64, types.Types[TUINT64], args[0])
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], x)
 		},
-		sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
 	addF("math/bits", "Len16",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			if s.config.PtrSize == 4 {
@@ -3432,7 +3433,7 @@ func init() {
 			x := s.newValue1(ssa.OpZeroExt16to64, types.Types[TUINT64], args[0])
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], x)
 		},
-		sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.ARM64, sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
 	addF("math/bits", "Len16",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBitLen16, types.Types[TINT], args[0])
@@ -3447,7 +3448,7 @@ func init() {
 			x := s.newValue1(ssa.OpZeroExt8to64, types.Types[TUINT64], args[0])
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], x)
 		},
-		sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.ARM64, sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
 	addF("math/bits", "Len8",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
 			return s.newValue1(ssa.OpBitLen8, types.Types[TINT], args[0])
@@ -3460,7 +3461,7 @@ func init() {
 			}
 			return s.newValue1(ssa.OpBitLen64, types.Types[TINT], args[0])
 		},
-		sys.AMD64, sys.ARM64, sys.ARM, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
+		sys.AMD64, sys.ARM64, sys.ARM, sys.Thumb, sys.S390X, sys.MIPS, sys.PPC64, sys.Wasm)
 	// LeadingZeros is handled because it trivially calls Len.
 	addF("math/bits", "Reverse64",
 		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
@@ -3659,6 +3660,61 @@ func init() {
 			return s.newValue3(ssa.OpDiv128u, types.NewTuple(types.Types[TUINT64], types.Types[TUINT64]), args[0], args[1], args[2])
 		},
 		sys.ArchAMD64)
+
+	/******** embedded/mmio ********/
+	add("embedded/mmio", "load32",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			v := s.newValue2(ssa.OpMMIOLoad32, types.NewTuple(types.Types[TUINT32], types.TypeMem), args[0], s.mem())
+			s.vars[&memVar] = s.newValue1(ssa.OpSelect1, types.TypeMem, v)
+			return s.newValue1(ssa.OpSelect0, types.Types[TUINT32], v)
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "load16",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			v := s.newValue2(ssa.OpMMIOLoad16, types.NewTuple(types.Types[TUINT16], types.TypeMem), args[0], s.mem())
+			s.vars[&memVar] = s.newValue1(ssa.OpSelect1, types.TypeMem, v)
+			return s.newValue1(ssa.OpSelect0, types.Types[TUINT16], v)
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "load8",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			v := s.newValue2(ssa.OpMMIOLoad8, types.NewTuple(types.Types[TUINT8], types.TypeMem), args[0], s.mem())
+			s.vars[&memVar] = s.newValue1(ssa.OpSelect1, types.TypeMem, v)
+			return s.newValue1(ssa.OpSelect0, types.Types[TUINT8], v)
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "store32",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue3(ssa.OpMMIOStore32, types.TypeMem, args[0], args[1], s.mem())
+			return nil
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "store16",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue3(ssa.OpMMIOStore16, types.TypeMem, args[0], args[1], s.mem())
+			return nil
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "store8",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue3(ssa.OpMMIOStore8, types.TypeMem, args[0], args[1], s.mem())
+			return nil
+		},
+		sys.ArchThumb)
+	add("embedded/mmio", "MB",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue1(ssa.OpMMIOMB, types.TypeMem, s.mem())
+			return nil
+		},
+		sys.ArchThumb)
+
+	/******** embedded/rtos ********/
+	add("embedded/rtos", "publicationBarrier",
+		func(s *state, n *Node, args []*ssa.Value) *ssa.Value {
+			s.vars[&memVar] = s.newValue1(ssa.OpPublicationBarrier, types.TypeMem, s.mem())
+			return nil
+		},
+		sys.ArchThumb)
 }
 
 // findIntrinsic returns a function which builds the SSA equivalent of the
@@ -5873,7 +5929,7 @@ func (s *SSAGenState) Call(v *ssa.Value) *obj.Prog {
 		switch thearch.LinkArch.Family {
 		case sys.AMD64, sys.I386, sys.PPC64, sys.S390X, sys.Wasm:
 			p.To.Type = obj.TYPE_REG
-		case sys.ARM, sys.ARM64, sys.MIPS, sys.MIPS64:
+		case sys.ARM, sys.Thumb, sys.ARM64, sys.MIPS, sys.MIPS64:
 			p.To.Type = obj.TYPE_MEM
 		default:
 			Fatalf("unknown indirect call family")
diff --git a/src/cmd/compile/internal/gc/walk.go b/src/cmd/compile/internal/gc/walk.go
index a8cc313b76..c81f03fb9c 100644
--- a/src/cmd/compile/internal/gc/walk.go
+++ b/src/cmd/compile/internal/gc/walk.go
@@ -1516,7 +1516,7 @@ func rtconvfn(src, dst *types.Type) (param, result types.EType) {
 	}
 
 	switch thearch.LinkArch.Family {
-	case sys.ARM, sys.MIPS:
+	case sys.ARM, sys.Thumb, sys.MIPS:
 		if src.IsFloat() {
 			switch dst.Etype {
 			case TINT64, TUINT64:
diff --git a/src/cmd/compile/internal/ssa/config.go b/src/cmd/compile/internal/ssa/config.go
index e46d937e42..1c07f4bf15 100644
--- a/src/cmd/compile/internal/ssa/config.go
+++ b/src/cmd/compile/internal/ssa/config.go
@@ -245,6 +245,17 @@ func NewConfig(arch string, types Types, ctxt *obj.Link, optimize bool) *Config
 		c.FPReg = framepointerRegARM
 		c.LinkReg = linkRegARM
 		c.hasGReg = true
+	case "thumb":
+		c.PtrSize = 4
+		c.RegSize = 4
+		c.lowerBlock = rewriteBlockThumb
+		c.lowerValue = rewriteValueThumb
+		c.registers = registersThumb[:]
+		c.gpRegMask = gpRegMaskThumb
+		c.fpRegMask = fpRegMaskThumb
+		c.FPReg = framepointerRegThumb
+		c.LinkReg = linkRegThumb
+		c.hasGReg = true
 	case "arm64":
 		c.PtrSize = 8
 		c.RegSize = 8
diff --git a/src/cmd/compile/internal/ssa/gen/Thumb.rules b/src/cmd/compile/internal/ssa/gen/Thumb.rules
new file mode 100644
index 0000000000..949602377c
--- /dev/null
+++ b/src/cmd/compile/internal/ssa/gen/Thumb.rules
@@ -0,0 +1,1346 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+(Add(Ptr|32|16|8) x y) -> (ADD x y)
+(Add(32|64)F x y) -> (ADD(F|D) x y)
+(Add32carry x y) -> (ADDS x y)
+(Add32withcarry x y c) -> (ADC x y c)
+
+(Sub(Ptr|32|16|8) x y) -> (SUB x y)
+(Sub(32|64)F x y) -> (SUB(F|D) x y)
+(Sub32carry x y) -> (SUBS x y)
+(Sub32withcarry x y c) -> (SBC x y c)
+
+(Mul(32|16|8) x y) -> (MUL x y)
+(Mul(32|64)F x y) -> (MUL(F|D) x y)
+(Hmul(32|32u) x y) -> (HMU(L|LU) x y)
+(Mul32uhilo x y) -> (MULLU x y)
+
+(Div32 x y) -> (DIV x y)
+(Div32u x y) -> (DIVU x y)
+(Div16 x y) -> (DIV (SignExt16to32 x) (SignExt16to32 y))
+(Div16u x y) -> (DIVU (ZeroExt16to32 x) (ZeroExt16to32 y))
+(Div8 x y) -> (DIV (SignExt8to32 x) (SignExt8to32 y))
+(Div8u x y) -> (DIVU (ZeroExt8to32 x) (ZeroExt8to32 y))
+(Div(32|64)F x y) -> (DIV(F|D) x y)
+
+(Mod32 x y) -> (SUB x (MUL <y.Type> y (DIV <x.Type> x y)))
+(Mod32u x y) -> (SUB x (MUL <y.Type> y (DIVU <x.Type> x y)))
+(Mod16 x y) -> (Mod32 (SignExt16to32 x) (SignExt16to32 y))
+(Mod16u x y) -> (Mod32u (ZeroExt16to32 x) (ZeroExt16to32 y))
+(Mod8 x y) -> (Mod32 (SignExt8to32 x) (SignExt8to32 y))
+(Mod8u x y) -> (Mod32u (ZeroExt8to32 x) (ZeroExt8to32 y))
+
+// (x + y) / 2 with x>=y -> (x - y) / 2 + y
+(Avg32u <t> x y) -> (ADD (SRLconst <t> (SUB <t> x y) [1]) y)
+
+(And(32|16|8) x y) -> (AND x y)
+(Or(32|16|8) x y) -> (OR x y)
+(Xor(32|16|8) x y) -> (XOR x y)
+
+// unary ops
+(Neg(32|16|8) x) -> (RSBconst [0] x)
+(Neg(32|64)F x) -> (NEG(F|D) x)
+
+(Com(32|16|8) x) -> (MVN x)
+
+(Sqrt x) -> (SQRTD x)
+
+(Ctz32NonZero x) -> (Ctz32 x)
+(Ctz16NonZero x) -> (Ctz32 x)
+(Ctz8NonZero x) -> (Ctz32 x)
+
+(Ctz32 <t> x) -> (CLZ <t> (RBIT <t> x))
+(Ctz16 <t> x) -> (CLZ <t> (RBIT <typ.UInt32> (ORconst <typ.UInt32> [0x10000] x)))
+(Ctz8 <t> x) -> (CLZ <t> (RBIT <typ.UInt32> (ORconst <typ.UInt32> [0x100] x)))
+
+// bit length
+(BitLen32 <t> x) -> (RSBconst [32] (CLZ <t> x))
+
+(Bswap32 x) -> (REV x)
+
+// boolean ops -- booleans are represented with 0=false, 1=true
+(AndB x y) -> (AND x y)
+(OrB x y) -> (OR x y)
+(EqB x y) -> (XORconst [1] (XOR <typ.Bool> x y))
+(NeqB x y) -> (XOR x y)
+(Not x) -> (XORconst [1] x)
+
+// shifts
+// hardware instruction uses only the low byte of the shift
+// we compare to 256 to ensure Go semantics for large shifts
+(Lsh32x32 x y) -> (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+(Lsh32x16 x y) -> (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Lsh32x8  x y) -> (SLL x (ZeroExt8to32 y))
+
+(Lsh16x32 x y) -> (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+(Lsh16x16 x y) -> (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Lsh16x8  x y) -> (SLL x (ZeroExt8to32 y))
+
+(Lsh8x32 x y) -> (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+(Lsh8x16 x y) -> (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Lsh8x8  x y) -> (SLL x (ZeroExt8to32 y))
+
+(Rsh32Ux32 x y) -> (CMOVWHSconst (SRL <x.Type> x y) (CMPconst [256] y) [0])
+(Rsh32Ux16 x y) -> (CMOVWHSconst (SRL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Rsh32Ux8  x y) -> (SRL x (ZeroExt8to32 y))
+
+(Rsh16Ux32 x y) -> (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) y) (CMPconst [256] y) [0])
+(Rsh16Ux16 x y) -> (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Rsh16Ux8  x y) -> (SRL (ZeroExt16to32 x) (ZeroExt8to32 y))
+
+(Rsh8Ux32 x y) -> (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) y) (CMPconst [256] y) [0])
+(Rsh8Ux16 x y) -> (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+(Rsh8Ux8  x y) -> (SRL (ZeroExt8to32 x) (ZeroExt8to32 y))
+
+(Rsh32x32 x y) -> (SRAcond x y (CMPconst [256] y))
+(Rsh32x16 x y) -> (SRAcond x (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+(Rsh32x8  x y) -> (SRA x (ZeroExt8to32 y))
+
+(Rsh16x32 x y) -> (SRAcond (SignExt16to32 x) y (CMPconst [256] y))
+(Rsh16x16 x y) -> (SRAcond (SignExt16to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+(Rsh16x8  x y) -> (SRA (SignExt16to32 x) (ZeroExt8to32 y))
+
+(Rsh8x32 x y) -> (SRAcond (SignExt8to32 x) y (CMPconst [256] y))
+(Rsh8x16 x y) -> (SRAcond (SignExt8to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+(Rsh8x8  x y) -> (SRA (SignExt8to32 x) (ZeroExt8to32 y))
+
+// constant shifts
+// generic opt rewrites all constant shifts to shift by Const64
+(Lsh32x64 x (Const64 [c])) && uint64(c) < 32 -> (SLLconst x [c])
+(Rsh32x64 x (Const64 [c])) && uint64(c) < 32 -> (SRAconst x [c])
+(Rsh32Ux64 x (Const64 [c])) && uint64(c) < 32 -> (SRLconst x [c])
+(Lsh16x64 x (Const64 [c])) && uint64(c) < 16 -> (SLLconst x [c])
+(Rsh16x64 x (Const64 [c])) && uint64(c) < 16 -> (SRAconst (SLLconst <typ.UInt32> x [16]) [c+16])
+(Rsh16Ux64 x (Const64 [c])) && uint64(c) < 16 -> (SRLconst (SLLconst <typ.UInt32> x [16]) [c+16])
+(Lsh8x64 x (Const64 [c])) && uint64(c) < 8 -> (SLLconst x [c])
+(Rsh8x64 x (Const64 [c])) && uint64(c) < 8 -> (SRAconst (SLLconst <typ.UInt32> x [24]) [c+24])
+(Rsh8Ux64 x (Const64 [c])) && uint64(c) < 8 -> (SRLconst (SLLconst <typ.UInt32> x [24]) [c+24])
+
+// large constant shifts
+(Lsh32x64 _ (Const64 [c])) && uint64(c) >= 32 -> (Const32 [0])
+(Rsh32Ux64 _ (Const64 [c])) && uint64(c) >= 32 -> (Const32 [0])
+(Lsh16x64 _ (Const64 [c])) && uint64(c) >= 16 -> (Const16 [0])
+(Rsh16Ux64 _ (Const64 [c])) && uint64(c) >= 16 -> (Const16 [0])
+(Lsh8x64 _ (Const64 [c])) && uint64(c) >= 8 -> (Const8 [0])
+(Rsh8Ux64 _ (Const64 [c])) && uint64(c) >= 8 -> (Const8 [0])
+
+// large constant signed right shift, we leave the sign bit
+(Rsh32x64 x (Const64 [c])) && uint64(c) >= 32 -> (SRAconst x [31])
+(Rsh16x64 x (Const64 [c])) && uint64(c) >= 16 -> (SRAconst (SLLconst <typ.UInt32> x [16]) [31])
+(Rsh8x64 x (Const64 [c])) && uint64(c) >= 8 -> (SRAconst (SLLconst <typ.UInt32> x [24]) [31])
+
+// constants
+(Const8 [val]) -> (MOVWconst [val])
+(Const16 [val]) -> (MOVWconst [val])
+(Const32 [val]) -> (MOVWconst [val])
+(Const32F [val]) -> (MOVFconst [val])
+(Const64F [val]) -> (MOVDconst [val])
+(ConstNil) -> (MOVWconst [0])
+(ConstBool [b]) -> (MOVWconst [b])
+
+// truncations
+// Because we ignore high parts of registers, truncates are just copies.
+(Trunc16to8 x) -> x
+(Trunc32to8 x) -> x
+(Trunc32to16 x) -> x
+
+// Zero-/Sign-extensions
+(ZeroExt8to16 x) -> (MOVBUreg x)
+(ZeroExt8to32 x) -> (MOVBUreg x)
+(ZeroExt16to32 x) -> (MOVHUreg x)
+
+(SignExt8to16 x) -> (MOVBreg x)
+(SignExt8to32 x) -> (MOVBreg x)
+(SignExt16to32 x) -> (MOVHreg x)
+
+(Signmask x) -> (SRAconst x [31])
+(Zeromask x) -> (SRAconst (RSBshiftRL <typ.Int32> x x [1]) [31]) // sign bit of uint32(x)>>1 - x
+(Slicemask <t> x) -> (SRAconst (RSBconst <t> [0] x) [31])
+
+// float <-> int conversion
+(Cvt32to32F x) -> (MOVWF x)
+(Cvt32to64F x) -> (MOVWD x)
+(Cvt32Uto32F x) -> (MOVWUF x)
+(Cvt32Uto64F x) -> (MOVWUD x)
+(Cvt32Fto32 x) -> (MOVFW x)
+(Cvt64Fto32 x) -> (MOVDW x)
+(Cvt32Fto32U x) -> (MOVFWU x)
+(Cvt64Fto32U x) -> (MOVDWU x)
+(Cvt32Fto64F x) -> (MOVFD x)
+(Cvt64Fto32F x) -> (MOVDF x)
+
+(Round(32|64)F x) -> x
+
+// comparisons
+(Eq8 x y)  -> (Equal (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Eq16 x y) -> (Equal (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Eq32 x y) -> (Equal (CMP x y))
+(EqPtr x y) -> (Equal (CMP x y))
+(Eq(32|64)F x y) -> (Equal (CMP(F|D) x y))
+
+(Neq8 x y)  -> (NotEqual (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Neq16 x y) -> (NotEqual (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Neq32 x y) -> (NotEqual (CMP x y))
+(NeqPtr x y) -> (NotEqual (CMP x y))
+(Neq(32|64)F x y) -> (NotEqual (CMP(F|D) x y))
+
+(Less8 x y)  -> (LessThan (CMP (SignExt8to32 x) (SignExt8to32 y)))
+(Less16 x y) -> (LessThan (CMP (SignExt16to32 x) (SignExt16to32 y)))
+(Less32 x y) -> (LessThan (CMP x y))
+(Less(32|64)F x y) -> (GreaterThan (CMP(F|D) y x)) // reverse operands to work around NaN
+
+(Less8U x y)  -> (LessThanU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Less16U x y) -> (LessThanU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Less32U x y) -> (LessThanU (CMP x y))
+
+(Leq8 x y)  -> (LessEqual (CMP (SignExt8to32 x) (SignExt8to32 y)))
+(Leq16 x y) -> (LessEqual (CMP (SignExt16to32 x) (SignExt16to32 y)))
+(Leq32 x y) -> (LessEqual (CMP x y))
+(Leq(32|64)F x y) -> (GreaterEqual (CMP(F|D) y x)) // reverse operands to work around NaN
+
+(Leq8U x y)  -> (LessEqualU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Leq16U x y) -> (LessEqualU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Leq32U x y) -> (LessEqualU (CMP x y))
+
+(Greater8 x y)  -> (GreaterThan (CMP (SignExt8to32 x) (SignExt8to32 y)))
+(Greater16 x y) -> (GreaterThan (CMP (SignExt16to32 x) (SignExt16to32 y)))
+(Greater32 x y) -> (GreaterThan (CMP x y))
+(Greater(32|64)F x y) -> (GreaterThan (CMP(F|D) x y))
+
+(Greater8U x y)  -> (GreaterThanU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Greater16U x y) -> (GreaterThanU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Greater32U x y) -> (GreaterThanU (CMP x y))
+
+(Geq8 x y)  -> (GreaterEqual (CMP (SignExt8to32 x) (SignExt8to32 y)))
+(Geq16 x y) -> (GreaterEqual (CMP (SignExt16to32 x) (SignExt16to32 y)))
+(Geq32 x y) -> (GreaterEqual (CMP x y))
+(Geq(32|64)F x y) -> (GreaterEqual (CMP(F|D) x y))
+
+(Geq8U x y)  -> (GreaterEqualU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+(Geq16U x y) -> (GreaterEqualU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+(Geq32U x y) -> (GreaterEqualU (CMP x y))
+
+(OffPtr [off] ptr:(SP)) -> (MOVWaddr [off] ptr)
+(OffPtr [off] ptr) -> (ADDconst [off] ptr)
+
+(Addr {sym} base) -> (MOVWaddr {sym} base)
+(LocalAddr {sym} base _) -> (MOVWaddr {sym} base)
+
+// loads
+(Load <t> ptr mem) && t.IsBoolean() -> (MOVBUload ptr mem)
+(Load <t> ptr mem) && (is8BitInt(t) && isSigned(t)) -> (MOVBload ptr mem)
+(Load <t> ptr mem) && (is8BitInt(t) && !isSigned(t)) -> (MOVBUload ptr mem)
+(Load <t> ptr mem) && (is16BitInt(t) && isSigned(t)) -> (MOVHload ptr mem)
+(Load <t> ptr mem) && (is16BitInt(t) && !isSigned(t)) -> (MOVHUload ptr mem)
+(Load <t> ptr mem) && (is32BitInt(t) || isPtr(t)) -> (MOVWload ptr mem)
+(Load <t> ptr mem) && is32BitFloat(t) -> (MOVFload ptr mem)
+(Load <t> ptr mem) && is64BitFloat(t) -> (MOVDload ptr mem)
+
+// stores
+(Store {t} ptr val mem) && t.(*types.Type).Size() == 1 -> (MOVBstore ptr val mem)
+(Store {t} ptr val mem) && t.(*types.Type).Size() == 2 -> (MOVHstore ptr val mem)
+(Store {t} ptr val mem) && t.(*types.Type).Size() == 4 && !is32BitFloat(val.Type) -> (MOVWstore ptr val mem)
+(Store {t} ptr val mem) && t.(*types.Type).Size() == 4 && is32BitFloat(val.Type) -> (MOVFstore ptr val mem)
+(Store {t} ptr val mem) && t.(*types.Type).Size() == 8 && is64BitFloat(val.Type) -> (MOVDstore ptr val mem)
+
+// mmio intrinsics
+(MMIOLoad8  ptr mem) -> (LoadOnce8 ptr mem)
+(MMIOLoad16 ptr mem) -> (LoadOnce16 ptr mem)
+(MMIOLoad32 ptr mem) -> (LoadOnce32 ptr mem)
+(MMIOStore8  ptr val mem) -> (StoreOnce8 ptr val mem)
+(MMIOStore16 ptr val mem) -> (StoreOnce16 ptr val mem)
+(MMIOStore32 ptr val mem) -> (StoreOnce32 ptr val mem)
+(MMIOMB mem) -> (DSB mem) // use DSB instead of DMB because of memory mapped CPU controll registers and to ensure memory update before the sleep (before WFI, WFE)
+
+// publication barrier
+(PublicationBarrier mem) -> (DMB_ST mem)
+
+// zero instructions
+(Zero [0] _ mem) -> mem
+(Zero [1] ptr mem) -> (MOVBstore ptr (MOVWconst [0]) mem)
+(Zero [2] {t} ptr mem) && t.(*types.Type).Alignment()%2 == 0 ->
+	(MOVHstore ptr (MOVWconst [0]) mem)
+(Zero [2] ptr mem) ->
+	(MOVBstore [1] ptr (MOVWconst [0])
+		(MOVBstore [0] ptr (MOVWconst [0]) mem))
+(Zero [4] {t} ptr mem) && t.(*types.Type).Alignment()%4 == 0 ->
+	(MOVWstore ptr (MOVWconst [0]) mem)
+(Zero [4] {t} ptr mem) && t.(*types.Type).Alignment()%2 == 0 ->
+	(MOVHstore [2] ptr (MOVWconst [0])
+		(MOVHstore [0] ptr (MOVWconst [0]) mem))
+(Zero [4] ptr mem) ->
+	(MOVBstore [3] ptr (MOVWconst [0])
+		(MOVBstore [2] ptr (MOVWconst [0])
+			(MOVBstore [1] ptr (MOVWconst [0])
+				(MOVBstore [0] ptr (MOVWconst [0]) mem))))
+
+(Zero [3] ptr mem) ->
+	(MOVBstore [2] ptr (MOVWconst [0])
+		(MOVBstore [1] ptr (MOVWconst [0])
+			(MOVBstore [0] ptr (MOVWconst [0]) mem)))
+
+// Medium zeroing uses a duff device
+// 4 and 128 are magic constants, see runtime/mkduff.go
+(Zero [s] {t} ptr mem)
+	&& s%4 == 0 && s > 4 && s <= 512
+	&& t.(*types.Type).Alignment()%4 == 0 && !config.noDuffDevice ->
+	(DUFFZERO [4 * (128 - s/4)] ptr (MOVWconst [0]) mem)
+
+// Large zeroing uses a loop
+(Zero [s] {t} ptr mem)
+	&& (s > 512 || config.noDuffDevice) || t.(*types.Type).Alignment()%4 != 0 ->
+	(LoweredZero [t.(*types.Type).Alignment()]
+		ptr
+		(ADDconst <ptr.Type> ptr [s-moveSize(t.(*types.Type).Alignment(), config)])
+		(MOVWconst [0])
+		mem)
+
+// moves
+(Move [0] _ _ mem) -> mem
+(Move [1] dst src mem) -> (MOVBstore dst (MOVBUload src mem) mem)
+(Move [2] {t} dst src mem) && t.(*types.Type).Alignment()%2 == 0 ->
+	(MOVHstore dst (MOVHUload src mem) mem)
+(Move [2] dst src mem) ->
+	(MOVBstore [1] dst (MOVBUload [1] src mem)
+		(MOVBstore dst (MOVBUload src mem) mem))
+(Move [4] {t} dst src mem) && t.(*types.Type).Alignment()%4 == 0 ->
+	(MOVWstore dst (MOVWload src mem) mem)
+(Move [4] {t} dst src mem) && t.(*types.Type).Alignment()%2 == 0 ->
+	(MOVHstore [2] dst (MOVHUload [2] src mem)
+		(MOVHstore dst (MOVHUload src mem) mem))
+(Move [4] dst src mem) ->
+	(MOVBstore [3] dst (MOVBUload [3] src mem)
+		(MOVBstore [2] dst (MOVBUload [2] src mem)
+			(MOVBstore [1] dst (MOVBUload [1] src mem)
+				(MOVBstore dst (MOVBUload src mem) mem))))
+
+(Move [3] dst src mem) ->
+	(MOVBstore [2] dst (MOVBUload [2] src mem)
+		(MOVBstore [1] dst (MOVBUload [1] src mem)
+			(MOVBstore dst (MOVBUload src mem) mem)))
+
+// Medium move uses a duff device
+// 8 and 128 are magic constants, see runtime/mkduff.go
+(Move [s] {t} dst src mem)
+	&& s%4 == 0 && s > 4 && s <= 512
+	&& t.(*types.Type).Alignment()%4 == 0 && !config.noDuffDevice ->
+	(DUFFCOPY [8 * (128 - s/4)] dst src mem)
+
+// Large move uses a loop
+(Move [s] {t} dst src mem)
+	&& (s > 512 || config.noDuffDevice) || t.(*types.Type).Alignment()%4 != 0 ->
+	(LoweredMove [t.(*types.Type).Alignment()]
+		dst
+		src
+		(ADDconst <src.Type> src [s-moveSize(t.(*types.Type).Alignment(), config)])
+		mem)
+
+// calls
+(StaticCall [argwid] {target} mem) -> (CALLstatic [argwid] {target} mem)
+(ClosureCall [argwid] entry closure mem) -> (CALLclosure [argwid] entry closure mem)
+(InterCall [argwid] entry mem) -> (CALLinter [argwid] entry mem)
+
+// checks
+(NilCheck ptr mem) -> (LoweredNilCheck ptr mem)
+(IsNonNil ptr) -> (NotEqual (CMPconst [0] ptr))
+(IsInBounds idx len) -> (LessThanU (CMP idx len))
+(IsSliceInBounds idx len) -> (LessEqualU (CMP idx len))
+
+// pseudo-ops
+(GetClosurePtr) -> (LoweredGetClosurePtr)
+(GetCallerSP) -> (LoweredGetCallerSP)
+(GetCallerPC) -> (LoweredGetCallerPC)
+
+// Absorb pseudo-ops into blocks.
+(If (Equal cc) yes no) -> (EQ cc yes no)
+(If (NotEqual cc) yes no) -> (NE cc yes no)
+(If (LessThan cc) yes no) -> (LT cc yes no)
+(If (LessThanU cc) yes no) -> (ULT cc yes no)
+(If (LessEqual cc) yes no) -> (LE cc yes no)
+(If (LessEqualU cc) yes no) -> (ULE cc yes no)
+(If (GreaterThan cc) yes no) -> (GT cc yes no)
+(If (GreaterThanU cc) yes no) -> (UGT cc yes no)
+(If (GreaterEqual cc) yes no) -> (GE cc yes no)
+(If (GreaterEqualU cc) yes no) -> (UGE cc yes no)
+
+(If cond yes no) -> (NE (CMPconst [0] cond) yes no)
+
+// Absorb boolean tests into block
+(NE (CMPconst [0] (Equal cc)) yes no) -> (EQ cc yes no)
+(NE (CMPconst [0] (NotEqual cc)) yes no) -> (NE cc yes no)
+(NE (CMPconst [0] (LessThan cc)) yes no) -> (LT cc yes no)
+(NE (CMPconst [0] (LessThanU cc)) yes no) -> (ULT cc yes no)
+(NE (CMPconst [0] (LessEqual cc)) yes no) -> (LE cc yes no)
+(NE (CMPconst [0] (LessEqualU cc)) yes no) -> (ULE cc yes no)
+(NE (CMPconst [0] (GreaterThan cc)) yes no) -> (GT cc yes no)
+(NE (CMPconst [0] (GreaterThanU cc)) yes no) -> (UGT cc yes no)
+(NE (CMPconst [0] (GreaterEqual cc)) yes no) -> (GE cc yes no)
+(NE (CMPconst [0] (GreaterEqualU cc)) yes no) -> (UGE cc yes no)
+
+// Write barrier.
+(WB {fn} destptr srcptr mem) -> (LoweredWB {fn} destptr srcptr mem)
+
+(PanicBounds [kind] x y mem) && boundsABI(kind) == 0 -> (LoweredPanicBoundsA [kind] x y mem)
+(PanicBounds [kind] x y mem) && boundsABI(kind) == 1 -> (LoweredPanicBoundsB [kind] x y mem)
+(PanicBounds [kind] x y mem) && boundsABI(kind) == 2 -> (LoweredPanicBoundsC [kind] x y mem)
+
+(PanicExtend [kind] hi lo y mem) && boundsABI(kind) == 0 -> (LoweredPanicExtendA [kind] hi lo y mem)
+(PanicExtend [kind] hi lo y mem) && boundsABI(kind) == 1 -> (LoweredPanicExtendB [kind] hi lo y mem)
+(PanicExtend [kind] hi lo y mem) && boundsABI(kind) == 2 -> (LoweredPanicExtendC [kind] hi lo y mem)
+
+// Optimizations
+
+// fold offset into address
+(ADDconst [off1] (MOVWaddr [off2] {sym} ptr)) -> (MOVWaddr [off1+off2] {sym} ptr)
+(SUBconst [off1] (MOVWaddr [off2] {sym} ptr)) -> (MOVWaddr [off2-off1] {sym} ptr)
+
+// fold address into load/store
+(MOVBload [off1] {sym} (ADDconst [off2] ptr) mem) -> (MOVBload [off1+off2] {sym} ptr mem)
+(MOVBload [off1] {sym} (SUBconst [off2] ptr) mem) -> (MOVBload [off1-off2] {sym} ptr mem)
+(MOVBUload [off1] {sym} (ADDconst [off2] ptr) mem) -> (MOVBUload [off1+off2] {sym} ptr mem)
+(MOVBUload [off1] {sym} (SUBconst [off2] ptr) mem) -> (MOVBUload [off1-off2] {sym} ptr mem)
+(MOVHload [off1] {sym} (ADDconst [off2] ptr) mem) -> (MOVHload [off1+off2] {sym} ptr mem)
+(MOVHload [off1] {sym} (SUBconst [off2] ptr) mem) -> (MOVHload [off1-off2] {sym} ptr mem)
+(MOVHUload [off1] {sym} (ADDconst [off2] ptr) mem) -> (MOVHUload [off1+off2] {sym} ptr mem)
+(MOVHUload [off1] {sym} (SUBconst [off2] ptr) mem) -> (MOVHUload [off1-off2] {sym} ptr mem)
+(MOVWload [off1] {sym} (ADDconst [off2] ptr) mem) -> (MOVWload [off1+off2] {sym} ptr mem)
+(MOVWload [off1] {sym} (SUBconst [off2] ptr) mem) -> (MOVWload [off1-off2] {sym} ptr mem)
+(MOVFload [off1] {sym} (ADDconst [off2] ptr) mem) -> (MOVFload [off1+off2] {sym} ptr mem)
+(MOVFload [off1] {sym} (SUBconst [off2] ptr) mem) -> (MOVFload [off1-off2] {sym} ptr mem)
+(MOVDload [off1] {sym} (ADDconst [off2] ptr) mem) -> (MOVDload [off1+off2] {sym} ptr mem)
+(MOVDload [off1] {sym} (SUBconst [off2] ptr) mem) -> (MOVDload [off1-off2] {sym} ptr mem)
+(LoadOnce8 [off1] {sym} (ADDconst [off2] ptr) mem) -> (LoadOnce8 [off1+off2] {sym} ptr mem)
+(LoadOnce8 [off1] {sym} (SUBconst [off2] ptr) mem) -> (LoadOnce8 [off1-off2] {sym} ptr mem)
+(LoadOnce16 [off1] {sym} (ADDconst [off2] ptr) mem) -> (LoadOnce16 [off1+off2] {sym} ptr mem)
+(LoadOnce16 [off1] {sym} (SUBconst [off2] ptr) mem) -> (LoadOnce16 [off1-off2] {sym} ptr mem)
+(LoadOnce32 [off1] {sym} (ADDconst [off2] ptr) mem) -> (LoadOnce32 [off1+off2] {sym} ptr mem)
+(LoadOnce32 [off1] {sym} (SUBconst [off2] ptr) mem) -> (LoadOnce32 [off1-off2] {sym} ptr mem)
+
+(MOVBstore [off1] {sym} (ADDconst [off2] ptr) val mem) -> (MOVBstore [off1+off2] {sym} ptr val mem)
+(MOVBstore [off1] {sym} (SUBconst [off2] ptr) val mem) -> (MOVBstore [off1-off2] {sym} ptr val mem)
+(MOVHstore [off1] {sym} (ADDconst [off2] ptr) val mem) -> (MOVHstore [off1+off2] {sym} ptr val mem)
+(MOVHstore [off1] {sym} (SUBconst [off2] ptr) val mem) -> (MOVHstore [off1-off2] {sym} ptr val mem)
+(MOVWstore [off1] {sym} (ADDconst [off2] ptr) val mem) -> (MOVWstore [off1+off2] {sym} ptr val mem)
+(MOVWstore [off1] {sym} (SUBconst [off2] ptr) val mem) -> (MOVWstore [off1-off2] {sym} ptr val mem)
+(MOVFstore [off1] {sym} (ADDconst [off2] ptr) val mem) -> (MOVFstore [off1+off2] {sym} ptr val mem)
+(MOVFstore [off1] {sym} (SUBconst [off2] ptr) val mem) -> (MOVFstore [off1-off2] {sym} ptr val mem)
+(MOVDstore [off1] {sym} (ADDconst [off2] ptr) val mem) -> (MOVDstore [off1+off2] {sym} ptr val mem)
+(MOVDstore [off1] {sym} (SUBconst [off2] ptr) val mem) -> (MOVDstore [off1-off2] {sym} ptr val mem)
+(StoreOnce8 [off1] {sym} (ADDconst [off2] ptr) val mem) -> (StoreOnce8 [off1+off2] {sym} ptr val mem)
+(StoreOnce8 [off1] {sym} (SUBconst [off2] ptr) val mem) -> (StoreOnce8 [off1-off2] {sym} ptr val mem)
+(StoreOnce16 [off1] {sym} (ADDconst [off2] ptr) val mem) -> (StoreOnce16 [off1+off2] {sym} ptr val mem)
+(StoreOnce16 [off1] {sym} (SUBconst [off2] ptr) val mem) -> (StoreOnce16 [off1-off2] {sym} ptr val mem)
+(StoreOnce32 [off1] {sym} (ADDconst [off2] ptr) val mem) -> (StoreOnce32 [off1+off2] {sym} ptr val mem)
+(StoreOnce32 [off1] {sym} (SUBconst [off2] ptr) val mem) -> (StoreOnce32 [off1-off2] {sym} ptr val mem)
+
+(MOVBload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(MOVBload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVBUload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(MOVBUload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVHload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(MOVHload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVHUload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(MOVHUload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVWload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(MOVWload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVFload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(MOVFload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(MOVDload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(MOVDload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(LoadOnce8 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(LoadOnce8 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(LoadOnce16 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(LoadOnce16 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+(LoadOnce8 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem) && canMergeSym(sym1,sym2) ->
+	(LoadOnce8 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+
+(MOVBstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) ->
+	(MOVBstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(MOVHstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) ->
+	(MOVHstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(MOVWstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) ->
+	(MOVWstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(MOVFstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) ->
+	(MOVFstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(MOVDstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) ->
+	(MOVDstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(StoreOnce8 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) ->
+	(StoreOnce8 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(StoreOnce16 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) ->
+	(StoreOnce16 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+(StoreOnce32 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem) && canMergeSym(sym1,sym2) ->
+	(StoreOnce32 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+
+// replace load from same location as preceding store with zero/sign extension (or copy in case of full width)
+(MOVBload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) -> (MOVBreg x)
+(MOVBUload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) -> (MOVBUreg x)
+(MOVHload [off] {sym} ptr (MOVHstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) -> (MOVHreg x)
+(MOVHUload [off] {sym} ptr (MOVHstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) -> (MOVHUreg x)
+(MOVWload [off] {sym} ptr (MOVWstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) -> x
+
+(MOVFload [off] {sym} ptr (MOVFstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) -> x
+(MOVDload [off] {sym} ptr (MOVDstore [off2] {sym2} ptr2 x _)) && sym == sym2 && off == off2 && isSamePtr(ptr, ptr2) -> x
+
+(MOVWloadidx ptr idx (MOVWstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) -> x
+(MOVWloadshiftLL ptr idx [c] (MOVWstoreshiftLL ptr2 idx [d] x _)) && c==d && isSamePtr(ptr, ptr2) -> x
+(MOVBUloadidx ptr idx (MOVBstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) -> (MOVBUreg x)
+(MOVBloadidx ptr idx (MOVBstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) -> (MOVBreg x)
+(MOVHUloadidx ptr idx (MOVHstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) -> (MOVHUreg x)
+(MOVHloadidx ptr idx (MOVHstoreidx ptr2 idx x _)) && isSamePtr(ptr, ptr2) -> (MOVHreg x)
+
+// fold constant into arithmatic ops
+(ADD x (MOVWconst [c])) -> (ADDconst [c] x)
+(SUB (MOVWconst [c]) x) -> (RSBconst [c] x)
+(SUB x (MOVWconst [c])) -> (SUBconst [c] x)
+(RSB (MOVWconst [c]) x) -> (SUBconst [c] x)
+(RSB x (MOVWconst [c])) -> (RSBconst [c] x)
+
+(ADDS x (MOVWconst [c])) -> (ADDSconst [c] x)
+(SUBS x (MOVWconst [c])) -> (SUBSconst [c] x)
+
+(ADC (MOVWconst [c]) x flags) -> (ADCconst [c] x flags)
+(ADC x (MOVWconst [c]) flags) -> (ADCconst [c] x flags)
+(SBC x (MOVWconst [c]) flags) -> (SBCconst [c] x flags)
+
+(AND x (MOVWconst [c])) -> (ANDconst [c] x)
+(OR  x (MOVWconst [c])) -> (ORconst [c] x)
+(XOR x (MOVWconst [c])) -> (XORconst [c] x)
+(BIC x (MOVWconst [c])) -> (BICconst [c] x)
+
+(SLL x (MOVWconst [c])) -> (SLLconst x [c&31]) // Note: I don't think we ever generate bad constant shifts (i.e. c>=32)
+(SRL x (MOVWconst [c])) -> (SRLconst x [c&31])
+(SRA x (MOVWconst [c])) -> (SRAconst x [c&31])
+
+(CMP x (MOVWconst [c])) -> (CMPconst [c] x)
+(CMP (MOVWconst [c]) x) -> (InvertFlags (CMPconst [c] x))
+(CMN x (MOVWconst [c])) -> (CMNconst [c] x)
+(TST x (MOVWconst [c])) -> (TSTconst [c] x)
+(TEQ x (MOVWconst [c])) -> (TEQconst [c] x)
+
+// don't extend after proper load
+// MOVWreg instruction is not emitted if src and dst registers are same, but it ensures the type.
+(MOVBreg x:(MOVBload _ _)) -> (MOVWreg x)
+(MOVBUreg x:(MOVBUload _ _)) -> (MOVWreg x)
+(MOVHreg x:(MOVBload _ _)) -> (MOVWreg x)
+(MOVHreg x:(MOVBUload _ _)) -> (MOVWreg x)
+(MOVHreg x:(MOVHload _ _)) -> (MOVWreg x)
+(MOVHUreg x:(MOVBUload _ _)) -> (MOVWreg x)
+(MOVHUreg x:(MOVHUload _ _)) -> (MOVWreg x)
+
+// fold extensions and ANDs together
+(MOVBUreg (ANDconst [c] x)) -> (ANDconst [c&0xff] x)
+(MOVHUreg (ANDconst [c] x)) -> (ANDconst [c&0xffff] x)
+(MOVBreg (ANDconst [c] x)) && c & 0x80 == 0 -> (ANDconst [c&0x7f] x)
+(MOVHreg (ANDconst [c] x)) && c & 0x8000 == 0 -> (ANDconst [c&0x7fff] x)
+
+// fold double extensions
+(MOVBreg x:(MOVBreg _)) -> (MOVWreg x)
+(MOVBUreg x:(MOVBUreg _)) -> (MOVWreg x)
+(MOVHreg x:(MOVBreg _)) -> (MOVWreg x)
+(MOVHreg x:(MOVBUreg _)) -> (MOVWreg x)
+(MOVHreg x:(MOVHreg _)) -> (MOVWreg x)
+(MOVHUreg x:(MOVBUreg _)) -> (MOVWreg x)
+(MOVHUreg x:(MOVHUreg _)) -> (MOVWreg x)
+
+// don't extend before store
+(MOVBstore [off] {sym} ptr (MOVBreg x) mem) -> (MOVBstore [off] {sym} ptr x mem)
+(MOVBstore [off] {sym} ptr (MOVBUreg x) mem) -> (MOVBstore [off] {sym} ptr x mem)
+(MOVBstore [off] {sym} ptr (MOVHreg x) mem) -> (MOVBstore [off] {sym} ptr x mem)
+(MOVBstore [off] {sym} ptr (MOVHUreg x) mem) -> (MOVBstore [off] {sym} ptr x mem)
+(MOVHstore [off] {sym} ptr (MOVHreg x) mem) -> (MOVHstore [off] {sym} ptr x mem)
+(MOVHstore [off] {sym} ptr (MOVHUreg x) mem) -> (MOVHstore [off] {sym} ptr x mem)
+
+// if a register move has only 1 use, just use the same register without emitting instruction
+// MOVWnop doesn't emit instruction, only for ensuring the type.
+(MOVWreg x) && x.Uses == 1 -> (MOVWnop x)
+
+// mul by constant
+(MUL x (MOVWconst [c])) && int32(c) == -1 -> (RSBconst [0] x)
+(MUL _ (MOVWconst [0])) -> (MOVWconst [0])
+(MUL x (MOVWconst [1])) -> x
+(MUL x (MOVWconst [c])) && isPowerOfTwo(c) -> (SLLconst [log2(c)] x)
+(MUL x (MOVWconst [c])) && isPowerOfTwo(c-1) && int32(c) >= 3 -> (ADDshiftLL x x [log2(c-1)])
+(MUL x (MOVWconst [c])) && isPowerOfTwo(c+1) && int32(c) >= 7 -> (RSBshiftLL x x [log2(c+1)])
+(MUL x (MOVWconst [c])) && c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c) -> (SLLconst [log2(c/3)] (ADDshiftLL <x.Type> x x [1]))
+(MUL x (MOVWconst [c])) && c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c) -> (SLLconst [log2(c/5)] (ADDshiftLL <x.Type> x x [2]))
+(MUL x (MOVWconst [c])) && c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c) -> (SLLconst [log2(c/7)] (RSBshiftLL <x.Type> x x [3]))
+(MUL x (MOVWconst [c])) && c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c) -> (SLLconst [log2(c/9)] (ADDshiftLL <x.Type> x x [3]))
+
+(MULA x (MOVWconst [c]) a) && int32(c) == -1 -> (SUB a x)
+(MULA _ (MOVWconst [0]) a) -> a
+(MULA x (MOVWconst [1]) a) -> (ADD x a)
+(MULA x (MOVWconst [c]) a) && isPowerOfTwo(c) -> (ADD (SLLconst <x.Type> [log2(c)] x) a)
+(MULA x (MOVWconst [c]) a) && isPowerOfTwo(c-1) && int32(c) >= 3 -> (ADD (ADDshiftLL <x.Type> x x [log2(c-1)]) a)
+(MULA x (MOVWconst [c]) a) && isPowerOfTwo(c+1) && int32(c) >= 7 -> (ADD (RSBshiftLL <x.Type> x x [log2(c+1)]) a)
+(MULA x (MOVWconst [c]) a) && c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c) -> (ADD (SLLconst <x.Type> [log2(c/3)] (ADDshiftLL <x.Type> x x [1])) a)
+(MULA x (MOVWconst [c]) a) && c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c) -> (ADD (SLLconst <x.Type> [log2(c/5)] (ADDshiftLL <x.Type> x x [2])) a)
+(MULA x (MOVWconst [c]) a) && c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c) -> (ADD (SLLconst <x.Type> [log2(c/7)] (RSBshiftLL <x.Type> x x [3])) a)
+(MULA x (MOVWconst [c]) a) && c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c) -> (ADD (SLLconst <x.Type> [log2(c/9)] (ADDshiftLL <x.Type> x x [3])) a)
+
+(MULA (MOVWconst [c]) x a) && int32(c) == -1 -> (SUB a x)
+(MULA (MOVWconst [0]) _ a) -> a
+(MULA (MOVWconst [1]) x a) -> (ADD x a)
+(MULA (MOVWconst [c]) x a) && isPowerOfTwo(c) -> (ADD (SLLconst <x.Type> [log2(c)] x) a)
+(MULA (MOVWconst [c]) x a) && isPowerOfTwo(c-1) && int32(c) >= 3 -> (ADD (ADDshiftLL <x.Type> x x [log2(c-1)]) a)
+(MULA (MOVWconst [c]) x a) && isPowerOfTwo(c+1) && int32(c) >= 7 -> (ADD (RSBshiftLL <x.Type> x x [log2(c+1)]) a)
+(MULA (MOVWconst [c]) x a) && c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c) -> (ADD (SLLconst <x.Type> [log2(c/3)] (ADDshiftLL <x.Type> x x [1])) a)
+(MULA (MOVWconst [c]) x a) && c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c) -> (ADD (SLLconst <x.Type> [log2(c/5)] (ADDshiftLL <x.Type> x x [2])) a)
+(MULA (MOVWconst [c]) x a) && c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c) -> (ADD (SLLconst <x.Type> [log2(c/7)] (RSBshiftLL <x.Type> x x [3])) a)
+(MULA (MOVWconst [c]) x a) && c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c) -> (ADD (SLLconst <x.Type> [log2(c/9)] (ADDshiftLL <x.Type> x x [3])) a)
+
+(MULS x (MOVWconst [c]) a) && int32(c) == -1 -> (ADD a x)
+(MULS _ (MOVWconst [0]) a) -> a
+(MULS x (MOVWconst [1]) a) -> (RSB x a)
+(MULS x (MOVWconst [c]) a) && isPowerOfTwo(c) -> (RSB (SLLconst <x.Type> [log2(c)] x) a)
+(MULS x (MOVWconst [c]) a) && isPowerOfTwo(c-1) && int32(c) >= 3 -> (RSB (ADDshiftLL <x.Type> x x [log2(c-1)]) a)
+(MULS x (MOVWconst [c]) a) && isPowerOfTwo(c+1) && int32(c) >= 7 -> (RSB (RSBshiftLL <x.Type> x x [log2(c+1)]) a)
+(MULS x (MOVWconst [c]) a) && c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c) -> (RSB (SLLconst <x.Type> [log2(c/3)] (ADDshiftLL <x.Type> x x [1])) a)
+(MULS x (MOVWconst [c]) a) && c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c) -> (RSB (SLLconst <x.Type> [log2(c/5)] (ADDshiftLL <x.Type> x x [2])) a)
+(MULS x (MOVWconst [c]) a) && c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c) -> (RSB (SLLconst <x.Type> [log2(c/7)] (RSBshiftLL <x.Type> x x [3])) a)
+(MULS x (MOVWconst [c]) a) && c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c) -> (RSB (SLLconst <x.Type> [log2(c/9)] (ADDshiftLL <x.Type> x x [3])) a)
+
+(MULS (MOVWconst [c]) x a) && int32(c) == -1 -> (ADD a x)
+(MULS (MOVWconst [0]) _ a) -> a
+(MULS (MOVWconst [1]) x a) -> (RSB x a)
+(MULS (MOVWconst [c]) x a) && isPowerOfTwo(c) -> (RSB (SLLconst <x.Type> [log2(c)] x) a)
+(MULS (MOVWconst [c]) x a) && isPowerOfTwo(c-1) && int32(c) >= 3 -> (RSB (ADDshiftLL <x.Type> x x [log2(c-1)]) a)
+(MULS (MOVWconst [c]) x a) && isPowerOfTwo(c+1) && int32(c) >= 7 -> (RSB (RSBshiftLL <x.Type> x x [log2(c+1)]) a)
+(MULS (MOVWconst [c]) x a) && c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c) -> (RSB (SLLconst <x.Type> [log2(c/3)] (ADDshiftLL <x.Type> x x [1])) a)
+(MULS (MOVWconst [c]) x a) && c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c) -> (RSB (SLLconst <x.Type> [log2(c/5)] (ADDshiftLL <x.Type> x x [2])) a)
+(MULS (MOVWconst [c]) x a) && c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c) -> (RSB (SLLconst <x.Type> [log2(c/7)] (RSBshiftLL <x.Type> x x [3])) a)
+(MULS (MOVWconst [c]) x a) && c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c) -> (RSB (SLLconst <x.Type> [log2(c/9)] (ADDshiftLL <x.Type> x x [3])) a)
+
+// div by constant
+(DIV x (MOVWconst [1])) -> x
+(DIVU x (MOVWconst [1])) -> x
+(DIV x (MOVWconst [c])) && isPowerOfTwo(c) -> (SRAconst [log2(c)] x)
+(DIVU x (MOVWconst [c])) && isPowerOfTwo(c) -> (SRLconst [log2(c)] x)
+
+// constant comparisons
+(CMPconst (MOVWconst [x]) [y]) && int32(x)==int32(y) -> (FlagEQ)
+(CMPconst (MOVWconst [x]) [y]) && int32(x)<int32(y) && uint32(x)<uint32(y) -> (FlagLT_ULT)
+(CMPconst (MOVWconst [x]) [y]) && int32(x)<int32(y) && uint32(x)>uint32(y) -> (FlagLT_UGT)
+(CMPconst (MOVWconst [x]) [y]) && int32(x)>int32(y) && uint32(x)<uint32(y) -> (FlagGT_ULT)
+(CMPconst (MOVWconst [x]) [y]) && int32(x)>int32(y) && uint32(x)>uint32(y) -> (FlagGT_UGT)
+(CMNconst (MOVWconst [x]) [y]) && int32(x)==int32(-y) -> (FlagEQ)
+(CMNconst (MOVWconst [x]) [y]) && int32(x)<int32(-y) && uint32(x)<uint32(-y) -> (FlagLT_ULT)
+(CMNconst (MOVWconst [x]) [y]) && int32(x)<int32(-y) && uint32(x)>uint32(-y) -> (FlagLT_UGT)
+(CMNconst (MOVWconst [x]) [y]) && int32(x)>int32(-y) && uint32(x)<uint32(-y) -> (FlagGT_ULT)
+(CMNconst (MOVWconst [x]) [y]) && int32(x)>int32(-y) && uint32(x)>uint32(-y) -> (FlagGT_UGT)
+(TSTconst (MOVWconst [x]) [y]) && int32(x&y)==0 -> (FlagEQ)
+(TSTconst (MOVWconst [x]) [y]) && int32(x&y)<0 -> (FlagLT_UGT)
+(TSTconst (MOVWconst [x]) [y]) && int32(x&y)>0 -> (FlagGT_UGT)
+(TEQconst (MOVWconst [x]) [y]) && int32(x^y)==0 -> (FlagEQ)
+(TEQconst (MOVWconst [x]) [y]) && int32(x^y)<0 -> (FlagLT_UGT)
+(TEQconst (MOVWconst [x]) [y]) && int32(x^y)>0 -> (FlagGT_UGT)
+
+// other known comparisons
+(CMPconst (MOVBUreg _) [c]) && 0xff < c -> (FlagLT_ULT)
+(CMPconst (MOVHUreg _) [c]) && 0xffff < c -> (FlagLT_ULT)
+(CMPconst (ANDconst _ [m]) [n]) && 0 <= int32(m) && int32(m) < int32(n) -> (FlagLT_ULT)
+(CMPconst (SRLconst _ [c]) [n]) && 0 <= n && 0 < c && c <= 32 && (1<<uint32(32-c)) <= uint32(n) -> (FlagLT_ULT)
+
+// absorb flag constants into branches
+(EQ (FlagEQ) yes no) -> (First nil yes no)
+(EQ (FlagLT_ULT) yes no) -> (First nil no yes)
+(EQ (FlagLT_UGT) yes no) -> (First nil no yes)
+(EQ (FlagGT_ULT) yes no) -> (First nil no yes)
+(EQ (FlagGT_UGT) yes no) -> (First nil no yes)
+
+(NE (FlagEQ) yes no) -> (First nil no yes)
+(NE (FlagLT_ULT) yes no) -> (First nil yes no)
+(NE (FlagLT_UGT) yes no) -> (First nil yes no)
+(NE (FlagGT_ULT) yes no) -> (First nil yes no)
+(NE (FlagGT_UGT) yes no) -> (First nil yes no)
+
+(LT (FlagEQ) yes no) -> (First nil no yes)
+(LT (FlagLT_ULT) yes no) -> (First nil yes no)
+(LT (FlagLT_UGT) yes no) -> (First nil yes no)
+(LT (FlagGT_ULT) yes no) -> (First nil no yes)
+(LT (FlagGT_UGT) yes no) -> (First nil no yes)
+
+(LE (FlagEQ) yes no) -> (First nil yes no)
+(LE (FlagLT_ULT) yes no) -> (First nil yes no)
+(LE (FlagLT_UGT) yes no) -> (First nil yes no)
+(LE (FlagGT_ULT) yes no) -> (First nil no yes)
+(LE (FlagGT_UGT) yes no) -> (First nil no yes)
+
+(GT (FlagEQ) yes no) -> (First nil no yes)
+(GT (FlagLT_ULT) yes no) -> (First nil no yes)
+(GT (FlagLT_UGT) yes no) -> (First nil no yes)
+(GT (FlagGT_ULT) yes no) -> (First nil yes no)
+(GT (FlagGT_UGT) yes no) -> (First nil yes no)
+
+(GE (FlagEQ) yes no) -> (First nil yes no)
+(GE (FlagLT_ULT) yes no) -> (First nil no yes)
+(GE (FlagLT_UGT) yes no) -> (First nil no yes)
+(GE (FlagGT_ULT) yes no) -> (First nil yes no)
+(GE (FlagGT_UGT) yes no) -> (First nil yes no)
+
+(ULT (FlagEQ) yes no) -> (First nil no yes)
+(ULT (FlagLT_ULT) yes no) -> (First nil yes no)
+(ULT (FlagLT_UGT) yes no) -> (First nil no yes)
+(ULT (FlagGT_ULT) yes no) -> (First nil yes no)
+(ULT (FlagGT_UGT) yes no) -> (First nil no yes)
+
+(ULE (FlagEQ) yes no) -> (First nil yes no)
+(ULE (FlagLT_ULT) yes no) -> (First nil yes no)
+(ULE (FlagLT_UGT) yes no) -> (First nil no yes)
+(ULE (FlagGT_ULT) yes no) -> (First nil yes no)
+(ULE (FlagGT_UGT) yes no) -> (First nil no yes)
+
+(UGT (FlagEQ) yes no) -> (First nil no yes)
+(UGT (FlagLT_ULT) yes no) -> (First nil no yes)
+(UGT (FlagLT_UGT) yes no) -> (First nil yes no)
+(UGT (FlagGT_ULT) yes no) -> (First nil no yes)
+(UGT (FlagGT_UGT) yes no) -> (First nil yes no)
+
+(UGE (FlagEQ) yes no) -> (First nil yes no)
+(UGE (FlagLT_ULT) yes no) -> (First nil no yes)
+(UGE (FlagLT_UGT) yes no) -> (First nil yes no)
+(UGE (FlagGT_ULT) yes no) -> (First nil no yes)
+(UGE (FlagGT_UGT) yes no) -> (First nil yes no)
+
+// absorb InvertFlags into branches
+(LT (InvertFlags cmp) yes no) -> (GT cmp yes no)
+(GT (InvertFlags cmp) yes no) -> (LT cmp yes no)
+(LE (InvertFlags cmp) yes no) -> (GE cmp yes no)
+(GE (InvertFlags cmp) yes no) -> (LE cmp yes no)
+(ULT (InvertFlags cmp) yes no) -> (UGT cmp yes no)
+(UGT (InvertFlags cmp) yes no) -> (ULT cmp yes no)
+(ULE (InvertFlags cmp) yes no) -> (UGE cmp yes no)
+(UGE (InvertFlags cmp) yes no) -> (ULE cmp yes no)
+(EQ (InvertFlags cmp) yes no) -> (EQ cmp yes no)
+(NE (InvertFlags cmp) yes no) -> (NE cmp yes no)
+
+// absorb flag constants into boolean values
+(Equal (FlagEQ)) -> (MOVWconst [1])
+(Equal (FlagLT_ULT)) -> (MOVWconst [0])
+(Equal (FlagLT_UGT)) -> (MOVWconst [0])
+(Equal (FlagGT_ULT)) -> (MOVWconst [0])
+(Equal (FlagGT_UGT)) -> (MOVWconst [0])
+
+(NotEqual (FlagEQ)) -> (MOVWconst [0])
+(NotEqual (FlagLT_ULT)) -> (MOVWconst [1])
+(NotEqual (FlagLT_UGT)) -> (MOVWconst [1])
+(NotEqual (FlagGT_ULT)) -> (MOVWconst [1])
+(NotEqual (FlagGT_UGT)) -> (MOVWconst [1])
+
+(LessThan (FlagEQ)) -> (MOVWconst [0])
+(LessThan (FlagLT_ULT)) -> (MOVWconst [1])
+(LessThan (FlagLT_UGT)) -> (MOVWconst [1])
+(LessThan (FlagGT_ULT)) -> (MOVWconst [0])
+(LessThan (FlagGT_UGT)) -> (MOVWconst [0])
+
+(LessThanU (FlagEQ)) -> (MOVWconst [0])
+(LessThanU (FlagLT_ULT)) -> (MOVWconst [1])
+(LessThanU (FlagLT_UGT)) -> (MOVWconst [0])
+(LessThanU (FlagGT_ULT)) -> (MOVWconst [1])
+(LessThanU (FlagGT_UGT)) -> (MOVWconst [0])
+
+(LessEqual (FlagEQ)) -> (MOVWconst [1])
+(LessEqual (FlagLT_ULT)) -> (MOVWconst [1])
+(LessEqual (FlagLT_UGT)) -> (MOVWconst [1])
+(LessEqual (FlagGT_ULT)) -> (MOVWconst [0])
+(LessEqual (FlagGT_UGT)) -> (MOVWconst [0])
+
+(LessEqualU (FlagEQ)) -> (MOVWconst [1])
+(LessEqualU (FlagLT_ULT)) -> (MOVWconst [1])
+(LessEqualU (FlagLT_UGT)) -> (MOVWconst [0])
+(LessEqualU (FlagGT_ULT)) -> (MOVWconst [1])
+(LessEqualU (FlagGT_UGT)) -> (MOVWconst [0])
+
+(GreaterThan (FlagEQ)) -> (MOVWconst [0])
+(GreaterThan (FlagLT_ULT)) -> (MOVWconst [0])
+(GreaterThan (FlagLT_UGT)) -> (MOVWconst [0])
+(GreaterThan (FlagGT_ULT)) -> (MOVWconst [1])
+(GreaterThan (FlagGT_UGT)) -> (MOVWconst [1])
+
+(GreaterThanU (FlagEQ)) -> (MOVWconst [0])
+(GreaterThanU (FlagLT_ULT)) -> (MOVWconst [0])
+(GreaterThanU (FlagLT_UGT)) -> (MOVWconst [1])
+(GreaterThanU (FlagGT_ULT)) -> (MOVWconst [0])
+(GreaterThanU (FlagGT_UGT)) -> (MOVWconst [1])
+
+(GreaterEqual (FlagEQ)) -> (MOVWconst [1])
+(GreaterEqual (FlagLT_ULT)) -> (MOVWconst [0])
+(GreaterEqual (FlagLT_UGT)) -> (MOVWconst [0])
+(GreaterEqual (FlagGT_ULT)) -> (MOVWconst [1])
+(GreaterEqual (FlagGT_UGT)) -> (MOVWconst [1])
+
+(GreaterEqualU (FlagEQ)) -> (MOVWconst [1])
+(GreaterEqualU (FlagLT_ULT)) -> (MOVWconst [0])
+(GreaterEqualU (FlagLT_UGT)) -> (MOVWconst [1])
+(GreaterEqualU (FlagGT_ULT)) -> (MOVWconst [0])
+(GreaterEqualU (FlagGT_UGT)) -> (MOVWconst [1])
+
+// absorb InvertFlags into boolean values
+(Equal (InvertFlags x)) -> (Equal x)
+(NotEqual (InvertFlags x)) -> (NotEqual x)
+(LessThan (InvertFlags x)) -> (GreaterThan x)
+(LessThanU (InvertFlags x)) -> (GreaterThanU x)
+(GreaterThan (InvertFlags x)) -> (LessThan x)
+(GreaterThanU (InvertFlags x)) -> (LessThanU x)
+(LessEqual (InvertFlags x)) -> (GreaterEqual x)
+(LessEqualU (InvertFlags x)) -> (GreaterEqualU x)
+(GreaterEqual (InvertFlags x)) -> (LessEqual x)
+(GreaterEqualU (InvertFlags x)) -> (LessEqualU x)
+
+// absorb flag constants into conditional instructions
+(CMOVWLSconst _ (FlagEQ) [c]) -> (MOVWconst [c])
+(CMOVWLSconst _ (FlagLT_ULT) [c]) -> (MOVWconst [c])
+(CMOVWLSconst x (FlagLT_UGT)) -> x
+(CMOVWLSconst _ (FlagGT_ULT) [c]) -> (MOVWconst [c])
+(CMOVWLSconst x (FlagGT_UGT)) -> x
+
+(CMOVWHSconst _ (FlagEQ) [c]) -> (MOVWconst [c])
+(CMOVWHSconst x (FlagLT_ULT)) -> x
+(CMOVWHSconst _ (FlagLT_UGT) [c]) -> (MOVWconst [c])
+(CMOVWHSconst x (FlagGT_ULT)) -> x
+(CMOVWHSconst _ (FlagGT_UGT) [c]) -> (MOVWconst [c])
+
+(CMOVWLSconst x (InvertFlags flags) [c]) -> (CMOVWHSconst x flags [c])
+(CMOVWHSconst x (InvertFlags flags) [c]) -> (CMOVWLSconst x flags [c])
+
+(SRAcond x _ (FlagEQ)) -> (SRAconst x [31])
+(SRAcond x y (FlagLT_ULT)) -> (SRA x y)
+(SRAcond x _ (FlagLT_UGT)) -> (SRAconst x [31])
+(SRAcond x y (FlagGT_ULT)) -> (SRA x y)
+(SRAcond x _ (FlagGT_UGT)) -> (SRAconst x [31])
+
+// remove redundant *const ops
+(ADDconst [0] x) -> x
+(SUBconst [0] x) -> x
+(ANDconst [0] _) -> (MOVWconst [0])
+(ANDconst [c] x) && int32(c)==-1 -> x
+(ORconst [0] x) -> x
+(ORconst [c] _) && int32(c)==-1 -> (MOVWconst [-1])
+(XORconst [0] x) -> x
+(BICconst [0] x) -> x
+(BICconst [c] _) && int32(c)==-1 -> (MOVWconst [0])
+
+// generic constant folding
+(ADDconst [c] (MOVWconst [d])) -> (MOVWconst [int64(int32(c+d))])
+(ADDconst [c] (ADDconst [d] x)) -> (ADDconst [int64(int32(c+d))] x)
+(ADDconst [c] (SUBconst [d] x)) -> (ADDconst [int64(int32(c-d))] x)
+(ADDconst [c] (RSBconst [d] x)) -> (RSBconst [int64(int32(c+d))] x)
+(ADCconst [c] (ADDconst [d] x) flags) -> (ADCconst [int64(int32(c+d))] x flags)
+(ADCconst [c] (SUBconst [d] x) flags) -> (ADCconst [int64(int32(c-d))] x flags)
+(SUBconst [c] (MOVWconst [d])) -> (MOVWconst [int64(int32(d-c))])
+(SUBconst [c] (SUBconst [d] x)) -> (ADDconst [int64(int32(-c-d))] x)
+(SUBconst [c] (ADDconst [d] x)) -> (ADDconst [int64(int32(-c+d))] x)
+(SUBconst [c] (RSBconst [d] x)) -> (RSBconst [int64(int32(-c+d))] x)
+(SBCconst [c] (ADDconst [d] x) flags) -> (SBCconst [int64(int32(c-d))] x flags)
+(SBCconst [c] (SUBconst [d] x) flags) -> (SBCconst [int64(int32(c+d))] x flags)
+(RSBconst [c] (MOVWconst [d])) -> (MOVWconst [int64(int32(c-d))])
+(RSBconst [c] (RSBconst [d] x)) -> (ADDconst [int64(int32(c-d))] x)
+(RSBconst [c] (ADDconst [d] x)) -> (RSBconst [int64(int32(c-d))] x)
+(RSBconst [c] (SUBconst [d] x)) -> (RSBconst [int64(int32(c+d))] x)
+(SLLconst [c] (MOVWconst [d])) -> (MOVWconst [int64(int32(uint32(d)<<uint64(c)))])
+(SRLconst [c] (MOVWconst [d])) -> (MOVWconst [int64(int32(uint32(d)>>uint64(c)))])
+(SRAconst [c] (MOVWconst [d])) -> (MOVWconst [int64(int32(d)>>uint64(c))])
+(MUL (MOVWconst [c]) (MOVWconst [d])) -> (MOVWconst [int64(int32(c*d))])
+(MULA (MOVWconst [c]) (MOVWconst [d]) a) -> (ADDconst [int64(int32(c*d))] a)
+(MULS (MOVWconst [c]) (MOVWconst [d]) a) -> (SUBconst [int64(int32(c*d))] a)
+(DIV (MOVWconst [c]) (MOVWconst [d])) -> (MOVWconst [int64(int32(c)/int32(d))])
+(DIVU (MOVWconst [c]) (MOVWconst [d])) -> (MOVWconst [int64(uint32(c)/uint32(d))])
+(ANDconst [c] (MOVWconst [d])) -> (MOVWconst [c&d])
+(ANDconst [c] (ANDconst [d] x)) -> (ANDconst [c&d] x)
+(ORconst [c] (MOVWconst [d])) -> (MOVWconst [c|d])
+(ORconst [c] (ORconst [d] x)) -> (ORconst [c|d] x)
+(XORconst [c] (MOVWconst [d])) -> (MOVWconst [c^d])
+(XORconst [c] (XORconst [d] x)) -> (XORconst [c^d] x)
+(BICconst [c] (MOVWconst [d])) -> (MOVWconst [d&^c])
+(BICconst [c] (BICconst [d] x)) -> (BICconst [int64(int32(c|d))] x)
+(MVN (MOVWconst [c])) -> (MOVWconst [^c])
+(MOVBreg (MOVWconst [c])) -> (MOVWconst [int64(int8(c))])
+(MOVBUreg (MOVWconst [c])) -> (MOVWconst [int64(uint8(c))])
+(MOVHreg (MOVWconst [c])) -> (MOVWconst [int64(int16(c))])
+(MOVHUreg (MOVWconst [c])) -> (MOVWconst [int64(uint16(c))])
+(MOVWreg (MOVWconst [c])) -> (MOVWconst [c])
+// BFX: Width = c >> 8, LSB = c & 0xff, result = d << (32 - Width - LSB) >> (32 - Width)
+(BFX [c] (MOVWconst [d])) -> (MOVWconst [int64(int32(d)<<(32-uint32(c&0xff)-uint32(c>>8))>>(32-uint32(c>>8)))])
+(BFXU [c] (MOVWconst [d])) -> (MOVWconst [int64(int32(uint32(d)<<(32-uint32(c&0xff)-uint32(c>>8))>>(32-uint32(c>>8))))])
+
+// absorb shifts into ops
+(ADD x (SLLconst [c] y)) -> (ADDshiftLL x y [c])
+(ADD x (SRLconst [c] y)) -> (ADDshiftRL x y [c])
+(ADD x (SRAconst [c] y)) -> (ADDshiftRA x y [c])
+(ADC x (SLLconst [c] y) flags) -> (ADCshiftLL x y [c] flags)
+(ADC (SLLconst [c] y) x flags) -> (ADCshiftLL x y [c] flags)
+(ADC x (SRLconst [c] y) flags) -> (ADCshiftRL x y [c] flags)
+(ADC (SRLconst [c] y) x flags) -> (ADCshiftRL x y [c] flags)
+(ADC x (SRAconst [c] y) flags) -> (ADCshiftRA x y [c] flags)
+(ADC (SRAconst [c] y) x flags) -> (ADCshiftRA x y [c] flags)
+(ADDS x (SLLconst [c] y)) -> (ADDSshiftLL x y [c])
+(ADDS x (SRLconst [c] y)) -> (ADDSshiftRL x y [c])
+(ADDS x (SRAconst [c] y)) -> (ADDSshiftRA x y [c])
+(SUB x (SLLconst [c] y)) -> (SUBshiftLL x y [c])
+(SUB (SLLconst [c] y) x) -> (RSBshiftLL x y [c])
+(SUB x (SRLconst [c] y)) -> (SUBshiftRL x y [c])
+(SUB (SRLconst [c] y) x) -> (RSBshiftRL x y [c])
+(SUB x (SRAconst [c] y)) -> (SUBshiftRA x y [c])
+(SUB (SRAconst [c] y) x) -> (RSBshiftRA x y [c])
+(SBC x (SLLconst [c] y) flags) -> (SBCshiftLL x y [c] flags)
+(SBC x (SRLconst [c] y) flags) -> (SBCshiftRL x y [c] flags)
+(SBC x (SRAconst [c] y) flags) -> (SBCshiftRA x y [c] flags)
+(SUBS x (SLLconst [c] y)) -> (SUBSshiftLL x y [c])
+(SUBS (SLLconst [c] y) x) -> (RSBSshiftLL x y [c])
+(SUBS x (SRLconst [c] y)) -> (SUBSshiftRL x y [c])
+(SUBS (SRLconst [c] y) x) -> (RSBSshiftRL x y [c])
+(SUBS x (SRAconst [c] y)) -> (SUBSshiftRA x y [c])
+(SUBS (SRAconst [c] y) x) -> (RSBSshiftRA x y [c])
+(RSB x (SLLconst [c] y)) -> (RSBshiftLL x y [c])
+(RSB (SLLconst [c] y) x) -> (SUBshiftLL x y [c])
+(RSB x (SRLconst [c] y)) -> (RSBshiftRL x y [c])
+(RSB (SRLconst [c] y) x) -> (SUBshiftRL x y [c])
+(RSB x (SRAconst [c] y)) -> (RSBshiftRA x y [c])
+(RSB (SRAconst [c] y) x) -> (SUBshiftRA x y [c])
+(AND x (SLLconst [c] y)) -> (ANDshiftLL x y [c])
+(AND x (SRLconst [c] y)) -> (ANDshiftRL x y [c])
+(AND x (SRAconst [c] y)) -> (ANDshiftRA x y [c])
+(OR x (SLLconst [c] y)) -> (ORshiftLL x y [c])
+(OR x (SRLconst [c] y)) -> (ORshiftRL x y [c])
+(OR x (SRAconst [c] y)) -> (ORshiftRA x y [c])
+(XOR x (SLLconst [c] y)) -> (XORshiftLL x y [c])
+(XOR x (SRLconst [c] y)) -> (XORshiftRL x y [c])
+(XOR x (SRAconst [c] y)) -> (XORshiftRA x y [c])
+(XOR x (SRRconst [c] y)) -> (XORshiftRR x y [c])
+(BIC x (SLLconst [c] y)) -> (BICshiftLL x y [c])
+(BIC x (SRLconst [c] y)) -> (BICshiftRL x y [c])
+(BIC x (SRAconst [c] y)) -> (BICshiftRA x y [c])
+(MVN (SLLconst [c] x)) -> (MVNshiftLL x [c])
+(MVN (SRLconst [c] x)) -> (MVNshiftRL x [c])
+(MVN (SRAconst [c] x)) -> (MVNshiftRA x [c])
+
+(CMP x (SLLconst [c] y)) -> (CMPshiftLL x y [c])
+(CMP (SLLconst [c] y) x) -> (InvertFlags (CMPshiftLL x y [c]))
+(CMP x (SRLconst [c] y)) -> (CMPshiftRL x y [c])
+(CMP (SRLconst [c] y) x) -> (InvertFlags (CMPshiftRL x y [c]))
+(CMP x (SRAconst [c] y)) -> (CMPshiftRA x y [c])
+(CMP (SRAconst [c] y) x) -> (InvertFlags (CMPshiftRA x y [c]))
+(TST x (SLLconst [c] y)) -> (TSTshiftLL x y [c])
+(TST x (SRLconst [c] y)) -> (TSTshiftRL x y [c])
+(TST x (SRAconst [c] y)) -> (TSTshiftRA x y [c])
+(TEQ x (SLLconst [c] y)) -> (TEQshiftLL x y [c])
+(TEQ x (SRLconst [c] y)) -> (TEQshiftRL x y [c])
+(TEQ x (SRAconst [c] y)) -> (TEQshiftRA x y [c])
+(CMN x (SLLconst [c] y)) -> (CMNshiftLL x y [c])
+(CMN x (SRLconst [c] y)) -> (CMNshiftRL x y [c])
+(CMN x (SRAconst [c] y)) -> (CMNshiftRA x y [c])
+
+// prefer *const ops to *shift ops
+(ADDshiftLL (MOVWconst [c]) x [d]) -> (ADDconst [c] (SLLconst <x.Type> x [d]))
+(ADDshiftRL (MOVWconst [c]) x [d]) -> (ADDconst [c] (SRLconst <x.Type> x [d]))
+(ADDshiftRA (MOVWconst [c]) x [d]) -> (ADDconst [c] (SRAconst <x.Type> x [d]))
+(ADCshiftLL (MOVWconst [c]) x [d] flags) -> (ADCconst [c] (SLLconst <x.Type> x [d]) flags)
+(ADCshiftRL (MOVWconst [c]) x [d] flags) -> (ADCconst [c] (SRLconst <x.Type> x [d]) flags)
+(ADCshiftRA (MOVWconst [c]) x [d] flags) -> (ADCconst [c] (SRAconst <x.Type> x [d]) flags)
+(ADDSshiftLL (MOVWconst [c]) x [d]) -> (ADDSconst [c] (SLLconst <x.Type> x [d]))
+(ADDSshiftRL (MOVWconst [c]) x [d]) -> (ADDSconst [c] (SRLconst <x.Type> x [d]))
+(ADDSshiftRA (MOVWconst [c]) x [d]) -> (ADDSconst [c] (SRAconst <x.Type> x [d]))
+(SUBshiftLL (MOVWconst [c]) x [d]) -> (RSBconst [c] (SLLconst <x.Type> x [d]))
+(SUBshiftRL (MOVWconst [c]) x [d]) -> (RSBconst [c] (SRLconst <x.Type> x [d]))
+(SUBshiftRA (MOVWconst [c]) x [d]) -> (RSBconst [c] (SRAconst <x.Type> x [d]))
+(SUBSshiftLL (MOVWconst [c]) x [d]) -> (RSBSconst [c] (SLLconst <x.Type> x [d]))
+(SUBSshiftRL (MOVWconst [c]) x [d]) -> (RSBSconst [c] (SRLconst <x.Type> x [d]))
+(SUBSshiftRA (MOVWconst [c]) x [d]) -> (RSBSconst [c] (SRAconst <x.Type> x [d]))
+(RSBshiftLL (MOVWconst [c]) x [d]) -> (SUBconst [c] (SLLconst <x.Type> x [d]))
+(RSBshiftRL (MOVWconst [c]) x [d]) -> (SUBconst [c] (SRLconst <x.Type> x [d]))
+(RSBshiftRA (MOVWconst [c]) x [d]) -> (SUBconst [c] (SRAconst <x.Type> x [d]))
+(RSBSshiftLL (MOVWconst [c]) x [d]) -> (SUBSconst [c] (SLLconst <x.Type> x [d]))
+(RSBSshiftRL (MOVWconst [c]) x [d]) -> (SUBSconst [c] (SRLconst <x.Type> x [d]))
+(RSBSshiftRA (MOVWconst [c]) x [d]) -> (SUBSconst [c] (SRAconst <x.Type> x [d]))
+(ANDshiftLL (MOVWconst [c]) x [d]) -> (ANDconst [c] (SLLconst <x.Type> x [d]))
+(ANDshiftRL (MOVWconst [c]) x [d]) -> (ANDconst [c] (SRLconst <x.Type> x [d]))
+(ANDshiftRA (MOVWconst [c]) x [d]) -> (ANDconst [c] (SRAconst <x.Type> x [d]))
+(ORshiftLL (MOVWconst [c]) x [d]) -> (ORconst [c] (SLLconst <x.Type> x [d]))
+(ORshiftRL (MOVWconst [c]) x [d]) -> (ORconst [c] (SRLconst <x.Type> x [d]))
+(ORshiftRA (MOVWconst [c]) x [d]) -> (ORconst [c] (SRAconst <x.Type> x [d]))
+(XORshiftLL (MOVWconst [c]) x [d]) -> (XORconst [c] (SLLconst <x.Type> x [d]))
+(XORshiftRL (MOVWconst [c]) x [d]) -> (XORconst [c] (SRLconst <x.Type> x [d]))
+(XORshiftRA (MOVWconst [c]) x [d]) -> (XORconst [c] (SRAconst <x.Type> x [d]))
+(XORshiftRR (MOVWconst [c]) x [d]) -> (XORconst [c] (SRRconst <x.Type> x [d]))
+(CMPshiftLL (MOVWconst [c]) x [d]) -> (InvertFlags (CMPconst [c] (SLLconst <x.Type> x [d])))
+(CMPshiftRL (MOVWconst [c]) x [d]) -> (InvertFlags (CMPconst [c] (SRLconst <x.Type> x [d])))
+(CMPshiftRA (MOVWconst [c]) x [d]) -> (InvertFlags (CMPconst [c] (SRAconst <x.Type> x [d])))
+(TSTshiftLL (MOVWconst [c]) x [d]) -> (TSTconst [c] (SLLconst <x.Type> x [d]))
+(TSTshiftRL (MOVWconst [c]) x [d]) -> (TSTconst [c] (SRLconst <x.Type> x [d]))
+(TSTshiftRA (MOVWconst [c]) x [d]) -> (TSTconst [c] (SRAconst <x.Type> x [d]))
+(TEQshiftLL (MOVWconst [c]) x [d]) -> (TEQconst [c] (SLLconst <x.Type> x [d]))
+(TEQshiftRL (MOVWconst [c]) x [d]) -> (TEQconst [c] (SRLconst <x.Type> x [d]))
+(TEQshiftRA (MOVWconst [c]) x [d]) -> (TEQconst [c] (SRAconst <x.Type> x [d]))
+(CMNshiftLL (MOVWconst [c]) x [d]) -> (CMNconst [c] (SLLconst <x.Type> x [d]))
+(CMNshiftRL (MOVWconst [c]) x [d]) -> (CMNconst [c] (SRLconst <x.Type> x [d]))
+(CMNshiftRA (MOVWconst [c]) x [d]) -> (CMNconst [c] (SRAconst <x.Type> x [d]))
+
+// constant folding in *shift ops
+(ADDshiftLL x (MOVWconst [c]) [d]) -> (ADDconst x [int64(int32(uint32(c)<<uint64(d)))])
+(ADDshiftRL x (MOVWconst [c]) [d]) -> (ADDconst x [int64(int32(uint32(c)>>uint64(d)))])
+(ADDshiftRA x (MOVWconst [c]) [d]) -> (ADDconst x [int64(int32(c)>>uint64(d))])
+(ADCshiftLL x (MOVWconst [c]) [d] flags) -> (ADCconst x [int64(int32(uint32(c)<<uint64(d)))] flags)
+(ADCshiftRL x (MOVWconst [c]) [d] flags) -> (ADCconst x [int64(int32(uint32(c)>>uint64(d)))] flags)
+(ADCshiftRA x (MOVWconst [c]) [d] flags) -> (ADCconst x [int64(int32(c)>>uint64(d))] flags)
+(ADDSshiftLL x (MOVWconst [c]) [d]) -> (ADDSconst x [int64(int32(uint32(c)<<uint64(d)))])
+(ADDSshiftRL x (MOVWconst [c]) [d]) -> (ADDSconst x [int64(int32(uint32(c)>>uint64(d)))])
+(ADDSshiftRA x (MOVWconst [c]) [d]) -> (ADDSconst x [int64(int32(c)>>uint64(d))])
+(SUBshiftLL x (MOVWconst [c]) [d]) -> (SUBconst x [int64(int32(uint32(c)<<uint64(d)))])
+(SUBshiftRL x (MOVWconst [c]) [d]) -> (SUBconst x [int64(int32(uint32(c)>>uint64(d)))])
+(SUBshiftRA x (MOVWconst [c]) [d]) -> (SUBconst x [int64(int32(c)>>uint64(d))])
+(SBCshiftLL x (MOVWconst [c]) [d] flags) -> (SBCconst x [int64(int32(uint32(c)<<uint64(d)))] flags)
+(SBCshiftRL x (MOVWconst [c]) [d] flags) -> (SBCconst x [int64(int32(uint32(c)>>uint64(d)))] flags)
+(SBCshiftRA x (MOVWconst [c]) [d] flags) -> (SBCconst x [int64(int32(c)>>uint64(d))] flags)
+(SUBSshiftLL x (MOVWconst [c]) [d]) -> (SUBSconst x [int64(int32(uint32(c)<<uint64(d)))])
+(SUBSshiftRL x (MOVWconst [c]) [d]) -> (SUBSconst x [int64(int32(uint32(c)>>uint64(d)))])
+(SUBSshiftRA x (MOVWconst [c]) [d]) -> (SUBSconst x [int64(int32(c)>>uint64(d))])
+(RSBshiftLL x (MOVWconst [c]) [d]) -> (RSBconst x [int64(int32(uint32(c)<<uint64(d)))])
+(RSBshiftRL x (MOVWconst [c]) [d]) -> (RSBconst x [int64(int32(uint32(c)>>uint64(d)))])
+(RSBshiftRA x (MOVWconst [c]) [d]) -> (RSBconst x [int64(int32(c)>>uint64(d))])
+(RSBSshiftLL x (MOVWconst [c]) [d]) -> (RSBSconst x [int64(int32(uint32(c)<<uint64(d)))])
+(RSBSshiftRL x (MOVWconst [c]) [d]) -> (RSBSconst x [int64(int32(uint32(c)>>uint64(d)))])
+(RSBSshiftRA x (MOVWconst [c]) [d]) -> (RSBSconst x [int64(int32(c)>>uint64(d))])
+(ANDshiftLL x (MOVWconst [c]) [d]) -> (ANDconst x [int64(int32(uint32(c)<<uint64(d)))])
+(ANDshiftRL x (MOVWconst [c]) [d]) -> (ANDconst x [int64(int32(uint32(c)>>uint64(d)))])
+(ANDshiftRA x (MOVWconst [c]) [d]) -> (ANDconst x [int64(int32(c)>>uint64(d))])
+(ORshiftLL x (MOVWconst [c]) [d]) -> (ORconst x [int64(int32(uint32(c)<<uint64(d)))])
+(ORshiftRL x (MOVWconst [c]) [d]) -> (ORconst x [int64(int32(uint32(c)>>uint64(d)))])
+(ORshiftRA x (MOVWconst [c]) [d]) -> (ORconst x [int64(int32(c)>>uint64(d))])
+(XORshiftLL x (MOVWconst [c]) [d]) -> (XORconst x [int64(int32(uint32(c)<<uint64(d)))])
+(XORshiftRL x (MOVWconst [c]) [d]) -> (XORconst x [int64(int32(uint32(c)>>uint64(d)))])
+(XORshiftRA x (MOVWconst [c]) [d]) -> (XORconst x [int64(int32(c)>>uint64(d))])
+(XORshiftRR x (MOVWconst [c]) [d]) -> (XORconst x [int64(int32(uint32(c)>>uint64(d)|uint32(c)<<uint64(32-d)))])
+(BICshiftLL x (MOVWconst [c]) [d]) -> (BICconst x [int64(int32(uint32(c)<<uint64(d)))])
+(BICshiftRL x (MOVWconst [c]) [d]) -> (BICconst x [int64(int32(uint32(c)>>uint64(d)))])
+(BICshiftRA x (MOVWconst [c]) [d]) -> (BICconst x [int64(int32(c)>>uint64(d))])
+(MVNshiftLL (MOVWconst [c]) [d]) -> (MOVWconst [^int64(uint32(c)<<uint64(d))])
+(MVNshiftRL (MOVWconst [c]) [d]) -> (MOVWconst [^int64(uint32(c)>>uint64(d))])
+(MVNshiftRA (MOVWconst [c]) [d]) -> (MOVWconst [^int64(int32(c)>>uint64(d))])
+(CMPshiftLL x (MOVWconst [c]) [d]) -> (CMPconst x [int64(int32(uint32(c)<<uint64(d)))])
+(CMPshiftRL x (MOVWconst [c]) [d]) -> (CMPconst x [int64(int32(uint32(c)>>uint64(d)))])
+(CMPshiftRA x (MOVWconst [c]) [d]) -> (CMPconst x [int64(int32(c)>>uint64(d))])
+(TSTshiftLL x (MOVWconst [c]) [d]) -> (TSTconst x [int64(int32(uint32(c)<<uint64(d)))])
+(TSTshiftRL x (MOVWconst [c]) [d]) -> (TSTconst x [int64(int32(uint32(c)>>uint64(d)))])
+(TSTshiftRA x (MOVWconst [c]) [d]) -> (TSTconst x [int64(int32(c)>>uint64(d))])
+(TEQshiftLL x (MOVWconst [c]) [d]) -> (TEQconst x [int64(int32(uint32(c)<<uint64(d)))])
+(TEQshiftRL x (MOVWconst [c]) [d]) -> (TEQconst x [int64(int32(uint32(c)>>uint64(d)))])
+(TEQshiftRA x (MOVWconst [c]) [d]) -> (TEQconst x [int64(int32(c)>>uint64(d))])
+(CMNshiftLL x (MOVWconst [c]) [d]) -> (CMNconst x [int64(int32(uint32(c)<<uint64(d)))])
+(CMNshiftRL x (MOVWconst [c]) [d]) -> (CMNconst x [int64(int32(uint32(c)>>uint64(d)))])
+(CMNshiftRA x (MOVWconst [c]) [d]) -> (CMNconst x [int64(int32(c)>>uint64(d))])
+
+// Generate rotates
+(ADDshiftLL [c] (SRLconst x [32-c]) x) -> (SRRconst [32-c] x)
+( ORshiftLL [c] (SRLconst x [32-c]) x) -> (SRRconst [32-c] x)
+(XORshiftLL [c] (SRLconst x [32-c]) x) -> (SRRconst [32-c] x)
+(ADDshiftRL [c] (SLLconst x [32-c]) x) -> (SRRconst [   c] x)
+( ORshiftRL [c] (SLLconst x [32-c]) x) -> (SRRconst [   c] x)
+(XORshiftRL [c] (SLLconst x [32-c]) x) -> (SRRconst [   c] x)
+
+(RotateLeft32 x (MOVWconst [c])) -> (SRRconst [-c&31] x)
+(RotateLeft16 <t> x (MOVWconst [c])) -> (Or16 (Lsh16x32 <t> x (MOVWconst [c&15])) (Rsh16Ux32 <t> x (MOVWconst [-c&15])))
+(RotateLeft8 <t> x (MOVWconst [c])) -> (Or8 (Lsh8x32 <t> x (MOVWconst [c&7])) (Rsh8Ux32 <t> x (MOVWconst [-c&7])))
+
+// ((x>>8) | (x<<8)) -> (REV16 x), the type of x is uint16, "|" can also be "^" or "+".
+((ADDshiftLL|ORshiftLL|XORshiftLL) <typ.UInt16> [8] (BFXU <typ.UInt16> [armBFAuxInt(8, 8)] x) x) -> (REV16 x)
+((ADDshiftLL|ORshiftLL|XORshiftLL) <typ.UInt16> [8] (SRLconst <typ.UInt16> [24] (SLLconst [16] x)) x) -> (REV16 x)
+
+// use indexed loads and stores
+(MOVWload [0] {sym} (ADD ptr idx) mem) && sym == nil -> (MOVWloadidx ptr idx mem)
+(MOVHload [0] {sym} (ADD ptr idx) mem) && sym == nil -> (MOVHloadidx ptr idx mem)
+(MOVHUload [0] {sym} (ADD ptr idx) mem) && sym == nil -> (MOVHUloadidx ptr idx mem)
+(MOVBload [0] {sym} (ADD ptr idx) mem) && sym == nil -> (MOVBloadidx ptr idx mem)
+(MOVBUload [0] {sym} (ADD ptr idx) mem) && sym == nil -> (MOVBUloadidx ptr idx mem)
+(LoadOnce8 [0] {sym} (ADD ptr idx) mem) && sym == nil -> (LoadOnce8idx ptr idx mem)
+(LoadOnce16 [0] {sym} (ADD ptr idx) mem) && sym == nil -> (LoadOnce16idx ptr idx mem)
+(LoadOnce32 [0] {sym} (ADD ptr idx) mem) && sym == nil -> (LoadOnce32idx ptr idx  mem)
+
+(MOVWstore [0] {sym} (ADD ptr idx) val mem) && sym == nil -> (MOVWstoreidx ptr idx val mem)
+(MOVBstore [0] {sym} (ADD ptr idx) val mem) && sym == nil -> (MOVBstoreidx ptr idx val mem)
+(MOVHstore [0] {sym} (ADD ptr idx) val mem) && sym == nil -> (MOVHstoreidx ptr idx val mem)
+(StoreOnce8 [0] {sym} (ADD ptr idx) val mem) && sym == nil -> (StoreOnce8idx ptr idx val mem)
+(StoreOnce16 [0] {sym} (ADD ptr idx) val mem) && sym == nil -> (StoreOnce16idx ptr idx val mem)
+(StoreOnce32 [0] {sym} (ADD ptr idx) val mem) && sym == nil -> (StoreOnce32idx ptr idx val mem)
+
+(MOVWload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 -> (MOVWloadshiftLL ptr idx [c] mem)
+(MOVHload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 -> (MOVHloadshiftLL ptr idx [c] mem)
+(MOVHUload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 -> (MOVHUloadshiftLL ptr idx [c] mem)
+(MOVBload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 -> (MOVBloadshiftLL ptr idx [c] mem)
+(MOVBUload [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 -> (MOVBUloadshiftLL ptr idx [c] mem)
+(LoadOnce8 [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 -> (LoadOnce8shiftLL ptr idx [c] mem)
+(LoadOnce16 [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 -> (LoadOnce16shiftLL ptr idx [c] mem)
+(LoadOnce32 [0] {sym} (ADDshiftLL ptr idx [c]) mem) && sym == nil && c <= 3 -> (LoadOnce32shiftLL ptr idx [c] mem)
+
+(MOVWstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 -> (MOVWstoreshiftLL ptr idx [c] val mem)
+(MOVHstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 -> (MOVHstoreshiftLL ptr idx [c] val mem)
+(MOVBstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 -> (MOVBstoreshiftLL ptr idx [c] val mem)
+(StoreOnce8 [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 -> (StoreOnce8shiftLL ptr idx [c] val mem)
+(StoreOnce16 [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 -> (StoreOnce16shiftLL ptr idx [c] val mem)
+(StoreOnce32 [0] {sym} (ADDshiftLL ptr idx [c]) val mem) && sym == nil && c <= 3 -> (StoreOnce32shiftLL ptr idx [c] val mem)
+
+// constant folding in indexed loads and stores
+(MOVWloadidx ptr (MOVWconst [c]) mem) -> (MOVWload [c] ptr mem)
+(MOVWloadidx (MOVWconst [c]) ptr mem) -> (MOVWload [c] ptr mem)
+(MOVHloadidx ptr (MOVWconst [c]) mem) -> (MOVHload [c] ptr mem)
+(MOVHloadidx (MOVWconst [c]) ptr mem) -> (MOVHload [c] ptr mem)
+(MOVHUloadidx ptr (MOVWconst [c]) mem) -> (MOVHUload [c] ptr mem)
+(MOVHUloadidx (MOVWconst [c]) ptr mem) -> (MOVHUload [c] ptr mem)
+(MOVBloadidx ptr (MOVWconst [c]) mem) -> (MOVBload [c] ptr mem)
+(MOVBloadidx (MOVWconst [c]) ptr mem) -> (MOVBload [c] ptr mem)
+(MOVBUloadidx ptr (MOVWconst [c]) mem) -> (MOVBUload [c] ptr mem)
+(MOVBUloadidx (MOVWconst [c]) ptr mem) -> (MOVBUload [c] ptr mem)
+(LoadOnce32idx ptr (MOVWconst [c]) mem) -> (LoadOnce32 [c] ptr mem)
+(LoadOnce32idx (MOVWconst [c]) ptr mem) -> (LoadOnce32 [c] ptr mem)
+(LoadOnce16idx ptr (MOVWconst [c]) mem) -> (LoadOnce16 [c] ptr mem)
+(LoadOnce16idx (MOVWconst [c]) ptr mem) -> (LoadOnce16 [c] ptr mem)
+(LoadOnce8idx ptr (MOVWconst [c]) mem) -> (LoadOnce8 [c] ptr mem)
+(LoadOnce8idx (MOVWconst [c]) ptr mem) -> (LoadOnce8 [c] ptr mem)
+
+(MOVWstoreidx ptr (MOVWconst [c]) val mem) -> (MOVWstore [c] ptr val mem)
+(MOVWstoreidx (MOVWconst [c]) ptr val mem) -> (MOVWstore [c] ptr val mem)
+(MOVHstoreidx ptr (MOVWconst [c]) val mem) -> (MOVHstore [c] ptr val mem)
+(MOVHstoreidx (MOVWconst [c]) ptr val mem) -> (MOVHstore [c] ptr val mem)
+(MOVBstoreidx ptr (MOVWconst [c]) val mem) -> (MOVBstore [c] ptr val mem)
+(MOVBstoreidx (MOVWconst [c]) ptr val mem) -> (MOVBstore [c] ptr val mem)
+(StoreOnce32idx ptr (MOVWconst [c]) val mem) -> (StoreOnce32 [c] ptr val mem)
+(StoreOnce32idx (MOVWconst [c]) ptr val mem) -> (StoreOnce32 [c] ptr val mem)
+(StoreOnce16idx ptr (MOVWconst [c]) val mem) -> (StoreOnce16 [c] ptr val mem)
+(StoreOnce16idx (MOVWconst [c]) ptr val mem) -> (StoreOnce16 [c] ptr val mem)
+(StoreOnce8idx ptr (MOVWconst [c]) val mem) -> (StoreOnce8 [c] ptr val mem)
+(StoreOnce8idx (MOVWconst [c]) ptr val mem) -> (StoreOnce8 [c] ptr val mem)
+
+(MOVWloadidx ptr (SLLconst idx [c]) mem) && c <= 3 -> (MOVWloadshiftLL ptr idx [c] mem)
+(MOVWloadidx (SLLconst idx [c]) ptr mem) && c <= 3 -> (MOVWloadshiftLL ptr idx [c] mem)
+(MOVHUloadidx ptr (SLLconst idx [c]) mem) && c <= 3 -> (MOVHUloadshiftLL ptr idx [c] mem)
+(MOVHUloadidx (SLLconst idx [c]) ptr mem) && c <= 3 -> (MOVHUloadshiftLL ptr idx [c] mem)
+(MOVHloadidx ptr (SLLconst idx [c]) mem) && c <= 3 -> (MOVHloadshiftLL ptr idx [c] mem)
+(MOVHloadidx (SLLconst idx [c]) ptr mem) && c <= 3 -> (MOVHloadshiftLL ptr idx [c] mem)
+(MOVBUloadidx ptr (SLLconst idx [c]) mem) && c <= 3 -> (MOVBUloadshiftLL ptr idx [c] mem)
+(MOVBUloadidx (SLLconst idx [c]) ptr mem) && c <= 3 -> (MOVBUloadshiftLL ptr idx [c] mem)
+(MOVBloadidx ptr (SLLconst idx [c]) mem) && c <= 3 -> (MOVBloadshiftLL ptr idx [c] mem)
+(MOVBloadidx (SLLconst idx [c]) ptr mem) && c <= 3 -> (MOVBloadshiftLL ptr idx [c] mem)
+(LoadOnce32idx ptr (SLLconst idx [c]) mem) && c <= 3 -> (LoadOnce32shiftLL ptr idx [c] mem)
+(LoadOnce16idx ptr (SLLconst idx [c]) mem) && c <= 3 -> (LoadOnce16shiftLL ptr idx [c] mem)
+(LoadOnce8idx ptr (SLLconst idx [c]) mem) && c <= 3 -> (LoadOnce8shiftLL ptr idx [c] mem)
+
+(MOVWstoreidx ptr (SLLconst idx [c]) val mem) && c <= 3 -> (MOVWstoreshiftLL ptr idx [c] val mem)
+(MOVWstoreidx (SLLconst idx [c]) ptr val mem) && c <= 3 -> (MOVWstoreshiftLL ptr idx [c] val mem)
+(MOVHstoreidx ptr (SLLconst idx [c]) val mem) && c <= 3 -> (MOVHstoreshiftLL ptr idx [c] val mem)
+(MOVHstoreidx (SLLconst idx [c]) ptr val mem) && c <= 3 -> (MOVHstoreshiftLL ptr idx [c] val mem)
+(MOVBstoreidx ptr (SLLconst idx [c]) val mem) && c <= 3 -> (MOVBstoreshiftLL ptr idx [c] val mem)
+(MOVBstoreidx (SLLconst idx [c]) ptr val mem) && c <= 3 -> (MOVBstoreshiftLL ptr idx [c] val mem)
+(StoreOnce32idx ptr (SLLconst idx [c]) val mem) && c <= 3 -> (StoreOnce32shiftLL ptr idx [c] val mem)
+(StoreOnce32idx (SLLconst idx [c]) ptr val mem) && c <= 3 -> (StoreOnce32shiftLL ptr idx [c] val mem)
+(StoreOnce16idx ptr (SLLconst idx [c]) val mem) && c <= 3 -> (StoreOnce16shiftLL ptr idx [c] val mem)
+(StoreOnce16idx (SLLconst idx [c]) ptr val mem) && c <= 3 -> (StoreOnce16shiftLL ptr idx [c] val mem)
+(StoreOnce8idx ptr (SLLconst idx [c]) val mem) && c <= 3 -> (StoreOnce8shiftLL ptr idx [c] val mem)
+(StoreOnce8idx (SLLconst idx [c]) ptr val mem) && c <= 3 -> (StoreOnce8shiftLL ptr idx [c] val mem)
+
+(MOVWloadshiftLL ptr (MOVWconst [c]) [d] mem) -> (MOVWload [int64(uint32(c)<<uint64(d))] ptr mem)
+
+(MOVWstoreshiftLL ptr (MOVWconst [c]) [d] val mem) -> (MOVWstore [int64(uint32(c)<<uint64(d))] ptr val mem)
+
+// generic simplifications
+(ADD x (RSBconst [0] y)) -> (SUB x y)
+(ADD <t> (RSBconst [c] x) (RSBconst [d] y)) -> (RSBconst [c+d] (ADD <t> x y))
+(SUB x x) -> (MOVWconst [0])
+(RSB x x) -> (MOVWconst [0])
+(AND x x) -> x
+(OR x x) -> x
+(XOR x x) -> (MOVWconst [0])
+(BIC x x) -> (MOVWconst [0])
+
+(ADD (MUL x y) a) -> (MULA x y a)
+(SUB a (MUL x y)) -> (MULS x y a)
+(RSB (MUL x y) a) -> (MULS x y a)
+
+(NEGF (MULF x y)) -> (NMULF x y)
+(NEGD (MULD x y)) -> (NMULD x y)
+(MULF (NEGF x) y) -> (NMULF x y)
+(MULD (NEGD x) y) -> (NMULD x y)
+(NMULF (NEGF x) y) -> (MULF x y)
+(NMULD (NEGD x) y) -> (MULD x y)
+
+// the result will overwrite the addend, since they are in the same register
+(ADDF a (MULF x y)) && a.Uses == 1 -> (MULAF a x y)
+(ADDF a (NMULF x y)) && a.Uses == 1 -> (MULSF a x y)
+(ADDD a (MULD x y)) && a.Uses == 1 -> (MULAD a x y)
+(ADDD a (NMULD x y)) && a.Uses == 1 -> (MULSD a x y)
+(SUBF a (MULF x y)) && a.Uses == 1 -> (MULSF a x y)
+(SUBF a (NMULF x y)) && a.Uses == 1 -> (MULAF a x y)
+(SUBD a (MULD x y)) && a.Uses == 1 -> (MULSD a x y)
+(SUBD a (NMULD x y)) && a.Uses == 1 -> (MULAD a x y)
+
+(AND x (MVN y)) -> (BIC x y)
+
+// simplification with *shift ops
+(SUBshiftLL x (SLLconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(SUBshiftRL x (SRLconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(SUBshiftRA x (SRAconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(RSBshiftLL x (SLLconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(RSBshiftRL x (SRLconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(RSBshiftRA x (SRAconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(ANDshiftLL x y:(SLLconst x [c]) [d]) && c==d -> y
+(ANDshiftRL x y:(SRLconst x [c]) [d]) && c==d -> y
+(ANDshiftRA x y:(SRAconst x [c]) [d]) && c==d -> y
+(ORshiftLL x y:(SLLconst x [c]) [d]) && c==d -> y
+(ORshiftRL x y:(SRLconst x [c]) [d]) && c==d -> y
+(ORshiftRA x y:(SRAconst x [c]) [d]) && c==d -> y
+(XORshiftLL x (SLLconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(XORshiftRL x (SRLconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(XORshiftRA x (SRAconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(BICshiftLL x (SLLconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(BICshiftRL x (SRLconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(BICshiftRA x (SRAconst x [c]) [d]) && c==d -> (MOVWconst [0])
+(AND x (MVNshiftLL y [c])) -> (BICshiftLL x y [c])
+(AND x (MVNshiftRL y [c])) -> (BICshiftRL x y [c])
+(AND x (MVNshiftRA y [c])) -> (BICshiftRA x y [c])
+
+// floating point optimizations
+(CMPF x (MOVFconst [0])) -> (CMPF0 x)
+(CMPD x (MOVDconst [0])) -> (CMPD0 x)
+
+// bit extraction
+(SRAconst (SLLconst x [c]) [d]) && uint64(d)>=uint64(c) && uint64(d)<=31 -> (BFX [(d-c)|(32-d)<<8] x)
+(SRLconst (SLLconst x [c]) [d]) && uint64(d)>=uint64(c) && uint64(d)<=31 -> (BFXU [(d-c)|(32-d)<<8] x)
+
+// comparison simplification
+(CMP x (RSBconst [0] y)) -> (CMN x y)
+(CMN x (RSBconst [0] y)) -> (CMP x y)
+(EQ (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 -> (EQ (CMP x y) yes no)
+(EQ (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 -> (EQ (CMP a (MUL <x.Type> x y)) yes no)
+(EQ (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 -> (EQ (CMPconst [c] x) yes no)
+(EQ (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 -> (EQ (CMPshiftLL x y [c]) yes no)
+(EQ (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 -> (EQ (CMPshiftRL x y [c]) yes no)
+(EQ (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 -> (EQ (CMPshiftRA x y [c]) yes no)
+(NE (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 -> (NE (CMP x y) yes no)
+(NE (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 -> (NE (CMP a (MUL <x.Type> x y)) yes no)
+(NE (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 -> (NE (CMPconst [c] x) yes no)
+(NE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 -> (NE (CMPshiftLL x y [c]) yes no)
+(NE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 -> (NE (CMPshiftRL x y [c]) yes no)
+(NE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 -> (NE (CMPshiftRA x y [c]) yes no)
+(EQ (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 -> (EQ (CMN x y) yes no)
+(EQ (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 -> (EQ (CMN a (MUL <x.Type> x y)) yes no)
+(EQ (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 -> (EQ (CMNconst [c] x) yes no)
+(EQ (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 -> (EQ (CMNshiftLL x y [c]) yes no)
+(EQ (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 -> (EQ (CMNshiftRL x y [c]) yes no)
+(EQ (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 -> (EQ (CMNshiftRA x y [c]) yes no)
+(NE (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 -> (NE (CMN x y) yes no)
+(NE (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 -> (NE (CMN a (MUL <x.Type> x y)) yes no)
+(NE (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 -> (NE (CMNconst [c] x) yes no)
+(NE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 -> (NE (CMNshiftLL x y [c]) yes no)
+(NE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 -> (NE (CMNshiftRL x y [c]) yes no)
+(NE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 -> (NE (CMNshiftRA x y [c]) yes no)
+(EQ (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 -> (EQ (TST x y) yes no)
+(EQ (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 -> (EQ (TSTconst [c] x) yes no)
+(EQ (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 -> (EQ (TSTshiftLL x y [c]) yes no)
+(EQ (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 -> (EQ (TSTshiftRL x y [c]) yes no)
+(EQ (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 -> (EQ (TSTshiftRA x y [c]) yes no)
+(NE (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 -> (NE (TST x y) yes no)
+(NE (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 -> (NE (TSTconst [c] x) yes no)
+(NE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 -> (NE (TSTshiftLL x y [c]) yes no)
+(NE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 -> (NE (TSTshiftRL x y [c]) yes no)
+(NE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 -> (NE (TSTshiftRA x y [c]) yes no)
+(EQ (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 -> (EQ (TEQ x y) yes no)
+(EQ (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 -> (EQ (TEQconst [c] x) yes no)
+(EQ (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 -> (EQ (TEQshiftLL x y [c]) yes no)
+(EQ (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 -> (EQ (TEQshiftRL x y [c]) yes no)
+(EQ (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 -> (EQ (TEQshiftRA x y [c]) yes no)
+(NE (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 -> (NE (TEQ x y) yes no)
+(NE (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 -> (NE (TEQconst [c] x) yes no)
+(NE (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 -> (NE (TEQshiftLL x y [c]) yes no)
+(NE (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 -> (NE (TEQshiftRL x y [c]) yes no)
+(NE (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 -> (NE (TEQshiftRA x y [c]) yes no)
+(LT (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 -> (LT (CMP x y) yes no)
+(LT (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 -> (LT (CMP a (MUL <x.Type> x y)) yes no)
+(LT (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 -> (LT (CMPconst [c] x) yes no)
+(LT (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 -> (LT (CMPshiftLL x y [c]) yes no)
+(LT (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 -> (LT (CMPshiftRL x y [c]) yes no)
+(LT (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 -> (LT (CMPshiftRA x y [c]) yes no)
+(LE (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 -> (LE (CMP x y) yes no)
+(LE (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 -> (LE (CMP a (MUL <x.Type> x y)) yes no)
+(LE (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 -> (LE (CMPconst [c] x) yes no)
+(LE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 -> (LE (CMPshiftLL x y [c]) yes no)
+(LE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 -> (LE (CMPshiftRL x y [c]) yes no)
+(LE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 -> (LE (CMPshiftRA x y [c]) yes no)
+(LT (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 -> (LT (CMN x y) yes no)
+(LT (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 -> (LT (CMN a (MUL <x.Type> x y)) yes no)
+(LT (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 -> (LT (CMNconst [c] x) yes no)
+(LT (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 -> (LT (CMNshiftLL x y [c]) yes no)
+(LT (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 -> (LT (CMNshiftRL x y [c]) yes no)
+(LT (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 -> (LT (CMNshiftRA x y [c]) yes no)
+(LE (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 -> (LE (CMN x y) yes no)
+(LE (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 -> (LE (CMN a (MUL <x.Type> x y)) yes no)
+(LE (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1  -> (LE (CMNconst [c] x) yes no)
+(LE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 -> (LE (CMNshiftLL x y [c]) yes no)
+(LE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 -> (LE (CMNshiftRL x y [c]) yes no)
+(LE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 -> (LE (CMNshiftRA x y [c]) yes no)
+(LT (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 -> (LT (TST x y) yes no)
+(LT (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 -> (LT (TSTconst [c] x) yes no)
+(LT (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 -> (LT (TSTshiftLL x y [c]) yes no)
+(LT (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 -> (LT (TSTshiftRL x y [c]) yes no)
+(LT (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 -> (LT (TSTshiftRA x y [c]) yes no)
+(LE (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 -> (LE (TST x y) yes no)
+(LE (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 -> (LE (TSTconst [c] x) yes no)
+(LE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 -> (LE (TSTshiftLL x y [c]) yes no)
+(LE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 -> (LE (TSTshiftRL x y [c]) yes no)
+(LE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 -> (LE (TSTshiftRA x y [c]) yes no)
+(LT (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 -> (LT (TEQ x y) yes no)
+(LT (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 -> (LT (TEQconst [c] x) yes no)
+(LT (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 -> (LT (TEQshiftLL x y [c]) yes no)
+(LT (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 -> (LT (TEQshiftRL x y [c]) yes no)
+(LT (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 -> (LT (TEQshiftRA x y [c]) yes no)
+(LE (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 -> (LE (TEQ x y) yes no)
+(LE (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1  -> (LE (TEQconst [c] x) yes no)
+(LE (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 -> (LE (TEQshiftLL x y [c]) yes no)
+(LE (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 -> (LE (TEQshiftRL x y [c]) yes no)
+(LE (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 -> (LE (TEQshiftRA x y [c]) yes no)
+(GT (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 -> (GT (CMP x y) yes no)
+(GT (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 -> (GT (CMP a (MUL <x.Type> x y)) yes no)
+(GT (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 -> (GT (CMPconst [c] x) yes no)
+(GT (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 -> (GT (CMPshiftLL x y [c]) yes no)
+(GT (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 -> (GT (CMPshiftRL x y [c]) yes no)
+(GT (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 -> (GT (CMPshiftRA x y [c]) yes no)
+(GE (CMPconst [0] l:(SUB x y)) yes no) && l.Uses==1 -> (GE (CMP x y) yes no)
+(GE (CMPconst [0] l:(MULS x y a)) yes no) && l.Uses==1 -> (GE (CMP a (MUL <x.Type> x y)) yes no)
+(GE (CMPconst [0] l:(SUBconst [c] x)) yes no) && l.Uses==1 -> (GE (CMPconst [c] x) yes no)
+(GE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no) && l.Uses==1 -> (GE (CMPshiftLL x y [c]) yes no)
+(GE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no) && l.Uses==1 -> (GE (CMPshiftRL x y [c]) yes no)
+(GE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no) && l.Uses==1 -> (GE (CMPshiftRA x y [c]) yes no)
+(GT (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 -> (GT (CMN x y) yes no)
+(GT (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 -> (GT (CMNconst [c] x) yes no)
+(GT (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 -> (GT (CMNshiftLL x y [c]) yes no)
+(GT (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 -> (GT (CMNshiftRL x y [c]) yes no)
+(GT (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 -> (GT (CMNshiftRA x y [c]) yes no)
+(GE (CMPconst [0] l:(ADD x y)) yes no) && l.Uses==1 -> (GE (CMN x y) yes no)
+(GE (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 -> (GE (CMN a (MUL <x.Type> x y)) yes no)
+(GE (CMPconst [0] l:(ADDconst [c] x)) yes no) && l.Uses==1 -> (GE (CMNconst [c] x) yes no)
+(GE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no) && l.Uses==1 -> (GE (CMNshiftLL x y [c]) yes no)
+(GE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no) && l.Uses==1 -> (GE (CMNshiftRL x y [c]) yes no)
+(GE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no) && l.Uses==1 -> (GE (CMNshiftRA x y [c]) yes no)
+(GT (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 -> (GT (TST x y) yes no)
+(GT (CMPconst [0] l:(MULA x y a)) yes no) && l.Uses==1 -> (GT (CMN a (MUL <x.Type> x y)) yes no)
+(GT (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 -> (GT (TSTconst [c] x) yes no)
+(GT (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 -> (GT (TSTshiftLL x y [c]) yes no)
+(GT (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 -> (GT (TSTshiftRL x y [c]) yes no)
+(GT (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 -> (GT (TSTshiftRA x y [c]) yes no)
+(GE (CMPconst [0] l:(AND x y)) yes no) && l.Uses==1 -> (GE (TST x y) yes no)
+(GE (CMPconst [0] l:(ANDconst [c] x)) yes no) && l.Uses==1 -> (GE (TSTconst [c] x) yes no)
+(GE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no) && l.Uses==1 -> (GE (TSTshiftLL x y [c]) yes no)
+(GE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no) && l.Uses==1 -> (GE (TSTshiftRL x y [c]) yes no)
+(GE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no) && l.Uses==1 -> (GE (TSTshiftRA x y [c]) yes no)
+(GT (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 -> (GT (TEQ x y) yes no)
+(GT (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 -> (GT (TEQconst [c] x) yes no)
+(GT (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 -> (GT (TEQshiftLL x y [c]) yes no)
+(GT (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 -> (GT (TEQshiftRL x y [c]) yes no)
+(GT (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 -> (GT (TEQshiftRA x y [c]) yes no)
+(GE (CMPconst [0] l:(XOR x y)) yes no) && l.Uses==1 -> (GE (TEQ x y) yes no)
+(GE (CMPconst [0] l:(XORconst [c] x)) yes no) && l.Uses==1 -> (GE (TEQconst [c] x) yes no)
+(GE (CMPconst [0] l:(XORshiftLL x y [c])) yes no) && l.Uses==1 -> (GE (TEQshiftLL x y [c]) yes no)
+(GE (CMPconst [0] l:(XORshiftRL x y [c])) yes no) && l.Uses==1 -> (GE (TEQshiftRL x y [c]) yes no)
+(GE (CMPconst [0] l:(XORshiftRA x y [c])) yes no) && l.Uses==1 -> (GE (TEQshiftRA x y [c]) yes no)
+
+(MOVBUload [off] {sym} (SB) _) && symIsRO(sym) -> (MOVWconst [int64(read8(sym, off))])
+(MOVHUload [off] {sym} (SB) _) && symIsRO(sym) -> (MOVWconst [int64(read16(sym, off, config.BigEndian))])
+(MOVWload [off] {sym} (SB) _) && symIsRO(sym) -> (MOVWconst [int64(int32(read32(sym, off, config.BigEndian)))])
diff --git a/src/cmd/compile/internal/ssa/gen/ThumbOps.go b/src/cmd/compile/internal/ssa/gen/ThumbOps.go
new file mode 100644
index 0000000000..67a51067dc
--- /dev/null
+++ b/src/cmd/compile/internal/ssa/gen/ThumbOps.go
@@ -0,0 +1,551 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build ignore
+
+package main
+
+import "strings"
+
+// Notes:
+//  - Integer types live in the low portion of registers. Upper portions are junk.
+//  - Boolean types use the low-order byte of a register. 0=false, 1=true.
+//    Upper bytes are junk.
+//  - *const instructions may use a constant larger than the instruction can encode.
+//    In this case the assembler expands to multiple instructions and uses tmp
+//    register (R7).
+
+// Suffixes encode the bit width of various instructions.
+// W (word)      = 32 bit
+// H (half word) = 16 bit
+// HU            = 16 bit unsigned
+// B (byte)      = 8 bit
+// BU            = 8 bit unsigned
+// F (float)     = 32 bit float
+// D (double)    = 64 bit float
+
+var regNamesThumb = []string{
+	"R0",
+	"R1",
+	"R2",
+	"R3",
+	"R4",
+	"R5",
+	"R6",
+	"R7", // tmp
+	"R8",
+	"R9",
+	"g", // aka R10
+	"R11",
+	"R12",
+	"SP",  // aka R13
+	"R14", // link
+	"R15", // pc
+
+	"F0",
+	"F1",
+	"F2",
+	"F3",
+	"F4",
+	"F5",
+	"F6",
+	"F7",
+	"F8",
+	"F9",
+	"F10",
+	"F11",
+	"F12",
+	"F13",
+	"F14",
+	"F15", // tmp
+
+	// pseudo-registers
+	"SB",
+}
+
+func init() {
+	// Make map from reg names to reg integers.
+	if len(regNamesThumb) > 64 {
+		panic("too many registers")
+	}
+	num := map[string]int{}
+	for i, name := range regNamesThumb {
+		num[name] = i
+	}
+	buildReg := func(s string) regMask {
+		m := regMask(0)
+		for _, r := range strings.Split(s, " ") {
+			if n, ok := num[r]; ok {
+				m |= regMask(1) << uint(n)
+				continue
+			}
+			panic("register " + r + " not found")
+		}
+		return m
+	}
+
+	// Common individual register masks
+	var (
+		gp         = buildReg("R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14")
+		gpg        = gp | buildReg("g")
+		gpsp       = gp | buildReg("SP")
+		gpspg      = gpg | buildReg("SP")
+		gpspsbg    = gpspg | buildReg("SB")
+		fp         = buildReg("F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15")
+		callerSave = gp | fp | buildReg("g") // runtime.setg (and anything calling it) may clobber g
+		r0         = buildReg("R0")
+		r1         = buildReg("R1")
+		r2         = buildReg("R2")
+		r3         = buildReg("R3")
+		r4         = buildReg("R4")
+	)
+	// Common regInfo
+	var (
+		gp01      = regInfo{inputs: nil, outputs: []regMask{gp}}
+		gp11      = regInfo{inputs: []regMask{gpg}, outputs: []regMask{gp}}
+		gp11carry = regInfo{inputs: []regMask{gpg}, outputs: []regMask{gp, 0}}
+		gp11sp    = regInfo{inputs: []regMask{gpspg}, outputs: []regMask{gp}}
+		gp1flags  = regInfo{inputs: []regMask{gpg}}
+		gp1flags1 = regInfo{inputs: []regMask{gp}, outputs: []regMask{gp}}
+		gp21      = regInfo{inputs: []regMask{gpg, gpg}, outputs: []regMask{gp}}
+		gp21carry = regInfo{inputs: []regMask{gpg, gpg}, outputs: []regMask{gp, 0}}
+		gp2flags  = regInfo{inputs: []regMask{gpg, gpg}}
+		gp2flags1 = regInfo{inputs: []regMask{gp, gp}, outputs: []regMask{gp}}
+		gp22      = regInfo{inputs: []regMask{gpg, gpg}, outputs: []regMask{gp, gp}}
+		gp31      = regInfo{inputs: []regMask{gp, gp, gp}, outputs: []regMask{gp}}
+		gpload    = regInfo{inputs: []regMask{gpspsbg}, outputs: []regMask{gp}}
+		gpstore   = regInfo{inputs: []regMask{gpspsbg, gpg}}
+		gp2load   = regInfo{inputs: []regMask{gpspsbg, gpg}, outputs: []regMask{gp}}
+		gp2store  = regInfo{inputs: []regMask{gpspsbg, gpg, gpg}}
+		fp01      = regInfo{inputs: nil, outputs: []regMask{fp}}
+		fp11      = regInfo{inputs: []regMask{fp}, outputs: []regMask{fp}}
+		fp1flags  = regInfo{inputs: []regMask{fp}}
+		fpgp      = regInfo{inputs: []regMask{fp}, outputs: []regMask{gp}, clobbers: buildReg("F15")} // int-float conversion uses F15 as tmp
+		gpfp      = regInfo{inputs: []regMask{gp}, outputs: []regMask{fp}, clobbers: buildReg("F15")}
+		fp21      = regInfo{inputs: []regMask{fp, fp}, outputs: []regMask{fp}}
+		fp31      = regInfo{inputs: []regMask{fp, fp, fp}, outputs: []regMask{fp}}
+		fp2flags  = regInfo{inputs: []regMask{fp, fp}}
+		fpload    = regInfo{inputs: []regMask{gpspsbg}, outputs: []regMask{fp}}
+		fpstore   = regInfo{inputs: []regMask{gpspsbg, fp}}
+		readflags = regInfo{inputs: nil, outputs: []regMask{gp}}
+	)
+	// Thumb instruction can be encoded as 32 or 16 bit - assembler decides. Unlike arm, many thumb
+	// instruction encodigs don't support .S bit. Especially all 16-bit data processing instructions
+	// don't support it and can clobber flags if outside of IT block.
+	//
+	// For example, for ADD instruction assembler chooses shortest encoding that may but may not set
+	// flags. For ADD.S it chooses shortest encoding that sets flags according to the result. For
+	// ADD.P assembler generates shortest encoding that don't touch flags.
+	//
+	// Current compiler have no idea about 16/32 bit instructions, IT blocks, .P modifier, so any data
+	// processing instructions tha can be 16-bit encoded should be marked clobberFlags. Even *const
+	// instructions that have no direct 16-bit encoding can be encoded by assebler as pair of
+	// `load litoffset(PC), REGTMP; op REGTMP, Rdn` and can clober flags.
+	//
+	// TODO: Use .P modifier if appropriate.
+	ops := []opData{
+		// binary ops
+		{name: "ADD", argLength: 2, reg: gp21, asm: "ADD", commutative: true, clobberFlags: true},      // arg0 + arg1
+		{name: "ADDconst", argLength: 1, reg: gp11sp, asm: "ADD", aux: "Int32", clobberFlags: true},    // arg0 + auxInt
+		{name: "SUB", argLength: 2, reg: gp21, asm: "SUB", clobberFlags: true},                         // arg0 - arg1
+		{name: "SUBconst", argLength: 1, reg: gp11, asm: "SUB", aux: "Int32", clobberFlags: true},      // arg0 - auxInt
+		{name: "RSB", argLength: 2, reg: gp21, asm: "RSB", clobberFlags: true},                         // arg1 - arg0
+		{name: "RSBconst", argLength: 1, reg: gp11, asm: "RSB", aux: "Int32", clobberFlags: true},      // auxInt - arg0
+		{name: "MUL", argLength: 2, reg: gp21, asm: "MUL", commutative: true, clobberFlags: true},      // arg0 * arg1
+		{name: "HMUL", argLength: 2, reg: gp21, asm: "MULL", commutative: true},                        // (arg0 * arg1) >> 32, signed
+		{name: "HMULU", argLength: 2, reg: gp21, asm: "MULLU", commutative: true},                      // (arg0 * arg1) >> 32, unsigned
+		{name: "DIV", argLength: 2, reg: gp21, asm: "DIV"},                                             // arg0 / arg1, signed
+		{name: "DIVU", argLength: 2, reg: gp21, asm: "DIVU"},                                           // arg0 / arg1, unsigned
+		{name: "ADDS", argLength: 2, reg: gp21carry, asm: "ADD", commutative: true},                    // arg0 + arg1, set carry flag
+		{name: "ADDSconst", argLength: 1, reg: gp11carry, asm: "ADD", aux: "Int32"},                    // arg0 + auxInt, set carry flag
+		{name: "ADC", argLength: 3, reg: gp2flags1, asm: "ADC", commutative: true, clobberFlags: true}, // arg0 + arg1 + carry, arg2=flags
+		{name: "ADCconst", argLength: 2, reg: gp1flags1, asm: "ADC", aux: "Int32", clobberFlags: true}, // arg0 + auxInt + carry, arg1=flags
+		{name: "SUBS", argLength: 2, reg: gp21carry, asm: "SUB"},                                       // arg0 - arg1, set carry flag
+		{name: "SUBSconst", argLength: 1, reg: gp11carry, asm: "SUB", aux: "Int32"},                    // arg0 - auxInt, set carry flag
+		{name: "RSBSconst", argLength: 1, reg: gp11carry, asm: "RSB", aux: "Int32"},                    // auxInt - arg0, set carry flag
+		{name: "SBC", argLength: 3, reg: gp2flags1, asm: "SBC", clobberFlags: true},                    // arg0 - arg1 - carry, arg2=flags
+		{name: "SBCconst", argLength: 2, reg: gp1flags1, asm: "SBC", aux: "Int32", clobberFlags: true}, // arg0 - auxInt - carry, arg1=flags
+
+		{name: "MULLU", argLength: 2, reg: gp22, asm: "MULLU", commutative: true}, // arg0 * arg1, high 32 bits in out0, low 32 bits in out1
+		{name: "MULA", argLength: 3, reg: gp31, asm: "MULA"},                      // arg0 * arg1 + arg2
+		{name: "MULS", argLength: 3, reg: gp31, asm: "MULS"},                      // arg2 - arg0 * arg1
+
+		{name: "ADDF", argLength: 2, reg: fp21, asm: "ADDF", commutative: true},   // arg0 + arg1
+		{name: "ADDD", argLength: 2, reg: fp21, asm: "ADDD", commutative: true},   // arg0 + arg1
+		{name: "SUBF", argLength: 2, reg: fp21, asm: "SUBF"},                      // arg0 - arg1
+		{name: "SUBD", argLength: 2, reg: fp21, asm: "SUBD"},                      // arg0 - arg1
+		{name: "MULF", argLength: 2, reg: fp21, asm: "MULF", commutative: true},   // arg0 * arg1
+		{name: "MULD", argLength: 2, reg: fp21, asm: "MULD", commutative: true},   // arg0 * arg1
+		{name: "NMULF", argLength: 2, reg: fp21, asm: "NMULF", commutative: true}, // -(arg0 * arg1)
+		{name: "NMULD", argLength: 2, reg: fp21, asm: "NMULD", commutative: true}, // -(arg0 * arg1)
+		{name: "DIVF", argLength: 2, reg: fp21, asm: "DIVF"},                      // arg0 / arg1
+		{name: "DIVD", argLength: 2, reg: fp21, asm: "DIVD"},                      // arg0 / arg1
+
+		{name: "MULAF", argLength: 3, reg: fp31, asm: "MULAF", resultInArg0: true}, // arg0 + (arg1 * arg2)
+		{name: "MULAD", argLength: 3, reg: fp31, asm: "MULAD", resultInArg0: true}, // arg0 + (arg1 * arg2)
+		{name: "MULSF", argLength: 3, reg: fp31, asm: "MULSF", resultInArg0: true}, // arg0 - (arg1 * arg2)
+		{name: "MULSD", argLength: 3, reg: fp31, asm: "MULSD", resultInArg0: true}, // arg0 - (arg1 * arg2)
+
+		{name: "AND", argLength: 2, reg: gp21, asm: "AND", commutative: true, clobberFlags: true}, // arg0 & arg1
+		{name: "ANDconst", argLength: 1, reg: gp11, asm: "AND", aux: "Int32", clobberFlags: true}, // arg0 & auxInt
+		{name: "OR", argLength: 2, reg: gp21, asm: "ORR", commutative: true, clobberFlags: true},  // arg0 | arg1
+		{name: "ORconst", argLength: 1, reg: gp11, asm: "ORR", aux: "Int32", clobberFlags: true},  // arg0 | auxInt
+		{name: "XOR", argLength: 2, reg: gp21, asm: "EOR", commutative: true, clobberFlags: true}, // arg0 ^ arg1
+		{name: "XORconst", argLength: 1, reg: gp11, asm: "EOR", aux: "Int32", clobberFlags: true}, // arg0 ^ auxInt
+		{name: "BIC", argLength: 2, reg: gp21, asm: "BIC", clobberFlags: true},                    // arg0 &^ arg1
+		{name: "BICconst", argLength: 1, reg: gp11, asm: "BIC", aux: "Int32", clobberFlags: true}, // arg0 &^ auxInt
+
+		// bit extractio1n, AuxInt = Width<<8 | LSB
+		{name: "BFX", argLength: 1, reg: gp11, asm: "BFX", aux: "Int32"},   // extract W bits from bit L in arg0, then signed extend
+		{name: "BFXU", argLength: 1, reg: gp11, asm: "BFXU", aux: "Int32"}, // extract W bits from bit L in arg0, then unsigned extend
+
+		// unary ops
+		{name: "MVN", argLength: 1, reg: gp11, asm: "MVN", clobberFlags: true}, // ^arg0
+
+		{name: "NEGF", argLength: 1, reg: fp11, asm: "NEGF"},   // -arg0, float32
+		{name: "NEGD", argLength: 1, reg: fp11, asm: "NEGD"},   // -arg0, float64
+		{name: "SQRTD", argLength: 1, reg: fp11, asm: "SQRTD"}, // sqrt(arg0), float64
+
+		{name: "CLZ", argLength: 1, reg: gp11, asm: "CLZ"},     // count leading zero
+		{name: "REV", argLength: 1, reg: gp11, asm: "REV"},     // reverse byte order
+		{name: "REV16", argLength: 1, reg: gp11, asm: "REV16"}, // reverse byte order in 16-bit halfwords
+		{name: "RBIT", argLength: 1, reg: gp11, asm: "RBIT"},   // reverse bit order
+
+		// shifts
+		{name: "SLL", argLength: 2, reg: gp21, asm: "SLL", clobberFlags: true},                    // arg0 << arg1, shift amount is mod 256
+		{name: "SLLconst", argLength: 1, reg: gp11, asm: "SLL", aux: "Int32", clobberFlags: true}, // arg0 << auxInt
+		{name: "SRL", argLength: 2, reg: gp21, asm: "SRL", clobberFlags: true},                    // arg0 >> arg1, unsigned, shift amount is mod 256
+		{name: "SRLconst", argLength: 1, reg: gp11, asm: "SRL", aux: "Int32", clobberFlags: true}, // arg0 >> auxInt, unsigned
+		{name: "SRA", argLength: 2, reg: gp21, asm: "SRA", clobberFlags: true},                    // arg0 >> arg1, signed, shift amount is mod 256
+		{name: "SRAconst", argLength: 1, reg: gp11, asm: "SRA", aux: "Int32", clobberFlags: true}, // arg0 >> auxInt, signed
+		{name: "SRRconst", argLength: 1, reg: gp11, aux: "Int32"},                                 // arg0 right rotate by auxInt bits
+
+		{name: "ADDshiftLL", argLength: 2, reg: gp21, asm: "ADD", aux: "Int32"}, // arg0 + arg1<<auxInt
+		{name: "ADDshiftRL", argLength: 2, reg: gp21, asm: "ADD", aux: "Int32"}, // arg0 + arg1>>auxInt, unsigned shift
+		{name: "ADDshiftRA", argLength: 2, reg: gp21, asm: "ADD", aux: "Int32"}, // arg0 + arg1>>auxInt, signed shift
+		{name: "SUBshiftLL", argLength: 2, reg: gp21, asm: "SUB", aux: "Int32"}, // arg0 - arg1<<auxInt
+		{name: "SUBshiftRL", argLength: 2, reg: gp21, asm: "SUB", aux: "Int32"}, // arg0 - arg1>>auxInt, unsigned shift
+		{name: "SUBshiftRA", argLength: 2, reg: gp21, asm: "SUB", aux: "Int32"}, // arg0 - arg1>>auxInt, signed shift
+		{name: "RSBshiftLL", argLength: 2, reg: gp21, asm: "RSB", aux: "Int32"}, // arg1<<auxInt - arg0
+		{name: "RSBshiftRL", argLength: 2, reg: gp21, asm: "RSB", aux: "Int32"}, // arg1>>auxInt - arg0, unsigned shift
+		{name: "RSBshiftRA", argLength: 2, reg: gp21, asm: "RSB", aux: "Int32"}, // arg1>>auxInt - arg0, signed shift
+		{name: "ANDshiftLL", argLength: 2, reg: gp21, asm: "AND", aux: "Int32"}, // arg0 & (arg1<<auxInt)
+		{name: "ANDshiftRL", argLength: 2, reg: gp21, asm: "AND", aux: "Int32"}, // arg0 & (arg1>>auxInt), unsigned shift
+		{name: "ANDshiftRA", argLength: 2, reg: gp21, asm: "AND", aux: "Int32"}, // arg0 & (arg1>>auxInt), signed shift
+		{name: "ORshiftLL", argLength: 2, reg: gp21, asm: "ORR", aux: "Int32"},  // arg0 | arg1<<auxInt
+		{name: "ORshiftRL", argLength: 2, reg: gp21, asm: "ORR", aux: "Int32"},  // arg0 | arg1>>auxInt, unsigned shift
+		{name: "ORshiftRA", argLength: 2, reg: gp21, asm: "ORR", aux: "Int32"},  // arg0 | arg1>>auxInt, signed shift
+		{name: "XORshiftLL", argLength: 2, reg: gp21, asm: "EOR", aux: "Int32"}, // arg0 ^ arg1<<auxInt
+		{name: "XORshiftRL", argLength: 2, reg: gp21, asm: "EOR", aux: "Int32"}, // arg0 ^ arg1>>auxInt, unsigned shift
+		{name: "XORshiftRA", argLength: 2, reg: gp21, asm: "EOR", aux: "Int32"}, // arg0 ^ arg1>>auxInt, signed shift
+		{name: "XORshiftRR", argLength: 2, reg: gp21, asm: "EOR", aux: "Int32"}, // arg0 ^ (arg1 right rotate by auxInt)
+		{name: "BICshiftLL", argLength: 2, reg: gp21, asm: "BIC", aux: "Int32"}, // arg0 &^ (arg1<<auxInt)
+		{name: "BICshiftRL", argLength: 2, reg: gp21, asm: "BIC", aux: "Int32"}, // arg0 &^ (arg1>>auxInt), unsigned shift
+		{name: "BICshiftRA", argLength: 2, reg: gp21, asm: "BIC", aux: "Int32"}, // arg0 &^ (arg1>>auxInt), signed shift
+		{name: "MVNshiftLL", argLength: 1, reg: gp11, asm: "MVN", aux: "Int32"}, // ^(arg0<<auxInt)
+		{name: "MVNshiftRL", argLength: 1, reg: gp11, asm: "MVN", aux: "Int32"}, // ^(arg0>>auxInt), unsigned shift
+		{name: "MVNshiftRA", argLength: 1, reg: gp11, asm: "MVN", aux: "Int32"}, // ^(arg0>>auxInt), signed shift
+
+		{name: "ADCshiftLL", argLength: 3, reg: gp2flags1, asm: "ADC", aux: "Int32"}, // arg0 + arg1<<auxInt + carry, arg2=flags
+		{name: "ADCshiftRL", argLength: 3, reg: gp2flags1, asm: "ADC", aux: "Int32"}, // arg0 + arg1>>auxInt + carry, unsigned shift, arg2=flags
+		{name: "ADCshiftRA", argLength: 3, reg: gp2flags1, asm: "ADC", aux: "Int32"}, // arg0 + arg1>>auxInt + carry, signed shift, arg2=flags
+		{name: "SBCshiftLL", argLength: 3, reg: gp2flags1, asm: "SBC", aux: "Int32"}, // arg0 - arg1<<auxInt - carry, arg2=flags
+		{name: "SBCshiftRL", argLength: 3, reg: gp2flags1, asm: "SBC", aux: "Int32"}, // arg0 - arg1>>auxInt - carry, unsigned shift, arg2=flags
+		{name: "SBCshiftRA", argLength: 3, reg: gp2flags1, asm: "SBC", aux: "Int32"}, // arg0 - arg1>>auxInt - carry, signed shift, arg2=flags
+
+		{name: "ADDSshiftLL", argLength: 2, reg: gp21carry, asm: "ADD", aux: "Int32"}, // arg0 + arg1<<auxInt, set carry flag
+		{name: "ADDSshiftRL", argLength: 2, reg: gp21carry, asm: "ADD", aux: "Int32"}, // arg0 + arg1>>auxInt, unsigned shift, set carry flag
+		{name: "ADDSshiftRA", argLength: 2, reg: gp21carry, asm: "ADD", aux: "Int32"}, // arg0 + arg1>>auxInt, signed shift, set carry flag
+		{name: "SUBSshiftLL", argLength: 2, reg: gp21carry, asm: "SUB", aux: "Int32"}, // arg0 - arg1<<auxInt, set carry flag
+		{name: "SUBSshiftRL", argLength: 2, reg: gp21carry, asm: "SUB", aux: "Int32"}, // arg0 - arg1>>auxInt, unsigned shift, set carry flag
+		{name: "SUBSshiftRA", argLength: 2, reg: gp21carry, asm: "SUB", aux: "Int32"}, // arg0 - arg1>>auxInt, signed shift, set carry flag
+		{name: "RSBSshiftLL", argLength: 2, reg: gp21carry, asm: "RSB", aux: "Int32"}, // arg1<<auxInt - arg0, set carry flag
+		{name: "RSBSshiftRL", argLength: 2, reg: gp21carry, asm: "RSB", aux: "Int32"}, // arg1>>auxInt - arg0, unsigned shift, set carry flag
+		{name: "RSBSshiftRA", argLength: 2, reg: gp21carry, asm: "RSB", aux: "Int32"}, // arg1>>auxInt - arg0, signed shift, set carry flag
+
+		// comparisons
+		{name: "CMP", argLength: 2, reg: gp2flags, asm: "CMP", typ: "Flags"},                    // arg0 compare to arg1
+		{name: "CMPconst", argLength: 1, reg: gp1flags, asm: "CMP", aux: "Int32", typ: "Flags"}, // arg0 compare to auxInt
+		{name: "CMN", argLength: 2, reg: gp2flags, asm: "CMN", typ: "Flags", commutative: true}, // arg0 compare to -arg1
+		{name: "CMNconst", argLength: 1, reg: gp1flags, asm: "CMN", aux: "Int32", typ: "Flags"}, // arg0 compare to -auxInt
+		{name: "TST", argLength: 2, reg: gp2flags, asm: "TST", typ: "Flags", commutative: true}, // arg0 & arg1 compare to 0
+		{name: "TSTconst", argLength: 1, reg: gp1flags, asm: "TST", aux: "Int32", typ: "Flags"}, // arg0 & auxInt compare to 0
+		{name: "TEQ", argLength: 2, reg: gp2flags, asm: "TEQ", typ: "Flags", commutative: true}, // arg0 ^ arg1 compare to 0
+		{name: "TEQconst", argLength: 1, reg: gp1flags, asm: "TEQ", aux: "Int32", typ: "Flags"}, // arg0 ^ auxInt compare to 0
+		{name: "CMPF", argLength: 2, reg: fp2flags, asm: "CMPF", typ: "Flags"},                  // arg0 compare to arg1, float32
+		{name: "CMPD", argLength: 2, reg: fp2flags, asm: "CMPD", typ: "Flags"},                  // arg0 compare to arg1, float64
+
+		{name: "CMPshiftLL", argLength: 2, reg: gp2flags, asm: "CMP", aux: "Int32", typ: "Flags"}, // arg0 compare to arg1<<auxInt
+		{name: "CMPshiftRL", argLength: 2, reg: gp2flags, asm: "CMP", aux: "Int32", typ: "Flags"}, // arg0 compare to arg1>>auxInt, unsigned shift
+		{name: "CMPshiftRA", argLength: 2, reg: gp2flags, asm: "CMP", aux: "Int32", typ: "Flags"}, // arg0 compare to arg1>>auxInt, signed shift
+		{name: "CMNshiftLL", argLength: 2, reg: gp2flags, asm: "CMN", aux: "Int32", typ: "Flags"}, // arg0 compare to -(arg1<<auxInt)
+		{name: "CMNshiftRL", argLength: 2, reg: gp2flags, asm: "CMN", aux: "Int32", typ: "Flags"}, // arg0 compare to -(arg1>>auxInt), unsigned shift
+		{name: "CMNshiftRA", argLength: 2, reg: gp2flags, asm: "CMN", aux: "Int32", typ: "Flags"}, // arg0 compare to -(arg1>>auxInt), signed shift
+		{name: "TSTshiftLL", argLength: 2, reg: gp2flags, asm: "TST", aux: "Int32", typ: "Flags"}, // arg0 & (arg1<<auxInt) compare to 0
+		{name: "TSTshiftRL", argLength: 2, reg: gp2flags, asm: "TST", aux: "Int32", typ: "Flags"}, // arg0 & (arg1>>auxInt) compare to 0, unsigned shift
+		{name: "TSTshiftRA", argLength: 2, reg: gp2flags, asm: "TST", aux: "Int32", typ: "Flags"}, // arg0 & (arg1>>auxInt) compare to 0, signed shift
+		{name: "TEQshiftLL", argLength: 2, reg: gp2flags, asm: "TEQ", aux: "Int32", typ: "Flags"}, // arg0 ^ (arg1<<auxInt) compare to 0
+		{name: "TEQshiftRL", argLength: 2, reg: gp2flags, asm: "TEQ", aux: "Int32", typ: "Flags"}, // arg0 ^ (arg1>>auxInt) compare to 0, unsigned shift
+		{name: "TEQshiftRA", argLength: 2, reg: gp2flags, asm: "TEQ", aux: "Int32", typ: "Flags"}, // arg0 ^ (arg1>>auxInt) compare to 0, signed shift
+
+		{name: "CMPF0", argLength: 1, reg: fp1flags, asm: "CMPF", typ: "Flags"}, // arg0 compare to 0, float32
+		{name: "CMPD0", argLength: 1, reg: fp1flags, asm: "CMPD", typ: "Flags"}, // arg0 compare to 0, float64
+
+		// moves
+		//{name: "MOVWconst", argLength: 0, reg: gp01, aux: "Int32", asm: "MOVW", typ: "UInt32", rematerializeable: true}, // 32 low bits of auxint
+		{name: "MOVWconst", argLength: 0, reg: gp01, aux: "Int32", asm: "MOVW", typ: "UInt32", clobberFlags: true},         // 32 low bits of auxint
+		{name: "MOVFconst", argLength: 0, reg: fp01, aux: "Float64", asm: "MOVF", typ: "Float32", rematerializeable: true}, // auxint as 64-bit float, convert to 32-bit float
+		{name: "MOVDconst", argLength: 0, reg: fp01, aux: "Float64", asm: "MOVD", typ: "Float64", rematerializeable: true}, // auxint as 64-bit float
+
+		{name: "MOVWaddr", argLength: 1, reg: regInfo{inputs: []regMask{buildReg("SP") | buildReg("SB")}, outputs: []regMask{gp}}, aux: "SymOff", asm: "MOVW", rematerializeable: true, symEffect: "Addr"}, // arg0 + auxInt + aux.(*gc.Sym), arg0=SP/SB
+
+		{name: "MOVBload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVB", typ: "Int8", faultOnNilArg0: true, symEffect: "Read"},     // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVBUload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVBU", typ: "UInt8", faultOnNilArg0: true, symEffect: "Read"},  // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVHload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVH", typ: "Int16", faultOnNilArg0: true, symEffect: "Read"},    // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVHUload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVHU", typ: "UInt16", faultOnNilArg0: true, symEffect: "Read"}, // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVWload", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVW", typ: "UInt32", faultOnNilArg0: true, symEffect: "Read"},   // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVFload", argLength: 2, reg: fpload, aux: "SymOff", asm: "MOVF", typ: "Float32", faultOnNilArg0: true, symEffect: "Read"},  // load from arg0 + auxInt + aux.  arg1=mem.
+		{name: "MOVDload", argLength: 2, reg: fpload, aux: "SymOff", asm: "MOVD", typ: "Float64", faultOnNilArg0: true, symEffect: "Read"},  // load from arg0 + auxInt + aux.  arg1=mem.
+
+		{name: "MOVBstore", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVB", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 1 byte of arg1 to arg0 + auxInt + aux.  arg2=mem.
+		{name: "MOVHstore", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVH", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 2 bytes of arg1 to arg0 + auxInt + aux.  arg2=mem.
+		{name: "MOVWstore", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVW", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 4 bytes of arg1 to arg0 + auxInt + aux.  arg2=mem.
+		{name: "MOVFstore", argLength: 3, reg: fpstore, aux: "SymOff", asm: "MOVF", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 4 bytes of arg1 to arg0 + auxInt + aux.  arg2=mem.
+		{name: "MOVDstore", argLength: 3, reg: fpstore, aux: "SymOff", asm: "MOVD", typ: "Mem", faultOnNilArg0: true, symEffect: "Write"}, // store 8 bytes of arg1 to arg0 + auxInt + aux.  arg2=mem.
+
+		{name: "MOVWloadidx", argLength: 3, reg: gp2load, asm: "MOVW", typ: "UInt32"},                     // load from arg0 + arg1. arg2=mem
+		{name: "MOVHloadidx", argLength: 3, reg: gp2load, asm: "MOVH", typ: "Int16"},                      // load from arg0 + arg1. arg2=mem
+		{name: "MOVHUloadidx", argLength: 3, reg: gp2load, asm: "MOVHU", typ: "UInt16"},                   // load from arg0 + arg1. arg2=mem
+		{name: "MOVBloadidx", argLength: 3, reg: gp2load, asm: "MOVB", typ: "Int8"},                       // load from arg0 + arg1. arg2=mem
+		{name: "MOVBUloadidx", argLength: 3, reg: gp2load, asm: "MOVBU", typ: "UInt8"},                    // load from arg0 + arg1. arg2=mem
+		{name: "MOVWloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVW", aux: "Int32", typ: "UInt32"},   // load from arg0 + arg1<<auxInt. arg2=mem
+		{name: "MOVHloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVH", aux: "Int32", typ: "Int16"},    // load from arg0 + arg1<<auxInt. arg2=mem
+		{name: "MOVHUloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVHU", aux: "Int32", typ: "UInt16"}, // load from arg0 + arg1<<auxInt. arg2=mem
+		{name: "MOVBloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVB", aux: "Int32", typ: "Int8"},     // load from arg0 + arg1<<auxInt. arg2=mem
+		{name: "MOVBUloadshiftLL", argLength: 3, reg: gp2load, asm: "MOVBU", aux: "Int32", typ: "UInt8"},  // load from arg0 + arg1<<auxInt. arg2=mem
+
+		{name: "MOVWstoreidx", argLength: 4, reg: gp2store, asm: "MOVW", typ: "Mem"},                   // store arg2 to arg0 + arg1. arg3=mem
+		{name: "MOVBstoreidx", argLength: 4, reg: gp2store, asm: "MOVB", typ: "Mem"},                   // store arg2 to arg0 + arg1. arg3=mem
+		{name: "MOVHstoreidx", argLength: 4, reg: gp2store, asm: "MOVH", typ: "Mem"},                   // store arg2 to arg0 + arg1. arg3=mem
+		{name: "MOVWstoreshiftLL", argLength: 4, reg: gp2store, asm: "MOVW", aux: "Int32", typ: "Mem"}, // store arg2 to arg0 + arg1<<auxInt. arg3=mem
+		{name: "MOVHstoreshiftLL", argLength: 4, reg: gp2store, asm: "MOVH", aux: "Int32", typ: "Mem"}, // store arg2 to arg0 + arg1<<auxInt. arg3=mem
+		{name: "MOVBstoreshiftLL", argLength: 4, reg: gp2store, asm: "MOVB", aux: "Int32", typ: "Mem"}, // store arg2 to arg0 + arg1<<auxInt. arg3=mem
+
+		{name: "MOVBreg", argLength: 1, reg: gp11, asm: "MOVBS"},  // move from arg0, sign-extended from byte
+		{name: "MOVBUreg", argLength: 1, reg: gp11, asm: "MOVBU"}, // move from arg0, unsign-extended from byte
+		{name: "MOVHreg", argLength: 1, reg: gp11, asm: "MOVHS"},  // move from arg0, sign-extended from half
+		{name: "MOVHUreg", argLength: 1, reg: gp11, asm: "MOVHU"}, // move from arg0, unsign-extended from half
+		{name: "MOVWreg", argLength: 1, reg: gp11, asm: "MOVW"},   // move from arg0
+
+		{name: "MOVWnop", argLength: 1, reg: regInfo{inputs: []regMask{gp}, outputs: []regMask{gp}}, resultInArg0: true}, // nop, return arg0 in same register
+
+		{name: "MOVWF", argLength: 1, reg: gpfp, asm: "MOVWF"},  // int32 -> float32
+		{name: "MOVWD", argLength: 1, reg: gpfp, asm: "MOVWD"},  // int32 -> float64
+		{name: "MOVWUF", argLength: 1, reg: gpfp, asm: "MOVWF"}, // uint32 -> float32, set U bit in the instruction
+		{name: "MOVWUD", argLength: 1, reg: gpfp, asm: "MOVWD"}, // uint32 -> float64, set U bit in the instruction
+		{name: "MOVFW", argLength: 1, reg: fpgp, asm: "MOVFW"},  // float32 -> int32
+		{name: "MOVDW", argLength: 1, reg: fpgp, asm: "MOVDW"},  // float64 -> int32
+		{name: "MOVFWU", argLength: 1, reg: fpgp, asm: "MOVFW"}, // float32 -> uint32, set U bit in the instruction
+		{name: "MOVDWU", argLength: 1, reg: fpgp, asm: "MOVDW"}, // float64 -> uint32, set U bit in the instruction
+		{name: "MOVFD", argLength: 1, reg: fp11, asm: "MOVFD"},  // float32 -> float64
+		{name: "MOVDF", argLength: 1, reg: fp11, asm: "MOVDF"},  // float64 -> float32
+
+		// conditional instructions, for lowering shifts
+		{name: "CMOVWHSconst", argLength: 2, reg: gp1flags1, asm: "MOVW", aux: "Int32", resultInArg0: true}, // replace arg0 w/ const if flags indicates HS, arg1=flags
+		{name: "CMOVWLSconst", argLength: 2, reg: gp1flags1, asm: "MOVW", aux: "Int32", resultInArg0: true}, // replace arg0 w/ const if flags indicates LS, arg1=flags
+		{name: "SRAcond", argLength: 3, reg: gp2flags1, asm: "SRA"},                                         // arg0 >> 31 if flags indicates HS, arg0 >> arg1 otherwise, signed shift, arg2=flags
+
+		// function calls
+		{name: "CALLstatic", argLength: 1, reg: regInfo{clobbers: callerSave}, aux: "SymOff", clobberFlags: true, call: true, symEffect: "None"},                           // call static function aux.(*obj.LSym).  arg0=mem, auxint=argsize, returns mem
+		{name: "CALLclosure", argLength: 3, reg: regInfo{inputs: []regMask{gpsp, buildReg("R11"), 0}, clobbers: callerSave}, aux: "Int64", clobberFlags: true, call: true}, // call function via closure.  arg0=codeptr, arg1=closure, arg2=mem, auxint=argsize, returns mem
+		{name: "CALLinter", argLength: 2, reg: regInfo{inputs: []regMask{gp}, clobbers: callerSave}, aux: "Int64", clobberFlags: true, call: true},                         // call fn by pointer.  arg0=codeptr, arg1=mem, auxint=argsize, returns mem
+
+		{name: "LoadOnce8", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVBU", typ: "(UInt8,Mem)", symEffect: "Read", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce16", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVHU", typ: "(UInt16,Mem)", symEffect: "Read", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce32", argLength: 2, reg: gpload, aux: "SymOff", asm: "MOVW", typ: "(UInt32,Mem)", symEffect: "Read", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce8", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVB", typ: "Mem", symEffect: "Write", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce16", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVH", typ: "Mem", symEffect: "Write", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce32", argLength: 3, reg: gpstore, aux: "SymOff", asm: "MOVW", typ: "Mem", symEffect: "Write", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "DSB", argLength: 1, typ: "Mem", asm: "DSB", hasSideEffects: true},
+		{name: "DMB_ST", argLength: 1, typ: "Mem", asm: "DMB", hasSideEffects: true},
+		{name: "LoadOnce8idx", argLength: 3, reg: gp2load, asm: "MOVB", typ: "(UInt8,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce16idx", argLength: 3, reg: gp2load, asm: "MOVH", typ: "(UInt16,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce32idx", argLength: 3, reg: gp2load, asm: "MOVW", typ: "(UInt32,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce8idx", argLength: 4, reg: gp2store, asm: "MOVB", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce16idx", argLength: 4, reg: gp2store, asm: "MOVH", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce32idx", argLength: 4, reg: gp2store, asm: "MOVW", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce8shiftLL", argLength: 3, reg: gp2load, aux: "Int32", asm: "MOVB", typ: "(UInt8,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce16shiftLL", argLength: 3, reg: gp2load, aux: "Int32", asm: "MOVH", typ: "(UInt16,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "LoadOnce32shiftLL", argLength: 3, reg: gp2load, aux: "Int32", asm: "MOVW", typ: "(UInt32,Mem)", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce8shiftLL", argLength: 4, reg: gp2store, aux: "Int32", asm: "MOVB", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce16shiftLL", argLength: 4, reg: gp2store, aux: "Int32", asm: "MOVH", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+		{name: "StoreOnce32shiftLL", argLength: 4, reg: gp2store, aux: "Int32", asm: "MOVW", typ: "Mem", hasSideEffects: true, faultOnNilArg0: true},
+
+		// pseudo-ops
+		{name: "LoweredNilCheck", argLength: 2, reg: regInfo{inputs: []regMask{gpg}}, nilCheck: true, faultOnNilArg0: true}, // panic if arg0 is nil.  arg1=mem.
+
+		{name: "Equal", argLength: 1, reg: readflags},         // bool, true flags encode x==y false otherwise.
+		{name: "NotEqual", argLength: 1, reg: readflags},      // bool, true flags encode x!=y false otherwise.
+		{name: "LessThan", argLength: 1, reg: readflags},      // bool, true flags encode signed x<y false otherwise.
+		{name: "LessEqual", argLength: 1, reg: readflags},     // bool, true flags encode signed x<=y false otherwise.
+		{name: "GreaterThan", argLength: 1, reg: readflags},   // bool, true flags encode signed x>y false otherwise.
+		{name: "GreaterEqual", argLength: 1, reg: readflags},  // bool, true flags encode signed x>=y false otherwise.
+		{name: "LessThanU", argLength: 1, reg: readflags},     // bool, true flags encode unsigned x<y false otherwise.
+		{name: "LessEqualU", argLength: 1, reg: readflags},    // bool, true flags encode unsigned x<=y false otherwise.
+		{name: "GreaterThanU", argLength: 1, reg: readflags},  // bool, true flags encode unsigned x>y false otherwise.
+		{name: "GreaterEqualU", argLength: 1, reg: readflags}, // bool, true flags encode unsigned x>=y false otherwise.
+
+		// duffzero (must be 4-byte aligned)
+		// arg0 = address of memory to zero (in R1, changed as side effect)
+		// arg1 = value to store (always zero)
+		// arg2 = mem
+		// auxint = offset into duffzero code to start executing
+		// returns mem
+		{
+			name:      "DUFFZERO",
+			aux:       "Int64",
+			argLength: 3,
+			reg: regInfo{
+				inputs:   []regMask{buildReg("R1"), buildReg("R0")},
+				clobbers: buildReg("R1 R14"),
+			},
+			faultOnNilArg0: true,
+		},
+
+		// duffcopy (must be 4-byte aligned)
+		// arg0 = address of dst memory (in R2, changed as side effect)
+		// arg1 = address of src memory (in R1, changed as side effect)
+		// arg2 = mem
+		// auxint = offset into duffcopy code to start executing
+		// returns mem
+		{
+			name:      "DUFFCOPY",
+			aux:       "Int64",
+			argLength: 3,
+			reg: regInfo{
+				inputs:   []regMask{buildReg("R2"), buildReg("R1")},
+				clobbers: buildReg("R0 R1 R2 R14"),
+			},
+			faultOnNilArg0: true,
+			faultOnNilArg1: true,
+		},
+
+		// large or unaligned zeroing
+		// arg0 = address of memory to zero (in R1, changed as side effect)
+		// arg1 = address of the last element to zero
+		// arg2 = value to store (always zero)
+		// arg3 = mem
+		// returns mem
+		//	MOVW.P	Rarg2, 4(R1)
+		//	CMP	R1, Rarg1
+		//	BLE	-2(PC)
+		{
+			name:      "LoweredZero",
+			aux:       "Int64",
+			argLength: 4,
+			reg: regInfo{
+				inputs:   []regMask{buildReg("R1"), gp, gp},
+				clobbers: buildReg("R1"),
+			},
+			clobberFlags:   true,
+			faultOnNilArg0: true,
+		},
+
+		// large or unaligned move
+		// arg0 = address of dst memory (in R2, changed as side effect)
+		// arg1 = address of src memory (in R1, changed as side effect)
+		// arg2 = address of the last element of src
+		// arg3 = mem
+		// returns mem
+		//	MOVW.P	4(R1), Rtmp
+		//	MOVW.P	Rtmp, 4(R2)
+		//	CMP	R1, Rarg2
+		//	BLE	-3(PC)
+		{
+			name:      "LoweredMove",
+			aux:       "Int64",
+			argLength: 4,
+			reg: regInfo{
+				inputs:   []regMask{buildReg("R2"), buildReg("R1"), gp},
+				clobbers: buildReg("R1 R2"),
+			},
+			clobberFlags:   true,
+			faultOnNilArg0: true,
+			faultOnNilArg1: true,
+		},
+
+		// Scheduler ensures LoweredGetClosurePtr occurs only in entry block,
+		// and sorts it to the very beginning of the block to prevent other
+		// use of R11 (thumb.REGCTXT, the closure pointer)
+		{name: "LoweredGetClosurePtr", reg: regInfo{outputs: []regMask{buildReg("R11")}}, zeroWidth: true},
+
+		// LoweredGetCallerSP returns the SP of the caller of the current function.
+		{name: "LoweredGetCallerSP", reg: gp01, rematerializeable: true},
+
+		// LoweredGetCallerPC evaluates to the PC to which its "caller" will return.
+		// I.e., if f calls g "calls" getcallerpc,
+		// the result should be the PC within f that g will return to.
+		// See runtime/stubs.go for a more detailed discussion.
+		{name: "LoweredGetCallerPC", reg: gp01, rematerializeable: true},
+
+		// There are three of these functions so that they can have three different register inputs.
+		// When we check 0 <= c <= cap (A), then 0 <= b <= c (B), then 0 <= a <= b (C), we want the
+		// default registers to match so we don't need to copy registers around unnecessarily.
+		{name: "LoweredPanicBoundsA", argLength: 3, aux: "Int64", reg: regInfo{inputs: []regMask{r2, r3}}, typ: "Mem"}, // arg0=idx, arg1=len, arg2=mem, returns memory. AuxInt contains report code (see PanicBounds in genericOps.go).
+		{name: "LoweredPanicBoundsB", argLength: 3, aux: "Int64", reg: regInfo{inputs: []regMask{r1, r2}}, typ: "Mem"}, // arg0=idx, arg1=len, arg2=mem, returns memory. AuxInt contains report code (see PanicBounds in genericOps.go).
+		{name: "LoweredPanicBoundsC", argLength: 3, aux: "Int64", reg: regInfo{inputs: []regMask{r0, r1}}, typ: "Mem"}, // arg0=idx, arg1=len, arg2=mem, returns memory. AuxInt contains report code (see PanicBounds in genericOps.go).
+		// Extend ops are the same as Bounds ops except the indexes are 64-bit.
+		{name: "LoweredPanicExtendA", argLength: 4, aux: "Int64", reg: regInfo{inputs: []regMask{r4, r2, r3}}, typ: "Mem"}, // arg0=idxHi, arg1=idxLo, arg2=len, arg3=mem, returns memory. AuxInt contains report code (see PanicExtend in genericOps.go).
+		{name: "LoweredPanicExtendB", argLength: 4, aux: "Int64", reg: regInfo{inputs: []regMask{r4, r1, r2}}, typ: "Mem"}, // arg0=idxHi, arg1=idxLo, arg2=len, arg3=mem, returns memory. AuxInt contains report code (see PanicExtend in genericOps.go).
+		{name: "LoweredPanicExtendC", argLength: 4, aux: "Int64", reg: regInfo{inputs: []regMask{r4, r0, r1}}, typ: "Mem"}, // arg0=idxHi, arg1=idxLo, arg2=len, arg3=mem, returns memory. AuxInt contains report code (see PanicExtend in genericOps.go).
+
+		// Constant flag values. For any comparison, there are 5 possible
+		// outcomes: the three from the signed total order (<,==,>) and the
+		// three from the unsigned total order. The == cases overlap.
+		// Note: there's a sixth "unordered" outcome for floating-point
+		// comparisons, but we don't use such a beast yet.
+		// These ops are for temporary use by rewrite rules. They
+		// cannot appear in the generated assembly.
+		{name: "FlagEQ"},     // equal
+		{name: "FlagLT_ULT"}, // signed < and unsigned <
+		{name: "FlagLT_UGT"}, // signed < and unsigned >
+		{name: "FlagGT_UGT"}, // signed > and unsigned <
+		{name: "FlagGT_ULT"}, // signed > and unsigned >
+
+		// (InvertFlags (CMP a b)) == (CMP b a)
+		// InvertFlags is a pseudo-op which can't appear in assembly output.
+		{name: "InvertFlags", argLength: 1}, // reverse direction of arg0
+
+		// LoweredWB invokes runtime.gcWriteBarrier. arg0=destptr, arg1=srcptr, arg2=mem, aux=runtime.gcWriteBarrier
+		// It saves all GP registers if necessary,
+		// but clobbers R14 (LR) because it's a call.
+		{name: "LoweredWB", argLength: 3, reg: regInfo{inputs: []regMask{buildReg("R2"), buildReg("R3")}, clobbers: (callerSave &^ gpg) | buildReg("R14")}, clobberFlags: true, aux: "Sym", symEffect: "None"},
+	}
+
+	blocks := []blockData{
+		{name: "EQ"},
+		{name: "NE"},
+		{name: "LT"},
+		{name: "LE"},
+		{name: "GT"},
+		{name: "GE"},
+		{name: "ULT"},
+		{name: "ULE"},
+		{name: "UGT"},
+		{name: "UGE"},
+	}
+
+	archs = append(archs, arch{
+		name:            "Thumb",
+		pkg:             "cmd/internal/obj/thumb",
+		genfile:         "../../thumb/ssa.go",
+		ops:             ops,
+		blocks:          blocks,
+		regnames:        regNamesThumb,
+		gpregmask:       gp,
+		fpregmask:       fp,
+		framepointerreg: -1, // not used
+		linkreg:         int8(num["R14"]),
+	})
+}
diff --git a/src/cmd/compile/internal/ssa/gen/genericOps.go b/src/cmd/compile/internal/ssa/gen/genericOps.go
index 8933aa51ef..51f443762a 100644
--- a/src/cmd/compile/internal/ssa/gen/genericOps.go
+++ b/src/cmd/compile/internal/ssa/gen/genericOps.go
@@ -553,6 +553,19 @@ var genericOps = []opData{
 	{name: "AtomicAdd32Variant", argLength: 3, typ: "(UInt32,Mem)", hasSideEffects: true}, // Do *arg0 += arg1.  arg2=memory.  Returns sum and new memory.
 	{name: "AtomicAdd64Variant", argLength: 3, typ: "(UInt64,Mem)", hasSideEffects: true}, // Do *arg0 += arg1.  arg2=memory.  Returns sum and new memory.
 
+	// MMIO operations need for semantically inlining functions in embedded/mmio
+	// package. Both load and store operations on I/O memory can cause side effects.
+	{name: "MMIOLoad32", argLength: 2, typ: "(UInt32,Mem)", hasSideEffects: true},
+	{name: "MMIOLoad16", argLength: 2, typ: "(UInt16,Mem)", hasSideEffects: true},
+	{name: "MMIOLoad8", argLength: 2, typ: "(UInt8,Mem)", hasSideEffects: true},
+	{name: "MMIOStore32", argLength: 3, typ: "Mem", hasSideEffects: true},
+	{name: "MMIOStore16", argLength: 3, typ: "Mem", hasSideEffects: true},
+	{name: "MMIOStore8", argLength: 3, typ: "Mem", hasSideEffects: true},
+	{name: "MMIOMB", argLength: 1, typ: "Mem", hasSideEffects: true},
+
+	// Publication barrier
+	{name: "PublicationBarrier", argLength: 1, typ: "Mem", hasSideEffects: true},
+
 	// Clobber experiment op
 	{name: "Clobber", argLength: 0, typ: "Void", aux: "SymOff", symEffect: "None"}, // write an invalid pointer value to the given pointer slot of a stack variable
 }
diff --git a/src/cmd/compile/internal/ssa/opGen.go b/src/cmd/compile/internal/ssa/opGen.go
index 00e49c97b7..b815d8d649 100644
--- a/src/cmd/compile/internal/ssa/opGen.go
+++ b/src/cmd/compile/internal/ssa/opGen.go
@@ -9,6 +9,7 @@ import (
 	"cmd/internal/obj/mips"
 	"cmd/internal/obj/ppc64"
 	"cmd/internal/obj/s390x"
+	"cmd/internal/obj/thumb"
 	"cmd/internal/obj/wasm"
 	"cmd/internal/obj/x86"
 )
@@ -120,6 +121,17 @@ const (
 	BlockS390XGTF
 	BlockS390XGEF
 
+	BlockThumbEQ
+	BlockThumbNE
+	BlockThumbLT
+	BlockThumbLE
+	BlockThumbGT
+	BlockThumbGE
+	BlockThumbULT
+	BlockThumbULE
+	BlockThumbUGT
+	BlockThumbUGE
+
 	BlockPlain
 	BlockIf
 	BlockDefer
@@ -236,6 +248,17 @@ var blockString = [...]string{
 	BlockS390XGTF: "GTF",
 	BlockS390XGEF: "GEF",
 
+	BlockThumbEQ:  "EQ",
+	BlockThumbNE:  "NE",
+	BlockThumbLT:  "LT",
+	BlockThumbLE:  "LE",
+	BlockThumbGT:  "GT",
+	BlockThumbGE:  "GE",
+	BlockThumbULT: "ULT",
+	BlockThumbULE: "ULE",
+	BlockThumbUGT: "UGT",
+	BlockThumbUGE: "UGE",
+
 	BlockPlain:  "Plain",
 	BlockIf:     "If",
 	BlockDefer:  "Defer",
@@ -2082,6 +2105,238 @@ const (
 	OpS390XLoweredMove
 	OpS390XLoweredZero
 
+	OpThumbADD
+	OpThumbADDconst
+	OpThumbSUB
+	OpThumbSUBconst
+	OpThumbRSB
+	OpThumbRSBconst
+	OpThumbMUL
+	OpThumbHMUL
+	OpThumbHMULU
+	OpThumbDIV
+	OpThumbDIVU
+	OpThumbADDS
+	OpThumbADDSconst
+	OpThumbADC
+	OpThumbADCconst
+	OpThumbSUBS
+	OpThumbSUBSconst
+	OpThumbRSBSconst
+	OpThumbSBC
+	OpThumbSBCconst
+	OpThumbMULLU
+	OpThumbMULA
+	OpThumbMULS
+	OpThumbADDF
+	OpThumbADDD
+	OpThumbSUBF
+	OpThumbSUBD
+	OpThumbMULF
+	OpThumbMULD
+	OpThumbNMULF
+	OpThumbNMULD
+	OpThumbDIVF
+	OpThumbDIVD
+	OpThumbMULAF
+	OpThumbMULAD
+	OpThumbMULSF
+	OpThumbMULSD
+	OpThumbAND
+	OpThumbANDconst
+	OpThumbOR
+	OpThumbORconst
+	OpThumbXOR
+	OpThumbXORconst
+	OpThumbBIC
+	OpThumbBICconst
+	OpThumbBFX
+	OpThumbBFXU
+	OpThumbMVN
+	OpThumbNEGF
+	OpThumbNEGD
+	OpThumbSQRTD
+	OpThumbCLZ
+	OpThumbREV
+	OpThumbREV16
+	OpThumbRBIT
+	OpThumbSLL
+	OpThumbSLLconst
+	OpThumbSRL
+	OpThumbSRLconst
+	OpThumbSRA
+	OpThumbSRAconst
+	OpThumbSRRconst
+	OpThumbADDshiftLL
+	OpThumbADDshiftRL
+	OpThumbADDshiftRA
+	OpThumbSUBshiftLL
+	OpThumbSUBshiftRL
+	OpThumbSUBshiftRA
+	OpThumbRSBshiftLL
+	OpThumbRSBshiftRL
+	OpThumbRSBshiftRA
+	OpThumbANDshiftLL
+	OpThumbANDshiftRL
+	OpThumbANDshiftRA
+	OpThumbORshiftLL
+	OpThumbORshiftRL
+	OpThumbORshiftRA
+	OpThumbXORshiftLL
+	OpThumbXORshiftRL
+	OpThumbXORshiftRA
+	OpThumbXORshiftRR
+	OpThumbBICshiftLL
+	OpThumbBICshiftRL
+	OpThumbBICshiftRA
+	OpThumbMVNshiftLL
+	OpThumbMVNshiftRL
+	OpThumbMVNshiftRA
+	OpThumbADCshiftLL
+	OpThumbADCshiftRL
+	OpThumbADCshiftRA
+	OpThumbSBCshiftLL
+	OpThumbSBCshiftRL
+	OpThumbSBCshiftRA
+	OpThumbADDSshiftLL
+	OpThumbADDSshiftRL
+	OpThumbADDSshiftRA
+	OpThumbSUBSshiftLL
+	OpThumbSUBSshiftRL
+	OpThumbSUBSshiftRA
+	OpThumbRSBSshiftLL
+	OpThumbRSBSshiftRL
+	OpThumbRSBSshiftRA
+	OpThumbCMP
+	OpThumbCMPconst
+	OpThumbCMN
+	OpThumbCMNconst
+	OpThumbTST
+	OpThumbTSTconst
+	OpThumbTEQ
+	OpThumbTEQconst
+	OpThumbCMPF
+	OpThumbCMPD
+	OpThumbCMPshiftLL
+	OpThumbCMPshiftRL
+	OpThumbCMPshiftRA
+	OpThumbCMNshiftLL
+	OpThumbCMNshiftRL
+	OpThumbCMNshiftRA
+	OpThumbTSTshiftLL
+	OpThumbTSTshiftRL
+	OpThumbTSTshiftRA
+	OpThumbTEQshiftLL
+	OpThumbTEQshiftRL
+	OpThumbTEQshiftRA
+	OpThumbCMPF0
+	OpThumbCMPD0
+	OpThumbMOVWconst
+	OpThumbMOVFconst
+	OpThumbMOVDconst
+	OpThumbMOVWaddr
+	OpThumbMOVBload
+	OpThumbMOVBUload
+	OpThumbMOVHload
+	OpThumbMOVHUload
+	OpThumbMOVWload
+	OpThumbMOVFload
+	OpThumbMOVDload
+	OpThumbMOVBstore
+	OpThumbMOVHstore
+	OpThumbMOVWstore
+	OpThumbMOVFstore
+	OpThumbMOVDstore
+	OpThumbMOVWloadidx
+	OpThumbMOVHloadidx
+	OpThumbMOVHUloadidx
+	OpThumbMOVBloadidx
+	OpThumbMOVBUloadidx
+	OpThumbMOVWloadshiftLL
+	OpThumbMOVHloadshiftLL
+	OpThumbMOVHUloadshiftLL
+	OpThumbMOVBloadshiftLL
+	OpThumbMOVBUloadshiftLL
+	OpThumbMOVWstoreidx
+	OpThumbMOVBstoreidx
+	OpThumbMOVHstoreidx
+	OpThumbMOVWstoreshiftLL
+	OpThumbMOVHstoreshiftLL
+	OpThumbMOVBstoreshiftLL
+	OpThumbMOVBreg
+	OpThumbMOVBUreg
+	OpThumbMOVHreg
+	OpThumbMOVHUreg
+	OpThumbMOVWreg
+	OpThumbMOVWnop
+	OpThumbMOVWF
+	OpThumbMOVWD
+	OpThumbMOVWUF
+	OpThumbMOVWUD
+	OpThumbMOVFW
+	OpThumbMOVDW
+	OpThumbMOVFWU
+	OpThumbMOVDWU
+	OpThumbMOVFD
+	OpThumbMOVDF
+	OpThumbCMOVWHSconst
+	OpThumbCMOVWLSconst
+	OpThumbSRAcond
+	OpThumbCALLstatic
+	OpThumbCALLclosure
+	OpThumbCALLinter
+	OpThumbLoadOnce8
+	OpThumbLoadOnce16
+	OpThumbLoadOnce32
+	OpThumbStoreOnce8
+	OpThumbStoreOnce16
+	OpThumbStoreOnce32
+	OpThumbDSB
+	OpThumbDMB_ST
+	OpThumbLoadOnce8idx
+	OpThumbLoadOnce16idx
+	OpThumbLoadOnce32idx
+	OpThumbStoreOnce8idx
+	OpThumbStoreOnce16idx
+	OpThumbStoreOnce32idx
+	OpThumbLoadOnce8shiftLL
+	OpThumbLoadOnce16shiftLL
+	OpThumbLoadOnce32shiftLL
+	OpThumbStoreOnce8shiftLL
+	OpThumbStoreOnce16shiftLL
+	OpThumbStoreOnce32shiftLL
+	OpThumbLoweredNilCheck
+	OpThumbEqual
+	OpThumbNotEqual
+	OpThumbLessThan
+	OpThumbLessEqual
+	OpThumbGreaterThan
+	OpThumbGreaterEqual
+	OpThumbLessThanU
+	OpThumbLessEqualU
+	OpThumbGreaterThanU
+	OpThumbGreaterEqualU
+	OpThumbDUFFZERO
+	OpThumbDUFFCOPY
+	OpThumbLoweredZero
+	OpThumbLoweredMove
+	OpThumbLoweredGetClosurePtr
+	OpThumbLoweredGetCallerSP
+	OpThumbLoweredGetCallerPC
+	OpThumbLoweredPanicBoundsA
+	OpThumbLoweredPanicBoundsB
+	OpThumbLoweredPanicBoundsC
+	OpThumbLoweredPanicExtendA
+	OpThumbLoweredPanicExtendB
+	OpThumbLoweredPanicExtendC
+	OpThumbFlagEQ
+	OpThumbFlagLT_ULT
+	OpThumbFlagLT_UGT
+	OpThumbFlagGT_UGT
+	OpThumbFlagGT_ULT
+	OpThumbInvertFlags
+	OpThumbLoweredWB
+
 	OpWasmLoweredStaticCall
 	OpWasmLoweredClosureCall
 	OpWasmLoweredInterCall
@@ -2531,6 +2786,14 @@ const (
 	OpAtomicOr8
 	OpAtomicAdd32Variant
 	OpAtomicAdd64Variant
+	OpMMIOLoad32
+	OpMMIOLoad16
+	OpMMIOLoad8
+	OpMMIOStore32
+	OpMMIOStore16
+	OpMMIOStore8
+	OpMMIOMB
+	OpPublicationBarrier
 	OpClobber
 )
 
@@ -27596,411 +27859,3562 @@ var opcodeTable = [...]opInfo{
 		},
 	},
 	{
-		name:   "FlagEQ",
-		argLen: 0,
-		reg:    regInfo{},
-	},
-	{
-		name:   "FlagLT",
-		argLen: 0,
-		reg:    regInfo{},
-	},
-	{
-		name:   "FlagGT",
-		argLen: 0,
-		reg:    regInfo{},
-	},
-	{
-		name:   "FlagOV",
-		argLen: 0,
-		reg:    regInfo{},
-	},
-	{
-		name:   "SYNC",
-		argLen: 1,
-		asm:    s390x.ASYNC,
-		reg:    regInfo{},
-	},
-	{
-		name:           "MOVBZatomicload",
-		auxType:        auxSymOff,
-		argLen:         2,
-		faultOnNilArg0: true,
-		symEffect:      SymRead,
-		asm:            s390x.AMOVBZ,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
-			},
-			outputs: []outputInfo{
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
-			},
-		},
-	},
-	{
-		name:           "MOVWZatomicload",
-		auxType:        auxSymOff,
-		argLen:         2,
-		faultOnNilArg0: true,
-		symEffect:      SymRead,
-		asm:            s390x.AMOVWZ,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
-			},
-			outputs: []outputInfo{
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
-			},
-		},
-	},
-	{
-		name:           "MOVDatomicload",
-		auxType:        auxSymOff,
-		argLen:         2,
-		faultOnNilArg0: true,
-		symEffect:      SymRead,
-		asm:            s390x.AMOVD,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
-			},
-			outputs: []outputInfo{
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
-			},
-		},
-	},
-	{
-		name:           "MOVWatomicstore",
-		auxType:        auxSymOff,
-		argLen:         3,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		hasSideEffects: true,
-		symEffect:      SymWrite,
-		asm:            s390x.AMOVW,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
-				{1, 56319},      // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-		},
-	},
-	{
-		name:           "MOVDatomicstore",
-		auxType:        auxSymOff,
-		argLen:         3,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		hasSideEffects: true,
-		symEffect:      SymWrite,
-		asm:            s390x.AMOVD,
+		name:   "FlagEQ",
+		argLen: 0,
+		reg:    regInfo{},
+	},
+	{
+		name:   "FlagLT",
+		argLen: 0,
+		reg:    regInfo{},
+	},
+	{
+		name:   "FlagGT",
+		argLen: 0,
+		reg:    regInfo{},
+	},
+	{
+		name:   "FlagOV",
+		argLen: 0,
+		reg:    regInfo{},
+	},
+	{
+		name:   "SYNC",
+		argLen: 1,
+		asm:    s390x.ASYNC,
+		reg:    regInfo{},
+	},
+	{
+		name:           "MOVBZatomicload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            s390x.AMOVBZ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVWZatomicload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            s390x.AMOVWZ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVDatomicload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            s390x.AMOVD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
+			},
+			outputs: []outputInfo{
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVWatomicstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymWrite,
+		asm:            s390x.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
+				{1, 56319},      // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+		},
+	},
+	{
+		name:           "MOVDatomicstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymWrite,
+		asm:            s390x.AMOVD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
+				{1, 56319},      // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+		},
+	},
+	{
+		name:           "LAA",
+		auxType:        auxSymOff,
+		argLen:         3,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRdWr,
+		asm:            s390x.ALAA,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
+				{1, 56319},      // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+			outputs: []outputInfo{
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LAAG",
+		auxType:        auxSymOff,
+		argLen:         3,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRdWr,
+		asm:            s390x.ALAAG,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
+				{1, 56319},      // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+			outputs: []outputInfo{
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "AddTupleFirst32",
+		argLen: 2,
+		reg:    regInfo{},
+	},
+	{
+		name:   "AddTupleFirst64",
+		argLen: 2,
+		reg:    regInfo{},
+	},
+	{
+		name:           "LoweredAtomicCas32",
+		auxType:        auxSymOff,
+		argLen:         4,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRdWr,
+		asm:            s390x.ACS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 1},     // R0
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+				{2, 56319}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+			clobbers: 1, // R0
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoweredAtomicCas64",
+		auxType:        auxSymOff,
+		argLen:         4,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRdWr,
+		asm:            s390x.ACSG,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 1},     // R0
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+				{2, 56319}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+			clobbers: 1, // R0
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoweredAtomicExchange32",
+		auxType:        auxSymOff,
+		argLen:         3,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRdWr,
+		asm:            s390x.ACS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+				{1, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 1}, // R0
+			},
+		},
+	},
+	{
+		name:           "LoweredAtomicExchange64",
+		auxType:        auxSymOff,
+		argLen:         3,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRdWr,
+		asm:            s390x.ACSG,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+				{1, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 1}, // R0
+			},
+		},
+	},
+	{
+		name:         "FLOGR",
+		argLen:       1,
+		clobberFlags: true,
+		asm:          s390x.AFLOGR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+			clobbers: 2, // R1
+			outputs: []outputInfo{
+				{0, 1}, // R0
+			},
+		},
+	},
+	{
+		name:         "POPCNT",
+		argLen:       1,
+		clobberFlags: true,
+		asm:          s390x.APOPCNT,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "SumBytes2",
+		argLen: 1,
+		reg:    regInfo{},
+	},
+	{
+		name:   "SumBytes4",
+		argLen: 1,
+		reg:    regInfo{},
+	},
+	{
+		name:   "SumBytes8",
+		argLen: 1,
+		reg:    regInfo{},
+	},
+	{
+		name:           "STMG2",
+		auxType:        auxSymOff,
+		argLen:         4,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            s390x.ASTMG,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 2},     // R1
+				{2, 4},     // R2
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+		},
+	},
+	{
+		name:           "STMG3",
+		auxType:        auxSymOff,
+		argLen:         5,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            s390x.ASTMG,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 2},     // R1
+				{2, 4},     // R2
+				{3, 8},     // R3
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+		},
+	},
+	{
+		name:           "STMG4",
+		auxType:        auxSymOff,
+		argLen:         6,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            s390x.ASTMG,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 2},     // R1
+				{2, 4},     // R2
+				{3, 8},     // R3
+				{4, 16},    // R4
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+		},
+	},
+	{
+		name:           "STM2",
+		auxType:        auxSymOff,
+		argLen:         4,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            s390x.ASTMY,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 2},     // R1
+				{2, 4},     // R2
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+		},
+	},
+	{
+		name:           "STM3",
+		auxType:        auxSymOff,
+		argLen:         5,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            s390x.ASTMY,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 2},     // R1
+				{2, 4},     // R2
+				{3, 8},     // R3
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+		},
+	},
+	{
+		name:           "STM4",
+		auxType:        auxSymOff,
+		argLen:         6,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            s390x.ASTMY,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 2},     // R1
+				{2, 4},     // R2
+				{3, 8},     // R3
+				{4, 16},    // R4
+				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+		},
+	},
+	{
+		name:           "LoweredMove",
+		auxType:        auxInt64,
+		argLen:         4,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		faultOnNilArg1: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 2},     // R1
+				{1, 4},     // R2
+				{2, 56319}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+			clobbers: 6, // R1 R2
+		},
+	},
+	{
+		name:           "LoweredZero",
+		auxType:        auxInt64,
+		argLen:         3,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 2},     // R1
+				{1, 56319}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+			},
+			clobbers: 2, // R1
+		},
+	},
+
+	{
+		name:         "ADD",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "ADDconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 32639}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SUB",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SUBconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "RSB",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "RSBconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "MUL",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AMUL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "HMUL",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "HMULU",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULLU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "DIV",
+		argLen: 2,
+		asm:    thumb.ADIV,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "DIVU",
+		argLen: 2,
+		asm:    thumb.ADIVU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "ADDS",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDSconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "ADC",
+		argLen:       3,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AADC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "ADCconst",
+		auxType:      auxInt32,
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.AADC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "SUBS",
+		argLen: 2,
+		asm:    thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBSconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBSconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SBC",
+		argLen:       3,
+		clobberFlags: true,
+		asm:          thumb.ASBC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SBCconst",
+		auxType:      auxInt32,
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASBC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "MULLU",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULLU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MULA",
+		argLen: 3,
+		asm:    thumb.AMULA,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{2, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MULS",
+		argLen: 3,
+		asm:    thumb.AMULS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{2, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "ADDF",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AADDF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:        "ADDD",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AADDD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "SUBF",
+		argLen: 2,
+		asm:    thumb.ASUBF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "SUBD",
+		argLen: 2,
+		asm:    thumb.ASUBD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:        "MULF",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:        "MULD",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.AMULD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:        "NMULF",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ANMULF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:        "NMULD",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ANMULD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "DIVF",
+		argLen: 2,
+		asm:    thumb.ADIVF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "DIVD",
+		argLen: 2,
+		asm:    thumb.ADIVD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "MULAF",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AMULAF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "MULAD",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AMULAD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "MULSF",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AMULSF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "MULSD",
+		argLen:       3,
+		resultInArg0: true,
+		asm:          thumb.AMULSD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{2, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "AND",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AAND,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "ANDconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AAND,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "OR",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AORR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "ORconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AORR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "XOR",
+		argLen:       2,
+		commutative:  true,
+		clobberFlags: true,
+		asm:          thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "XORconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "BIC",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "BICconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BFX",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ABFX,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BFXU",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ABFXU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "MVN",
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.AMVN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "NEGF",
+		argLen: 1,
+		asm:    thumb.ANEGF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "NEGD",
+		argLen: 1,
+		asm:    thumb.ANEGD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "SQRTD",
+		argLen: 1,
+		asm:    thumb.ASQRTD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "CLZ",
+		argLen: 1,
+		asm:    thumb.ACLZ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "REV",
+		argLen: 1,
+		asm:    thumb.AREV,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "REV16",
+		argLen: 1,
+		asm:    thumb.AREV16,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "RBIT",
+		argLen: 1,
+		asm:    thumb.ARBIT,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SLL",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASLL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SLLconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASLL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRL",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASRL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRLconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASRL,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRA",
+		argLen:       2,
+		clobberFlags: true,
+		asm:          thumb.ASRA,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "SRAconst",
+		auxType:      auxInt32,
+		argLen:       1,
+		clobberFlags: true,
+		asm:          thumb.ASRA,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SRRconst",
+		auxType: auxInt32,
+		argLen:  1,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ANDshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AAND,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ANDshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AAND,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ANDshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AAND,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ORshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AORR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "XORshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "XORshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "XORshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "XORshiftRR",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AEOR,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BICshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BICshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "BICshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ABIC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MVNshiftLL",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.AMVN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MVNshiftRL",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.AMVN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MVNshiftRA",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.AMVN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADCshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AADC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADCshiftRL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AADC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADCshiftRA",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AADC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SBCshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.ASBC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SBCshiftRL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.ASBC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SBCshiftRA",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.ASBC,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDSshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDSshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "ADDSshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.AADD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBSshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBSshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "SUBSshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ASUB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBSshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBSshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "RSBSshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ARSB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{1, 0},
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "CMP",
+		argLen: 2,
+		asm:    thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMPconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "CMN",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMNconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "TST",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TSTconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:        "TEQ",
+		argLen:      2,
+		commutative: true,
+		asm:         thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TEQconst",
+		auxType: auxInt32,
+		argLen:  1,
+		asm:     thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "CMPF",
+		argLen: 2,
+		asm:    thumb.ACMPF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "CMPD",
+		argLen: 2,
+		asm:    thumb.ACMPD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:    "CMPshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMPshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMPshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMP,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMNshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMNshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "CMNshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ACMN,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TSTshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TSTshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TSTshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATST,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TEQshiftLL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TEQshiftRL",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "TEQshiftRA",
+		auxType: auxInt32,
+		argLen:  2,
+		asm:     thumb.ATEQ,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{1, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "CMPF0",
+		argLen: 1,
+		asm:    thumb.ACMPF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "CMPD0",
+		argLen: 1,
+		asm:    thumb.ACMPD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "MOVWconst",
+		auxType:      auxInt32,
+		argLen:       0,
+		clobberFlags: true,
+		asm:          thumb.AMOVW,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:              "MOVFconst",
+		auxType:           auxFloat64,
+		argLen:            0,
+		rematerializeable: true,
+		asm:               thumb.AMOVF,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:              "MOVDconst",
+		auxType:           auxFloat64,
+		argLen:            0,
+		rematerializeable: true,
+		asm:               thumb.AMOVD,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:              "MOVWaddr",
+		auxType:           auxSymOff,
+		argLen:            1,
+		rematerializeable: true,
+		symEffect:         SymAddr,
+		asm:               thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294975488}, // SP SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVBload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVBUload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVHload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVHUload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVWload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "MOVFload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:           "MOVDload",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:           "MOVBstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "MOVHstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "MOVWstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "MOVFstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:           "MOVDstore",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+				{1, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVWloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVHloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVHUloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVBloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVBUloadidx",
+		argLen: 3,
+		asm:    thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVWloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVHloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVHUloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVBloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "MOVBUloadshiftLL",
+		auxType: auxInt32,
+		argLen:  3,
+		asm:     thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVWstoreidx",
+		argLen: 4,
+		asm:    thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:   "MOVBstoreidx",
+		argLen: 4,
+		asm:    thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:   "MOVHstoreidx",
+		argLen: 4,
+		asm:    thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:    "MOVWstoreshiftLL",
+		auxType: auxInt32,
+		argLen:  4,
+		asm:     thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:    "MOVHstoreshiftLL",
+		auxType: auxInt32,
+		argLen:  4,
+		asm:     thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:    "MOVBstoreshiftLL",
+		auxType: auxInt32,
+		argLen:  4,
+		asm:     thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:   "MOVBreg",
+		argLen: 1,
+		asm:    thumb.AMOVBS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVBUreg",
+		argLen: 1,
+		asm:    thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVHreg",
+		argLen: 1,
+		asm:    thumb.AMOVHS,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVHUreg",
+		argLen: 1,
+		asm:    thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVWreg",
+		argLen: 1,
+		asm:    thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "MOVWnop",
+		argLen:       1,
+		resultInArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVWF",
+		argLen: 1,
+		asm:    thumb.AMOVWF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVWD",
+		argLen: 1,
+		asm:    thumb.AMOVWD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVWUF",
+		argLen: 1,
+		asm:    thumb.AMOVWF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVWUD",
+		argLen: 1,
+		asm:    thumb.AMOVWD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVFW",
+		argLen: 1,
+		asm:    thumb.AMOVFW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVDW",
+		argLen: 1,
+		asm:    thumb.AMOVDW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVFWU",
+		argLen: 1,
+		asm:    thumb.AMOVFW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVDWU",
+		argLen: 1,
+		asm:    thumb.AMOVDW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			clobbers: 2147483648, // F15
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "MOVFD",
+		argLen: 1,
+		asm:    thumb.AMOVFD,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:   "MOVDF",
+		argLen: 1,
+		asm:    thumb.AMOVDF,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+			outputs: []outputInfo{
+				{0, 4294901760}, // F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+			},
+		},
+	},
+	{
+		name:         "CMOVWHSconst",
+		auxType:      auxInt32,
+		argLen:       2,
+		resultInArg0: true,
+		asm:          thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "CMOVWLSconst",
+		auxType:      auxInt32,
+		argLen:       2,
+		resultInArg0: true,
+		asm:          thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "SRAcond",
+		argLen: 3,
+		asm:    thumb.ASRA,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:         "CALLstatic",
+		auxType:      auxSymOff,
+		argLen:       1,
+		clobberFlags: true,
+		call:         true,
+		symEffect:    SymNone,
+		reg: regInfo{
+			clobbers: 4294926207, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+		},
+	},
+	{
+		name:         "CALLclosure",
+		auxType:      auxInt64,
+		argLen:       3,
+		clobberFlags: true,
+		call:         true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 2048},  // R11
+				{0, 31615}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 SP R14
+			},
+			clobbers: 4294926207, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+		},
+	},
+	{
+		name:         "CALLinter",
+		auxType:      auxInt64,
+		argLen:       2,
+		clobberFlags: true,
+		call:         true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 4294926207, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
+		},
+	},
+	{
+		name:           "LoadOnce8",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVBU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce16",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVHU,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce32",
+		auxType:        auxSymOff,
+		argLen:         2,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymRead,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "StoreOnce8",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce16",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce32",
+		auxType:        auxSymOff,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		symEffect:      SymWrite,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "DSB",
+		argLen:         1,
+		hasSideEffects: true,
+		asm:            thumb.ADSB,
+		reg:            regInfo{},
+	},
+	{
+		name:           "DMB_ST",
+		argLen:         1,
+		hasSideEffects: true,
+		asm:            thumb.ADMB,
+		reg:            regInfo{},
+	},
+	{
+		name:           "LoadOnce8idx",
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce16idx",
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce32idx",
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "StoreOnce8idx",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce16idx",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce32idx",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "LoadOnce8shiftLL",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce16shiftLL",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "LoadOnce32shiftLL",
+		auxType:        auxInt32,
+		argLen:         3,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "StoreOnce8shiftLL",
+		auxType:        auxInt32,
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVB,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce16shiftLL",
+		auxType:        auxInt32,
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVH,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "StoreOnce32shiftLL",
+		auxType:        auxInt32,
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		asm:            thumb.AMOVW,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{2, 24447},      // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+				{0, 4294999935}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 SP R14 SB
+			},
+		},
+	},
+	{
+		name:           "LoweredNilCheck",
+		argLen:         2,
+		nilCheck:       true,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 24447}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 g R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "Equal",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "NotEqual",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "LessThan",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "LessEqual",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "GreaterThan",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "GreaterEqual",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "LessThanU",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "LessEqualU",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "GreaterThanU",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:   "GreaterEqualU",
+		argLen: 1,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:           "DUFFZERO",
+		auxType:        auxInt64,
+		argLen:         3,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 2}, // R1
+				{1, 1}, // R0
+			},
+			clobbers: 16386, // R1 R14
+		},
+	},
+	{
+		name:           "DUFFCOPY",
+		auxType:        auxInt64,
+		argLen:         3,
+		faultOnNilArg0: true,
+		faultOnNilArg1: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4}, // R2
+				{1, 2}, // R1
+			},
+			clobbers: 16391, // R0 R1 R2 R14
+		},
+	},
+	{
+		name:           "LoweredZero",
+		auxType:        auxInt64,
+		argLen:         4,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 2},     // R1
+				{1, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+				{2, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 2, // R1
+		},
+	},
+	{
+		name:           "LoweredMove",
+		auxType:        auxInt64,
+		argLen:         4,
+		clobberFlags:   true,
+		faultOnNilArg0: true,
+		faultOnNilArg1: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4},     // R2
+				{1, 2},     // R1
+				{2, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+			clobbers: 6, // R1 R2
+		},
+	},
+	{
+		name:      "LoweredGetClosurePtr",
+		argLen:    0,
+		zeroWidth: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 2048}, // R11
+			},
+		},
+	},
+	{
+		name:              "LoweredGetCallerSP",
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:              "LoweredGetCallerPC",
+		argLen:            0,
+		rematerializeable: true,
+		reg: regInfo{
+			outputs: []outputInfo{
+				{0, 23423}, // R0 R1 R2 R3 R4 R5 R6 R8 R9 R11 R12 R14
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicBoundsA",
+		auxType: auxInt64,
+		argLen:  3,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 4}, // R2
+				{1, 8}, // R3
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicBoundsB",
+		auxType: auxInt64,
+		argLen:  3,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 2}, // R1
+				{1, 4}, // R2
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicBoundsC",
+		auxType: auxInt64,
+		argLen:  3,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 1}, // R0
+				{1, 2}, // R1
+			},
+		},
+	},
+	{
+		name:    "LoweredPanicExtendA",
+		auxType: auxInt64,
+		argLen:  4,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
-				{1, 56319},      // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+				{0, 16}, // R4
+				{1, 4},  // R2
+				{2, 8},  // R3
 			},
 		},
 	},
 	{
-		name:           "LAA",
-		auxType:        auxSymOff,
-		argLen:         3,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		hasSideEffects: true,
-		symEffect:      SymRdWr,
-		asm:            s390x.ALAA,
+		name:    "LoweredPanicExtendB",
+		auxType: auxInt64,
+		argLen:  4,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
-				{1, 56319},      // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-			outputs: []outputInfo{
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+				{0, 16}, // R4
+				{1, 2},  // R1
+				{2, 4},  // R2
 			},
 		},
 	},
 	{
-		name:           "LAAG",
-		auxType:        auxSymOff,
-		argLen:         3,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		hasSideEffects: true,
-		symEffect:      SymRdWr,
-		asm:            s390x.ALAAG,
+		name:    "LoweredPanicExtendC",
+		auxType: auxInt64,
+		argLen:  4,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 4295023614}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP SB
-				{1, 56319},      // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-			outputs: []outputInfo{
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
+				{0, 16}, // R4
+				{1, 1},  // R0
+				{2, 2},  // R1
 			},
 		},
 	},
 	{
-		name:   "AddTupleFirst32",
-		argLen: 2,
+		name:   "FlagEQ",
+		argLen: 0,
 		reg:    regInfo{},
 	},
 	{
-		name:   "AddTupleFirst64",
-		argLen: 2,
+		name:   "FlagLT_ULT",
+		argLen: 0,
 		reg:    regInfo{},
 	},
 	{
-		name:           "LoweredAtomicCas32",
-		auxType:        auxSymOff,
-		argLen:         4,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		hasSideEffects: true,
-		symEffect:      SymRdWr,
-		asm:            s390x.ACS,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 1},     // R0
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-				{2, 56319}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-			clobbers: 1, // R0
-			outputs: []outputInfo{
-				{1, 0},
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
-			},
-		},
-	},
-	{
-		name:           "LoweredAtomicCas64",
-		auxType:        auxSymOff,
-		argLen:         4,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		hasSideEffects: true,
-		symEffect:      SymRdWr,
-		asm:            s390x.ACSG,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 1},     // R0
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-				{2, 56319}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-			clobbers: 1, // R0
-			outputs: []outputInfo{
-				{1, 0},
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
-			},
-		},
-	},
-	{
-		name:           "LoweredAtomicExchange32",
-		auxType:        auxSymOff,
-		argLen:         3,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		hasSideEffects: true,
-		symEffect:      SymRdWr,
-		asm:            s390x.ACS,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-				{1, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-			outputs: []outputInfo{
-				{1, 0},
-				{0, 1}, // R0
-			},
-		},
-	},
-	{
-		name:           "LoweredAtomicExchange64",
-		auxType:        auxSymOff,
-		argLen:         3,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		hasSideEffects: true,
-		symEffect:      SymRdWr,
-		asm:            s390x.ACSG,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-				{1, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-			outputs: []outputInfo{
-				{1, 0},
-				{0, 1}, // R0
-			},
-		},
-	},
-	{
-		name:         "FLOGR",
-		argLen:       1,
-		clobberFlags: true,
-		asm:          s390x.AFLOGR,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
-			},
-			clobbers: 2, // R1
-			outputs: []outputInfo{
-				{0, 1}, // R0
-			},
-		},
-	},
-	{
-		name:         "POPCNT",
-		argLen:       1,
-		clobberFlags: true,
-		asm:          s390x.APOPCNT,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
-			},
-			outputs: []outputInfo{
-				{0, 23551}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14
-			},
-		},
+		name:   "FlagLT_UGT",
+		argLen: 0,
+		reg:    regInfo{},
 	},
 	{
-		name:   "SumBytes2",
-		argLen: 1,
+		name:   "FlagGT_UGT",
+		argLen: 0,
 		reg:    regInfo{},
 	},
 	{
-		name:   "SumBytes4",
-		argLen: 1,
+		name:   "FlagGT_ULT",
+		argLen: 0,
 		reg:    regInfo{},
 	},
 	{
-		name:   "SumBytes8",
+		name:   "InvertFlags",
 		argLen: 1,
 		reg:    regInfo{},
 	},
 	{
-		name:           "STMG2",
-		auxType:        auxSymOff,
-		argLen:         4,
-		faultOnNilArg0: true,
-		symEffect:      SymWrite,
-		asm:            s390x.ASTMG,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 2},     // R1
-				{2, 4},     // R2
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-		},
-	},
-	{
-		name:           "STMG3",
-		auxType:        auxSymOff,
-		argLen:         5,
-		faultOnNilArg0: true,
-		symEffect:      SymWrite,
-		asm:            s390x.ASTMG,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 2},     // R1
-				{2, 4},     // R2
-				{3, 8},     // R3
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-		},
-	},
-	{
-		name:           "STMG4",
-		auxType:        auxSymOff,
-		argLen:         6,
-		faultOnNilArg0: true,
-		symEffect:      SymWrite,
-		asm:            s390x.ASTMG,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 2},     // R1
-				{2, 4},     // R2
-				{3, 8},     // R3
-				{4, 16},    // R4
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-		},
-	},
-	{
-		name:           "STM2",
-		auxType:        auxSymOff,
-		argLen:         4,
-		faultOnNilArg0: true,
-		symEffect:      SymWrite,
-		asm:            s390x.ASTMY,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 2},     // R1
-				{2, 4},     // R2
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-		},
-	},
-	{
-		name:           "STM3",
-		auxType:        auxSymOff,
-		argLen:         5,
-		faultOnNilArg0: true,
-		symEffect:      SymWrite,
-		asm:            s390x.ASTMY,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 2},     // R1
-				{2, 4},     // R2
-				{3, 8},     // R3
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-		},
-	},
-	{
-		name:           "STM4",
-		auxType:        auxSymOff,
-		argLen:         6,
-		faultOnNilArg0: true,
-		symEffect:      SymWrite,
-		asm:            s390x.ASTMY,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{1, 2},     // R1
-				{2, 4},     // R2
-				{3, 8},     // R3
-				{4, 16},    // R4
-				{0, 56318}, // R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-		},
-	},
-	{
-		name:           "LoweredMove",
-		auxType:        auxInt64,
-		argLen:         4,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
-		faultOnNilArg1: true,
-		reg: regInfo{
-			inputs: []inputInfo{
-				{0, 2},     // R1
-				{1, 4},     // R2
-				{2, 56319}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
-			},
-			clobbers: 6, // R1 R2
-		},
-	},
-	{
-		name:           "LoweredZero",
-		auxType:        auxInt64,
-		argLen:         3,
-		clobberFlags:   true,
-		faultOnNilArg0: true,
+		name:         "LoweredWB",
+		auxType:      auxSym,
+		argLen:       3,
+		clobberFlags: true,
+		symEffect:    SymNone,
 		reg: regInfo{
 			inputs: []inputInfo{
-				{0, 2},     // R1
-				{1, 56319}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R11 R12 R14 SP
+				{0, 4}, // R2
+				{1, 8}, // R3
 			},
-			clobbers: 2, // R1
+			clobbers: 4294918144, // R14 F0 F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13 F14 F15
 		},
 	},
 
@@ -31118,6 +34532,54 @@ var opcodeTable = [...]opInfo{
 		hasSideEffects: true,
 		generic:        true,
 	},
+	{
+		name:           "MMIOLoad32",
+		argLen:         2,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOLoad16",
+		argLen:         2,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOLoad8",
+		argLen:         2,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOStore32",
+		argLen:         3,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOStore16",
+		argLen:         3,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOStore8",
+		argLen:         3,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "MMIOMB",
+		argLen:         1,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "PublicationBarrier",
+		argLen:         1,
+		hasSideEffects: true,
+		generic:        true,
+	},
 	{
 		name:      "Clobber",
 		auxType:   auxSymOff,
@@ -31545,6 +35007,46 @@ var fpRegMaskS390X = regMask(4294901760)
 var specialRegMaskS390X = regMask(0)
 var framepointerRegS390X = int8(-1)
 var linkRegS390X = int8(14)
+var registersThumb = [...]Register{
+	{0, thumb.REG_R0, 0, "R0"},
+	{1, thumb.REG_R1, 1, "R1"},
+	{2, thumb.REG_R2, 2, "R2"},
+	{3, thumb.REG_R3, 3, "R3"},
+	{4, thumb.REG_R4, 4, "R4"},
+	{5, thumb.REG_R5, 5, "R5"},
+	{6, thumb.REG_R6, 6, "R6"},
+	{7, thumb.REG_R7, -1, "R7"},
+	{8, thumb.REG_R8, 7, "R8"},
+	{9, thumb.REG_R9, 8, "R9"},
+	{10, thumb.REGG, -1, "g"},
+	{11, thumb.REG_R11, 9, "R11"},
+	{12, thumb.REG_R12, 10, "R12"},
+	{13, thumb.REGSP, -1, "SP"},
+	{14, thumb.REG_R14, 11, "R14"},
+	{15, thumb.REG_R15, -1, "R15"},
+	{16, thumb.REG_F0, -1, "F0"},
+	{17, thumb.REG_F1, -1, "F1"},
+	{18, thumb.REG_F2, -1, "F2"},
+	{19, thumb.REG_F3, -1, "F3"},
+	{20, thumb.REG_F4, -1, "F4"},
+	{21, thumb.REG_F5, -1, "F5"},
+	{22, thumb.REG_F6, -1, "F6"},
+	{23, thumb.REG_F7, -1, "F7"},
+	{24, thumb.REG_F8, -1, "F8"},
+	{25, thumb.REG_F9, -1, "F9"},
+	{26, thumb.REG_F10, -1, "F10"},
+	{27, thumb.REG_F11, -1, "F11"},
+	{28, thumb.REG_F12, -1, "F12"},
+	{29, thumb.REG_F13, -1, "F13"},
+	{30, thumb.REG_F14, -1, "F14"},
+	{31, thumb.REG_F15, -1, "F15"},
+	{32, 0, -1, "SB"},
+}
+var gpRegMaskThumb = regMask(23423)
+var fpRegMaskThumb = regMask(4294901760)
+var specialRegMaskThumb = regMask(0)
+var framepointerRegThumb = int8(-1)
+var linkRegThumb = int8(14)
 var registersWasm = [...]Register{
 	{0, wasm.REG_R0, 0, "R0"},
 	{1, wasm.REG_R1, 1, "R1"},
diff --git a/src/cmd/compile/internal/ssa/regalloc.go b/src/cmd/compile/internal/ssa/regalloc.go
index 8abbf61507..3a7b7b67be 100644
--- a/src/cmd/compile/internal/ssa/regalloc.go
+++ b/src/cmd/compile/internal/ssa/regalloc.go
@@ -596,7 +596,8 @@ func (s *regAllocState) init(f *Func) {
 			// Leaf functions don't save/restore the link register.
 			s.allocatable &^= 1 << uint(s.f.Config.LinkReg)
 		}
-		if s.f.Config.arch == "arm" && objabi.GOARM == 5 {
+		if s.f.Config.arch == "arm" && objabi.GOARM == 5 || s.f.Config.arch == "thumb" && objabi.GOARM&0xF != 0xD {
+			// TODO: handle 32-bit FPU (GOARCH=thumb GOARM=0x7F)
 			// On ARMv5 we insert softfloat calls at each FP instruction.
 			// This clobbers LR almost everywhere. Disable allocating LR
 			// on ARMv5.
@@ -607,7 +608,7 @@ func (s *regAllocState) init(f *Func) {
 		switch s.f.Config.arch {
 		case "amd64":
 			s.allocatable &^= 1 << 15 // R15
-		case "arm":
+		case "arm", "thumb":
 			s.allocatable &^= 1 << 9 // R9
 		case "ppc64le": // R2 already reserved.
 			// nothing to do
diff --git a/src/cmd/compile/internal/ssa/rewrite.go b/src/cmd/compile/internal/ssa/rewrite.go
index cd23fe87e5..0f0b07e39b 100644
--- a/src/cmd/compile/internal/ssa/rewrite.go
+++ b/src/cmd/compile/internal/ssa/rewrite.go
@@ -1061,7 +1061,7 @@ func isInlinableMemmove(dst, src *Value, sz int64, c *Config) bool {
 		return sz <= 8
 	case "s390x":
 		return sz <= 8 || disjoint(dst, sz, src, sz)
-	case "arm", "mips", "mips64", "mipsle", "mips64le":
+	case "arm", "thumb", "mips", "mips64", "mipsle", "mips64le":
 		return sz <= 4
 	}
 	return false
diff --git a/src/cmd/compile/internal/ssa/rewriteThumb.go b/src/cmd/compile/internal/ssa/rewriteThumb.go
new file mode 100644
index 0000000000..8237f52862
--- /dev/null
+++ b/src/cmd/compile/internal/ssa/rewriteThumb.go
@@ -0,0 +1,22701 @@
+// Code generated from gen/Thumb.rules; DO NOT EDIT.
+// generated with: cd gen; go run *.go
+
+package ssa
+
+import "fmt"
+import "math"
+import "cmd/internal/obj"
+import "cmd/internal/objabi"
+import "cmd/compile/internal/types"
+
+var _ = fmt.Println   // in case not otherwise used
+var _ = math.MinInt8  // in case not otherwise used
+var _ = obj.ANOP      // in case not otherwise used
+var _ = objabi.GOROOT // in case not otherwise used
+var _ = types.TypeMem // in case not otherwise used
+
+func rewriteValueThumb(v *Value) bool {
+	switch v.Op {
+	case OpAdd16:
+		return rewriteValueThumb_OpAdd16_0(v)
+	case OpAdd32:
+		return rewriteValueThumb_OpAdd32_0(v)
+	case OpAdd32F:
+		return rewriteValueThumb_OpAdd32F_0(v)
+	case OpAdd32carry:
+		return rewriteValueThumb_OpAdd32carry_0(v)
+	case OpAdd32withcarry:
+		return rewriteValueThumb_OpAdd32withcarry_0(v)
+	case OpAdd64F:
+		return rewriteValueThumb_OpAdd64F_0(v)
+	case OpAdd8:
+		return rewriteValueThumb_OpAdd8_0(v)
+	case OpAddPtr:
+		return rewriteValueThumb_OpAddPtr_0(v)
+	case OpAddr:
+		return rewriteValueThumb_OpAddr_0(v)
+	case OpAnd16:
+		return rewriteValueThumb_OpAnd16_0(v)
+	case OpAnd32:
+		return rewriteValueThumb_OpAnd32_0(v)
+	case OpAnd8:
+		return rewriteValueThumb_OpAnd8_0(v)
+	case OpAndB:
+		return rewriteValueThumb_OpAndB_0(v)
+	case OpAvg32u:
+		return rewriteValueThumb_OpAvg32u_0(v)
+	case OpBitLen32:
+		return rewriteValueThumb_OpBitLen32_0(v)
+	case OpBswap32:
+		return rewriteValueThumb_OpBswap32_0(v)
+	case OpClosureCall:
+		return rewriteValueThumb_OpClosureCall_0(v)
+	case OpCom16:
+		return rewriteValueThumb_OpCom16_0(v)
+	case OpCom32:
+		return rewriteValueThumb_OpCom32_0(v)
+	case OpCom8:
+		return rewriteValueThumb_OpCom8_0(v)
+	case OpConst16:
+		return rewriteValueThumb_OpConst16_0(v)
+	case OpConst32:
+		return rewriteValueThumb_OpConst32_0(v)
+	case OpConst32F:
+		return rewriteValueThumb_OpConst32F_0(v)
+	case OpConst64F:
+		return rewriteValueThumb_OpConst64F_0(v)
+	case OpConst8:
+		return rewriteValueThumb_OpConst8_0(v)
+	case OpConstBool:
+		return rewriteValueThumb_OpConstBool_0(v)
+	case OpConstNil:
+		return rewriteValueThumb_OpConstNil_0(v)
+	case OpCtz16:
+		return rewriteValueThumb_OpCtz16_0(v)
+	case OpCtz16NonZero:
+		return rewriteValueThumb_OpCtz16NonZero_0(v)
+	case OpCtz32:
+		return rewriteValueThumb_OpCtz32_0(v)
+	case OpCtz32NonZero:
+		return rewriteValueThumb_OpCtz32NonZero_0(v)
+	case OpCtz8:
+		return rewriteValueThumb_OpCtz8_0(v)
+	case OpCtz8NonZero:
+		return rewriteValueThumb_OpCtz8NonZero_0(v)
+	case OpCvt32Fto32:
+		return rewriteValueThumb_OpCvt32Fto32_0(v)
+	case OpCvt32Fto32U:
+		return rewriteValueThumb_OpCvt32Fto32U_0(v)
+	case OpCvt32Fto64F:
+		return rewriteValueThumb_OpCvt32Fto64F_0(v)
+	case OpCvt32Uto32F:
+		return rewriteValueThumb_OpCvt32Uto32F_0(v)
+	case OpCvt32Uto64F:
+		return rewriteValueThumb_OpCvt32Uto64F_0(v)
+	case OpCvt32to32F:
+		return rewriteValueThumb_OpCvt32to32F_0(v)
+	case OpCvt32to64F:
+		return rewriteValueThumb_OpCvt32to64F_0(v)
+	case OpCvt64Fto32:
+		return rewriteValueThumb_OpCvt64Fto32_0(v)
+	case OpCvt64Fto32F:
+		return rewriteValueThumb_OpCvt64Fto32F_0(v)
+	case OpCvt64Fto32U:
+		return rewriteValueThumb_OpCvt64Fto32U_0(v)
+	case OpDiv16:
+		return rewriteValueThumb_OpDiv16_0(v)
+	case OpDiv16u:
+		return rewriteValueThumb_OpDiv16u_0(v)
+	case OpDiv32:
+		return rewriteValueThumb_OpDiv32_0(v)
+	case OpDiv32F:
+		return rewriteValueThumb_OpDiv32F_0(v)
+	case OpDiv32u:
+		return rewriteValueThumb_OpDiv32u_0(v)
+	case OpDiv64F:
+		return rewriteValueThumb_OpDiv64F_0(v)
+	case OpDiv8:
+		return rewriteValueThumb_OpDiv8_0(v)
+	case OpDiv8u:
+		return rewriteValueThumb_OpDiv8u_0(v)
+	case OpEq16:
+		return rewriteValueThumb_OpEq16_0(v)
+	case OpEq32:
+		return rewriteValueThumb_OpEq32_0(v)
+	case OpEq32F:
+		return rewriteValueThumb_OpEq32F_0(v)
+	case OpEq64F:
+		return rewriteValueThumb_OpEq64F_0(v)
+	case OpEq8:
+		return rewriteValueThumb_OpEq8_0(v)
+	case OpEqB:
+		return rewriteValueThumb_OpEqB_0(v)
+	case OpEqPtr:
+		return rewriteValueThumb_OpEqPtr_0(v)
+	case OpGeq16:
+		return rewriteValueThumb_OpGeq16_0(v)
+	case OpGeq16U:
+		return rewriteValueThumb_OpGeq16U_0(v)
+	case OpGeq32:
+		return rewriteValueThumb_OpGeq32_0(v)
+	case OpGeq32F:
+		return rewriteValueThumb_OpGeq32F_0(v)
+	case OpGeq32U:
+		return rewriteValueThumb_OpGeq32U_0(v)
+	case OpGeq64F:
+		return rewriteValueThumb_OpGeq64F_0(v)
+	case OpGeq8:
+		return rewriteValueThumb_OpGeq8_0(v)
+	case OpGeq8U:
+		return rewriteValueThumb_OpGeq8U_0(v)
+	case OpGetCallerPC:
+		return rewriteValueThumb_OpGetCallerPC_0(v)
+	case OpGetCallerSP:
+		return rewriteValueThumb_OpGetCallerSP_0(v)
+	case OpGetClosurePtr:
+		return rewriteValueThumb_OpGetClosurePtr_0(v)
+	case OpGreater16:
+		return rewriteValueThumb_OpGreater16_0(v)
+	case OpGreater16U:
+		return rewriteValueThumb_OpGreater16U_0(v)
+	case OpGreater32:
+		return rewriteValueThumb_OpGreater32_0(v)
+	case OpGreater32F:
+		return rewriteValueThumb_OpGreater32F_0(v)
+	case OpGreater32U:
+		return rewriteValueThumb_OpGreater32U_0(v)
+	case OpGreater64F:
+		return rewriteValueThumb_OpGreater64F_0(v)
+	case OpGreater8:
+		return rewriteValueThumb_OpGreater8_0(v)
+	case OpGreater8U:
+		return rewriteValueThumb_OpGreater8U_0(v)
+	case OpHmul32:
+		return rewriteValueThumb_OpHmul32_0(v)
+	case OpHmul32u:
+		return rewriteValueThumb_OpHmul32u_0(v)
+	case OpInterCall:
+		return rewriteValueThumb_OpInterCall_0(v)
+	case OpIsInBounds:
+		return rewriteValueThumb_OpIsInBounds_0(v)
+	case OpIsNonNil:
+		return rewriteValueThumb_OpIsNonNil_0(v)
+	case OpIsSliceInBounds:
+		return rewriteValueThumb_OpIsSliceInBounds_0(v)
+	case OpLeq16:
+		return rewriteValueThumb_OpLeq16_0(v)
+	case OpLeq16U:
+		return rewriteValueThumb_OpLeq16U_0(v)
+	case OpLeq32:
+		return rewriteValueThumb_OpLeq32_0(v)
+	case OpLeq32F:
+		return rewriteValueThumb_OpLeq32F_0(v)
+	case OpLeq32U:
+		return rewriteValueThumb_OpLeq32U_0(v)
+	case OpLeq64F:
+		return rewriteValueThumb_OpLeq64F_0(v)
+	case OpLeq8:
+		return rewriteValueThumb_OpLeq8_0(v)
+	case OpLeq8U:
+		return rewriteValueThumb_OpLeq8U_0(v)
+	case OpLess16:
+		return rewriteValueThumb_OpLess16_0(v)
+	case OpLess16U:
+		return rewriteValueThumb_OpLess16U_0(v)
+	case OpLess32:
+		return rewriteValueThumb_OpLess32_0(v)
+	case OpLess32F:
+		return rewriteValueThumb_OpLess32F_0(v)
+	case OpLess32U:
+		return rewriteValueThumb_OpLess32U_0(v)
+	case OpLess64F:
+		return rewriteValueThumb_OpLess64F_0(v)
+	case OpLess8:
+		return rewriteValueThumb_OpLess8_0(v)
+	case OpLess8U:
+		return rewriteValueThumb_OpLess8U_0(v)
+	case OpLoad:
+		return rewriteValueThumb_OpLoad_0(v)
+	case OpLocalAddr:
+		return rewriteValueThumb_OpLocalAddr_0(v)
+	case OpLsh16x16:
+		return rewriteValueThumb_OpLsh16x16_0(v)
+	case OpLsh16x32:
+		return rewriteValueThumb_OpLsh16x32_0(v)
+	case OpLsh16x64:
+		return rewriteValueThumb_OpLsh16x64_0(v)
+	case OpLsh16x8:
+		return rewriteValueThumb_OpLsh16x8_0(v)
+	case OpLsh32x16:
+		return rewriteValueThumb_OpLsh32x16_0(v)
+	case OpLsh32x32:
+		return rewriteValueThumb_OpLsh32x32_0(v)
+	case OpLsh32x64:
+		return rewriteValueThumb_OpLsh32x64_0(v)
+	case OpLsh32x8:
+		return rewriteValueThumb_OpLsh32x8_0(v)
+	case OpLsh8x16:
+		return rewriteValueThumb_OpLsh8x16_0(v)
+	case OpLsh8x32:
+		return rewriteValueThumb_OpLsh8x32_0(v)
+	case OpLsh8x64:
+		return rewriteValueThumb_OpLsh8x64_0(v)
+	case OpLsh8x8:
+		return rewriteValueThumb_OpLsh8x8_0(v)
+	case OpMMIOLoad16:
+		return rewriteValueThumb_OpMMIOLoad16_0(v)
+	case OpMMIOLoad32:
+		return rewriteValueThumb_OpMMIOLoad32_0(v)
+	case OpMMIOLoad8:
+		return rewriteValueThumb_OpMMIOLoad8_0(v)
+	case OpMMIOMB:
+		return rewriteValueThumb_OpMMIOMB_0(v)
+	case OpMMIOStore16:
+		return rewriteValueThumb_OpMMIOStore16_0(v)
+	case OpMMIOStore32:
+		return rewriteValueThumb_OpMMIOStore32_0(v)
+	case OpMMIOStore8:
+		return rewriteValueThumb_OpMMIOStore8_0(v)
+	case OpMod16:
+		return rewriteValueThumb_OpMod16_0(v)
+	case OpMod16u:
+		return rewriteValueThumb_OpMod16u_0(v)
+	case OpMod32:
+		return rewriteValueThumb_OpMod32_0(v)
+	case OpMod32u:
+		return rewriteValueThumb_OpMod32u_0(v)
+	case OpMod8:
+		return rewriteValueThumb_OpMod8_0(v)
+	case OpMod8u:
+		return rewriteValueThumb_OpMod8u_0(v)
+	case OpMove:
+		return rewriteValueThumb_OpMove_0(v)
+	case OpMul16:
+		return rewriteValueThumb_OpMul16_0(v)
+	case OpMul32:
+		return rewriteValueThumb_OpMul32_0(v)
+	case OpMul32F:
+		return rewriteValueThumb_OpMul32F_0(v)
+	case OpMul32uhilo:
+		return rewriteValueThumb_OpMul32uhilo_0(v)
+	case OpMul64F:
+		return rewriteValueThumb_OpMul64F_0(v)
+	case OpMul8:
+		return rewriteValueThumb_OpMul8_0(v)
+	case OpNeg16:
+		return rewriteValueThumb_OpNeg16_0(v)
+	case OpNeg32:
+		return rewriteValueThumb_OpNeg32_0(v)
+	case OpNeg32F:
+		return rewriteValueThumb_OpNeg32F_0(v)
+	case OpNeg64F:
+		return rewriteValueThumb_OpNeg64F_0(v)
+	case OpNeg8:
+		return rewriteValueThumb_OpNeg8_0(v)
+	case OpNeq16:
+		return rewriteValueThumb_OpNeq16_0(v)
+	case OpNeq32:
+		return rewriteValueThumb_OpNeq32_0(v)
+	case OpNeq32F:
+		return rewriteValueThumb_OpNeq32F_0(v)
+	case OpNeq64F:
+		return rewriteValueThumb_OpNeq64F_0(v)
+	case OpNeq8:
+		return rewriteValueThumb_OpNeq8_0(v)
+	case OpNeqB:
+		return rewriteValueThumb_OpNeqB_0(v)
+	case OpNeqPtr:
+		return rewriteValueThumb_OpNeqPtr_0(v)
+	case OpNilCheck:
+		return rewriteValueThumb_OpNilCheck_0(v)
+	case OpNot:
+		return rewriteValueThumb_OpNot_0(v)
+	case OpOffPtr:
+		return rewriteValueThumb_OpOffPtr_0(v)
+	case OpOr16:
+		return rewriteValueThumb_OpOr16_0(v)
+	case OpOr32:
+		return rewriteValueThumb_OpOr32_0(v)
+	case OpOr8:
+		return rewriteValueThumb_OpOr8_0(v)
+	case OpOrB:
+		return rewriteValueThumb_OpOrB_0(v)
+	case OpPanicBounds:
+		return rewriteValueThumb_OpPanicBounds_0(v)
+	case OpPanicExtend:
+		return rewriteValueThumb_OpPanicExtend_0(v)
+	case OpPublicationBarrier:
+		return rewriteValueThumb_OpPublicationBarrier_0(v)
+	case OpRotateLeft16:
+		return rewriteValueThumb_OpRotateLeft16_0(v)
+	case OpRotateLeft32:
+		return rewriteValueThumb_OpRotateLeft32_0(v)
+	case OpRotateLeft8:
+		return rewriteValueThumb_OpRotateLeft8_0(v)
+	case OpRound32F:
+		return rewriteValueThumb_OpRound32F_0(v)
+	case OpRound64F:
+		return rewriteValueThumb_OpRound64F_0(v)
+	case OpRsh16Ux16:
+		return rewriteValueThumb_OpRsh16Ux16_0(v)
+	case OpRsh16Ux32:
+		return rewriteValueThumb_OpRsh16Ux32_0(v)
+	case OpRsh16Ux64:
+		return rewriteValueThumb_OpRsh16Ux64_0(v)
+	case OpRsh16Ux8:
+		return rewriteValueThumb_OpRsh16Ux8_0(v)
+	case OpRsh16x16:
+		return rewriteValueThumb_OpRsh16x16_0(v)
+	case OpRsh16x32:
+		return rewriteValueThumb_OpRsh16x32_0(v)
+	case OpRsh16x64:
+		return rewriteValueThumb_OpRsh16x64_0(v)
+	case OpRsh16x8:
+		return rewriteValueThumb_OpRsh16x8_0(v)
+	case OpRsh32Ux16:
+		return rewriteValueThumb_OpRsh32Ux16_0(v)
+	case OpRsh32Ux32:
+		return rewriteValueThumb_OpRsh32Ux32_0(v)
+	case OpRsh32Ux64:
+		return rewriteValueThumb_OpRsh32Ux64_0(v)
+	case OpRsh32Ux8:
+		return rewriteValueThumb_OpRsh32Ux8_0(v)
+	case OpRsh32x16:
+		return rewriteValueThumb_OpRsh32x16_0(v)
+	case OpRsh32x32:
+		return rewriteValueThumb_OpRsh32x32_0(v)
+	case OpRsh32x64:
+		return rewriteValueThumb_OpRsh32x64_0(v)
+	case OpRsh32x8:
+		return rewriteValueThumb_OpRsh32x8_0(v)
+	case OpRsh8Ux16:
+		return rewriteValueThumb_OpRsh8Ux16_0(v)
+	case OpRsh8Ux32:
+		return rewriteValueThumb_OpRsh8Ux32_0(v)
+	case OpRsh8Ux64:
+		return rewriteValueThumb_OpRsh8Ux64_0(v)
+	case OpRsh8Ux8:
+		return rewriteValueThumb_OpRsh8Ux8_0(v)
+	case OpRsh8x16:
+		return rewriteValueThumb_OpRsh8x16_0(v)
+	case OpRsh8x32:
+		return rewriteValueThumb_OpRsh8x32_0(v)
+	case OpRsh8x64:
+		return rewriteValueThumb_OpRsh8x64_0(v)
+	case OpRsh8x8:
+		return rewriteValueThumb_OpRsh8x8_0(v)
+	case OpSignExt16to32:
+		return rewriteValueThumb_OpSignExt16to32_0(v)
+	case OpSignExt8to16:
+		return rewriteValueThumb_OpSignExt8to16_0(v)
+	case OpSignExt8to32:
+		return rewriteValueThumb_OpSignExt8to32_0(v)
+	case OpSignmask:
+		return rewriteValueThumb_OpSignmask_0(v)
+	case OpSlicemask:
+		return rewriteValueThumb_OpSlicemask_0(v)
+	case OpSqrt:
+		return rewriteValueThumb_OpSqrt_0(v)
+	case OpStaticCall:
+		return rewriteValueThumb_OpStaticCall_0(v)
+	case OpStore:
+		return rewriteValueThumb_OpStore_0(v)
+	case OpSub16:
+		return rewriteValueThumb_OpSub16_0(v)
+	case OpSub32:
+		return rewriteValueThumb_OpSub32_0(v)
+	case OpSub32F:
+		return rewriteValueThumb_OpSub32F_0(v)
+	case OpSub32carry:
+		return rewriteValueThumb_OpSub32carry_0(v)
+	case OpSub32withcarry:
+		return rewriteValueThumb_OpSub32withcarry_0(v)
+	case OpSub64F:
+		return rewriteValueThumb_OpSub64F_0(v)
+	case OpSub8:
+		return rewriteValueThumb_OpSub8_0(v)
+	case OpSubPtr:
+		return rewriteValueThumb_OpSubPtr_0(v)
+	case OpThumbADC:
+		return rewriteValueThumb_OpThumbADC_0(v) || rewriteValueThumb_OpThumbADC_10(v)
+	case OpThumbADCconst:
+		return rewriteValueThumb_OpThumbADCconst_0(v)
+	case OpThumbADCshiftLL:
+		return rewriteValueThumb_OpThumbADCshiftLL_0(v)
+	case OpThumbADCshiftRA:
+		return rewriteValueThumb_OpThumbADCshiftRA_0(v)
+	case OpThumbADCshiftRL:
+		return rewriteValueThumb_OpThumbADCshiftRL_0(v)
+	case OpThumbADD:
+		return rewriteValueThumb_OpThumbADD_0(v) || rewriteValueThumb_OpThumbADD_10(v)
+	case OpThumbADDD:
+		return rewriteValueThumb_OpThumbADDD_0(v)
+	case OpThumbADDF:
+		return rewriteValueThumb_OpThumbADDF_0(v)
+	case OpThumbADDS:
+		return rewriteValueThumb_OpThumbADDS_0(v)
+	case OpThumbADDSshiftLL:
+		return rewriteValueThumb_OpThumbADDSshiftLL_0(v)
+	case OpThumbADDSshiftRA:
+		return rewriteValueThumb_OpThumbADDSshiftRA_0(v)
+	case OpThumbADDSshiftRL:
+		return rewriteValueThumb_OpThumbADDSshiftRL_0(v)
+	case OpThumbADDconst:
+		return rewriteValueThumb_OpThumbADDconst_0(v)
+	case OpThumbADDshiftLL:
+		return rewriteValueThumb_OpThumbADDshiftLL_0(v)
+	case OpThumbADDshiftRA:
+		return rewriteValueThumb_OpThumbADDshiftRA_0(v)
+	case OpThumbADDshiftRL:
+		return rewriteValueThumb_OpThumbADDshiftRL_0(v)
+	case OpThumbAND:
+		return rewriteValueThumb_OpThumbAND_0(v) || rewriteValueThumb_OpThumbAND_10(v)
+	case OpThumbANDconst:
+		return rewriteValueThumb_OpThumbANDconst_0(v)
+	case OpThumbANDshiftLL:
+		return rewriteValueThumb_OpThumbANDshiftLL_0(v)
+	case OpThumbANDshiftRA:
+		return rewriteValueThumb_OpThumbANDshiftRA_0(v)
+	case OpThumbANDshiftRL:
+		return rewriteValueThumb_OpThumbANDshiftRL_0(v)
+	case OpThumbBFX:
+		return rewriteValueThumb_OpThumbBFX_0(v)
+	case OpThumbBFXU:
+		return rewriteValueThumb_OpThumbBFXU_0(v)
+	case OpThumbBIC:
+		return rewriteValueThumb_OpThumbBIC_0(v)
+	case OpThumbBICconst:
+		return rewriteValueThumb_OpThumbBICconst_0(v)
+	case OpThumbBICshiftLL:
+		return rewriteValueThumb_OpThumbBICshiftLL_0(v)
+	case OpThumbBICshiftRA:
+		return rewriteValueThumb_OpThumbBICshiftRA_0(v)
+	case OpThumbBICshiftRL:
+		return rewriteValueThumb_OpThumbBICshiftRL_0(v)
+	case OpThumbCMN:
+		return rewriteValueThumb_OpThumbCMN_0(v)
+	case OpThumbCMNconst:
+		return rewriteValueThumb_OpThumbCMNconst_0(v)
+	case OpThumbCMNshiftLL:
+		return rewriteValueThumb_OpThumbCMNshiftLL_0(v)
+	case OpThumbCMNshiftRA:
+		return rewriteValueThumb_OpThumbCMNshiftRA_0(v)
+	case OpThumbCMNshiftRL:
+		return rewriteValueThumb_OpThumbCMNshiftRL_0(v)
+	case OpThumbCMOVWHSconst:
+		return rewriteValueThumb_OpThumbCMOVWHSconst_0(v)
+	case OpThumbCMOVWLSconst:
+		return rewriteValueThumb_OpThumbCMOVWLSconst_0(v)
+	case OpThumbCMP:
+		return rewriteValueThumb_OpThumbCMP_0(v)
+	case OpThumbCMPD:
+		return rewriteValueThumb_OpThumbCMPD_0(v)
+	case OpThumbCMPF:
+		return rewriteValueThumb_OpThumbCMPF_0(v)
+	case OpThumbCMPconst:
+		return rewriteValueThumb_OpThumbCMPconst_0(v)
+	case OpThumbCMPshiftLL:
+		return rewriteValueThumb_OpThumbCMPshiftLL_0(v)
+	case OpThumbCMPshiftRA:
+		return rewriteValueThumb_OpThumbCMPshiftRA_0(v)
+	case OpThumbCMPshiftRL:
+		return rewriteValueThumb_OpThumbCMPshiftRL_0(v)
+	case OpThumbDIV:
+		return rewriteValueThumb_OpThumbDIV_0(v)
+	case OpThumbDIVU:
+		return rewriteValueThumb_OpThumbDIVU_0(v)
+	case OpThumbEqual:
+		return rewriteValueThumb_OpThumbEqual_0(v)
+	case OpThumbGreaterEqual:
+		return rewriteValueThumb_OpThumbGreaterEqual_0(v)
+	case OpThumbGreaterEqualU:
+		return rewriteValueThumb_OpThumbGreaterEqualU_0(v)
+	case OpThumbGreaterThan:
+		return rewriteValueThumb_OpThumbGreaterThan_0(v)
+	case OpThumbGreaterThanU:
+		return rewriteValueThumb_OpThumbGreaterThanU_0(v)
+	case OpThumbLessEqual:
+		return rewriteValueThumb_OpThumbLessEqual_0(v)
+	case OpThumbLessEqualU:
+		return rewriteValueThumb_OpThumbLessEqualU_0(v)
+	case OpThumbLessThan:
+		return rewriteValueThumb_OpThumbLessThan_0(v)
+	case OpThumbLessThanU:
+		return rewriteValueThumb_OpThumbLessThanU_0(v)
+	case OpThumbLoadOnce16:
+		return rewriteValueThumb_OpThumbLoadOnce16_0(v)
+	case OpThumbLoadOnce16idx:
+		return rewriteValueThumb_OpThumbLoadOnce16idx_0(v)
+	case OpThumbLoadOnce32:
+		return rewriteValueThumb_OpThumbLoadOnce32_0(v)
+	case OpThumbLoadOnce32idx:
+		return rewriteValueThumb_OpThumbLoadOnce32idx_0(v)
+	case OpThumbLoadOnce8:
+		return rewriteValueThumb_OpThumbLoadOnce8_0(v)
+	case OpThumbLoadOnce8idx:
+		return rewriteValueThumb_OpThumbLoadOnce8idx_0(v)
+	case OpThumbMOVBUload:
+		return rewriteValueThumb_OpThumbMOVBUload_0(v)
+	case OpThumbMOVBUloadidx:
+		return rewriteValueThumb_OpThumbMOVBUloadidx_0(v)
+	case OpThumbMOVBUreg:
+		return rewriteValueThumb_OpThumbMOVBUreg_0(v)
+	case OpThumbMOVBload:
+		return rewriteValueThumb_OpThumbMOVBload_0(v)
+	case OpThumbMOVBloadidx:
+		return rewriteValueThumb_OpThumbMOVBloadidx_0(v)
+	case OpThumbMOVBreg:
+		return rewriteValueThumb_OpThumbMOVBreg_0(v)
+	case OpThumbMOVBstore:
+		return rewriteValueThumb_OpThumbMOVBstore_0(v)
+	case OpThumbMOVBstoreidx:
+		return rewriteValueThumb_OpThumbMOVBstoreidx_0(v)
+	case OpThumbMOVDload:
+		return rewriteValueThumb_OpThumbMOVDload_0(v)
+	case OpThumbMOVDstore:
+		return rewriteValueThumb_OpThumbMOVDstore_0(v)
+	case OpThumbMOVFload:
+		return rewriteValueThumb_OpThumbMOVFload_0(v)
+	case OpThumbMOVFstore:
+		return rewriteValueThumb_OpThumbMOVFstore_0(v)
+	case OpThumbMOVHUload:
+		return rewriteValueThumb_OpThumbMOVHUload_0(v)
+	case OpThumbMOVHUloadidx:
+		return rewriteValueThumb_OpThumbMOVHUloadidx_0(v)
+	case OpThumbMOVHUreg:
+		return rewriteValueThumb_OpThumbMOVHUreg_0(v)
+	case OpThumbMOVHload:
+		return rewriteValueThumb_OpThumbMOVHload_0(v)
+	case OpThumbMOVHloadidx:
+		return rewriteValueThumb_OpThumbMOVHloadidx_0(v)
+	case OpThumbMOVHreg:
+		return rewriteValueThumb_OpThumbMOVHreg_0(v)
+	case OpThumbMOVHstore:
+		return rewriteValueThumb_OpThumbMOVHstore_0(v)
+	case OpThumbMOVHstoreidx:
+		return rewriteValueThumb_OpThumbMOVHstoreidx_0(v)
+	case OpThumbMOVWload:
+		return rewriteValueThumb_OpThumbMOVWload_0(v)
+	case OpThumbMOVWloadidx:
+		return rewriteValueThumb_OpThumbMOVWloadidx_0(v)
+	case OpThumbMOVWloadshiftLL:
+		return rewriteValueThumb_OpThumbMOVWloadshiftLL_0(v)
+	case OpThumbMOVWreg:
+		return rewriteValueThumb_OpThumbMOVWreg_0(v)
+	case OpThumbMOVWstore:
+		return rewriteValueThumb_OpThumbMOVWstore_0(v)
+	case OpThumbMOVWstoreidx:
+		return rewriteValueThumb_OpThumbMOVWstoreidx_0(v)
+	case OpThumbMOVWstoreshiftLL:
+		return rewriteValueThumb_OpThumbMOVWstoreshiftLL_0(v)
+	case OpThumbMUL:
+		return rewriteValueThumb_OpThumbMUL_0(v) || rewriteValueThumb_OpThumbMUL_10(v) || rewriteValueThumb_OpThumbMUL_20(v)
+	case OpThumbMULA:
+		return rewriteValueThumb_OpThumbMULA_0(v) || rewriteValueThumb_OpThumbMULA_10(v) || rewriteValueThumb_OpThumbMULA_20(v)
+	case OpThumbMULD:
+		return rewriteValueThumb_OpThumbMULD_0(v)
+	case OpThumbMULF:
+		return rewriteValueThumb_OpThumbMULF_0(v)
+	case OpThumbMULS:
+		return rewriteValueThumb_OpThumbMULS_0(v) || rewriteValueThumb_OpThumbMULS_10(v) || rewriteValueThumb_OpThumbMULS_20(v)
+	case OpThumbMVN:
+		return rewriteValueThumb_OpThumbMVN_0(v)
+	case OpThumbMVNshiftLL:
+		return rewriteValueThumb_OpThumbMVNshiftLL_0(v)
+	case OpThumbMVNshiftRA:
+		return rewriteValueThumb_OpThumbMVNshiftRA_0(v)
+	case OpThumbMVNshiftRL:
+		return rewriteValueThumb_OpThumbMVNshiftRL_0(v)
+	case OpThumbNEGD:
+		return rewriteValueThumb_OpThumbNEGD_0(v)
+	case OpThumbNEGF:
+		return rewriteValueThumb_OpThumbNEGF_0(v)
+	case OpThumbNMULD:
+		return rewriteValueThumb_OpThumbNMULD_0(v)
+	case OpThumbNMULF:
+		return rewriteValueThumb_OpThumbNMULF_0(v)
+	case OpThumbNotEqual:
+		return rewriteValueThumb_OpThumbNotEqual_0(v)
+	case OpThumbOR:
+		return rewriteValueThumb_OpThumbOR_0(v)
+	case OpThumbORconst:
+		return rewriteValueThumb_OpThumbORconst_0(v)
+	case OpThumbORshiftLL:
+		return rewriteValueThumb_OpThumbORshiftLL_0(v)
+	case OpThumbORshiftRA:
+		return rewriteValueThumb_OpThumbORshiftRA_0(v)
+	case OpThumbORshiftRL:
+		return rewriteValueThumb_OpThumbORshiftRL_0(v)
+	case OpThumbRSB:
+		return rewriteValueThumb_OpThumbRSB_0(v)
+	case OpThumbRSBSshiftLL:
+		return rewriteValueThumb_OpThumbRSBSshiftLL_0(v)
+	case OpThumbRSBSshiftRA:
+		return rewriteValueThumb_OpThumbRSBSshiftRA_0(v)
+	case OpThumbRSBSshiftRL:
+		return rewriteValueThumb_OpThumbRSBSshiftRL_0(v)
+	case OpThumbRSBconst:
+		return rewriteValueThumb_OpThumbRSBconst_0(v)
+	case OpThumbRSBshiftLL:
+		return rewriteValueThumb_OpThumbRSBshiftLL_0(v)
+	case OpThumbRSBshiftRA:
+		return rewriteValueThumb_OpThumbRSBshiftRA_0(v)
+	case OpThumbRSBshiftRL:
+		return rewriteValueThumb_OpThumbRSBshiftRL_0(v)
+	case OpThumbSBC:
+		return rewriteValueThumb_OpThumbSBC_0(v)
+	case OpThumbSBCconst:
+		return rewriteValueThumb_OpThumbSBCconst_0(v)
+	case OpThumbSBCshiftLL:
+		return rewriteValueThumb_OpThumbSBCshiftLL_0(v)
+	case OpThumbSBCshiftRA:
+		return rewriteValueThumb_OpThumbSBCshiftRA_0(v)
+	case OpThumbSBCshiftRL:
+		return rewriteValueThumb_OpThumbSBCshiftRL_0(v)
+	case OpThumbSLL:
+		return rewriteValueThumb_OpThumbSLL_0(v)
+	case OpThumbSLLconst:
+		return rewriteValueThumb_OpThumbSLLconst_0(v)
+	case OpThumbSRA:
+		return rewriteValueThumb_OpThumbSRA_0(v)
+	case OpThumbSRAcond:
+		return rewriteValueThumb_OpThumbSRAcond_0(v)
+	case OpThumbSRAconst:
+		return rewriteValueThumb_OpThumbSRAconst_0(v)
+	case OpThumbSRL:
+		return rewriteValueThumb_OpThumbSRL_0(v)
+	case OpThumbSRLconst:
+		return rewriteValueThumb_OpThumbSRLconst_0(v)
+	case OpThumbSUB:
+		return rewriteValueThumb_OpThumbSUB_0(v)
+	case OpThumbSUBD:
+		return rewriteValueThumb_OpThumbSUBD_0(v)
+	case OpThumbSUBF:
+		return rewriteValueThumb_OpThumbSUBF_0(v)
+	case OpThumbSUBS:
+		return rewriteValueThumb_OpThumbSUBS_0(v)
+	case OpThumbSUBSshiftLL:
+		return rewriteValueThumb_OpThumbSUBSshiftLL_0(v)
+	case OpThumbSUBSshiftRA:
+		return rewriteValueThumb_OpThumbSUBSshiftRA_0(v)
+	case OpThumbSUBSshiftRL:
+		return rewriteValueThumb_OpThumbSUBSshiftRL_0(v)
+	case OpThumbSUBconst:
+		return rewriteValueThumb_OpThumbSUBconst_0(v)
+	case OpThumbSUBshiftLL:
+		return rewriteValueThumb_OpThumbSUBshiftLL_0(v)
+	case OpThumbSUBshiftRA:
+		return rewriteValueThumb_OpThumbSUBshiftRA_0(v)
+	case OpThumbSUBshiftRL:
+		return rewriteValueThumb_OpThumbSUBshiftRL_0(v)
+	case OpThumbStoreOnce16:
+		return rewriteValueThumb_OpThumbStoreOnce16_0(v)
+	case OpThumbStoreOnce16idx:
+		return rewriteValueThumb_OpThumbStoreOnce16idx_0(v)
+	case OpThumbStoreOnce32:
+		return rewriteValueThumb_OpThumbStoreOnce32_0(v)
+	case OpThumbStoreOnce32idx:
+		return rewriteValueThumb_OpThumbStoreOnce32idx_0(v)
+	case OpThumbStoreOnce8:
+		return rewriteValueThumb_OpThumbStoreOnce8_0(v)
+	case OpThumbStoreOnce8idx:
+		return rewriteValueThumb_OpThumbStoreOnce8idx_0(v)
+	case OpThumbTEQ:
+		return rewriteValueThumb_OpThumbTEQ_0(v)
+	case OpThumbTEQconst:
+		return rewriteValueThumb_OpThumbTEQconst_0(v)
+	case OpThumbTEQshiftLL:
+		return rewriteValueThumb_OpThumbTEQshiftLL_0(v)
+	case OpThumbTEQshiftRA:
+		return rewriteValueThumb_OpThumbTEQshiftRA_0(v)
+	case OpThumbTEQshiftRL:
+		return rewriteValueThumb_OpThumbTEQshiftRL_0(v)
+	case OpThumbTST:
+		return rewriteValueThumb_OpThumbTST_0(v)
+	case OpThumbTSTconst:
+		return rewriteValueThumb_OpThumbTSTconst_0(v)
+	case OpThumbTSTshiftLL:
+		return rewriteValueThumb_OpThumbTSTshiftLL_0(v)
+	case OpThumbTSTshiftRA:
+		return rewriteValueThumb_OpThumbTSTshiftRA_0(v)
+	case OpThumbTSTshiftRL:
+		return rewriteValueThumb_OpThumbTSTshiftRL_0(v)
+	case OpThumbXOR:
+		return rewriteValueThumb_OpThumbXOR_0(v) || rewriteValueThumb_OpThumbXOR_10(v)
+	case OpThumbXORconst:
+		return rewriteValueThumb_OpThumbXORconst_0(v)
+	case OpThumbXORshiftLL:
+		return rewriteValueThumb_OpThumbXORshiftLL_0(v)
+	case OpThumbXORshiftRA:
+		return rewriteValueThumb_OpThumbXORshiftRA_0(v)
+	case OpThumbXORshiftRL:
+		return rewriteValueThumb_OpThumbXORshiftRL_0(v)
+	case OpThumbXORshiftRR:
+		return rewriteValueThumb_OpThumbXORshiftRR_0(v)
+	case OpTrunc16to8:
+		return rewriteValueThumb_OpTrunc16to8_0(v)
+	case OpTrunc32to16:
+		return rewriteValueThumb_OpTrunc32to16_0(v)
+	case OpTrunc32to8:
+		return rewriteValueThumb_OpTrunc32to8_0(v)
+	case OpWB:
+		return rewriteValueThumb_OpWB_0(v)
+	case OpXor16:
+		return rewriteValueThumb_OpXor16_0(v)
+	case OpXor32:
+		return rewriteValueThumb_OpXor32_0(v)
+	case OpXor8:
+		return rewriteValueThumb_OpXor8_0(v)
+	case OpZero:
+		return rewriteValueThumb_OpZero_0(v)
+	case OpZeroExt16to32:
+		return rewriteValueThumb_OpZeroExt16to32_0(v)
+	case OpZeroExt8to16:
+		return rewriteValueThumb_OpZeroExt8to16_0(v)
+	case OpZeroExt8to32:
+		return rewriteValueThumb_OpZeroExt8to32_0(v)
+	case OpZeromask:
+		return rewriteValueThumb_OpZeromask_0(v)
+	}
+	return false
+}
+func rewriteValueThumb_OpAdd16_0(v *Value) bool {
+	// match: (Add16 x y)
+	// cond:
+	// result: (ADD x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbADD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAdd32_0(v *Value) bool {
+	// match: (Add32 x y)
+	// cond:
+	// result: (ADD x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbADD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAdd32F_0(v *Value) bool {
+	// match: (Add32F x y)
+	// cond:
+	// result: (ADDF x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbADDF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAdd32carry_0(v *Value) bool {
+	// match: (Add32carry x y)
+	// cond:
+	// result: (ADDS x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbADDS)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAdd32withcarry_0(v *Value) bool {
+	// match: (Add32withcarry x y c)
+	// cond:
+	// result: (ADC x y c)
+	for {
+		c := v.Args[2]
+		x := v.Args[0]
+		y := v.Args[1]
+		v.reset(OpThumbADC)
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(c)
+		return true
+	}
+}
+func rewriteValueThumb_OpAdd64F_0(v *Value) bool {
+	// match: (Add64F x y)
+	// cond:
+	// result: (ADDD x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbADDD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAdd8_0(v *Value) bool {
+	// match: (Add8 x y)
+	// cond:
+	// result: (ADD x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbADD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAddPtr_0(v *Value) bool {
+	// match: (AddPtr x y)
+	// cond:
+	// result: (ADD x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbADD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAddr_0(v *Value) bool {
+	// match: (Addr {sym} base)
+	// cond:
+	// result: (MOVWaddr {sym} base)
+	for {
+		sym := v.Aux
+		base := v.Args[0]
+		v.reset(OpThumbMOVWaddr)
+		v.Aux = sym
+		v.AddArg(base)
+		return true
+	}
+}
+func rewriteValueThumb_OpAnd16_0(v *Value) bool {
+	// match: (And16 x y)
+	// cond:
+	// result: (AND x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbAND)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAnd32_0(v *Value) bool {
+	// match: (And32 x y)
+	// cond:
+	// result: (AND x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbAND)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAnd8_0(v *Value) bool {
+	// match: (And8 x y)
+	// cond:
+	// result: (AND x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbAND)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAndB_0(v *Value) bool {
+	// match: (AndB x y)
+	// cond:
+	// result: (AND x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbAND)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpAvg32u_0(v *Value) bool {
+	b := v.Block
+	// match: (Avg32u <t> x y)
+	// cond:
+	// result: (ADD (SRLconst <t> (SUB <t> x y) [1]) y)
+	for {
+		t := v.Type
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, t)
+		v0.AuxInt = 1
+		v1 := b.NewValue0(v.Pos, OpThumbSUB, t)
+		v1.AddArg(x)
+		v1.AddArg(y)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpBitLen32_0(v *Value) bool {
+	b := v.Block
+	// match: (BitLen32 <t> x)
+	// cond:
+	// result: (RSBconst [32] (CLZ <t> x))
+	for {
+		t := v.Type
+		x := v.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = 32
+		v0 := b.NewValue0(v.Pos, OpThumbCLZ, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpBswap32_0(v *Value) bool {
+	// match: (Bswap32 x)
+	// cond:
+	// result: (REV x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbREV)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpClosureCall_0(v *Value) bool {
+	// match: (ClosureCall [argwid] entry closure mem)
+	// cond:
+	// result: (CALLclosure [argwid] entry closure mem)
+	for {
+		argwid := v.AuxInt
+		mem := v.Args[2]
+		entry := v.Args[0]
+		closure := v.Args[1]
+		v.reset(OpThumbCALLclosure)
+		v.AuxInt = argwid
+		v.AddArg(entry)
+		v.AddArg(closure)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpCom16_0(v *Value) bool {
+	// match: (Com16 x)
+	// cond:
+	// result: (MVN x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMVN)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCom32_0(v *Value) bool {
+	// match: (Com32 x)
+	// cond:
+	// result: (MVN x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMVN)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCom8_0(v *Value) bool {
+	// match: (Com8 x)
+	// cond:
+	// result: (MVN x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMVN)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpConst16_0(v *Value) bool {
+	// match: (Const16 [val])
+	// cond:
+	// result: (MOVWconst [val])
+	for {
+		val := v.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = val
+		return true
+	}
+}
+func rewriteValueThumb_OpConst32_0(v *Value) bool {
+	// match: (Const32 [val])
+	// cond:
+	// result: (MOVWconst [val])
+	for {
+		val := v.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = val
+		return true
+	}
+}
+func rewriteValueThumb_OpConst32F_0(v *Value) bool {
+	// match: (Const32F [val])
+	// cond:
+	// result: (MOVFconst [val])
+	for {
+		val := v.AuxInt
+		v.reset(OpThumbMOVFconst)
+		v.AuxInt = val
+		return true
+	}
+}
+func rewriteValueThumb_OpConst64F_0(v *Value) bool {
+	// match: (Const64F [val])
+	// cond:
+	// result: (MOVDconst [val])
+	for {
+		val := v.AuxInt
+		v.reset(OpThumbMOVDconst)
+		v.AuxInt = val
+		return true
+	}
+}
+func rewriteValueThumb_OpConst8_0(v *Value) bool {
+	// match: (Const8 [val])
+	// cond:
+	// result: (MOVWconst [val])
+	for {
+		val := v.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = val
+		return true
+	}
+}
+func rewriteValueThumb_OpConstBool_0(v *Value) bool {
+	// match: (ConstBool [b])
+	// cond:
+	// result: (MOVWconst [b])
+	for {
+		b := v.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = b
+		return true
+	}
+}
+func rewriteValueThumb_OpConstNil_0(v *Value) bool {
+	// match: (ConstNil)
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Ctz16 <t> x)
+	// cond:
+	// result: (CLZ <t> (RBIT <typ.UInt32> (ORconst <typ.UInt32> [0x10000] x)))
+	for {
+		t := v.Type
+		x := v.Args[0]
+		v.reset(OpThumbCLZ)
+		v.Type = t
+		v0 := b.NewValue0(v.Pos, OpThumbRBIT, typ.UInt32)
+		v1 := b.NewValue0(v.Pos, OpThumbORconst, typ.UInt32)
+		v1.AuxInt = 0x10000
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz16NonZero_0(v *Value) bool {
+	// match: (Ctz16NonZero x)
+	// cond:
+	// result: (Ctz32 x)
+	for {
+		x := v.Args[0]
+		v.reset(OpCtz32)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz32_0(v *Value) bool {
+	b := v.Block
+	// match: (Ctz32 <t> x)
+	// cond:
+	// result: (CLZ <t> (RBIT <t> x))
+	for {
+		t := v.Type
+		x := v.Args[0]
+		v.reset(OpThumbCLZ)
+		v.Type = t
+		v0 := b.NewValue0(v.Pos, OpThumbRBIT, t)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz32NonZero_0(v *Value) bool {
+	// match: (Ctz32NonZero x)
+	// cond:
+	// result: (Ctz32 x)
+	for {
+		x := v.Args[0]
+		v.reset(OpCtz32)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Ctz8 <t> x)
+	// cond:
+	// result: (CLZ <t> (RBIT <typ.UInt32> (ORconst <typ.UInt32> [0x100] x)))
+	for {
+		t := v.Type
+		x := v.Args[0]
+		v.reset(OpThumbCLZ)
+		v.Type = t
+		v0 := b.NewValue0(v.Pos, OpThumbRBIT, typ.UInt32)
+		v1 := b.NewValue0(v.Pos, OpThumbORconst, typ.UInt32)
+		v1.AuxInt = 0x100
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpCtz8NonZero_0(v *Value) bool {
+	// match: (Ctz8NonZero x)
+	// cond:
+	// result: (Ctz32 x)
+	for {
+		x := v.Args[0]
+		v.reset(OpCtz32)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt32Fto32_0(v *Value) bool {
+	// match: (Cvt32Fto32 x)
+	// cond:
+	// result: (MOVFW x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVFW)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt32Fto32U_0(v *Value) bool {
+	// match: (Cvt32Fto32U x)
+	// cond:
+	// result: (MOVFWU x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVFWU)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt32Fto64F_0(v *Value) bool {
+	// match: (Cvt32Fto64F x)
+	// cond:
+	// result: (MOVFD x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVFD)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt32Uto32F_0(v *Value) bool {
+	// match: (Cvt32Uto32F x)
+	// cond:
+	// result: (MOVWUF x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVWUF)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt32Uto64F_0(v *Value) bool {
+	// match: (Cvt32Uto64F x)
+	// cond:
+	// result: (MOVWUD x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVWUD)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt32to32F_0(v *Value) bool {
+	// match: (Cvt32to32F x)
+	// cond:
+	// result: (MOVWF x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVWF)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt32to64F_0(v *Value) bool {
+	// match: (Cvt32to64F x)
+	// cond:
+	// result: (MOVWD x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVWD)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt64Fto32_0(v *Value) bool {
+	// match: (Cvt64Fto32 x)
+	// cond:
+	// result: (MOVDW x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVDW)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt64Fto32F_0(v *Value) bool {
+	// match: (Cvt64Fto32F x)
+	// cond:
+	// result: (MOVDF x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVDF)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpCvt64Fto32U_0(v *Value) bool {
+	// match: (Cvt64Fto32U x)
+	// cond:
+	// result: (MOVDWU x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVDWU)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Div16 x y)
+	// cond:
+	// result: (DIV (SignExt16to32 x) (SignExt16to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbDIV)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv16u_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Div16u x y)
+	// cond:
+	// result: (DIVU (ZeroExt16to32 x) (ZeroExt16to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbDIVU)
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv32_0(v *Value) bool {
+	// match: (Div32 x y)
+	// cond:
+	// result: (DIV x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbDIV)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv32F_0(v *Value) bool {
+	// match: (Div32F x y)
+	// cond:
+	// result: (DIVF x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbDIVF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv32u_0(v *Value) bool {
+	// match: (Div32u x y)
+	// cond:
+	// result: (DIVU x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbDIVU)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv64F_0(v *Value) bool {
+	// match: (Div64F x y)
+	// cond:
+	// result: (DIVD x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbDIVD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Div8 x y)
+	// cond:
+	// result: (DIV (SignExt8to32 x) (SignExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbDIV)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpDiv8u_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Div8u x y)
+	// cond:
+	// result: (DIVU (ZeroExt8to32 x) (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbDIVU)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Eq16 x y)
+	// cond:
+	// result: (Equal (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq32_0(v *Value) bool {
+	b := v.Block
+	// match: (Eq32 x y)
+	// cond:
+	// result: (Equal (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq32F_0(v *Value) bool {
+	b := v.Block
+	// match: (Eq32F x y)
+	// cond:
+	// result: (Equal (CMPF x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq64F_0(v *Value) bool {
+	b := v.Block
+	// match: (Eq64F x y)
+	// cond:
+	// result: (Equal (CMPD x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEq8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Eq8 x y)
+	// cond:
+	// result: (Equal (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEqB_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (EqB x y)
+	// cond:
+	// result: (XORconst [1] (XOR <typ.Bool> x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbXORconst)
+		v.AuxInt = 1
+		v0 := b.NewValue0(v.Pos, OpThumbXOR, typ.Bool)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpEqPtr_0(v *Value) bool {
+	b := v.Block
+	// match: (EqPtr x y)
+	// cond:
+	// result: (Equal (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGeq16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Geq16 x y)
+	// cond:
+	// result: (GreaterEqual (CMP (SignExt16to32 x) (SignExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGeq16U_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Geq16U x y)
+	// cond:
+	// result: (GreaterEqualU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGeq32_0(v *Value) bool {
+	b := v.Block
+	// match: (Geq32 x y)
+	// cond:
+	// result: (GreaterEqual (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGeq32F_0(v *Value) bool {
+	b := v.Block
+	// match: (Geq32F x y)
+	// cond:
+	// result: (GreaterEqual (CMPF x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGeq32U_0(v *Value) bool {
+	b := v.Block
+	// match: (Geq32U x y)
+	// cond:
+	// result: (GreaterEqualU (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGeq64F_0(v *Value) bool {
+	b := v.Block
+	// match: (Geq64F x y)
+	// cond:
+	// result: (GreaterEqual (CMPD x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGeq8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Geq8 x y)
+	// cond:
+	// result: (GreaterEqual (CMP (SignExt8to32 x) (SignExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGeq8U_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Geq8U x y)
+	// cond:
+	// result: (GreaterEqualU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGetCallerPC_0(v *Value) bool {
+	// match: (GetCallerPC)
+	// cond:
+	// result: (LoweredGetCallerPC)
+	for {
+		v.reset(OpThumbLoweredGetCallerPC)
+		return true
+	}
+}
+func rewriteValueThumb_OpGetCallerSP_0(v *Value) bool {
+	// match: (GetCallerSP)
+	// cond:
+	// result: (LoweredGetCallerSP)
+	for {
+		v.reset(OpThumbLoweredGetCallerSP)
+		return true
+	}
+}
+func rewriteValueThumb_OpGetClosurePtr_0(v *Value) bool {
+	// match: (GetClosurePtr)
+	// cond:
+	// result: (LoweredGetClosurePtr)
+	for {
+		v.reset(OpThumbLoweredGetClosurePtr)
+		return true
+	}
+}
+func rewriteValueThumb_OpGreater16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Greater16 x y)
+	// cond:
+	// result: (GreaterThan (CMP (SignExt16to32 x) (SignExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGreater16U_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Greater16U x y)
+	// cond:
+	// result: (GreaterThanU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGreater32_0(v *Value) bool {
+	b := v.Block
+	// match: (Greater32 x y)
+	// cond:
+	// result: (GreaterThan (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGreater32F_0(v *Value) bool {
+	b := v.Block
+	// match: (Greater32F x y)
+	// cond:
+	// result: (GreaterThan (CMPF x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGreater32U_0(v *Value) bool {
+	b := v.Block
+	// match: (Greater32U x y)
+	// cond:
+	// result: (GreaterThanU (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGreater64F_0(v *Value) bool {
+	b := v.Block
+	// match: (Greater64F x y)
+	// cond:
+	// result: (GreaterThan (CMPD x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGreater8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Greater8 x y)
+	// cond:
+	// result: (GreaterThan (CMP (SignExt8to32 x) (SignExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpGreater8U_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Greater8U x y)
+	// cond:
+	// result: (GreaterThanU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpHmul32_0(v *Value) bool {
+	// match: (Hmul32 x y)
+	// cond:
+	// result: (HMUL x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbHMUL)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpHmul32u_0(v *Value) bool {
+	// match: (Hmul32u x y)
+	// cond:
+	// result: (HMULU x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbHMULU)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpInterCall_0(v *Value) bool {
+	// match: (InterCall [argwid] entry mem)
+	// cond:
+	// result: (CALLinter [argwid] entry mem)
+	for {
+		argwid := v.AuxInt
+		mem := v.Args[1]
+		entry := v.Args[0]
+		v.reset(OpThumbCALLinter)
+		v.AuxInt = argwid
+		v.AddArg(entry)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpIsInBounds_0(v *Value) bool {
+	b := v.Block
+	// match: (IsInBounds idx len)
+	// cond:
+	// result: (LessThanU (CMP idx len))
+	for {
+		len := v.Args[1]
+		idx := v.Args[0]
+		v.reset(OpThumbLessThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(idx)
+		v0.AddArg(len)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpIsNonNil_0(v *Value) bool {
+	b := v.Block
+	// match: (IsNonNil ptr)
+	// cond:
+	// result: (NotEqual (CMPconst [0] ptr))
+	for {
+		ptr := v.Args[0]
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = 0
+		v0.AddArg(ptr)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpIsSliceInBounds_0(v *Value) bool {
+	b := v.Block
+	// match: (IsSliceInBounds idx len)
+	// cond:
+	// result: (LessEqualU (CMP idx len))
+	for {
+		len := v.Args[1]
+		idx := v.Args[0]
+		v.reset(OpThumbLessEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(idx)
+		v0.AddArg(len)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Leq16 x y)
+	// cond:
+	// result: (LessEqual (CMP (SignExt16to32 x) (SignExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq16U_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Leq16U x y)
+	// cond:
+	// result: (LessEqualU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq32_0(v *Value) bool {
+	b := v.Block
+	// match: (Leq32 x y)
+	// cond:
+	// result: (LessEqual (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq32F_0(v *Value) bool {
+	b := v.Block
+	// match: (Leq32F x y)
+	// cond:
+	// result: (GreaterEqual (CMPF y x))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg(y)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq32U_0(v *Value) bool {
+	b := v.Block
+	// match: (Leq32U x y)
+	// cond:
+	// result: (LessEqualU (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq64F_0(v *Value) bool {
+	b := v.Block
+	// match: (Leq64F x y)
+	// cond:
+	// result: (GreaterEqual (CMPD y x))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg(y)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Leq8 x y)
+	// cond:
+	// result: (LessEqual (CMP (SignExt8to32 x) (SignExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLeq8U_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Leq8U x y)
+	// cond:
+	// result: (LessEqualU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessEqualU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Less16 x y)
+	// cond:
+	// result: (LessThan (CMP (SignExt16to32 x) (SignExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess16U_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Less16U x y)
+	// cond:
+	// result: (LessThanU (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess32_0(v *Value) bool {
+	b := v.Block
+	// match: (Less32 x y)
+	// cond:
+	// result: (LessThan (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess32F_0(v *Value) bool {
+	b := v.Block
+	// match: (Less32F x y)
+	// cond:
+	// result: (GreaterThan (CMPF y x))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg(y)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess32U_0(v *Value) bool {
+	b := v.Block
+	// match: (Less32U x y)
+	// cond:
+	// result: (LessThanU (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess64F_0(v *Value) bool {
+	b := v.Block
+	// match: (Less64F x y)
+	// cond:
+	// result: (GreaterThan (CMPD y x))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg(y)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Less8 x y)
+	// cond:
+	// result: (LessThan (CMP (SignExt8to32 x) (SignExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessThan)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLess8U_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Less8U x y)
+	// cond:
+	// result: (LessThanU (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbLessThanU)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLoad_0(v *Value) bool {
+	// match: (Load <t> ptr mem)
+	// cond: t.IsBoolean()
+	// result: (MOVBUload ptr mem)
+	for {
+		t := v.Type
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(t.IsBoolean()) {
+			break
+		}
+		v.reset(OpThumbMOVBUload)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is8BitInt(t) && isSigned(t))
+	// result: (MOVBload ptr mem)
+	for {
+		t := v.Type
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(is8BitInt(t) && isSigned(t)) {
+			break
+		}
+		v.reset(OpThumbMOVBload)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is8BitInt(t) && !isSigned(t))
+	// result: (MOVBUload ptr mem)
+	for {
+		t := v.Type
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(is8BitInt(t) && !isSigned(t)) {
+			break
+		}
+		v.reset(OpThumbMOVBUload)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is16BitInt(t) && isSigned(t))
+	// result: (MOVHload ptr mem)
+	for {
+		t := v.Type
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(is16BitInt(t) && isSigned(t)) {
+			break
+		}
+		v.reset(OpThumbMOVHload)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is16BitInt(t) && !isSigned(t))
+	// result: (MOVHUload ptr mem)
+	for {
+		t := v.Type
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(is16BitInt(t) && !isSigned(t)) {
+			break
+		}
+		v.reset(OpThumbMOVHUload)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: (is32BitInt(t) || isPtr(t))
+	// result: (MOVWload ptr mem)
+	for {
+		t := v.Type
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(is32BitInt(t) || isPtr(t)) {
+			break
+		}
+		v.reset(OpThumbMOVWload)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: is32BitFloat(t)
+	// result: (MOVFload ptr mem)
+	for {
+		t := v.Type
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(is32BitFloat(t)) {
+			break
+		}
+		v.reset(OpThumbMOVFload)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Load <t> ptr mem)
+	// cond: is64BitFloat(t)
+	// result: (MOVDload ptr mem)
+	for {
+		t := v.Type
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(is64BitFloat(t)) {
+			break
+		}
+		v.reset(OpThumbMOVDload)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpLocalAddr_0(v *Value) bool {
+	// match: (LocalAddr {sym} base _)
+	// cond:
+	// result: (MOVWaddr {sym} base)
+	for {
+		sym := v.Aux
+		_ = v.Args[1]
+		base := v.Args[0]
+		v.reset(OpThumbMOVWaddr)
+		v.Aux = sym
+		v.AddArg(base)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh16x16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh16x16 x y)
+	// cond:
+	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = 256
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v3.AddArg(y)
+		v2.AddArg(v3)
+		v.AddArg(v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh16x32_0(v *Value) bool {
+	b := v.Block
+	// match: (Lsh16x32 x y)
+	// cond:
+	// result: (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = 256
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh16x64_0(v *Value) bool {
+	// match: (Lsh16x64 x (Const64 [c]))
+	// cond: uint64(c) < 16
+	// result: (SLLconst x [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 16) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (Lsh16x64 _ (Const64 [c]))
+	// cond: uint64(c) >= 16
+	// result: (Const16 [0])
+	for {
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 16) {
+			break
+		}
+		v.reset(OpConst16)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpLsh16x8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh16x8 x y)
+	// cond:
+	// result: (SLL x (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSLL)
+		v.AddArg(x)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh32x16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh32x16 x y)
+	// cond:
+	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = 256
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v3.AddArg(y)
+		v2.AddArg(v3)
+		v.AddArg(v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh32x32_0(v *Value) bool {
+	b := v.Block
+	// match: (Lsh32x32 x y)
+	// cond:
+	// result: (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = 256
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh32x64_0(v *Value) bool {
+	// match: (Lsh32x64 x (Const64 [c]))
+	// cond: uint64(c) < 32
+	// result: (SLLconst x [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 32) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (Lsh32x64 _ (Const64 [c]))
+	// cond: uint64(c) >= 32
+	// result: (Const32 [0])
+	for {
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 32) {
+			break
+		}
+		v.reset(OpConst32)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpLsh32x8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh32x8 x y)
+	// cond:
+	// result: (SLL x (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSLL)
+		v.AddArg(x)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh8x16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh8x16 x y)
+	// cond:
+	// result: (CMOVWHSconst (SLL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = 256
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v3.AddArg(y)
+		v2.AddArg(v3)
+		v.AddArg(v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh8x32_0(v *Value) bool {
+	b := v.Block
+	// match: (Lsh8x32 x y)
+	// cond:
+	// result: (CMOVWHSconst (SLL <x.Type> x y) (CMPconst [256] y) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSLL, x.Type)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = 256
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpLsh8x64_0(v *Value) bool {
+	// match: (Lsh8x64 x (Const64 [c]))
+	// cond: uint64(c) < 8
+	// result: (SLLconst x [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 8) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (Lsh8x64 _ (Const64 [c]))
+	// cond: uint64(c) >= 8
+	// result: (Const8 [0])
+	for {
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 8) {
+			break
+		}
+		v.reset(OpConst8)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpLsh8x8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Lsh8x8 x y)
+	// cond:
+	// result: (SLL x (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSLL)
+		v.AddArg(x)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOLoad16_0(v *Value) bool {
+	// match: (MMIOLoad16 ptr mem)
+	// cond:
+	// result: (LoadOnce16 ptr mem)
+	for {
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		v.reset(OpThumbLoadOnce16)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOLoad32_0(v *Value) bool {
+	// match: (MMIOLoad32 ptr mem)
+	// cond:
+	// result: (LoadOnce32 ptr mem)
+	for {
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		v.reset(OpThumbLoadOnce32)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOLoad8_0(v *Value) bool {
+	// match: (MMIOLoad8 ptr mem)
+	// cond:
+	// result: (LoadOnce8 ptr mem)
+	for {
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		v.reset(OpThumbLoadOnce8)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOMB_0(v *Value) bool {
+	// match: (MMIOMB mem)
+	// cond:
+	// result: (DSB mem)
+	for {
+		mem := v.Args[0]
+		v.reset(OpThumbDSB)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOStore16_0(v *Value) bool {
+	// match: (MMIOStore16 ptr val mem)
+	// cond:
+	// result: (StoreOnce16 ptr val mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce16)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOStore32_0(v *Value) bool {
+	// match: (MMIOStore32 ptr val mem)
+	// cond:
+	// result: (StoreOnce32 ptr val mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce32)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMMIOStore8_0(v *Value) bool {
+	// match: (MMIOStore8 ptr val mem)
+	// cond:
+	// result: (StoreOnce8 ptr val mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce8)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Mod16 x y)
+	// cond:
+	// result: (Mod32 (SignExt16to32 x) (SignExt16to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpMod32)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod16u_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Mod16u x y)
+	// cond:
+	// result: (Mod32u (ZeroExt16to32 x) (ZeroExt16to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpMod32u)
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod32_0(v *Value) bool {
+	b := v.Block
+	// match: (Mod32 x y)
+	// cond:
+	// result: (SUB x (MUL <y.Type> y (DIV <x.Type> x y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUB)
+		v.AddArg(x)
+		v0 := b.NewValue0(v.Pos, OpThumbMUL, y.Type)
+		v0.AddArg(y)
+		v1 := b.NewValue0(v.Pos, OpThumbDIV, x.Type)
+		v1.AddArg(x)
+		v1.AddArg(y)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod32u_0(v *Value) bool {
+	b := v.Block
+	// match: (Mod32u x y)
+	// cond:
+	// result: (SUB x (MUL <y.Type> y (DIVU <x.Type> x y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUB)
+		v.AddArg(x)
+		v0 := b.NewValue0(v.Pos, OpThumbMUL, y.Type)
+		v0.AddArg(y)
+		v1 := b.NewValue0(v.Pos, OpThumbDIVU, x.Type)
+		v1.AddArg(x)
+		v1.AddArg(y)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Mod8 x y)
+	// cond:
+	// result: (Mod32 (SignExt8to32 x) (SignExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpMod32)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpMod8u_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Mod8u x y)
+	// cond:
+	// result: (Mod32u (ZeroExt8to32 x) (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpMod32u)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpMove_0(v *Value) bool {
+	b := v.Block
+	config := b.Func.Config
+	typ := &b.Func.Config.Types
+	// match: (Move [0] _ _ mem)
+	// cond:
+	// result: mem
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		mem := v.Args[2]
+		v.reset(OpCopy)
+		v.Type = mem.Type
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Move [1] dst src mem)
+	// cond:
+	// result: (MOVBstore dst (MOVBUload src mem) mem)
+	for {
+		if v.AuxInt != 1 {
+			break
+		}
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		v.reset(OpThumbMOVBstore)
+		v.AddArg(dst)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v0.AddArg(src)
+		v0.AddArg(mem)
+		v.AddArg(v0)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Move [2] {t} dst src mem)
+	// cond: t.(*types.Type).Alignment()%2 == 0
+	// result: (MOVHstore dst (MOVHUload src mem) mem)
+	for {
+		if v.AuxInt != 2 {
+			break
+		}
+		t := v.Aux
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		if !(t.(*types.Type).Alignment()%2 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AddArg(dst)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVHUload, typ.UInt16)
+		v0.AddArg(src)
+		v0.AddArg(mem)
+		v.AddArg(v0)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Move [2] dst src mem)
+	// cond:
+	// result: (MOVBstore [1] dst (MOVBUload [1] src mem) (MOVBstore dst (MOVBUload src mem) mem))
+	for {
+		if v.AuxInt != 2 {
+			break
+		}
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = 1
+		v.AddArg(dst)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v0.AuxInt = 1
+		v0.AddArg(src)
+		v0.AddArg(mem)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AddArg(dst)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v2.AddArg(src)
+		v2.AddArg(mem)
+		v1.AddArg(v2)
+		v1.AddArg(mem)
+		v.AddArg(v1)
+		return true
+	}
+	// match: (Move [4] {t} dst src mem)
+	// cond: t.(*types.Type).Alignment()%4 == 0
+	// result: (MOVWstore dst (MOVWload src mem) mem)
+	for {
+		if v.AuxInt != 4 {
+			break
+		}
+		t := v.Aux
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		if !(t.(*types.Type).Alignment()%4 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVWstore)
+		v.AddArg(dst)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWload, typ.UInt32)
+		v0.AddArg(src)
+		v0.AddArg(mem)
+		v.AddArg(v0)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Move [4] {t} dst src mem)
+	// cond: t.(*types.Type).Alignment()%2 == 0
+	// result: (MOVHstore [2] dst (MOVHUload [2] src mem) (MOVHstore dst (MOVHUload src mem) mem))
+	for {
+		if v.AuxInt != 4 {
+			break
+		}
+		t := v.Aux
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		if !(t.(*types.Type).Alignment()%2 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = 2
+		v.AddArg(dst)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVHUload, typ.UInt16)
+		v0.AuxInt = 2
+		v0.AddArg(src)
+		v0.AddArg(mem)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVHstore, types.TypeMem)
+		v1.AddArg(dst)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVHUload, typ.UInt16)
+		v2.AddArg(src)
+		v2.AddArg(mem)
+		v1.AddArg(v2)
+		v1.AddArg(mem)
+		v.AddArg(v1)
+		return true
+	}
+	// match: (Move [4] dst src mem)
+	// cond:
+	// result: (MOVBstore [3] dst (MOVBUload [3] src mem) (MOVBstore [2] dst (MOVBUload [2] src mem) (MOVBstore [1] dst (MOVBUload [1] src mem) (MOVBstore dst (MOVBUload src mem) mem))))
+	for {
+		if v.AuxInt != 4 {
+			break
+		}
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = 3
+		v.AddArg(dst)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v0.AuxInt = 3
+		v0.AddArg(src)
+		v0.AddArg(mem)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = 2
+		v1.AddArg(dst)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v2.AuxInt = 2
+		v2.AddArg(src)
+		v2.AddArg(mem)
+		v1.AddArg(v2)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v3.AuxInt = 1
+		v3.AddArg(dst)
+		v4 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v4.AuxInt = 1
+		v4.AddArg(src)
+		v4.AddArg(mem)
+		v3.AddArg(v4)
+		v5 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v5.AddArg(dst)
+		v6 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v6.AddArg(src)
+		v6.AddArg(mem)
+		v5.AddArg(v6)
+		v5.AddArg(mem)
+		v3.AddArg(v5)
+		v1.AddArg(v3)
+		v.AddArg(v1)
+		return true
+	}
+	// match: (Move [3] dst src mem)
+	// cond:
+	// result: (MOVBstore [2] dst (MOVBUload [2] src mem) (MOVBstore [1] dst (MOVBUload [1] src mem) (MOVBstore dst (MOVBUload src mem) mem)))
+	for {
+		if v.AuxInt != 3 {
+			break
+		}
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = 2
+		v.AddArg(dst)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v0.AuxInt = 2
+		v0.AddArg(src)
+		v0.AddArg(mem)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = 1
+		v1.AddArg(dst)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v2.AuxInt = 1
+		v2.AddArg(src)
+		v2.AddArg(mem)
+		v1.AddArg(v2)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v3.AddArg(dst)
+		v4 := b.NewValue0(v.Pos, OpThumbMOVBUload, typ.UInt8)
+		v4.AddArg(src)
+		v4.AddArg(mem)
+		v3.AddArg(v4)
+		v3.AddArg(mem)
+		v1.AddArg(v3)
+		v.AddArg(v1)
+		return true
+	}
+	// match: (Move [s] {t} dst src mem)
+	// cond: s%4 == 0 && s > 4 && s <= 512 && t.(*types.Type).Alignment()%4 == 0 && !config.noDuffDevice
+	// result: (DUFFCOPY [8 * (128 - s/4)] dst src mem)
+	for {
+		s := v.AuxInt
+		t := v.Aux
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		if !(s%4 == 0 && s > 4 && s <= 512 && t.(*types.Type).Alignment()%4 == 0 && !config.noDuffDevice) {
+			break
+		}
+		v.reset(OpThumbDUFFCOPY)
+		v.AuxInt = 8 * (128 - s/4)
+		v.AddArg(dst)
+		v.AddArg(src)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Move [s] {t} dst src mem)
+	// cond: (s > 512 || config.noDuffDevice) || t.(*types.Type).Alignment()%4 != 0
+	// result: (LoweredMove [t.(*types.Type).Alignment()] dst src (ADDconst <src.Type> src [s-moveSize(t.(*types.Type).Alignment(), config)]) mem)
+	for {
+		s := v.AuxInt
+		t := v.Aux
+		mem := v.Args[2]
+		dst := v.Args[0]
+		src := v.Args[1]
+		if !((s > 512 || config.noDuffDevice) || t.(*types.Type).Alignment()%4 != 0) {
+			break
+		}
+		v.reset(OpThumbLoweredMove)
+		v.AuxInt = t.(*types.Type).Alignment()
+		v.AddArg(dst)
+		v.AddArg(src)
+		v0 := b.NewValue0(v.Pos, OpThumbADDconst, src.Type)
+		v0.AuxInt = s - moveSize(t.(*types.Type).Alignment(), config)
+		v0.AddArg(src)
+		v.AddArg(v0)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpMul16_0(v *Value) bool {
+	// match: (Mul16 x y)
+	// cond:
+	// result: (MUL x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbMUL)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpMul32_0(v *Value) bool {
+	// match: (Mul32 x y)
+	// cond:
+	// result: (MUL x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbMUL)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpMul32F_0(v *Value) bool {
+	// match: (Mul32F x y)
+	// cond:
+	// result: (MULF x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbMULF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpMul32uhilo_0(v *Value) bool {
+	// match: (Mul32uhilo x y)
+	// cond:
+	// result: (MULLU x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbMULLU)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpMul64F_0(v *Value) bool {
+	// match: (Mul64F x y)
+	// cond:
+	// result: (MULD x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbMULD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpMul8_0(v *Value) bool {
+	// match: (Mul8 x y)
+	// cond:
+	// result: (MUL x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbMUL)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeg16_0(v *Value) bool {
+	// match: (Neg16 x)
+	// cond:
+	// result: (RSBconst [0] x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = 0
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeg32_0(v *Value) bool {
+	// match: (Neg32 x)
+	// cond:
+	// result: (RSBconst [0] x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = 0
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeg32F_0(v *Value) bool {
+	// match: (Neg32F x)
+	// cond:
+	// result: (NEGF x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbNEGF)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeg64F_0(v *Value) bool {
+	// match: (Neg64F x)
+	// cond:
+	// result: (NEGD x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbNEGD)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeg8_0(v *Value) bool {
+	// match: (Neg8 x)
+	// cond:
+	// result: (RSBconst [0] x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = 0
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Neq16 x y)
+	// cond:
+	// result: (NotEqual (CMP (ZeroExt16to32 x) (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq32_0(v *Value) bool {
+	b := v.Block
+	// match: (Neq32 x y)
+	// cond:
+	// result: (NotEqual (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq32F_0(v *Value) bool {
+	b := v.Block
+	// match: (Neq32F x y)
+	// cond:
+	// result: (NotEqual (CMPF x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPF, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq64F_0(v *Value) bool {
+	b := v.Block
+	// match: (Neq64F x y)
+	// cond:
+	// result: (NotEqual (CMPD x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPD, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeq8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Neq8 x y)
+	// cond:
+	// result: (NotEqual (CMP (ZeroExt8to32 x) (ZeroExt8to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeqB_0(v *Value) bool {
+	// match: (NeqB x y)
+	// cond:
+	// result: (XOR x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbXOR)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpNeqPtr_0(v *Value) bool {
+	b := v.Block
+	// match: (NeqPtr x y)
+	// cond:
+	// result: (NotEqual (CMP x y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbNotEqual)
+		v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpNilCheck_0(v *Value) bool {
+	// match: (NilCheck ptr mem)
+	// cond:
+	// result: (LoweredNilCheck ptr mem)
+	for {
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		v.reset(OpThumbLoweredNilCheck)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpNot_0(v *Value) bool {
+	// match: (Not x)
+	// cond:
+	// result: (XORconst [1] x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbXORconst)
+		v.AuxInt = 1
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpOffPtr_0(v *Value) bool {
+	// match: (OffPtr [off] ptr:(SP))
+	// cond:
+	// result: (MOVWaddr [off] ptr)
+	for {
+		off := v.AuxInt
+		ptr := v.Args[0]
+		if ptr.Op != OpSP {
+			break
+		}
+		v.reset(OpThumbMOVWaddr)
+		v.AuxInt = off
+		v.AddArg(ptr)
+		return true
+	}
+	// match: (OffPtr [off] ptr)
+	// cond:
+	// result: (ADDconst [off] ptr)
+	for {
+		off := v.AuxInt
+		ptr := v.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = off
+		v.AddArg(ptr)
+		return true
+	}
+}
+func rewriteValueThumb_OpOr16_0(v *Value) bool {
+	// match: (Or16 x y)
+	// cond:
+	// result: (OR x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbOR)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpOr32_0(v *Value) bool {
+	// match: (Or32 x y)
+	// cond:
+	// result: (OR x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbOR)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpOr8_0(v *Value) bool {
+	// match: (Or8 x y)
+	// cond:
+	// result: (OR x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbOR)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpOrB_0(v *Value) bool {
+	// match: (OrB x y)
+	// cond:
+	// result: (OR x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbOR)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpPanicBounds_0(v *Value) bool {
+	// match: (PanicBounds [kind] x y mem)
+	// cond: boundsABI(kind) == 0
+	// result: (LoweredPanicBoundsA [kind] x y mem)
+	for {
+		kind := v.AuxInt
+		mem := v.Args[2]
+		x := v.Args[0]
+		y := v.Args[1]
+		if !(boundsABI(kind) == 0) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicBoundsA)
+		v.AuxInt = kind
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (PanicBounds [kind] x y mem)
+	// cond: boundsABI(kind) == 1
+	// result: (LoweredPanicBoundsB [kind] x y mem)
+	for {
+		kind := v.AuxInt
+		mem := v.Args[2]
+		x := v.Args[0]
+		y := v.Args[1]
+		if !(boundsABI(kind) == 1) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicBoundsB)
+		v.AuxInt = kind
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (PanicBounds [kind] x y mem)
+	// cond: boundsABI(kind) == 2
+	// result: (LoweredPanicBoundsC [kind] x y mem)
+	for {
+		kind := v.AuxInt
+		mem := v.Args[2]
+		x := v.Args[0]
+		y := v.Args[1]
+		if !(boundsABI(kind) == 2) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicBoundsC)
+		v.AuxInt = kind
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpPanicExtend_0(v *Value) bool {
+	// match: (PanicExtend [kind] hi lo y mem)
+	// cond: boundsABI(kind) == 0
+	// result: (LoweredPanicExtendA [kind] hi lo y mem)
+	for {
+		kind := v.AuxInt
+		mem := v.Args[3]
+		hi := v.Args[0]
+		lo := v.Args[1]
+		y := v.Args[2]
+		if !(boundsABI(kind) == 0) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicExtendA)
+		v.AuxInt = kind
+		v.AddArg(hi)
+		v.AddArg(lo)
+		v.AddArg(y)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (PanicExtend [kind] hi lo y mem)
+	// cond: boundsABI(kind) == 1
+	// result: (LoweredPanicExtendB [kind] hi lo y mem)
+	for {
+		kind := v.AuxInt
+		mem := v.Args[3]
+		hi := v.Args[0]
+		lo := v.Args[1]
+		y := v.Args[2]
+		if !(boundsABI(kind) == 1) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicExtendB)
+		v.AuxInt = kind
+		v.AddArg(hi)
+		v.AddArg(lo)
+		v.AddArg(y)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (PanicExtend [kind] hi lo y mem)
+	// cond: boundsABI(kind) == 2
+	// result: (LoweredPanicExtendC [kind] hi lo y mem)
+	for {
+		kind := v.AuxInt
+		mem := v.Args[3]
+		hi := v.Args[0]
+		lo := v.Args[1]
+		y := v.Args[2]
+		if !(boundsABI(kind) == 2) {
+			break
+		}
+		v.reset(OpThumbLoweredPanicExtendC)
+		v.AuxInt = kind
+		v.AddArg(hi)
+		v.AddArg(lo)
+		v.AddArg(y)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpPublicationBarrier_0(v *Value) bool {
+	// match: (PublicationBarrier mem)
+	// cond:
+	// result: (DMB_ST mem)
+	for {
+		mem := v.Args[0]
+		v.reset(OpThumbDMB_ST)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpRotateLeft16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (RotateLeft16 <t> x (MOVWconst [c]))
+	// cond:
+	// result: (Or16 (Lsh16x32 <t> x (MOVWconst [c&15])) (Rsh16Ux32 <t> x (MOVWconst [-c&15])))
+	for {
+		t := v.Type
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpOr16)
+		v0 := b.NewValue0(v.Pos, OpLsh16x32, t)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v1.AuxInt = c & 15
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v2 := b.NewValue0(v.Pos, OpRsh16Ux32, t)
+		v2.AddArg(x)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v3.AuxInt = -c & 15
+		v2.AddArg(v3)
+		v.AddArg(v2)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRotateLeft32_0(v *Value) bool {
+	// match: (RotateLeft32 x (MOVWconst [c]))
+	// cond:
+	// result: (SRRconst [-c&31] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = -c & 31
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRotateLeft8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (RotateLeft8 <t> x (MOVWconst [c]))
+	// cond:
+	// result: (Or8 (Lsh8x32 <t> x (MOVWconst [c&7])) (Rsh8Ux32 <t> x (MOVWconst [-c&7])))
+	for {
+		t := v.Type
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpOr8)
+		v0 := b.NewValue0(v.Pos, OpLsh8x32, t)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v1.AuxInt = c & 7
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v2 := b.NewValue0(v.Pos, OpRsh8Ux32, t)
+		v2.AddArg(x)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v3.AuxInt = -c & 7
+		v2.AddArg(v3)
+		v.AddArg(v2)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRound32F_0(v *Value) bool {
+	// match: (Round32F x)
+	// cond:
+	// result: x
+	for {
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpRound64F_0(v *Value) bool {
+	// match: (Round64F x)
+	// cond:
+	// result: x
+	for {
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16Ux16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16Ux16 x y)
+	// cond:
+	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		v3 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v3.AuxInt = 256
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v4.AddArg(y)
+		v3.AddArg(v4)
+		v.AddArg(v3)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16Ux32_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16Ux32 x y)
+	// cond:
+	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt16to32 x) y) (CMPconst [256] y) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = 256
+		v2.AddArg(y)
+		v.AddArg(v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16Ux64_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16Ux64 x (Const64 [c]))
+	// cond: uint64(c) < 16
+	// result: (SRLconst (SLLconst <typ.UInt32> x [16]) [c+16])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 16) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = c + 16
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = 16
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (Rsh16Ux64 _ (Const64 [c]))
+	// cond: uint64(c) >= 16
+	// result: (Const16 [0])
+	for {
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 16) {
+			break
+		}
+		v.reset(OpConst16)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh16Ux8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16Ux8 x y)
+	// cond:
+	// result: (SRL (ZeroExt16to32 x) (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRL)
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16x16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16x16 x y)
+	// cond:
+	// result: (SRAcond (SignExt16to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = 256
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v3.AddArg(y)
+		v2.AddArg(v3)
+		v.AddArg(v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16x32_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16x32 x y)
+	// cond:
+	// result: (SRAcond (SignExt16to32 x) y (CMPconst [256] y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(y)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = 256
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh16x64_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16x64 x (Const64 [c]))
+	// cond: uint64(c) < 16
+	// result: (SRAconst (SLLconst <typ.UInt32> x [16]) [c+16])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 16) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = c + 16
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = 16
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (Rsh16x64 x (Const64 [c]))
+	// cond: uint64(c) >= 16
+	// result: (SRAconst (SLLconst <typ.UInt32> x [16]) [31])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 16) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = 16
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh16x8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh16x8 x y)
+	// cond:
+	// result: (SRA (SignExt16to32 x) (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRA)
+		v0 := b.NewValue0(v.Pos, OpSignExt16to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32Ux16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh32Ux16 x y)
+	// cond:
+	// result: (CMOVWHSconst (SRL <x.Type> x (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v0.AddArg(x)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = 256
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v3.AddArg(y)
+		v2.AddArg(v3)
+		v.AddArg(v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32Ux32_0(v *Value) bool {
+	b := v.Block
+	// match: (Rsh32Ux32 x y)
+	// cond:
+	// result: (CMOVWHSconst (SRL <x.Type> x y) (CMPconst [256] y) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = 256
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32Ux64_0(v *Value) bool {
+	// match: (Rsh32Ux64 x (Const64 [c]))
+	// cond: uint64(c) < 32
+	// result: (SRLconst x [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 32) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (Rsh32Ux64 _ (Const64 [c]))
+	// cond: uint64(c) >= 32
+	// result: (Const32 [0])
+	for {
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 32) {
+			break
+		}
+		v.reset(OpConst32)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh32Ux8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh32Ux8 x y)
+	// cond:
+	// result: (SRL x (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRL)
+		v.AddArg(x)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32x16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh32x16 x y)
+	// cond:
+	// result: (SRAcond x (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRAcond)
+		v.AddArg(x)
+		v0 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = 256
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v1.AddArg(v2)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32x32_0(v *Value) bool {
+	b := v.Block
+	// match: (Rsh32x32 x y)
+	// cond:
+	// result: (SRAcond x y (CMPconst [256] y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRAcond)
+		v.AddArg(x)
+		v.AddArg(y)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = 256
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh32x64_0(v *Value) bool {
+	// match: (Rsh32x64 x (Const64 [c]))
+	// cond: uint64(c) < 32
+	// result: (SRAconst x [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 32) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (Rsh32x64 x (Const64 [c]))
+	// cond: uint64(c) >= 32
+	// result: (SRAconst x [31])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 32) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh32x8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh32x8 x y)
+	// cond:
+	// result: (SRA x (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRA)
+		v.AddArg(x)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8Ux16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8Ux16 x y)
+	// cond:
+	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) (ZeroExt16to32 y)) (CMPconst [256] (ZeroExt16to32 y)) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v2.AddArg(y)
+		v0.AddArg(v2)
+		v.AddArg(v0)
+		v3 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v3.AuxInt = 256
+		v4 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v4.AddArg(y)
+		v3.AddArg(v4)
+		v.AddArg(v3)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8Ux32_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8Ux32 x y)
+	// cond:
+	// result: (CMOVWHSconst (SRL <x.Type> (ZeroExt8to32 x) y) (CMPconst [256] y) [0])
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = 0
+		v0 := b.NewValue0(v.Pos, OpThumbSRL, x.Type)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = 256
+		v2.AddArg(y)
+		v.AddArg(v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8Ux64_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8Ux64 x (Const64 [c]))
+	// cond: uint64(c) < 8
+	// result: (SRLconst (SLLconst <typ.UInt32> x [24]) [c+24])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 8) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = c + 24
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = 24
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (Rsh8Ux64 _ (Const64 [c]))
+	// cond: uint64(c) >= 8
+	// result: (Const8 [0])
+	for {
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 8) {
+			break
+		}
+		v.reset(OpConst8)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh8Ux8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8Ux8 x y)
+	// cond:
+	// result: (SRL (ZeroExt8to32 x) (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRL)
+		v0 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8x16_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8x16 x y)
+	// cond:
+	// result: (SRAcond (SignExt8to32 x) (ZeroExt16to32 y) (CMPconst [256] (ZeroExt16to32 y)))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		v2 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v2.AuxInt = 256
+		v3 := b.NewValue0(v.Pos, OpZeroExt16to32, typ.UInt32)
+		v3.AddArg(y)
+		v2.AddArg(v3)
+		v.AddArg(v2)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8x32_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8x32 x y)
+	// cond:
+	// result: (SRAcond (SignExt8to32 x) y (CMPconst [256] y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRAcond)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(y)
+		v1 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v1.AuxInt = 256
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpRsh8x64_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8x64 x (Const64 [c]))
+	// cond: uint64(c) < 8
+	// result: (SRAconst (SLLconst <typ.UInt32> x [24]) [c+24])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) < 8) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = c + 24
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = 24
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (Rsh8x64 x (Const64 [c]))
+	// cond: uint64(c) >= 8
+	// result: (SRAconst (SLLconst <typ.UInt32> x [24]) [31])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpConst64 {
+			break
+		}
+		c := v_1.AuxInt
+		if !(uint64(c) >= 8) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, typ.UInt32)
+		v0.AuxInt = 24
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpRsh8x8_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Rsh8x8 x y)
+	// cond:
+	// result: (SRA (SignExt8to32 x) (ZeroExt8to32 y))
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSRA)
+		v0 := b.NewValue0(v.Pos, OpSignExt8to32, typ.Int32)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpZeroExt8to32, typ.UInt32)
+		v1.AddArg(y)
+		v.AddArg(v1)
+		return true
+	}
+}
+func rewriteValueThumb_OpSignExt16to32_0(v *Value) bool {
+	// match: (SignExt16to32 x)
+	// cond:
+	// result: (MOVHreg x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVHreg)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpSignExt8to16_0(v *Value) bool {
+	// match: (SignExt8to16 x)
+	// cond:
+	// result: (MOVBreg x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpSignExt8to32_0(v *Value) bool {
+	// match: (SignExt8to32 x)
+	// cond:
+	// result: (MOVBreg x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpSignmask_0(v *Value) bool {
+	// match: (Signmask x)
+	// cond:
+	// result: (SRAconst x [31])
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpSlicemask_0(v *Value) bool {
+	b := v.Block
+	// match: (Slicemask <t> x)
+	// cond:
+	// result: (SRAconst (RSBconst <t> [0] x) [31])
+	for {
+		t := v.Type
+		x := v.Args[0]
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v0 := b.NewValue0(v.Pos, OpThumbRSBconst, t)
+		v0.AuxInt = 0
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteValueThumb_OpSqrt_0(v *Value) bool {
+	// match: (Sqrt x)
+	// cond:
+	// result: (SQRTD x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbSQRTD)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpStaticCall_0(v *Value) bool {
+	// match: (StaticCall [argwid] {target} mem)
+	// cond:
+	// result: (CALLstatic [argwid] {target} mem)
+	for {
+		argwid := v.AuxInt
+		target := v.Aux
+		mem := v.Args[0]
+		v.reset(OpThumbCALLstatic)
+		v.AuxInt = argwid
+		v.Aux = target
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpStore_0(v *Value) bool {
+	// match: (Store {t} ptr val mem)
+	// cond: t.(*types.Type).Size() == 1
+	// result: (MOVBstore ptr val mem)
+	for {
+		t := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		val := v.Args[1]
+		if !(t.(*types.Type).Size() == 1) {
+			break
+		}
+		v.reset(OpThumbMOVBstore)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Store {t} ptr val mem)
+	// cond: t.(*types.Type).Size() == 2
+	// result: (MOVHstore ptr val mem)
+	for {
+		t := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		val := v.Args[1]
+		if !(t.(*types.Type).Size() == 2) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Store {t} ptr val mem)
+	// cond: t.(*types.Type).Size() == 4 && !is32BitFloat(val.Type)
+	// result: (MOVWstore ptr val mem)
+	for {
+		t := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		val := v.Args[1]
+		if !(t.(*types.Type).Size() == 4 && !is32BitFloat(val.Type)) {
+			break
+		}
+		v.reset(OpThumbMOVWstore)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Store {t} ptr val mem)
+	// cond: t.(*types.Type).Size() == 4 && is32BitFloat(val.Type)
+	// result: (MOVFstore ptr val mem)
+	for {
+		t := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		val := v.Args[1]
+		if !(t.(*types.Type).Size() == 4 && is32BitFloat(val.Type)) {
+			break
+		}
+		v.reset(OpThumbMOVFstore)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Store {t} ptr val mem)
+	// cond: t.(*types.Type).Size() == 8 && is64BitFloat(val.Type)
+	// result: (MOVDstore ptr val mem)
+	for {
+		t := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		val := v.Args[1]
+		if !(t.(*types.Type).Size() == 8 && is64BitFloat(val.Type)) {
+			break
+		}
+		v.reset(OpThumbMOVDstore)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpSub16_0(v *Value) bool {
+	// match: (Sub16 x y)
+	// cond:
+	// result: (SUB x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUB)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpSub32_0(v *Value) bool {
+	// match: (Sub32 x y)
+	// cond:
+	// result: (SUB x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUB)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpSub32F_0(v *Value) bool {
+	// match: (Sub32F x y)
+	// cond:
+	// result: (SUBF x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUBF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpSub32carry_0(v *Value) bool {
+	// match: (Sub32carry x y)
+	// cond:
+	// result: (SUBS x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUBS)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpSub32withcarry_0(v *Value) bool {
+	// match: (Sub32withcarry x y c)
+	// cond:
+	// result: (SBC x y c)
+	for {
+		c := v.Args[2]
+		x := v.Args[0]
+		y := v.Args[1]
+		v.reset(OpThumbSBC)
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(c)
+		return true
+	}
+}
+func rewriteValueThumb_OpSub64F_0(v *Value) bool {
+	// match: (Sub64F x y)
+	// cond:
+	// result: (SUBD x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUBD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpSub8_0(v *Value) bool {
+	// match: (Sub8 x y)
+	// cond:
+	// result: (SUB x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUB)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpSubPtr_0(v *Value) bool {
+	// match: (SubPtr x y)
+	// cond:
+	// result: (SUB x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbSUB)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpThumbADC_0(v *Value) bool {
+	// match: (ADC (MOVWconst [c]) x flags)
+	// cond:
+	// result: (ADCconst [c] x flags)
+	for {
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		v.reset(OpThumbADCconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC x (MOVWconst [c]) flags)
+	// cond:
+	// result: (ADCconst [c] x flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADCconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC x (MOVWconst [c]) flags)
+	// cond:
+	// result: (ADCconst [c] x flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADCconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC (MOVWconst [c]) x flags)
+	// cond:
+	// result: (ADCconst [c] x flags)
+	for {
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		v.reset(OpThumbADCconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC x (SLLconst [c] y) flags)
+	// cond:
+	// result: (ADCshiftLL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADCshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC (SLLconst [c] y) x flags)
+	// cond:
+	// result: (ADCshiftLL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		x := v.Args[1]
+		v.reset(OpThumbADCshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC (SLLconst [c] y) x flags)
+	// cond:
+	// result: (ADCshiftLL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		x := v.Args[1]
+		v.reset(OpThumbADCshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC x (SLLconst [c] y) flags)
+	// cond:
+	// result: (ADCshiftLL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADCshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC x (SRLconst [c] y) flags)
+	// cond:
+	// result: (ADCshiftRL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADCshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC (SRLconst [c] y) x flags)
+	// cond:
+	// result: (ADCshiftRL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		x := v.Args[1]
+		v.reset(OpThumbADCshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADC_10(v *Value) bool {
+	// match: (ADC (SRLconst [c] y) x flags)
+	// cond:
+	// result: (ADCshiftRL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		x := v.Args[1]
+		v.reset(OpThumbADCshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC x (SRLconst [c] y) flags)
+	// cond:
+	// result: (ADCshiftRL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADCshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC x (SRAconst [c] y) flags)
+	// cond:
+	// result: (ADCshiftRA x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADCshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC (SRAconst [c] y) x flags)
+	// cond:
+	// result: (ADCshiftRA x y [c] flags)
+	for {
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		x := v.Args[1]
+		v.reset(OpThumbADCshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC (SRAconst [c] y) x flags)
+	// cond:
+	// result: (ADCshiftRA x y [c] flags)
+	for {
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		x := v.Args[1]
+		v.reset(OpThumbADCshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADC x (SRAconst [c] y) flags)
+	// cond:
+	// result: (ADCshiftRA x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADCshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADCconst_0(v *Value) bool {
+	// match: (ADCconst [c] (ADDconst [d] x) flags)
+	// cond:
+	// result: (ADCconst [int64(int32(c+d))] x flags)
+	for {
+		c := v.AuxInt
+		flags := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int64(int32(c + d))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADCconst [c] (SUBconst [d] x) flags)
+	// cond:
+	// result: (ADCconst [int64(int32(c-d))] x flags)
+	for {
+		c := v.AuxInt
+		flags := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int64(int32(c - d))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADCshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (ADCshiftLL (MOVWconst [c]) x [d] flags)
+	// cond:
+	// result: (ADCconst [c] (SLLconst <x.Type> x [d]) flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		v.reset(OpThumbADCconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADCshiftLL x (MOVWconst [c]) [d] flags)
+	// cond:
+	// result: (ADCconst x [int64(int32(uint32(c)<<uint64(d)))] flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADCshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (ADCshiftRA (MOVWconst [c]) x [d] flags)
+	// cond:
+	// result: (ADCconst [c] (SRAconst <x.Type> x [d]) flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		v.reset(OpThumbADCconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADCshiftRA x (MOVWconst [c]) [d] flags)
+	// cond:
+	// result: (ADCconst x [int64(int32(c)>>uint64(d))] flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADCshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (ADCshiftRL (MOVWconst [c]) x [d] flags)
+	// cond:
+	// result: (ADCconst [c] (SRLconst <x.Type> x [d]) flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		v.reset(OpThumbADCconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (ADCshiftRL x (MOVWconst [c]) [d] flags)
+	// cond:
+	// result: (ADCconst x [int64(int32(uint32(c)>>uint64(d)))] flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADCconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADD_0(v *Value) bool {
+	// match: (ADD x (MOVWconst [c]))
+	// cond:
+	// result: (ADDconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADD (MOVWconst [c]) x)
+	// cond:
+	// result: (ADDconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADD x (SLLconst [c] y))
+	// cond:
+	// result: (ADDshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADDshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADD (SLLconst [c] y) x)
+	// cond:
+	// result: (ADDshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbADDshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADD x (SRLconst [c] y))
+	// cond:
+	// result: (ADDshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADDshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADD (SRLconst [c] y) x)
+	// cond:
+	// result: (ADDshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbADDshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADD x (SRAconst [c] y))
+	// cond:
+	// result: (ADDshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADDshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADD (SRAconst [c] y) x)
+	// cond:
+	// result: (ADDshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbADDshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADD x (RSBconst [0] y))
+	// cond:
+	// result: (SUB x y)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbRSBconst {
+			break
+		}
+		if v_1.AuxInt != 0 {
+			break
+		}
+		y := v_1.Args[0]
+		v.reset(OpThumbSUB)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADD (RSBconst [0] y) x)
+	// cond:
+	// result: (SUB x y)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		if v_0.AuxInt != 0 {
+			break
+		}
+		y := v_0.Args[0]
+		v.reset(OpThumbSUB)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADD_10(v *Value) bool {
+	b := v.Block
+	// match: (ADD <t> (RSBconst [c] x) (RSBconst [d] y))
+	// cond:
+	// result: (RSBconst [c+d] (ADD <t> x y))
+	for {
+		t := v.Type
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbRSBconst {
+			break
+		}
+		d := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = c + d
+		v0 := b.NewValue0(v.Pos, OpThumbADD, t)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADD <t> (RSBconst [d] y) (RSBconst [c] x))
+	// cond:
+	// result: (RSBconst [c+d] (ADD <t> x y))
+	for {
+		t := v.Type
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		d := v_0.AuxInt
+		y := v_0.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbRSBconst {
+			break
+		}
+		c := v_1.AuxInt
+		x := v_1.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = c + d
+		v0 := b.NewValue0(v.Pos, OpThumbADD, t)
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADD (MUL x y) a)
+	// cond:
+	// result: (MULA x y a)
+	for {
+		a := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMUL {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		v.reset(OpThumbMULA)
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(a)
+		return true
+	}
+	// match: (ADD a (MUL x y))
+	// cond:
+	// result: (MULA x y a)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMUL {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		v.reset(OpThumbMULA)
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDD_0(v *Value) bool {
+	// match: (ADDD a (MULD x y))
+	// cond: a.Uses == 1
+	// result: (MULAD a x y)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMULD {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULAD)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDD (MULD x y) a)
+	// cond: a.Uses == 1
+	// result: (MULAD a x y)
+	for {
+		a := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMULD {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULAD)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDD a (NMULD x y))
+	// cond: a.Uses == 1
+	// result: (MULSD a x y)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbNMULD {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULSD)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDD (NMULD x y) a)
+	// cond: a.Uses == 1
+	// result: (MULSD a x y)
+	for {
+		a := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbNMULD {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULSD)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDF_0(v *Value) bool {
+	// match: (ADDF a (MULF x y))
+	// cond: a.Uses == 1
+	// result: (MULAF a x y)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMULF {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULAF)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDF (MULF x y) a)
+	// cond: a.Uses == 1
+	// result: (MULAF a x y)
+	for {
+		a := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMULF {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULAF)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDF a (NMULF x y))
+	// cond: a.Uses == 1
+	// result: (MULSF a x y)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbNMULF {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULSF)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDF (NMULF x y) a)
+	// cond: a.Uses == 1
+	// result: (MULSF a x y)
+	for {
+		a := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbNMULF {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULSF)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDS_0(v *Value) bool {
+	// match: (ADDS x (MOVWconst [c]))
+	// cond:
+	// result: (ADDSconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDS (MOVWconst [c]) x)
+	// cond:
+	// result: (ADDSconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDS x (SLLconst [c] y))
+	// cond:
+	// result: (ADDSshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADDSshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDS (SLLconst [c] y) x)
+	// cond:
+	// result: (ADDSshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbADDSshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDS x (SRLconst [c] y))
+	// cond:
+	// result: (ADDSshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADDSshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDS (SRLconst [c] y) x)
+	// cond:
+	// result: (ADDSshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbADDSshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDS x (SRAconst [c] y))
+	// cond:
+	// result: (ADDSshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbADDSshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (ADDS (SRAconst [c] y) x)
+	// cond:
+	// result: (ADDSshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbADDSshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDSshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (ADDSshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ADDSconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDSshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ADDSconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDSshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (ADDSshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ADDSconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDSshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ADDSconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDSshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (ADDSshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ADDSconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDSshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ADDSconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADDSconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDconst_0(v *Value) bool {
+	// match: (ADDconst [off1] (MOVWaddr [off2] {sym} ptr))
+	// cond:
+	// result: (MOVWaddr [off1+off2] {sym} ptr)
+	for {
+		off1 := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym := v_0.Aux
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVWaddr)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		return true
+	}
+	// match: (ADDconst [0] x)
+	// cond:
+	// result: x
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(c+d))])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(c + d))
+		return true
+	}
+	// match: (ADDconst [c] (ADDconst [d] x))
+	// cond:
+	// result: (ADDconst [int64(int32(c+d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(c + d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDconst [c] (SUBconst [d] x))
+	// cond:
+	// result: (ADDconst [int64(int32(c-d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(c - d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDconst [c] (RSBconst [d] x))
+	// cond:
+	// result: (RSBconst [int64(int32(c+d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int64(int32(c + d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDshiftLL_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (ADDshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ADDconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ADDconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDshiftLL [c] (SRLconst x [32-c]) x)
+	// cond:
+	// result: (SRRconst [32-c] x)
+	for {
+		c := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		if v_0.AuxInt != 32-c {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = 32 - c
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDshiftLL <typ.UInt16> [8] (BFXU <typ.UInt16> [armBFAuxInt(8, 8)] x) x)
+	// cond:
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 {
+			break
+		}
+		if v.AuxInt != 8 {
+			break
+		}
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbBFXU {
+			break
+		}
+		if v_0.Type != typ.UInt16 {
+			break
+		}
+		if v_0.AuxInt != armBFAuxInt(8, 8) {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDshiftLL <typ.UInt16> [8] (SRLconst <typ.UInt16> [24] (SLLconst [16] x)) x)
+	// cond:
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 {
+			break
+		}
+		if v.AuxInt != 8 {
+			break
+		}
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		if v_0.Type != typ.UInt16 {
+			break
+		}
+		if v_0.AuxInt != 24 {
+			break
+		}
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbSLLconst {
+			break
+		}
+		if v_0_0.AuxInt != 16 {
+			break
+		}
+		if x != v_0_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (ADDshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ADDconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ADDconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbADDshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (ADDshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ADDconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ADDshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ADDconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ADDshiftRL [c] (SLLconst x [32-c]) x)
+	// cond:
+	// result: (SRRconst [ c] x)
+	for {
+		c := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		if v_0.AuxInt != 32-c {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbAND_0(v *Value) bool {
+	// match: (AND x (MOVWconst [c]))
+	// cond:
+	// result: (ANDconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (AND (MOVWconst [c]) x)
+	// cond:
+	// result: (ANDconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (AND x (SLLconst [c] y))
+	// cond:
+	// result: (ANDshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbANDshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND (SLLconst [c] y) x)
+	// cond:
+	// result: (ANDshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbANDshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND x (SRLconst [c] y))
+	// cond:
+	// result: (ANDshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbANDshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND (SRLconst [c] y) x)
+	// cond:
+	// result: (ANDshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbANDshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND x (SRAconst [c] y))
+	// cond:
+	// result: (ANDshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbANDshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND (SRAconst [c] y) x)
+	// cond:
+	// result: (ANDshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbANDshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND x x)
+	// cond:
+	// result: x
+	for {
+		x := v.Args[1]
+		if x != v.Args[0] {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (AND x (MVN y))
+	// cond:
+	// result: (BIC x y)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMVN {
+			break
+		}
+		y := v_1.Args[0]
+		v.reset(OpThumbBIC)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbAND_10(v *Value) bool {
+	// match: (AND (MVN y) x)
+	// cond:
+	// result: (BIC x y)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMVN {
+			break
+		}
+		y := v_0.Args[0]
+		v.reset(OpThumbBIC)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND x (MVNshiftLL y [c]))
+	// cond:
+	// result: (BICshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMVNshiftLL {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND (MVNshiftLL y [c]) x)
+	// cond:
+	// result: (BICshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMVNshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbBICshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND x (MVNshiftRL y [c]))
+	// cond:
+	// result: (BICshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMVNshiftRL {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND (MVNshiftRL y [c]) x)
+	// cond:
+	// result: (BICshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMVNshiftRL {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbBICshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND x (MVNshiftRA y [c]))
+	// cond:
+	// result: (BICshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMVNshiftRA {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (AND (MVNshiftRA y [c]) x)
+	// cond:
+	// result: (BICshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMVNshiftRA {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbBICshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbANDconst_0(v *Value) bool {
+	// match: (ANDconst [0] _)
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (ANDconst [c] x)
+	// cond: int32(c)==-1
+	// result: x
+	for {
+		c := v.AuxInt
+		x := v.Args[0]
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [c&d])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c & d
+		return true
+	}
+	// match: (ANDconst [c] (ANDconst [d] x))
+	// cond:
+	// result: (ANDconst [c&d] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c & d
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbANDshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (ANDshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ANDconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ANDshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ANDconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDshiftLL x y:(SLLconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		y := v.Args[1]
+		if y.Op != OpThumbSLLconst {
+			break
+		}
+		c := y.AuxInt
+		if x != y.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = y.Type
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbANDshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (ANDshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ANDconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ANDshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ANDconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDshiftRA x y:(SRAconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		y := v.Args[1]
+		if y.Op != OpThumbSRAconst {
+			break
+		}
+		c := y.AuxInt
+		if x != y.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = y.Type
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbANDshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (ANDshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ANDconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ANDshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ANDconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbANDconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ANDshiftRL x y:(SRLconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		y := v.Args[1]
+		if y.Op != OpThumbSRLconst {
+			break
+		}
+		c := y.AuxInt
+		if x != y.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = y.Type
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBFX_0(v *Value) bool {
+	// match: (BFX [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(d)<<(32-uint32(c&0xff)-uint32(c>>8))>>(32-uint32(c>>8)))])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(d) << (32 - uint32(c&0xff) - uint32(c>>8)) >> (32 - uint32(c>>8)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBFXU_0(v *Value) bool {
+	// match: (BFXU [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(uint32(d)<<(32-uint32(c&0xff)-uint32(c>>8))>>(32-uint32(c>>8))))])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(uint32(d) << (32 - uint32(c&0xff) - uint32(c>>8)) >> (32 - uint32(c>>8))))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBIC_0(v *Value) bool {
+	// match: (BIC x (MOVWconst [c]))
+	// cond:
+	// result: (BICconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbBICconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (BIC x (SLLconst [c] y))
+	// cond:
+	// result: (BICshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (BIC x (SRLconst [c] y))
+	// cond:
+	// result: (BICshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (BIC x (SRAconst [c] y))
+	// cond:
+	// result: (BICshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbBICshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (BIC x x)
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		x := v.Args[1]
+		if x != v.Args[0] {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBICconst_0(v *Value) bool {
+	// match: (BICconst [0] x)
+	// cond:
+	// result: x
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (BICconst [c] _)
+	// cond: int32(c)==-1
+	// result: (MOVWconst [0])
+	for {
+		c := v.AuxInt
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (BICconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [d&^c])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = d &^ c
+		return true
+	}
+	// match: (BICconst [c] (BICconst [d] x))
+	// cond:
+	// result: (BICconst [int64(int32(c|d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbBICconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int64(int32(c | d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBICshiftLL_0(v *Value) bool {
+	// match: (BICshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (BICconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (BICshiftLL x (SLLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBICshiftRA_0(v *Value) bool {
+	// match: (BICshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (BICconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (BICshiftRA x (SRAconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbBICshiftRL_0(v *Value) bool {
+	// match: (BICshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (BICconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbBICconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (BICshiftRL x (SRLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMN_0(v *Value) bool {
+	// match: (CMN x (MOVWconst [c]))
+	// cond:
+	// result: (CMNconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (CMN (MOVWconst [c]) x)
+	// cond:
+	// result: (CMNconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (CMN x (SLLconst [c] y))
+	// cond:
+	// result: (CMNshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbCMNshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMN (SLLconst [c] y) x)
+	// cond:
+	// result: (CMNshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbCMNshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMN x (SRLconst [c] y))
+	// cond:
+	// result: (CMNshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbCMNshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMN (SRLconst [c] y) x)
+	// cond:
+	// result: (CMNshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbCMNshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMN x (SRAconst [c] y))
+	// cond:
+	// result: (CMNshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbCMNshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMN (SRAconst [c] y) x)
+	// cond:
+	// result: (CMNshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbCMNshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMN x (RSBconst [0] y))
+	// cond:
+	// result: (CMP x y)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbRSBconst {
+			break
+		}
+		if v_1.AuxInt != 0 {
+			break
+		}
+		y := v_1.Args[0]
+		v.reset(OpThumbCMP)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMN (RSBconst [0] y) x)
+	// cond:
+	// result: (CMP x y)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		if v_0.AuxInt != 0 {
+			break
+		}
+		y := v_0.Args[0]
+		v.reset(OpThumbCMP)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMNconst_0(v *Value) bool {
+	// match: (CMNconst (MOVWconst [x]) [y])
+	// cond: int32(x)==int32(-y)
+	// result: (FlagEQ)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) == int32(-y)) {
+			break
+		}
+		v.reset(OpThumbFlagEQ)
+		return true
+	}
+	// match: (CMNconst (MOVWconst [x]) [y])
+	// cond: int32(x)<int32(-y) && uint32(x)<uint32(-y)
+	// result: (FlagLT_ULT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) < int32(-y) && uint32(x) < uint32(-y)) {
+			break
+		}
+		v.reset(OpThumbFlagLT_ULT)
+		return true
+	}
+	// match: (CMNconst (MOVWconst [x]) [y])
+	// cond: int32(x)<int32(-y) && uint32(x)>uint32(-y)
+	// result: (FlagLT_UGT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) < int32(-y) && uint32(x) > uint32(-y)) {
+			break
+		}
+		v.reset(OpThumbFlagLT_UGT)
+		return true
+	}
+	// match: (CMNconst (MOVWconst [x]) [y])
+	// cond: int32(x)>int32(-y) && uint32(x)<uint32(-y)
+	// result: (FlagGT_ULT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) > int32(-y) && uint32(x) < uint32(-y)) {
+			break
+		}
+		v.reset(OpThumbFlagGT_ULT)
+		return true
+	}
+	// match: (CMNconst (MOVWconst [x]) [y])
+	// cond: int32(x)>int32(-y) && uint32(x)>uint32(-y)
+	// result: (FlagGT_UGT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) > int32(-y) && uint32(x) > uint32(-y)) {
+			break
+		}
+		v.reset(OpThumbFlagGT_UGT)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMNshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (CMNshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (CMNconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMNshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (CMNconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMNshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (CMNshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (CMNconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMNshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (CMNconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMNshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (CMNshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (CMNconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMNshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (CMNconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbCMNconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMOVWHSconst_0(v *Value) bool {
+	// match: (CMOVWHSconst _ (FlagEQ) [c])
+	// cond:
+	// result: (MOVWconst [c])
+	for {
+		c := v.AuxInt
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c
+		return true
+	}
+	// match: (CMOVWHSconst x (FlagLT_ULT))
+	// cond:
+	// result: x
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (CMOVWHSconst _ (FlagLT_UGT) [c])
+	// cond:
+	// result: (MOVWconst [c])
+	for {
+		c := v.AuxInt
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c
+		return true
+	}
+	// match: (CMOVWHSconst x (FlagGT_ULT))
+	// cond:
+	// result: x
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (CMOVWHSconst _ (FlagGT_UGT) [c])
+	// cond:
+	// result: (MOVWconst [c])
+	for {
+		c := v.AuxInt
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c
+		return true
+	}
+	// match: (CMOVWHSconst x (InvertFlags flags) [c])
+	// cond:
+	// result: (CMOVWLSconst x flags [c])
+	for {
+		c := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbInvertFlags {
+			break
+		}
+		flags := v_1.Args[0]
+		v.reset(OpThumbCMOVWLSconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMOVWLSconst_0(v *Value) bool {
+	// match: (CMOVWLSconst _ (FlagEQ) [c])
+	// cond:
+	// result: (MOVWconst [c])
+	for {
+		c := v.AuxInt
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c
+		return true
+	}
+	// match: (CMOVWLSconst _ (FlagLT_ULT) [c])
+	// cond:
+	// result: (MOVWconst [c])
+	for {
+		c := v.AuxInt
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c
+		return true
+	}
+	// match: (CMOVWLSconst x (FlagLT_UGT))
+	// cond:
+	// result: x
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (CMOVWLSconst _ (FlagGT_ULT) [c])
+	// cond:
+	// result: (MOVWconst [c])
+	for {
+		c := v.AuxInt
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c
+		return true
+	}
+	// match: (CMOVWLSconst x (FlagGT_UGT))
+	// cond:
+	// result: x
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (CMOVWLSconst x (InvertFlags flags) [c])
+	// cond:
+	// result: (CMOVWHSconst x flags [c])
+	for {
+		c := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbInvertFlags {
+			break
+		}
+		flags := v_1.Args[0]
+		v.reset(OpThumbCMOVWHSconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMP_0(v *Value) bool {
+	b := v.Block
+	// match: (CMP x (MOVWconst [c]))
+	// cond:
+	// result: (CMPconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbCMPconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (CMP (MOVWconst [c]) x)
+	// cond:
+	// result: (InvertFlags (CMPconst [c] x))
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = c
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMP x (SLLconst [c] y))
+	// cond:
+	// result: (CMPshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbCMPshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMP (SLLconst [c] y) x)
+	// cond:
+	// result: (InvertFlags (CMPshiftLL x y [c]))
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+		v0.AuxInt = c
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMP x (SRLconst [c] y))
+	// cond:
+	// result: (CMPshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbCMPshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMP (SRLconst [c] y) x)
+	// cond:
+	// result: (InvertFlags (CMPshiftRL x y [c]))
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+		v0.AuxInt = c
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMP x (SRAconst [c] y))
+	// cond:
+	// result: (CMPshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbCMPshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (CMP (SRAconst [c] y) x)
+	// cond:
+	// result: (InvertFlags (CMPshiftRA x y [c]))
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+		v0.AuxInt = c
+		v0.AddArg(x)
+		v0.AddArg(y)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMP x (RSBconst [0] y))
+	// cond:
+	// result: (CMN x y)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbRSBconst {
+			break
+		}
+		if v_1.AuxInt != 0 {
+			break
+		}
+		y := v_1.Args[0]
+		v.reset(OpThumbCMN)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPD_0(v *Value) bool {
+	// match: (CMPD x (MOVDconst [0]))
+	// cond:
+	// result: (CMPD0 x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVDconst {
+			break
+		}
+		if v_1.AuxInt != 0 {
+			break
+		}
+		v.reset(OpThumbCMPD0)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPF_0(v *Value) bool {
+	// match: (CMPF x (MOVFconst [0]))
+	// cond:
+	// result: (CMPF0 x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVFconst {
+			break
+		}
+		if v_1.AuxInt != 0 {
+			break
+		}
+		v.reset(OpThumbCMPF0)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPconst_0(v *Value) bool {
+	// match: (CMPconst (MOVWconst [x]) [y])
+	// cond: int32(x)==int32(y)
+	// result: (FlagEQ)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) == int32(y)) {
+			break
+		}
+		v.reset(OpThumbFlagEQ)
+		return true
+	}
+	// match: (CMPconst (MOVWconst [x]) [y])
+	// cond: int32(x)<int32(y) && uint32(x)<uint32(y)
+	// result: (FlagLT_ULT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) < int32(y) && uint32(x) < uint32(y)) {
+			break
+		}
+		v.reset(OpThumbFlagLT_ULT)
+		return true
+	}
+	// match: (CMPconst (MOVWconst [x]) [y])
+	// cond: int32(x)<int32(y) && uint32(x)>uint32(y)
+	// result: (FlagLT_UGT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) < int32(y) && uint32(x) > uint32(y)) {
+			break
+		}
+		v.reset(OpThumbFlagLT_UGT)
+		return true
+	}
+	// match: (CMPconst (MOVWconst [x]) [y])
+	// cond: int32(x)>int32(y) && uint32(x)<uint32(y)
+	// result: (FlagGT_ULT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) > int32(y) && uint32(x) < uint32(y)) {
+			break
+		}
+		v.reset(OpThumbFlagGT_ULT)
+		return true
+	}
+	// match: (CMPconst (MOVWconst [x]) [y])
+	// cond: int32(x)>int32(y) && uint32(x)>uint32(y)
+	// result: (FlagGT_UGT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x) > int32(y) && uint32(x) > uint32(y)) {
+			break
+		}
+		v.reset(OpThumbFlagGT_UGT)
+		return true
+	}
+	// match: (CMPconst (MOVBUreg _) [c])
+	// cond: 0xff < c
+	// result: (FlagLT_ULT)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVBUreg {
+			break
+		}
+		if !(0xff < c) {
+			break
+		}
+		v.reset(OpThumbFlagLT_ULT)
+		return true
+	}
+	// match: (CMPconst (MOVHUreg _) [c])
+	// cond: 0xffff < c
+	// result: (FlagLT_ULT)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVHUreg {
+			break
+		}
+		if !(0xffff < c) {
+			break
+		}
+		v.reset(OpThumbFlagLT_ULT)
+		return true
+	}
+	// match: (CMPconst (ANDconst _ [m]) [n])
+	// cond: 0 <= int32(m) && int32(m) < int32(n)
+	// result: (FlagLT_ULT)
+	for {
+		n := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		m := v_0.AuxInt
+		if !(0 <= int32(m) && int32(m) < int32(n)) {
+			break
+		}
+		v.reset(OpThumbFlagLT_ULT)
+		return true
+	}
+	// match: (CMPconst (SRLconst _ [c]) [n])
+	// cond: 0 <= n && 0 < c && c <= 32 && (1<<uint32(32-c)) <= uint32(n)
+	// result: (FlagLT_ULT)
+	for {
+		n := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(0 <= n && 0 < c && c <= 32 && (1<<uint32(32-c)) <= uint32(n)) {
+			break
+		}
+		v.reset(OpThumbFlagLT_ULT)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (CMPshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (InvertFlags (CMPconst [c] (SLLconst <x.Type> x [d])))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = c
+		v1 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v1.AuxInt = d
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMPshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (CMPconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbCMPconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (CMPshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (InvertFlags (CMPconst [c] (SRAconst <x.Type> x [d])))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = c
+		v1 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v1.AuxInt = d
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMPshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (CMPconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbCMPconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbCMPshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (CMPshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (InvertFlags (CMPconst [c] (SRLconst <x.Type> x [d])))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbInvertFlags)
+		v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+		v0.AuxInt = c
+		v1 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v1.AuxInt = d
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (CMPshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (CMPconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbCMPconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbDIV_0(v *Value) bool {
+	// match: (DIV x (MOVWconst [1]))
+	// cond:
+	// result: x
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_1.AuxInt != 1 {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (DIV x (MOVWconst [c]))
+	// cond: isPowerOfTwo(c)
+	// result: (SRAconst [log2(c)] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c)) {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = log2(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (DIV (MOVWconst [c]) (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(c)/int32(d))])
+	for {
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_1.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(c) / int32(d))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbDIVU_0(v *Value) bool {
+	// match: (DIVU x (MOVWconst [1]))
+	// cond:
+	// result: x
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_1.AuxInt != 1 {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (DIVU x (MOVWconst [c]))
+	// cond: isPowerOfTwo(c)
+	// result: (SRLconst [log2(c)] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c)) {
+			break
+		}
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = log2(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (DIVU (MOVWconst [c]) (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(uint32(c)/uint32(d))])
+	for {
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_1.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(uint32(c) / uint32(d))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbEqual_0(v *Value) bool {
+	// match: (Equal (FlagEQ))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (Equal (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (Equal (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (Equal (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (Equal (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (Equal (InvertFlags x))
+	// cond:
+	// result: (Equal x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbEqual)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbGreaterEqual_0(v *Value) bool {
+	// match: (GreaterEqual (FlagEQ))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterEqual (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterEqual (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterEqual (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterEqual (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterEqual (InvertFlags x))
+	// cond:
+	// result: (LessEqual x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbLessEqual)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbGreaterEqualU_0(v *Value) bool {
+	// match: (GreaterEqualU (FlagEQ))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterEqualU (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterEqualU (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterEqualU (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterEqualU (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterEqualU (InvertFlags x))
+	// cond:
+	// result: (LessEqualU x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbLessEqualU)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbGreaterThan_0(v *Value) bool {
+	// match: (GreaterThan (FlagEQ))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterThan (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterThan (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterThan (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterThan (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterThan (InvertFlags x))
+	// cond:
+	// result: (LessThan x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbLessThan)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbGreaterThanU_0(v *Value) bool {
+	// match: (GreaterThanU (FlagEQ))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterThanU (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterThanU (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterThanU (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (GreaterThanU (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (GreaterThanU (InvertFlags x))
+	// cond:
+	// result: (LessThanU x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbLessThanU)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLessEqual_0(v *Value) bool {
+	// match: (LessEqual (FlagEQ))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessEqual (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessEqual (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessEqual (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessEqual (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessEqual (InvertFlags x))
+	// cond:
+	// result: (GreaterEqual x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbGreaterEqual)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLessEqualU_0(v *Value) bool {
+	// match: (LessEqualU (FlagEQ))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessEqualU (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessEqualU (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessEqualU (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessEqualU (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessEqualU (InvertFlags x))
+	// cond:
+	// result: (GreaterEqualU x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbGreaterEqualU)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLessThan_0(v *Value) bool {
+	// match: (LessThan (FlagEQ))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessThan (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessThan (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessThan (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessThan (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessThan (InvertFlags x))
+	// cond:
+	// result: (GreaterThan x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbGreaterThan)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLessThanU_0(v *Value) bool {
+	// match: (LessThanU (FlagEQ))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessThanU (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessThanU (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessThanU (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (LessThanU (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (LessThanU (InvertFlags x))
+	// cond:
+	// result: (GreaterThanU x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbGreaterThanU)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce16_0(v *Value) bool {
+	// match: (LoadOnce16 [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (LoadOnce16 [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce16 [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (LoadOnce16 [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce16 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (LoadOnce16 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce16 [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (LoadOnce16idx ptr idx mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbLoadOnce16idx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce16 [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (LoadOnce16shiftLL ptr idx [c] mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce16shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce16idx_0(v *Value) bool {
+	// match: (LoadOnce16idx ptr (MOVWconst [c]) mem)
+	// cond:
+	// result: (LoadOnce16 [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce16idx (MOVWconst [c]) ptr mem)
+	// cond:
+	// result: (LoadOnce16 [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		v.reset(OpThumbLoadOnce16)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce16idx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce16shiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce16shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce32_0(v *Value) bool {
+	// match: (LoadOnce32 [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (LoadOnce32 [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce32 [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (LoadOnce32 [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce32 [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (LoadOnce32idx ptr idx mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbLoadOnce32idx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce32 [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (LoadOnce32shiftLL ptr idx [c] mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce32shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce32idx_0(v *Value) bool {
+	// match: (LoadOnce32idx ptr (MOVWconst [c]) mem)
+	// cond:
+	// result: (LoadOnce32 [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce32idx (MOVWconst [c]) ptr mem)
+	// cond:
+	// result: (LoadOnce32 [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		v.reset(OpThumbLoadOnce32)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce32idx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce32shiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce32shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce8_0(v *Value) bool {
+	// match: (LoadOnce8 [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (LoadOnce8 [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce8 [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (LoadOnce8 [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce8 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (LoadOnce8 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce8 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (LoadOnce8 [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce8 [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (LoadOnce8idx ptr idx mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbLoadOnce8idx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce8 [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (LoadOnce8shiftLL ptr idx [c] mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce8shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbLoadOnce8idx_0(v *Value) bool {
+	// match: (LoadOnce8idx ptr (MOVWconst [c]) mem)
+	// cond:
+	// result: (LoadOnce8 [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce8idx (MOVWconst [c]) ptr mem)
+	// cond:
+	// result: (LoadOnce8 [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		v.reset(OpThumbLoadOnce8)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (LoadOnce8idx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (LoadOnce8shiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbLoadOnce8shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBUload_0(v *Value) bool {
+	// match: (MOVBUload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVBUload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBUload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVBUload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBUload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVBUload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBUload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: (MOVBUreg x)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVBstore {
+			break
+		}
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		_ = v_1.Args[2]
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVBUloadidx ptr idx mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVBUloadidx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBUload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVBUloadshiftLL ptr idx [c] mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBUloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBUload [off] {sym} (SB) _)
+	// cond: symIsRO(sym)
+	// result: (MOVWconst [int64(read8(sym, off))])
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpSB {
+			break
+		}
+		if !(symIsRO(sym)) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(read8(sym, off))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBUloadidx_0(v *Value) bool {
+	// match: (MOVBUloadidx ptr idx (MOVBstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: (MOVBUreg x)
+	for {
+		_ = v.Args[2]
+		ptr := v.Args[0]
+		idx := v.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbMOVBstoreidx {
+			break
+		}
+		_ = v_2.Args[3]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] {
+			break
+		}
+		x := v_2.Args[2]
+		if !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUloadidx ptr (MOVWconst [c]) mem)
+	// cond:
+	// result: (MOVBUload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBUloadidx (MOVWconst [c]) ptr mem)
+	// cond:
+	// result: (MOVBUload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		v.reset(OpThumbMOVBUload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBUloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVBUloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBUloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBUloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVBUloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBUloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBUreg_0(v *Value) bool {
+	// match: (MOVBUreg x:(MOVBUload _ _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBUload {
+			break
+		}
+		_ = x.Args[1]
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg (ANDconst [c] x))
+	// cond:
+	// result: (ANDconst [c&0xff] x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c & 0xff
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg x:(MOVBUreg _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBUreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBUreg (MOVWconst [c]))
+	// cond:
+	// result: (MOVWconst [int64(uint8(c))])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(uint8(c))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBload_0(v *Value) bool {
+	// match: (MOVBload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVBload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVBload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVBload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBload [off] {sym} ptr (MOVBstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: (MOVBreg x)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVBstore {
+			break
+		}
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		_ = v_1.Args[2]
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVBloadidx ptr idx mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVBloadidx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVBloadshiftLL ptr idx [c] mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBloadidx_0(v *Value) bool {
+	// match: (MOVBloadidx ptr idx (MOVBstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: (MOVBreg x)
+	for {
+		_ = v.Args[2]
+		ptr := v.Args[0]
+		idx := v.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbMOVBstoreidx {
+			break
+		}
+		_ = v_2.Args[3]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] {
+			break
+		}
+		x := v_2.Args[2]
+		if !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVBreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBloadidx ptr (MOVWconst [c]) mem)
+	// cond:
+	// result: (MOVBload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBloadidx (MOVWconst [c]) ptr mem)
+	// cond:
+	// result: (MOVBload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		v.reset(OpThumbMOVBload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVBloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVBloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBreg_0(v *Value) bool {
+	// match: (MOVBreg x:(MOVBload _ _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBload {
+			break
+		}
+		_ = x.Args[1]
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg (ANDconst [c] x))
+	// cond: c & 0x80 == 0
+	// result: (ANDconst [c&0x7f] x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		if !(c&0x80 == 0) {
+			break
+		}
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c & 0x7f
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg x:(MOVBreg _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVBreg (MOVWconst [c]))
+	// cond:
+	// result: (MOVWconst [int64(int8(c))])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int8(c))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBstore_0(v *Value) bool {
+	// match: (MOVBstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVBstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVBstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVBstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstore [off] {sym} ptr (MOVBreg x) mem)
+	// cond:
+	// result: (MOVBstore [off] {sym} ptr x mem)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVBreg {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = off
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(x)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstore [off] {sym} ptr (MOVBUreg x) mem)
+	// cond:
+	// result: (MOVBstore [off] {sym} ptr x mem)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVBUreg {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = off
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(x)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstore [off] {sym} ptr (MOVHreg x) mem)
+	// cond:
+	// result: (MOVBstore [off] {sym} ptr x mem)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVHreg {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = off
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(x)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstore [off] {sym} ptr (MOVHUreg x) mem)
+	// cond:
+	// result: (MOVBstore [off] {sym} ptr x mem)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVHUreg {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = off
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(x)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstore [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (MOVBstoreidx ptr idx val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVBstoreidx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVBstoreshiftLL ptr idx [c] val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVBstoreidx_0(v *Value) bool {
+	// match: (MOVBstoreidx ptr (MOVWconst [c]) val mem)
+	// cond:
+	// result: (MOVBstore [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		val := v.Args[2]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstoreidx (MOVWconst [c]) ptr val mem)
+	// cond:
+	// result: (MOVBstore [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		val := v.Args[2]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstoreidx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (MOVBstoreshiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVBstoreidx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (MOVBstoreshiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVBstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVDload_0(v *Value) bool {
+	// match: (MOVDload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVDload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVDload)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVDload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVDload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVDload)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVDload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVDload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVDload)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVDload [off] {sym} ptr (MOVDstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVDstore {
+			break
+		}
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		_ = v_1.Args[2]
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVDstore_0(v *Value) bool {
+	// match: (MOVDstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVDstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVDstore)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVDstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVDstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVDstore)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVDstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVDstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVDstore)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVFload_0(v *Value) bool {
+	// match: (MOVFload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVFload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVFload)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVFload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVFload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVFload)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVFload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVFload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVFload)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVFload [off] {sym} ptr (MOVFstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVFstore {
+			break
+		}
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		_ = v_1.Args[2]
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVFstore_0(v *Value) bool {
+	// match: (MOVFstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVFstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVFstore)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVFstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVFstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVFstore)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVFstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVFstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVFstore)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHUload_0(v *Value) bool {
+	b := v.Block
+	config := b.Func.Config
+	// match: (MOVHUload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVHUload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHUload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVHUload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHUload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVHUload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHUload [off] {sym} ptr (MOVHstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: (MOVHUreg x)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVHstore {
+			break
+		}
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		_ = v_1.Args[2]
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVHUloadidx ptr idx mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVHUloadidx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHUload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVHUloadshiftLL ptr idx [c] mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHUloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHUload [off] {sym} (SB) _)
+	// cond: symIsRO(sym)
+	// result: (MOVWconst [int64(read16(sym, off, config.BigEndian))])
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpSB {
+			break
+		}
+		if !(symIsRO(sym)) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(read16(sym, off, config.BigEndian))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHUloadidx_0(v *Value) bool {
+	// match: (MOVHUloadidx ptr idx (MOVHstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: (MOVHUreg x)
+	for {
+		_ = v.Args[2]
+		ptr := v.Args[0]
+		idx := v.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbMOVHstoreidx {
+			break
+		}
+		_ = v_2.Args[3]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] {
+			break
+		}
+		x := v_2.Args[2]
+		if !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHUreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUloadidx ptr (MOVWconst [c]) mem)
+	// cond:
+	// result: (MOVHUload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHUloadidx (MOVWconst [c]) ptr mem)
+	// cond:
+	// result: (MOVHUload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		v.reset(OpThumbMOVHUload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHUloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVHUloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHUloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHUloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVHUloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHUloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHUreg_0(v *Value) bool {
+	// match: (MOVHUreg x:(MOVBUload _ _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBUload {
+			break
+		}
+		_ = x.Args[1]
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVHUload _ _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVHUload {
+			break
+		}
+		_ = x.Args[1]
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg (ANDconst [c] x))
+	// cond:
+	// result: (ANDconst [c&0xffff] x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c & 0xffff
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVBUreg _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBUreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg x:(MOVHUreg _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVHUreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHUreg (MOVWconst [c]))
+	// cond:
+	// result: (MOVWconst [int64(uint16(c))])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(uint16(c))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHload_0(v *Value) bool {
+	// match: (MOVHload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVHload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVHload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVHload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHload [off] {sym} ptr (MOVHstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: (MOVHreg x)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVHstore {
+			break
+		}
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		_ = v_1.Args[2]
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVHloadidx ptr idx mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVHloadidx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVHloadshiftLL ptr idx [c] mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHloadidx_0(v *Value) bool {
+	// match: (MOVHloadidx ptr idx (MOVHstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: (MOVHreg x)
+	for {
+		_ = v.Args[2]
+		ptr := v.Args[0]
+		idx := v.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbMOVHstoreidx {
+			break
+		}
+		_ = v_2.Args[3]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] {
+			break
+		}
+		x := v_2.Args[2]
+		if !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpThumbMOVHreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHloadidx ptr (MOVWconst [c]) mem)
+	// cond:
+	// result: (MOVHload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHloadidx (MOVWconst [c]) ptr mem)
+	// cond:
+	// result: (MOVHload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		v.reset(OpThumbMOVHload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVHloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVHloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHreg_0(v *Value) bool {
+	// match: (MOVHreg x:(MOVBload _ _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBload {
+			break
+		}
+		_ = x.Args[1]
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBUload _ _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBUload {
+			break
+		}
+		_ = x.Args[1]
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVHload _ _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVHload {
+			break
+		}
+		_ = x.Args[1]
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (ANDconst [c] x))
+	// cond: c & 0x8000 == 0
+	// result: (ANDconst [c&0x7fff] x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbANDconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		if !(c&0x8000 == 0) {
+			break
+		}
+		v.reset(OpThumbANDconst)
+		v.AuxInt = c & 0x7fff
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBreg _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVBUreg _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVBUreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg x:(MOVHreg _))
+	// cond:
+	// result: (MOVWreg x)
+	for {
+		x := v.Args[0]
+		if x.Op != OpThumbMOVHreg {
+			break
+		}
+		v.reset(OpThumbMOVWreg)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVHreg (MOVWconst [c]))
+	// cond:
+	// result: (MOVWconst [int64(int16(c))])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int16(c))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHstore_0(v *Value) bool {
+	// match: (MOVHstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVHstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVHstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVHstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstore [off] {sym} ptr (MOVHreg x) mem)
+	// cond:
+	// result: (MOVHstore [off] {sym} ptr x mem)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVHreg {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = off
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(x)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstore [off] {sym} ptr (MOVHUreg x) mem)
+	// cond:
+	// result: (MOVHstore [off] {sym} ptr x mem)
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVHUreg {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = off
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(x)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstore [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (MOVHstoreidx ptr idx val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVHstoreidx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVHstoreshiftLL ptr idx [c] val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVHstoreidx_0(v *Value) bool {
+	// match: (MOVHstoreidx ptr (MOVWconst [c]) val mem)
+	// cond:
+	// result: (MOVHstore [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		val := v.Args[2]
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstoreidx (MOVWconst [c]) ptr val mem)
+	// cond:
+	// result: (MOVHstore [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		val := v.Args[2]
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstoreidx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (MOVHstoreshiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVHstoreidx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (MOVHstoreshiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVHstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWload_0(v *Value) bool {
+	b := v.Block
+	config := b.Func.Config
+	// match: (MOVWload [off1] {sym} (ADDconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVWload [off1+off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWload [off1] {sym} (SUBconst [off2] ptr) mem)
+	// cond:
+	// result: (MOVWload [off1-off2] {sym} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWload [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVWload [off1+off2] {mergeSym(sym1,sym2)} ptr mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWload [off] {sym} ptr (MOVWstore [off2] {sym2} ptr2 x _))
+	// cond: sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWstore {
+			break
+		}
+		off2 := v_1.AuxInt
+		sym2 := v_1.Aux
+		_ = v_1.Args[2]
+		ptr2 := v_1.Args[0]
+		x := v_1.Args[1]
+		if !(sym == sym2 && off == off2 && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVWload [0] {sym} (ADD ptr idx) mem)
+	// cond: sym == nil
+	// result: (MOVWloadidx ptr idx mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVWloadidx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWload [0] {sym} (ADDshiftLL ptr idx [c]) mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVWloadshiftLL ptr idx [c] mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWload [off] {sym} (SB) _)
+	// cond: symIsRO(sym)
+	// result: (MOVWconst [int64(int32(read32(sym, off, config.BigEndian)))])
+	for {
+		off := v.AuxInt
+		sym := v.Aux
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpSB {
+			break
+		}
+		if !(symIsRO(sym)) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(read32(sym, off, config.BigEndian)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWloadidx_0(v *Value) bool {
+	// match: (MOVWloadidx ptr idx (MOVWstoreidx ptr2 idx x _))
+	// cond: isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		_ = v.Args[2]
+		ptr := v.Args[0]
+		idx := v.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbMOVWstoreidx {
+			break
+		}
+		_ = v_2.Args[3]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] {
+			break
+		}
+		x := v_2.Args[2]
+		if !(isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVWloadidx ptr (MOVWconst [c]) mem)
+	// cond:
+	// result: (MOVWload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWloadidx (MOVWconst [c]) ptr mem)
+	// cond:
+	// result: (MOVWload [c] ptr mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWloadidx ptr (SLLconst idx [c]) mem)
+	// cond: c <= 3
+	// result: (MOVWloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWloadidx (SLLconst idx [c]) ptr mem)
+	// cond: c <= 3
+	// result: (MOVWloadshiftLL ptr idx [c] mem)
+	for {
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWloadshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWloadshiftLL_0(v *Value) bool {
+	// match: (MOVWloadshiftLL ptr idx [c] (MOVWstoreshiftLL ptr2 idx [d] x _))
+	// cond: c==d && isSamePtr(ptr, ptr2)
+	// result: x
+	for {
+		c := v.AuxInt
+		_ = v.Args[2]
+		ptr := v.Args[0]
+		idx := v.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbMOVWstoreshiftLL {
+			break
+		}
+		d := v_2.AuxInt
+		_ = v_2.Args[3]
+		ptr2 := v_2.Args[0]
+		if idx != v_2.Args[1] {
+			break
+		}
+		x := v_2.Args[2]
+		if !(c == d && isSamePtr(ptr, ptr2)) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVWloadshiftLL ptr (MOVWconst [c]) [d] mem)
+	// cond:
+	// result: (MOVWload [int64(uint32(c)<<uint64(d))] ptr mem)
+	for {
+		d := v.AuxInt
+		mem := v.Args[2]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbMOVWload)
+		v.AuxInt = int64(uint32(c) << uint64(d))
+		v.AddArg(ptr)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWreg_0(v *Value) bool {
+	// match: (MOVWreg x)
+	// cond: x.Uses == 1
+	// result: (MOVWnop x)
+	for {
+		x := v.Args[0]
+		if !(x.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMOVWnop)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MOVWreg (MOVWconst [c]))
+	// cond:
+	// result: (MOVWconst [c])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWstore_0(v *Value) bool {
+	// match: (MOVWstore [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVWstore [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWstore [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// cond:
+	// result: (MOVWstore [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWstore [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (MOVWstore [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWstore [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (MOVWstoreidx ptr idx val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbMOVWstoreidx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWstore [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (MOVWstoreshiftLL ptr idx [c] val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWstoreidx_0(v *Value) bool {
+	// match: (MOVWstoreidx ptr (MOVWconst [c]) val mem)
+	// cond:
+	// result: (MOVWstore [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		val := v.Args[2]
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWstoreidx (MOVWconst [c]) ptr val mem)
+	// cond:
+	// result: (MOVWstore [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		val := v.Args[2]
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWstoreidx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (MOVWstoreshiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (MOVWstoreidx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (MOVWstoreshiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbMOVWstoreshiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMOVWstoreshiftLL_0(v *Value) bool {
+	// match: (MOVWstoreshiftLL ptr (MOVWconst [c]) [d] val mem)
+	// cond:
+	// result: (MOVWstore [int64(uint32(c)<<uint64(d))] ptr val mem)
+	for {
+		d := v.AuxInt
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		val := v.Args[2]
+		v.reset(OpThumbMOVWstore)
+		v.AuxInt = int64(uint32(c) << uint64(d))
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMUL_0(v *Value) bool {
+	// match: (MUL x (MOVWconst [c]))
+	// cond: int32(c) == -1
+	// result: (RSBconst [0] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = 0
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL (MOVWconst [c]) x)
+	// cond: int32(c) == -1
+	// result: (RSBconst [0] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = 0
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL _ (MOVWconst [0]))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		_ = v.Args[1]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_1.AuxInt != 0 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (MUL (MOVWconst [0]) _)
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_0.AuxInt != 0 {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (MUL x (MOVWconst [1]))
+	// cond:
+	// result: x
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_1.AuxInt != 1 {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL (MOVWconst [1]) x)
+	// cond:
+	// result: x
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_0.AuxInt != 1 {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: isPowerOfTwo(c)
+	// result: (SLLconst [log2(c)] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL (MOVWconst [c]) x)
+	// cond: isPowerOfTwo(c)
+	// result: (SLLconst [log2(c)] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(isPowerOfTwo(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: isPowerOfTwo(c-1) && int32(c) >= 3
+	// result: (ADDshiftLL x x [log2(c-1)])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c-1) && int32(c) >= 3) {
+			break
+		}
+		v.reset(OpThumbADDshiftLL)
+		v.AuxInt = log2(c - 1)
+		v.AddArg(x)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL (MOVWconst [c]) x)
+	// cond: isPowerOfTwo(c-1) && int32(c) >= 3
+	// result: (ADDshiftLL x x [log2(c-1)])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(isPowerOfTwo(c-1) && int32(c) >= 3) {
+			break
+		}
+		v.reset(OpThumbADDshiftLL)
+		v.AuxInt = log2(c - 1)
+		v.AddArg(x)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMUL_10(v *Value) bool {
+	b := v.Block
+	// match: (MUL x (MOVWconst [c]))
+	// cond: isPowerOfTwo(c+1) && int32(c) >= 7
+	// result: (RSBshiftLL x x [log2(c+1)])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c+1) && int32(c) >= 7) {
+			break
+		}
+		v.reset(OpThumbRSBshiftLL)
+		v.AuxInt = log2(c + 1)
+		v.AddArg(x)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL (MOVWconst [c]) x)
+	// cond: isPowerOfTwo(c+1) && int32(c) >= 7
+	// result: (RSBshiftLL x x [log2(c+1)])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(isPowerOfTwo(c+1) && int32(c) >= 7) {
+			break
+		}
+		v.reset(OpThumbRSBshiftLL)
+		v.AuxInt = log2(c + 1)
+		v.AddArg(x)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)
+	// result: (SLLconst [log2(c/3)] (ADDshiftLL <x.Type> x x [1]))
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c / 3)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = 1
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MUL (MOVWconst [c]) x)
+	// cond: c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)
+	// result: (SLLconst [log2(c/3)] (ADDshiftLL <x.Type> x x [1]))
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c / 3)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = 1
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)
+	// result: (SLLconst [log2(c/5)] (ADDshiftLL <x.Type> x x [2]))
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c / 5)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = 2
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MUL (MOVWconst [c]) x)
+	// cond: c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)
+	// result: (SLLconst [log2(c/5)] (ADDshiftLL <x.Type> x x [2]))
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c / 5)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = 2
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)
+	// result: (SLLconst [log2(c/7)] (RSBshiftLL <x.Type> x x [3]))
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c / 7)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = 3
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MUL (MOVWconst [c]) x)
+	// cond: c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)
+	// result: (SLLconst [log2(c/7)] (RSBshiftLL <x.Type> x x [3]))
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c / 7)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = 3
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MUL x (MOVWconst [c]))
+	// cond: c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)
+	// result: (SLLconst [log2(c/9)] (ADDshiftLL <x.Type> x x [3]))
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c / 9)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = 3
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (MUL (MOVWconst [c]) x)
+	// cond: c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)
+	// result: (SLLconst [log2(c/9)] (ADDshiftLL <x.Type> x x [3]))
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		if !(c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = log2(c / 9)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = 3
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMUL_20(v *Value) bool {
+	// match: (MUL (MOVWconst [c]) (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(c*d))])
+	for {
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_1.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(c * d))
+		return true
+	}
+	// match: (MUL (MOVWconst [d]) (MOVWconst [c]))
+	// cond:
+	// result: (MOVWconst [int64(int32(c*d))])
+	for {
+		_ = v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(c * d))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULA_0(v *Value) bool {
+	b := v.Block
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: int32(c) == -1
+	// result: (SUB a x)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbSUB)
+		v.AddArg(a)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MULA _ (MOVWconst [0]) a)
+	// cond:
+	// result: a
+	for {
+		a := v.Args[2]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_1.AuxInt != 0 {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = a.Type
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [1]) a)
+	// cond:
+	// result: (ADD x a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_1.AuxInt != 1 {
+			break
+		}
+		v.reset(OpThumbADD)
+		v.AddArg(x)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c)] x) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo(c-1) && int32(c) >= 3
+	// result: (ADD (ADDshiftLL <x.Type> x x [log2(c-1)]) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c-1) && int32(c) >= 3) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = log2(c - 1)
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo(c+1) && int32(c) >= 7
+	// result: (ADD (RSBshiftLL <x.Type> x x [log2(c+1)]) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c+1) && int32(c) >= 7) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = log2(c + 1)
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c/3)] (ADDshiftLL <x.Type> x x [1])) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 3)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 1
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c/5)] (ADDshiftLL <x.Type> x x [2])) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 5)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 2
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c/7)] (RSBshiftLL <x.Type> x x [3])) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 7)
+		v1 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v1.AuxInt = 3
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA x (MOVWconst [c]) a)
+	// cond: c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c/9)] (ADDshiftLL <x.Type> x x [3])) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 9)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 3
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULA_10(v *Value) bool {
+	b := v.Block
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: int32(c) == -1
+	// result: (SUB a x)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbSUB)
+		v.AddArg(a)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MULA (MOVWconst [0]) _ a)
+	// cond:
+	// result: a
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_0.AuxInt != 0 {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = a.Type
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [1]) x a)
+	// cond:
+	// result: (ADD x a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_0.AuxInt != 1 {
+			break
+		}
+		x := v.Args[1]
+		v.reset(OpThumbADD)
+		v.AddArg(x)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c)] x) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(isPowerOfTwo(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo(c-1) && int32(c) >= 3
+	// result: (ADD (ADDshiftLL <x.Type> x x [log2(c-1)]) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(isPowerOfTwo(c-1) && int32(c) >= 3) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = log2(c - 1)
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo(c+1) && int32(c) >= 7
+	// result: (ADD (RSBshiftLL <x.Type> x x [log2(c+1)]) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(isPowerOfTwo(c+1) && int32(c) >= 7) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = log2(c + 1)
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c/3)] (ADDshiftLL <x.Type> x x [1])) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 3)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 1
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c/5)] (ADDshiftLL <x.Type> x x [2])) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 5)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 2
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c/7)] (RSBshiftLL <x.Type> x x [3])) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 7)
+		v1 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v1.AuxInt = 3
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULA (MOVWconst [c]) x a)
+	// cond: c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)
+	// result: (ADD (SLLconst <x.Type> [log2(c/9)] (ADDshiftLL <x.Type> x x [3])) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 9)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 3
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULA_20(v *Value) bool {
+	// match: (MULA (MOVWconst [c]) (MOVWconst [d]) a)
+	// cond:
+	// result: (ADDconst [int64(int32(c*d))] a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_1.AuxInt
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(c * d))
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULD_0(v *Value) bool {
+	// match: (MULD (NEGD x) y)
+	// cond:
+	// result: (NMULD x y)
+	for {
+		y := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbNEGD {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbNMULD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (MULD y (NEGD x))
+	// cond:
+	// result: (NMULD x y)
+	for {
+		_ = v.Args[1]
+		y := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbNEGD {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbNMULD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULF_0(v *Value) bool {
+	// match: (MULF (NEGF x) y)
+	// cond:
+	// result: (NMULF x y)
+	for {
+		y := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbNEGF {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbNMULF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (MULF y (NEGF x))
+	// cond:
+	// result: (NMULF x y)
+	for {
+		_ = v.Args[1]
+		y := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbNEGF {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbNMULF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULS_0(v *Value) bool {
+	b := v.Block
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: int32(c) == -1
+	// result: (ADD a x)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v.AddArg(a)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MULS _ (MOVWconst [0]) a)
+	// cond:
+	// result: a
+	for {
+		a := v.Args[2]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_1.AuxInt != 0 {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = a.Type
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [1]) a)
+	// cond:
+	// result: (RSB x a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_1.AuxInt != 1 {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v.AddArg(x)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c)] x) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo(c-1) && int32(c) >= 3
+	// result: (RSB (ADDshiftLL <x.Type> x x [log2(c-1)]) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c-1) && int32(c) >= 3) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = log2(c - 1)
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: isPowerOfTwo(c+1) && int32(c) >= 7
+	// result: (RSB (RSBshiftLL <x.Type> x x [log2(c+1)]) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(isPowerOfTwo(c+1) && int32(c) >= 7) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = log2(c + 1)
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c/3)] (ADDshiftLL <x.Type> x x [1])) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 3)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 1
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c/5)] (ADDshiftLL <x.Type> x x [2])) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 5)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 2
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c/7)] (RSBshiftLL <x.Type> x x [3])) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 7)
+		v1 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v1.AuxInt = 3
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS x (MOVWconst [c]) a)
+	// cond: c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c/9)] (ADDshiftLL <x.Type> x x [3])) a)
+	for {
+		a := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		if !(c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 9)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 3
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULS_10(v *Value) bool {
+	b := v.Block
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: int32(c) == -1
+	// result: (ADD a x)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbADD)
+		v.AddArg(a)
+		v.AddArg(x)
+		return true
+	}
+	// match: (MULS (MOVWconst [0]) _ a)
+	// cond:
+	// result: a
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_0.AuxInt != 0 {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = a.Type
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [1]) x a)
+	// cond:
+	// result: (RSB x a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		if v_0.AuxInt != 1 {
+			break
+		}
+		x := v.Args[1]
+		v.reset(OpThumbRSB)
+		v.AddArg(x)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c)] x) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(isPowerOfTwo(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo(c-1) && int32(c) >= 3
+	// result: (RSB (ADDshiftLL <x.Type> x x [log2(c-1)]) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(isPowerOfTwo(c-1) && int32(c) >= 3) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v0.AuxInt = log2(c - 1)
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: isPowerOfTwo(c+1) && int32(c) >= 7
+	// result: (RSB (RSBshiftLL <x.Type> x x [log2(c+1)]) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(isPowerOfTwo(c+1) && int32(c) >= 7) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v0.AuxInt = log2(c + 1)
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c/3)] (ADDshiftLL <x.Type> x x [1])) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(c%3 == 0 && isPowerOfTwo(c/3) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 3)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 1
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c/5)] (ADDshiftLL <x.Type> x x [2])) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(c%5 == 0 && isPowerOfTwo(c/5) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 5)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 2
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c/7)] (RSBshiftLL <x.Type> x x [3])) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(c%7 == 0 && isPowerOfTwo(c/7) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 7)
+		v1 := b.NewValue0(v.Pos, OpThumbRSBshiftLL, x.Type)
+		v1.AuxInt = 3
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	// match: (MULS (MOVWconst [c]) x a)
+	// cond: c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)
+	// result: (RSB (SLLconst <x.Type> [log2(c/9)] (ADDshiftLL <x.Type> x x [3])) a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v.Args[1]
+		if !(c%9 == 0 && isPowerOfTwo(c/9) && is32Bit(c)) {
+			break
+		}
+		v.reset(OpThumbRSB)
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = log2(c / 9)
+		v1 := b.NewValue0(v.Pos, OpThumbADDshiftLL, x.Type)
+		v1.AuxInt = 3
+		v1.AddArg(x)
+		v1.AddArg(x)
+		v0.AddArg(v1)
+		v.AddArg(v0)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMULS_20(v *Value) bool {
+	// match: (MULS (MOVWconst [c]) (MOVWconst [d]) a)
+	// cond:
+	// result: (SUBconst [int64(int32(c*d))] a)
+	for {
+		a := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_1.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int64(int32(c * d))
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMVN_0(v *Value) bool {
+	// match: (MVN (MOVWconst [c]))
+	// cond:
+	// result: (MOVWconst [^c])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = ^c
+		return true
+	}
+	// match: (MVN (SLLconst [c] x))
+	// cond:
+	// result: (MVNshiftLL x [c])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbMVNshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (MVN (SRLconst [c] x))
+	// cond:
+	// result: (MVNshiftRL x [c])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbMVNshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (MVN (SRAconst [c] x))
+	// cond:
+	// result: (MVNshiftRA x [c])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbMVNshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMVNshiftLL_0(v *Value) bool {
+	// match: (MVNshiftLL (MOVWconst [c]) [d])
+	// cond:
+	// result: (MOVWconst [^int64(uint32(c)<<uint64(d))])
+	for {
+		d := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = ^int64(uint32(c) << uint64(d))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMVNshiftRA_0(v *Value) bool {
+	// match: (MVNshiftRA (MOVWconst [c]) [d])
+	// cond:
+	// result: (MOVWconst [^int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = ^int64(int32(c) >> uint64(d))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbMVNshiftRL_0(v *Value) bool {
+	// match: (MVNshiftRL (MOVWconst [c]) [d])
+	// cond:
+	// result: (MOVWconst [^int64(uint32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = ^int64(uint32(c) >> uint64(d))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNEGD_0(v *Value) bool {
+	// match: (NEGD (MULD x y))
+	// cond:
+	// result: (NMULD x y)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMULD {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		v.reset(OpThumbNMULD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNEGF_0(v *Value) bool {
+	// match: (NEGF (MULF x y))
+	// cond:
+	// result: (NMULF x y)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMULF {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		v.reset(OpThumbNMULF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNMULD_0(v *Value) bool {
+	// match: (NMULD (NEGD x) y)
+	// cond:
+	// result: (MULD x y)
+	for {
+		y := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbNEGD {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMULD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (NMULD y (NEGD x))
+	// cond:
+	// result: (MULD x y)
+	for {
+		_ = v.Args[1]
+		y := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbNEGD {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbMULD)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNMULF_0(v *Value) bool {
+	// match: (NMULF (NEGF x) y)
+	// cond:
+	// result: (MULF x y)
+	for {
+		y := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbNEGF {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbMULF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (NMULF y (NEGF x))
+	// cond:
+	// result: (MULF x y)
+	for {
+		_ = v.Args[1]
+		y := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbNEGF {
+			break
+		}
+		x := v_1.Args[0]
+		v.reset(OpThumbMULF)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbNotEqual_0(v *Value) bool {
+	// match: (NotEqual (FlagEQ))
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (NotEqual (FlagLT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (NotEqual (FlagLT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (NotEqual (FlagGT_ULT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (NotEqual (FlagGT_UGT))
+	// cond:
+	// result: (MOVWconst [1])
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 1
+		return true
+	}
+	// match: (NotEqual (InvertFlags x))
+	// cond:
+	// result: (NotEqual x)
+	for {
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbInvertFlags {
+			break
+		}
+		x := v_0.Args[0]
+		v.reset(OpThumbNotEqual)
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbOR_0(v *Value) bool {
+	// match: (OR x (MOVWconst [c]))
+	// cond:
+	// result: (ORconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbORconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (OR (MOVWconst [c]) x)
+	// cond:
+	// result: (ORconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbORconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (OR x (SLLconst [c] y))
+	// cond:
+	// result: (ORshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbORshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (OR (SLLconst [c] y) x)
+	// cond:
+	// result: (ORshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbORshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (OR x (SRLconst [c] y))
+	// cond:
+	// result: (ORshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbORshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (OR (SRLconst [c] y) x)
+	// cond:
+	// result: (ORshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbORshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (OR x (SRAconst [c] y))
+	// cond:
+	// result: (ORshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbORshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (OR (SRAconst [c] y) x)
+	// cond:
+	// result: (ORshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbORshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (OR x x)
+	// cond:
+	// result: x
+	for {
+		x := v.Args[1]
+		if x != v.Args[0] {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORconst_0(v *Value) bool {
+	// match: (ORconst [0] x)
+	// cond:
+	// result: x
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORconst [c] _)
+	// cond: int32(c)==-1
+	// result: (MOVWconst [-1])
+	for {
+		c := v.AuxInt
+		if !(int32(c) == -1) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = -1
+		return true
+	}
+	// match: (ORconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [c|d])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c | d
+		return true
+	}
+	// match: (ORconst [c] (ORconst [d] x))
+	// cond:
+	// result: (ORconst [c|d] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbORconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbORconst)
+		v.AuxInt = c | d
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORshiftLL_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (ORshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ORconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbORconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ORshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ORconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbORconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftLL [c] (SRLconst x [32-c]) x)
+	// cond:
+	// result: (SRRconst [32-c] x)
+	for {
+		c := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		if v_0.AuxInt != 32-c {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = 32 - c
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftLL <typ.UInt16> [8] (BFXU <typ.UInt16> [armBFAuxInt(8, 8)] x) x)
+	// cond:
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 {
+			break
+		}
+		if v.AuxInt != 8 {
+			break
+		}
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbBFXU {
+			break
+		}
+		if v_0.Type != typ.UInt16 {
+			break
+		}
+		if v_0.AuxInt != armBFAuxInt(8, 8) {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftLL <typ.UInt16> [8] (SRLconst <typ.UInt16> [24] (SLLconst [16] x)) x)
+	// cond:
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 {
+			break
+		}
+		if v.AuxInt != 8 {
+			break
+		}
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		if v_0.Type != typ.UInt16 {
+			break
+		}
+		if v_0.AuxInt != 24 {
+			break
+		}
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbSLLconst {
+			break
+		}
+		if v_0_0.AuxInt != 16 {
+			break
+		}
+		if x != v_0_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftLL x y:(SLLconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		y := v.Args[1]
+		if y.Op != OpThumbSLLconst {
+			break
+		}
+		c := y.AuxInt
+		if x != y.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = y.Type
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (ORshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ORconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbORconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ORshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ORconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbORconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftRA x y:(SRAconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		y := v.Args[1]
+		if y.Op != OpThumbSRAconst {
+			break
+		}
+		c := y.AuxInt
+		if x != y.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = y.Type
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbORshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (ORshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (ORconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbORconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (ORshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (ORconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbORconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftRL [c] (SLLconst x [32-c]) x)
+	// cond:
+	// result: (SRRconst [ c] x)
+	for {
+		c := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		if v_0.AuxInt != 32-c {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (ORshiftRL x y:(SRLconst x [c]) [d])
+	// cond: c==d
+	// result: y
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		y := v.Args[1]
+		if y.Op != OpThumbSRLconst {
+			break
+		}
+		c := y.AuxInt
+		if x != y.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpCopy)
+		v.Type = y.Type
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSB_0(v *Value) bool {
+	// match: (RSB (MOVWconst [c]) x)
+	// cond:
+	// result: (SUBconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSB x (MOVWconst [c]))
+	// cond:
+	// result: (RSBconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSB x (SLLconst [c] y))
+	// cond:
+	// result: (RSBshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbRSBshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (RSB (SLLconst [c] y) x)
+	// cond:
+	// result: (SUBshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbSUBshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (RSB x (SRLconst [c] y))
+	// cond:
+	// result: (RSBshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbRSBshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (RSB (SRLconst [c] y) x)
+	// cond:
+	// result: (SUBshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbSUBshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (RSB x (SRAconst [c] y))
+	// cond:
+	// result: (RSBshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbRSBshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (RSB (SRAconst [c] y) x)
+	// cond:
+	// result: (SUBshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbSUBshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (RSB x x)
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		x := v.Args[1]
+		if x != v.Args[0] {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (RSB (MUL x y) a)
+	// cond:
+	// result: (MULS x y a)
+	for {
+		a := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMUL {
+			break
+		}
+		y := v_0.Args[1]
+		x := v_0.Args[0]
+		v.reset(OpThumbMULS)
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBSshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (RSBSshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (SUBSconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBSshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (RSBSconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBSshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (RSBSshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (SUBSconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBSshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (RSBSconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBSshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (RSBSshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (SUBSconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBSshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (RSBSconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBconst_0(v *Value) bool {
+	// match: (RSBconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(c-d))])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(c - d))
+		return true
+	}
+	// match: (RSBconst [c] (RSBconst [d] x))
+	// cond:
+	// result: (ADDconst [int64(int32(c-d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(c - d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBconst [c] (ADDconst [d] x))
+	// cond:
+	// result: (RSBconst [int64(int32(c-d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int64(int32(c - d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBconst [c] (SUBconst [d] x))
+	// cond:
+	// result: (RSBconst [int64(int32(c+d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int64(int32(c + d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (RSBshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (SUBconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (RSBconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBshiftLL x (SLLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (RSBshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (SUBconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (RSBconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBshiftRA x (SRAconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbRSBshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (RSBshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (SUBconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (RSBshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (RSBconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (RSBshiftRL x (SRLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBC_0(v *Value) bool {
+	// match: (SBC x (MOVWconst [c]) flags)
+	// cond:
+	// result: (SBCconst [c] x flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (SBC x (SLLconst [c] y) flags)
+	// cond:
+	// result: (SBCshiftLL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSBCshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (SBC x (SRLconst [c] y) flags)
+	// cond:
+	// result: (SBCshiftRL x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSBCshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (SBC x (SRAconst [c] y) flags)
+	// cond:
+	// result: (SBCshiftRA x y [c] flags)
+	for {
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSBCshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBCconst_0(v *Value) bool {
+	// match: (SBCconst [c] (ADDconst [d] x) flags)
+	// cond:
+	// result: (SBCconst [int64(int32(c-d))] x flags)
+	for {
+		c := v.AuxInt
+		flags := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int64(int32(c - d))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	// match: (SBCconst [c] (SUBconst [d] x) flags)
+	// cond:
+	// result: (SBCconst [int64(int32(c+d))] x flags)
+	for {
+		c := v.AuxInt
+		flags := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int64(int32(c + d))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBCshiftLL_0(v *Value) bool {
+	// match: (SBCshiftLL x (MOVWconst [c]) [d] flags)
+	// cond:
+	// result: (SBCconst x [int64(int32(uint32(c)<<uint64(d)))] flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBCshiftRA_0(v *Value) bool {
+	// match: (SBCshiftRA x (MOVWconst [c]) [d] flags)
+	// cond:
+	// result: (SBCconst x [int64(int32(c)>>uint64(d))] flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSBCshiftRL_0(v *Value) bool {
+	// match: (SBCshiftRL x (MOVWconst [c]) [d] flags)
+	// cond:
+	// result: (SBCconst x [int64(int32(uint32(c)>>uint64(d)))] flags)
+	for {
+		d := v.AuxInt
+		flags := v.Args[2]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSBCconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		v.AddArg(flags)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSLL_0(v *Value) bool {
+	// match: (SLL x (MOVWconst [c]))
+	// cond:
+	// result: (SLLconst x [c&31])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSLLconst)
+		v.AuxInt = c & 31
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSLLconst_0(v *Value) bool {
+	// match: (SLLconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(uint32(d)<<uint64(c)))])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(uint32(d) << uint64(c)))
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRA_0(v *Value) bool {
+	// match: (SRA x (MOVWconst [c]))
+	// cond:
+	// result: (SRAconst x [c&31])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = c & 31
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRAcond_0(v *Value) bool {
+	// match: (SRAcond x _ (FlagEQ))
+	// cond:
+	// result: (SRAconst x [31])
+	for {
+		_ = v.Args[2]
+		x := v.Args[0]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbFlagEQ {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v.AddArg(x)
+		return true
+	}
+	// match: (SRAcond x y (FlagLT_ULT))
+	// cond:
+	// result: (SRA x y)
+	for {
+		_ = v.Args[2]
+		x := v.Args[0]
+		y := v.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbFlagLT_ULT {
+			break
+		}
+		v.reset(OpThumbSRA)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SRAcond x _ (FlagLT_UGT))
+	// cond:
+	// result: (SRAconst x [31])
+	for {
+		_ = v.Args[2]
+		x := v.Args[0]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbFlagLT_UGT {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v.AddArg(x)
+		return true
+	}
+	// match: (SRAcond x y (FlagGT_ULT))
+	// cond:
+	// result: (SRA x y)
+	for {
+		_ = v.Args[2]
+		x := v.Args[0]
+		y := v.Args[1]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbFlagGT_ULT {
+			break
+		}
+		v.reset(OpThumbSRA)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SRAcond x _ (FlagGT_UGT))
+	// cond:
+	// result: (SRAconst x [31])
+	for {
+		_ = v.Args[2]
+		x := v.Args[0]
+		v_2 := v.Args[2]
+		if v_2.Op != OpThumbFlagGT_UGT {
+			break
+		}
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRAconst_0(v *Value) bool {
+	// match: (SRAconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(d)>>uint64(c))])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(d) >> uint64(c))
+		return true
+	}
+	// match: (SRAconst (SLLconst x [c]) [d])
+	// cond: uint64(d)>=uint64(c) && uint64(d)<=31
+	// result: (BFX [(d-c)|(32-d)<<8] x)
+	for {
+		d := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		if !(uint64(d) >= uint64(c) && uint64(d) <= 31) {
+			break
+		}
+		v.reset(OpThumbBFX)
+		v.AuxInt = (d - c) | (32-d)<<8
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRL_0(v *Value) bool {
+	// match: (SRL x (MOVWconst [c]))
+	// cond:
+	// result: (SRLconst x [c&31])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSRLconst)
+		v.AuxInt = c & 31
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSRLconst_0(v *Value) bool {
+	// match: (SRLconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(uint32(d)>>uint64(c)))])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(uint32(d) >> uint64(c)))
+		return true
+	}
+	// match: (SRLconst (SLLconst x [c]) [d])
+	// cond: uint64(d)>=uint64(c) && uint64(d)<=31
+	// result: (BFXU [(d-c)|(32-d)<<8] x)
+	for {
+		d := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		x := v_0.Args[0]
+		if !(uint64(d) >= uint64(c) && uint64(d) <= 31) {
+			break
+		}
+		v.reset(OpThumbBFXU)
+		v.AuxInt = (d - c) | (32-d)<<8
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUB_0(v *Value) bool {
+	// match: (SUB (MOVWconst [c]) x)
+	// cond:
+	// result: (RSBconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUB x (MOVWconst [c]))
+	// cond:
+	// result: (SUBconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUB x (SLLconst [c] y))
+	// cond:
+	// result: (SUBshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUB (SLLconst [c] y) x)
+	// cond:
+	// result: (RSBshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbRSBshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUB x (SRLconst [c] y))
+	// cond:
+	// result: (SUBshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUB (SRLconst [c] y) x)
+	// cond:
+	// result: (RSBshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbRSBshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUB x (SRAconst [c] y))
+	// cond:
+	// result: (SUBshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUB (SRAconst [c] y) x)
+	// cond:
+	// result: (RSBshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbRSBshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUB x x)
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		x := v.Args[1]
+		if x != v.Args[0] {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	// match: (SUB a (MUL x y))
+	// cond:
+	// result: (MULS x y a)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMUL {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		v.reset(OpThumbMULS)
+		v.AddArg(x)
+		v.AddArg(y)
+		v.AddArg(a)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBD_0(v *Value) bool {
+	// match: (SUBD a (MULD x y))
+	// cond: a.Uses == 1
+	// result: (MULSD a x y)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMULD {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULSD)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUBD a (NMULD x y))
+	// cond: a.Uses == 1
+	// result: (MULAD a x y)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbNMULD {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULAD)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBF_0(v *Value) bool {
+	// match: (SUBF a (MULF x y))
+	// cond: a.Uses == 1
+	// result: (MULSF a x y)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMULF {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULSF)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUBF a (NMULF x y))
+	// cond: a.Uses == 1
+	// result: (MULAF a x y)
+	for {
+		_ = v.Args[1]
+		a := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbNMULF {
+			break
+		}
+		y := v_1.Args[1]
+		x := v_1.Args[0]
+		if !(a.Uses == 1) {
+			break
+		}
+		v.reset(OpThumbMULAF)
+		v.AddArg(a)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBS_0(v *Value) bool {
+	// match: (SUBS x (MOVWconst [c]))
+	// cond:
+	// result: (SUBSconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBS x (SLLconst [c] y))
+	// cond:
+	// result: (SUBSshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBSshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUBS (SLLconst [c] y) x)
+	// cond:
+	// result: (RSBSshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbRSBSshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUBS x (SRLconst [c] y))
+	// cond:
+	// result: (SUBSshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBSshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUBS (SRLconst [c] y) x)
+	// cond:
+	// result: (RSBSshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbRSBSshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUBS x (SRAconst [c] y))
+	// cond:
+	// result: (SUBSshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbSUBSshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (SUBS (SRAconst [c] y) x)
+	// cond:
+	// result: (RSBSshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbRSBSshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBSshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (SUBSshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (RSBSconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBSshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (SUBSconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBSshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (SUBSshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (RSBSconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBSshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (SUBSconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBSshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (SUBSshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (RSBSconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbRSBSconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBSshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (SUBSconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSUBSconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBconst_0(v *Value) bool {
+	// match: (SUBconst [off1] (MOVWaddr [off2] {sym} ptr))
+	// cond:
+	// result: (MOVWaddr [off2-off1] {sym} ptr)
+	for {
+		off1 := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym := v_0.Aux
+		ptr := v_0.Args[0]
+		v.reset(OpThumbMOVWaddr)
+		v.AuxInt = off2 - off1
+		v.Aux = sym
+		v.AddArg(ptr)
+		return true
+	}
+	// match: (SUBconst [0] x)
+	// cond:
+	// result: x
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [int64(int32(d-c))])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = int64(int32(d - c))
+		return true
+	}
+	// match: (SUBconst [c] (SUBconst [d] x))
+	// cond:
+	// result: (ADDconst [int64(int32(-c-d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(-c - d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBconst [c] (ADDconst [d] x))
+	// cond:
+	// result: (ADDconst [int64(int32(-c+d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbADDconst)
+		v.AuxInt = int64(int32(-c + d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBconst [c] (RSBconst [d] x))
+	// cond:
+	// result: (RSBconst [int64(int32(-c+d))] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbRSBconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = int64(int32(-c + d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (SUBshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (RSBconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (SUBconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBshiftLL x (SLLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (SUBshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (RSBconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (SUBconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBshiftRA x (SRAconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbSUBshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (SUBshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (RSBconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbRSBconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (SUBshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (SUBconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbSUBconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (SUBshiftRL x (SRLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce16_0(v *Value) bool {
+	// match: (StoreOnce16 [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// cond:
+	// result: (StoreOnce16 [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce16 [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// cond:
+	// result: (StoreOnce16 [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce16 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (StoreOnce16 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce16 [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (StoreOnce16idx ptr idx val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbStoreOnce16idx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce16 [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (StoreOnce16shiftLL ptr idx [c] val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce16shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce16idx_0(v *Value) bool {
+	// match: (StoreOnce16idx ptr (MOVWconst [c]) val mem)
+	// cond:
+	// result: (StoreOnce16 [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		val := v.Args[2]
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce16idx (MOVWconst [c]) ptr val mem)
+	// cond:
+	// result: (StoreOnce16 [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		val := v.Args[2]
+		v.reset(OpThumbStoreOnce16)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce16idx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce16shiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce16shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce16idx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (StoreOnce16shiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce16shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce32_0(v *Value) bool {
+	// match: (StoreOnce32 [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// cond:
+	// result: (StoreOnce32 [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce32 [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// cond:
+	// result: (StoreOnce32 [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce32 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (StoreOnce32 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce32 [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (StoreOnce32idx ptr idx val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbStoreOnce32idx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce32 [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (StoreOnce32shiftLL ptr idx [c] val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce32shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce32idx_0(v *Value) bool {
+	// match: (StoreOnce32idx ptr (MOVWconst [c]) val mem)
+	// cond:
+	// result: (StoreOnce32 [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		val := v.Args[2]
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce32idx (MOVWconst [c]) ptr val mem)
+	// cond:
+	// result: (StoreOnce32 [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		val := v.Args[2]
+		v.reset(OpThumbStoreOnce32)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce32idx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce32shiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce32shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce32idx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (StoreOnce32shiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce32shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce8_0(v *Value) bool {
+	// match: (StoreOnce8 [off1] {sym} (ADDconst [off2] ptr) val mem)
+	// cond:
+	// result: (StoreOnce8 [off1+off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = off1 + off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce8 [off1] {sym} (SUBconst [off2] ptr) val mem)
+	// cond:
+	// result: (StoreOnce8 [off1-off2] {sym} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSUBconst {
+			break
+		}
+		off2 := v_0.AuxInt
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = off1 - off2
+		v.Aux = sym
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce8 [off1] {sym1} (MOVWaddr [off2] {sym2} ptr) val mem)
+	// cond: canMergeSym(sym1,sym2)
+	// result: (StoreOnce8 [off1+off2] {mergeSym(sym1,sym2)} ptr val mem)
+	for {
+		off1 := v.AuxInt
+		sym1 := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWaddr {
+			break
+		}
+		off2 := v_0.AuxInt
+		sym2 := v_0.Aux
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(canMergeSym(sym1, sym2)) {
+			break
+		}
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = off1 + off2
+		v.Aux = mergeSym(sym1, sym2)
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce8 [0] {sym} (ADD ptr idx) val mem)
+	// cond: sym == nil
+	// result: (StoreOnce8idx ptr idx val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADD {
+			break
+		}
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil) {
+			break
+		}
+		v.reset(OpThumbStoreOnce8idx)
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce8 [0] {sym} (ADDshiftLL ptr idx [c]) val mem)
+	// cond: sym == nil && c <= 3
+	// result: (StoreOnce8shiftLL ptr idx [c] val mem)
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		sym := v.Aux
+		mem := v.Args[2]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbADDshiftLL {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[1]
+		ptr := v_0.Args[0]
+		val := v.Args[1]
+		if !(sym == nil && c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce8shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbStoreOnce8idx_0(v *Value) bool {
+	// match: (StoreOnce8idx ptr (MOVWconst [c]) val mem)
+	// cond:
+	// result: (StoreOnce8 [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		val := v.Args[2]
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce8idx (MOVWconst [c]) ptr val mem)
+	// cond:
+	// result: (StoreOnce8 [c] ptr val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		ptr := v.Args[1]
+		val := v.Args[2]
+		v.reset(OpThumbStoreOnce8)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce8idx ptr (SLLconst idx [c]) val mem)
+	// cond: c <= 3
+	// result: (StoreOnce8shiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		ptr := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		idx := v_1.Args[0]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce8shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (StoreOnce8idx (SLLconst idx [c]) ptr val mem)
+	// cond: c <= 3
+	// result: (StoreOnce8shiftLL ptr idx [c] val mem)
+	for {
+		mem := v.Args[3]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		idx := v_0.Args[0]
+		ptr := v.Args[1]
+		val := v.Args[2]
+		if !(c <= 3) {
+			break
+		}
+		v.reset(OpThumbStoreOnce8shiftLL)
+		v.AuxInt = c
+		v.AddArg(ptr)
+		v.AddArg(idx)
+		v.AddArg(val)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQ_0(v *Value) bool {
+	// match: (TEQ x (MOVWconst [c]))
+	// cond:
+	// result: (TEQconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (TEQ (MOVWconst [c]) x)
+	// cond:
+	// result: (TEQconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (TEQ x (SLLconst [c] y))
+	// cond:
+	// result: (TEQshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbTEQshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TEQ (SLLconst [c] y) x)
+	// cond:
+	// result: (TEQshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbTEQshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TEQ x (SRLconst [c] y))
+	// cond:
+	// result: (TEQshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbTEQshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TEQ (SRLconst [c] y) x)
+	// cond:
+	// result: (TEQshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbTEQshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TEQ x (SRAconst [c] y))
+	// cond:
+	// result: (TEQshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbTEQshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TEQ (SRAconst [c] y) x)
+	// cond:
+	// result: (TEQshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbTEQshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQconst_0(v *Value) bool {
+	// match: (TEQconst (MOVWconst [x]) [y])
+	// cond: int32(x^y)==0
+	// result: (FlagEQ)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x^y) == 0) {
+			break
+		}
+		v.reset(OpThumbFlagEQ)
+		return true
+	}
+	// match: (TEQconst (MOVWconst [x]) [y])
+	// cond: int32(x^y)<0
+	// result: (FlagLT_UGT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x^y) < 0) {
+			break
+		}
+		v.reset(OpThumbFlagLT_UGT)
+		return true
+	}
+	// match: (TEQconst (MOVWconst [x]) [y])
+	// cond: int32(x^y)>0
+	// result: (FlagGT_UGT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x^y) > 0) {
+			break
+		}
+		v.reset(OpThumbFlagGT_UGT)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (TEQshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (TEQconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TEQshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (TEQconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (TEQshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (TEQconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TEQshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (TEQconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTEQshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (TEQshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (TEQconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TEQshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (TEQconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbTEQconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTST_0(v *Value) bool {
+	// match: (TST x (MOVWconst [c]))
+	// cond:
+	// result: (TSTconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (TST (MOVWconst [c]) x)
+	// cond:
+	// result: (TSTconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (TST x (SLLconst [c] y))
+	// cond:
+	// result: (TSTshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbTSTshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TST (SLLconst [c] y) x)
+	// cond:
+	// result: (TSTshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbTSTshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TST x (SRLconst [c] y))
+	// cond:
+	// result: (TSTshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbTSTshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TST (SRLconst [c] y) x)
+	// cond:
+	// result: (TSTshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbTSTshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TST x (SRAconst [c] y))
+	// cond:
+	// result: (TSTshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbTSTshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (TST (SRAconst [c] y) x)
+	// cond:
+	// result: (TSTshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbTSTshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTSTconst_0(v *Value) bool {
+	// match: (TSTconst (MOVWconst [x]) [y])
+	// cond: int32(x&y)==0
+	// result: (FlagEQ)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x&y) == 0) {
+			break
+		}
+		v.reset(OpThumbFlagEQ)
+		return true
+	}
+	// match: (TSTconst (MOVWconst [x]) [y])
+	// cond: int32(x&y)<0
+	// result: (FlagLT_UGT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x&y) < 0) {
+			break
+		}
+		v.reset(OpThumbFlagLT_UGT)
+		return true
+	}
+	// match: (TSTconst (MOVWconst [x]) [y])
+	// cond: int32(x&y)>0
+	// result: (FlagGT_UGT)
+	for {
+		y := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		x := v_0.AuxInt
+		if !(int32(x&y) > 0) {
+			break
+		}
+		v.reset(OpThumbFlagGT_UGT)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTSTshiftLL_0(v *Value) bool {
+	b := v.Block
+	// match: (TSTshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (TSTconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TSTshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (TSTconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTSTshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (TSTshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (TSTconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TSTshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (TSTconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbTSTshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (TSTshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (TSTconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (TSTshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (TSTconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbTSTconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXOR_0(v *Value) bool {
+	// match: (XOR x (MOVWconst [c]))
+	// cond:
+	// result: (XORconst [c] x)
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (XOR (MOVWconst [c]) x)
+	// cond:
+	// result: (XORconst [c] x)
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (XOR x (SLLconst [c] y))
+	// cond:
+	// result: (XORshiftLL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbXORshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (XOR (SLLconst [c] y) x)
+	// cond:
+	// result: (XORshiftLL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbXORshiftLL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (XOR x (SRLconst [c] y))
+	// cond:
+	// result: (XORshiftRL x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbXORshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (XOR (SRLconst [c] y) x)
+	// cond:
+	// result: (XORshiftRL x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbXORshiftRL)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (XOR x (SRAconst [c] y))
+	// cond:
+	// result: (XORshiftRA x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbXORshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (XOR (SRAconst [c] y) x)
+	// cond:
+	// result: (XORshiftRA x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbXORshiftRA)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (XOR x (SRRconst [c] y))
+	// cond:
+	// result: (XORshiftRR x y [c])
+	for {
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRRconst {
+			break
+		}
+		c := v_1.AuxInt
+		y := v_1.Args[0]
+		v.reset(OpThumbXORshiftRR)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	// match: (XOR (SRRconst [c] y) x)
+	// cond:
+	// result: (XORshiftRR x y [c])
+	for {
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRRconst {
+			break
+		}
+		c := v_0.AuxInt
+		y := v_0.Args[0]
+		v.reset(OpThumbXORshiftRR)
+		v.AuxInt = c
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXOR_10(v *Value) bool {
+	// match: (XOR x x)
+	// cond:
+	// result: (MOVWconst [0])
+	for {
+		x := v.Args[1]
+		if x != v.Args[0] {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORconst_0(v *Value) bool {
+	// match: (XORconst [0] x)
+	// cond:
+	// result: x
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORconst [c] (MOVWconst [d]))
+	// cond:
+	// result: (MOVWconst [c^d])
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		d := v_0.AuxInt
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = c ^ d
+		return true
+	}
+	// match: (XORconst [c] (XORconst [d] x))
+	// cond:
+	// result: (XORconst [c^d] x)
+	for {
+		c := v.AuxInt
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbXORconst {
+			break
+		}
+		d := v_0.AuxInt
+		x := v_0.Args[0]
+		v.reset(OpThumbXORconst)
+		v.AuxInt = c ^ d
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORshiftLL_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (XORshiftLL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (XORconst [c] (SLLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSLLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (XORshiftLL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (XORconst x [int64(int32(uint32(c)<<uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int64(int32(uint32(c) << uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftLL [c] (SRLconst x [32-c]) x)
+	// cond:
+	// result: (SRRconst [32-c] x)
+	for {
+		c := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		if v_0.AuxInt != 32-c {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = 32 - c
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftLL <typ.UInt16> [8] (BFXU <typ.UInt16> [armBFAuxInt(8, 8)] x) x)
+	// cond:
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 {
+			break
+		}
+		if v.AuxInt != 8 {
+			break
+		}
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbBFXU {
+			break
+		}
+		if v_0.Type != typ.UInt16 {
+			break
+		}
+		if v_0.AuxInt != armBFAuxInt(8, 8) {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftLL <typ.UInt16> [8] (SRLconst <typ.UInt16> [24] (SLLconst [16] x)) x)
+	// cond:
+	// result: (REV16 x)
+	for {
+		if v.Type != typ.UInt16 {
+			break
+		}
+		if v.AuxInt != 8 {
+			break
+		}
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSRLconst {
+			break
+		}
+		if v_0.Type != typ.UInt16 {
+			break
+		}
+		if v_0.AuxInt != 24 {
+			break
+		}
+		v_0_0 := v_0.Args[0]
+		if v_0_0.Op != OpThumbSLLconst {
+			break
+		}
+		if v_0_0.AuxInt != 16 {
+			break
+		}
+		if x != v_0_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbREV16)
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftLL x (SLLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSLLconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORshiftRA_0(v *Value) bool {
+	b := v.Block
+	// match: (XORshiftRA (MOVWconst [c]) x [d])
+	// cond:
+	// result: (XORconst [c] (SRAconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRAconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (XORshiftRA x (MOVWconst [c]) [d])
+	// cond:
+	// result: (XORconst x [int64(int32(c)>>uint64(d))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int64(int32(c) >> uint64(d))
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftRA x (SRAconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRAconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORshiftRL_0(v *Value) bool {
+	b := v.Block
+	// match: (XORshiftRL (MOVWconst [c]) x [d])
+	// cond:
+	// result: (XORconst [c] (SRLconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRLconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (XORshiftRL x (MOVWconst [c]) [d])
+	// cond:
+	// result: (XORconst x [int64(int32(uint32(c)>>uint64(d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int64(int32(uint32(c) >> uint64(d)))
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftRL [c] (SLLconst x [32-c]) x)
+	// cond:
+	// result: (SRRconst [ c] x)
+	for {
+		c := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbSLLconst {
+			break
+		}
+		if v_0.AuxInt != 32-c {
+			break
+		}
+		if x != v_0.Args[0] {
+			break
+		}
+		v.reset(OpThumbSRRconst)
+		v.AuxInt = c
+		v.AddArg(x)
+		return true
+	}
+	// match: (XORshiftRL x (SRLconst x [c]) [d])
+	// cond: c==d
+	// result: (MOVWconst [0])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbSRLconst {
+			break
+		}
+		c := v_1.AuxInt
+		if x != v_1.Args[0] {
+			break
+		}
+		if !(c == d) {
+			break
+		}
+		v.reset(OpThumbMOVWconst)
+		v.AuxInt = 0
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpThumbXORshiftRR_0(v *Value) bool {
+	b := v.Block
+	// match: (XORshiftRR (MOVWconst [c]) x [d])
+	// cond:
+	// result: (XORconst [c] (SRRconst <x.Type> x [d]))
+	for {
+		d := v.AuxInt
+		x := v.Args[1]
+		v_0 := v.Args[0]
+		if v_0.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_0.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = c
+		v0 := b.NewValue0(v.Pos, OpThumbSRRconst, x.Type)
+		v0.AuxInt = d
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+	// match: (XORshiftRR x (MOVWconst [c]) [d])
+	// cond:
+	// result: (XORconst x [int64(int32(uint32(c)>>uint64(d)|uint32(c)<<uint64(32-d)))])
+	for {
+		d := v.AuxInt
+		_ = v.Args[1]
+		x := v.Args[0]
+		v_1 := v.Args[1]
+		if v_1.Op != OpThumbMOVWconst {
+			break
+		}
+		c := v_1.AuxInt
+		v.reset(OpThumbXORconst)
+		v.AuxInt = int64(int32(uint32(c)>>uint64(d) | uint32(c)<<uint64(32-d)))
+		v.AddArg(x)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpTrunc16to8_0(v *Value) bool {
+	// match: (Trunc16to8 x)
+	// cond:
+	// result: x
+	for {
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpTrunc32to16_0(v *Value) bool {
+	// match: (Trunc32to16 x)
+	// cond:
+	// result: x
+	for {
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpTrunc32to8_0(v *Value) bool {
+	// match: (Trunc32to8 x)
+	// cond:
+	// result: x
+	for {
+		x := v.Args[0]
+		v.reset(OpCopy)
+		v.Type = x.Type
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpWB_0(v *Value) bool {
+	// match: (WB {fn} destptr srcptr mem)
+	// cond:
+	// result: (LoweredWB {fn} destptr srcptr mem)
+	for {
+		fn := v.Aux
+		mem := v.Args[2]
+		destptr := v.Args[0]
+		srcptr := v.Args[1]
+		v.reset(OpThumbLoweredWB)
+		v.Aux = fn
+		v.AddArg(destptr)
+		v.AddArg(srcptr)
+		v.AddArg(mem)
+		return true
+	}
+}
+func rewriteValueThumb_OpXor16_0(v *Value) bool {
+	// match: (Xor16 x y)
+	// cond:
+	// result: (XOR x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbXOR)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpXor32_0(v *Value) bool {
+	// match: (Xor32 x y)
+	// cond:
+	// result: (XOR x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbXOR)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpXor8_0(v *Value) bool {
+	// match: (Xor8 x y)
+	// cond:
+	// result: (XOR x y)
+	for {
+		y := v.Args[1]
+		x := v.Args[0]
+		v.reset(OpThumbXOR)
+		v.AddArg(x)
+		v.AddArg(y)
+		return true
+	}
+}
+func rewriteValueThumb_OpZero_0(v *Value) bool {
+	b := v.Block
+	config := b.Func.Config
+	typ := &b.Func.Config.Types
+	// match: (Zero [0] _ mem)
+	// cond:
+	// result: mem
+	for {
+		if v.AuxInt != 0 {
+			break
+		}
+		mem := v.Args[1]
+		v.reset(OpCopy)
+		v.Type = mem.Type
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Zero [1] ptr mem)
+	// cond:
+	// result: (MOVBstore ptr (MOVWconst [0]) mem)
+	for {
+		if v.AuxInt != 1 {
+			break
+		}
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		v.reset(OpThumbMOVBstore)
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = 0
+		v.AddArg(v0)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Zero [2] {t} ptr mem)
+	// cond: t.(*types.Type).Alignment()%2 == 0
+	// result: (MOVHstore ptr (MOVWconst [0]) mem)
+	for {
+		if v.AuxInt != 2 {
+			break
+		}
+		t := v.Aux
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(t.(*types.Type).Alignment()%2 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = 0
+		v.AddArg(v0)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Zero [2] ptr mem)
+	// cond:
+	// result: (MOVBstore [1] ptr (MOVWconst [0]) (MOVBstore [0] ptr (MOVWconst [0]) mem))
+	for {
+		if v.AuxInt != 2 {
+			break
+		}
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = 1
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = 0
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = 0
+		v1.AddArg(ptr)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v2.AuxInt = 0
+		v1.AddArg(v2)
+		v1.AddArg(mem)
+		v.AddArg(v1)
+		return true
+	}
+	// match: (Zero [4] {t} ptr mem)
+	// cond: t.(*types.Type).Alignment()%4 == 0
+	// result: (MOVWstore ptr (MOVWconst [0]) mem)
+	for {
+		if v.AuxInt != 4 {
+			break
+		}
+		t := v.Aux
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(t.(*types.Type).Alignment()%4 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVWstore)
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = 0
+		v.AddArg(v0)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Zero [4] {t} ptr mem)
+	// cond: t.(*types.Type).Alignment()%2 == 0
+	// result: (MOVHstore [2] ptr (MOVWconst [0]) (MOVHstore [0] ptr (MOVWconst [0]) mem))
+	for {
+		if v.AuxInt != 4 {
+			break
+		}
+		t := v.Aux
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(t.(*types.Type).Alignment()%2 == 0) {
+			break
+		}
+		v.reset(OpThumbMOVHstore)
+		v.AuxInt = 2
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = 0
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVHstore, types.TypeMem)
+		v1.AuxInt = 0
+		v1.AddArg(ptr)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v2.AuxInt = 0
+		v1.AddArg(v2)
+		v1.AddArg(mem)
+		v.AddArg(v1)
+		return true
+	}
+	// match: (Zero [4] ptr mem)
+	// cond:
+	// result: (MOVBstore [3] ptr (MOVWconst [0]) (MOVBstore [2] ptr (MOVWconst [0]) (MOVBstore [1] ptr (MOVWconst [0]) (MOVBstore [0] ptr (MOVWconst [0]) mem))))
+	for {
+		if v.AuxInt != 4 {
+			break
+		}
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = 3
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = 0
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = 2
+		v1.AddArg(ptr)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v2.AuxInt = 0
+		v1.AddArg(v2)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v3.AuxInt = 1
+		v3.AddArg(ptr)
+		v4 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v4.AuxInt = 0
+		v3.AddArg(v4)
+		v5 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v5.AuxInt = 0
+		v5.AddArg(ptr)
+		v6 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v6.AuxInt = 0
+		v5.AddArg(v6)
+		v5.AddArg(mem)
+		v3.AddArg(v5)
+		v1.AddArg(v3)
+		v.AddArg(v1)
+		return true
+	}
+	// match: (Zero [3] ptr mem)
+	// cond:
+	// result: (MOVBstore [2] ptr (MOVWconst [0]) (MOVBstore [1] ptr (MOVWconst [0]) (MOVBstore [0] ptr (MOVWconst [0]) mem)))
+	for {
+		if v.AuxInt != 3 {
+			break
+		}
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		v.reset(OpThumbMOVBstore)
+		v.AuxInt = 2
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = 0
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v1.AuxInt = 1
+		v1.AddArg(ptr)
+		v2 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v2.AuxInt = 0
+		v1.AddArg(v2)
+		v3 := b.NewValue0(v.Pos, OpThumbMOVBstore, types.TypeMem)
+		v3.AuxInt = 0
+		v3.AddArg(ptr)
+		v4 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v4.AuxInt = 0
+		v3.AddArg(v4)
+		v3.AddArg(mem)
+		v1.AddArg(v3)
+		v.AddArg(v1)
+		return true
+	}
+	// match: (Zero [s] {t} ptr mem)
+	// cond: s%4 == 0 && s > 4 && s <= 512 && t.(*types.Type).Alignment()%4 == 0 && !config.noDuffDevice
+	// result: (DUFFZERO [4 * (128 - s/4)] ptr (MOVWconst [0]) mem)
+	for {
+		s := v.AuxInt
+		t := v.Aux
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !(s%4 == 0 && s > 4 && s <= 512 && t.(*types.Type).Alignment()%4 == 0 && !config.noDuffDevice) {
+			break
+		}
+		v.reset(OpThumbDUFFZERO)
+		v.AuxInt = 4 * (128 - s/4)
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v0.AuxInt = 0
+		v.AddArg(v0)
+		v.AddArg(mem)
+		return true
+	}
+	// match: (Zero [s] {t} ptr mem)
+	// cond: (s > 512 || config.noDuffDevice) || t.(*types.Type).Alignment()%4 != 0
+	// result: (LoweredZero [t.(*types.Type).Alignment()] ptr (ADDconst <ptr.Type> ptr [s-moveSize(t.(*types.Type).Alignment(), config)]) (MOVWconst [0]) mem)
+	for {
+		s := v.AuxInt
+		t := v.Aux
+		mem := v.Args[1]
+		ptr := v.Args[0]
+		if !((s > 512 || config.noDuffDevice) || t.(*types.Type).Alignment()%4 != 0) {
+			break
+		}
+		v.reset(OpThumbLoweredZero)
+		v.AuxInt = t.(*types.Type).Alignment()
+		v.AddArg(ptr)
+		v0 := b.NewValue0(v.Pos, OpThumbADDconst, ptr.Type)
+		v0.AuxInt = s - moveSize(t.(*types.Type).Alignment(), config)
+		v0.AddArg(ptr)
+		v.AddArg(v0)
+		v1 := b.NewValue0(v.Pos, OpThumbMOVWconst, typ.UInt32)
+		v1.AuxInt = 0
+		v.AddArg(v1)
+		v.AddArg(mem)
+		return true
+	}
+	return false
+}
+func rewriteValueThumb_OpZeroExt16to32_0(v *Value) bool {
+	// match: (ZeroExt16to32 x)
+	// cond:
+	// result: (MOVHUreg x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVHUreg)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpZeroExt8to16_0(v *Value) bool {
+	// match: (ZeroExt8to16 x)
+	// cond:
+	// result: (MOVBUreg x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpZeroExt8to32_0(v *Value) bool {
+	// match: (ZeroExt8to32 x)
+	// cond:
+	// result: (MOVBUreg x)
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbMOVBUreg)
+		v.AddArg(x)
+		return true
+	}
+}
+func rewriteValueThumb_OpZeromask_0(v *Value) bool {
+	b := v.Block
+	typ := &b.Func.Config.Types
+	// match: (Zeromask x)
+	// cond:
+	// result: (SRAconst (RSBshiftRL <typ.Int32> x x [1]) [31])
+	for {
+		x := v.Args[0]
+		v.reset(OpThumbSRAconst)
+		v.AuxInt = 31
+		v0 := b.NewValue0(v.Pos, OpThumbRSBshiftRL, typ.Int32)
+		v0.AuxInt = 1
+		v0.AddArg(x)
+		v0.AddArg(x)
+		v.AddArg(v0)
+		return true
+	}
+}
+func rewriteBlockThumb(b *Block) bool {
+	config := b.Func.Config
+	typ := &config.Types
+	_ = typ
+	v := b.Control
+	_ = v
+	switch b.Kind {
+	case BlockThumbEQ:
+		// match: (EQ (FlagEQ) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (EQ (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (EQ (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (EQ (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (EQ (InvertFlags cmp) yes no)
+		// cond:
+		// result: (EQ cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbEQ
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMP x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMP a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMPconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMPshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMPshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMPshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMN x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMN a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMNconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMNshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMNshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (CMNshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TST x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTST, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TSTconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TSTshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TSTshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TSTshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQ x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTEQ, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (EQ (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (EQ (TEQshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbEQ
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbGE:
+		// match: (GE (FlagEQ) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GE (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GE (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (InvertFlags cmp) yes no)
+		// cond:
+		// result: (LE cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbLE
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMP x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMP a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMPconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMPshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMPshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMPshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMN x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMN a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMNconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMNshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMNshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (CMNshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TST x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTST, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TSTconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TSTshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TSTshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TSTshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TEQ x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQ, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TEQconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TEQshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TEQshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GE (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GE (TEQshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbGT:
+		// match: (GT (FlagEQ) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GT (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GT (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (GT (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (InvertFlags cmp) yes no)
+		// cond:
+		// result: (LT cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbLT
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMP x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMP a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMPconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMPshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMPshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMPshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMN x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMNconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMNshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMNshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMNshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TST x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTST, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (CMN a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TSTconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TSTshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TSTshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TSTshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TEQ x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQ, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TEQconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TEQshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TEQshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (GT (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (GT (TEQshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbGT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+	case BlockIf:
+		// match: (If (Equal cc) yes no)
+		// cond:
+		// result: (EQ cc yes no)
+		for v.Op == OpThumbEqual {
+			cc := v.Args[0]
+			b.Kind = BlockThumbEQ
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (NotEqual cc) yes no)
+		// cond:
+		// result: (NE cc yes no)
+		for v.Op == OpThumbNotEqual {
+			cc := v.Args[0]
+			b.Kind = BlockThumbNE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (LessThan cc) yes no)
+		// cond:
+		// result: (LT cc yes no)
+		for v.Op == OpThumbLessThan {
+			cc := v.Args[0]
+			b.Kind = BlockThumbLT
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (LessThanU cc) yes no)
+		// cond:
+		// result: (ULT cc yes no)
+		for v.Op == OpThumbLessThanU {
+			cc := v.Args[0]
+			b.Kind = BlockThumbULT
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (LessEqual cc) yes no)
+		// cond:
+		// result: (LE cc yes no)
+		for v.Op == OpThumbLessEqual {
+			cc := v.Args[0]
+			b.Kind = BlockThumbLE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (LessEqualU cc) yes no)
+		// cond:
+		// result: (ULE cc yes no)
+		for v.Op == OpThumbLessEqualU {
+			cc := v.Args[0]
+			b.Kind = BlockThumbULE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (GreaterThan cc) yes no)
+		// cond:
+		// result: (GT cc yes no)
+		for v.Op == OpThumbGreaterThan {
+			cc := v.Args[0]
+			b.Kind = BlockThumbGT
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (GreaterThanU cc) yes no)
+		// cond:
+		// result: (UGT cc yes no)
+		for v.Op == OpThumbGreaterThanU {
+			cc := v.Args[0]
+			b.Kind = BlockThumbUGT
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (GreaterEqual cc) yes no)
+		// cond:
+		// result: (GE cc yes no)
+		for v.Op == OpThumbGreaterEqual {
+			cc := v.Args[0]
+			b.Kind = BlockThumbGE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If (GreaterEqualU cc) yes no)
+		// cond:
+		// result: (UGE cc yes no)
+		for v.Op == OpThumbGreaterEqualU {
+			cc := v.Args[0]
+			b.Kind = BlockThumbUGE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (If cond yes no)
+		// cond:
+		// result: (NE (CMPconst [0] cond) yes no)
+		for {
+			cond := b.Control
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = 0
+			v0.AddArg(cond)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbLE:
+		// match: (LE (FlagEQ) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LE (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LE (InvertFlags cmp) yes no)
+		// cond:
+		// result: (GE cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbGE
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMP x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMP a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMPconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMPshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMPshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMPshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMN x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMN a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMNconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMNshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMNshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (CMNshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TST x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTST, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TSTconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TSTshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TSTshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TSTshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TEQ x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQ, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TEQconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TEQshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TEQshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LE (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LE (TEQshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbLT:
+		// match: (LT (FlagEQ) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LT (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LT (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (LT (InvertFlags cmp) yes no)
+		// cond:
+		// result: (GT cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbGT
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMP x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMP a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMPconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMPshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMPshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMPshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMN x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMN a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMNconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMNshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMNshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (CMNshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TST x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTST, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TSTconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TSTshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TSTshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TSTshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TEQ x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQ, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TEQconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TEQshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TEQshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (LT (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (LT (TEQshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbLT
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbNE:
+		// match: (NE (CMPconst [0] (Equal cc)) yes no)
+		// cond:
+		// result: (EQ cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbEqual {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbEQ
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (NotEqual cc)) yes no)
+		// cond:
+		// result: (NE cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbNotEqual {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbNE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (LessThan cc)) yes no)
+		// cond:
+		// result: (LT cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbLessThan {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbLT
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (LessThanU cc)) yes no)
+		// cond:
+		// result: (ULT cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbLessThanU {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbULT
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (LessEqual cc)) yes no)
+		// cond:
+		// result: (LE cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbLessEqual {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbLE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (LessEqualU cc)) yes no)
+		// cond:
+		// result: (ULE cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbLessEqualU {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbULE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (GreaterThan cc)) yes no)
+		// cond:
+		// result: (GT cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbGreaterThan {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbGT
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (GreaterThanU cc)) yes no)
+		// cond:
+		// result: (UGT cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbGreaterThanU {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbUGT
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (GreaterEqual cc)) yes no)
+		// cond:
+		// result: (GE cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbGreaterEqual {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbGE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] (GreaterEqualU cc)) yes no)
+		// cond:
+		// result: (UGE cc yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			v_0 := v.Args[0]
+			if v_0.Op != OpThumbGreaterEqualU {
+				break
+			}
+			cc := v_0.Args[0]
+			b.Kind = BlockThumbUGE
+			b.SetControl(cc)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (FlagEQ) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (NE (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (InvertFlags cmp) yes no)
+		// cond:
+		// result: (NE cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbNE
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUB x y)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMP x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUB {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(MULS x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMP a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULS {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMP, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUBconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMPconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUBshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMPshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUBshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMPshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(SUBshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMPshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbSUBshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMPshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADD x y)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMN x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADD {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(MULA x y a)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMN a (MUL <x.Type> x y)) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbMULA {
+				break
+			}
+			a := l.Args[2]
+			x := l.Args[0]
+			y := l.Args[1]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMN, types.TypeFlags)
+			v0.AddArg(a)
+			v1 := b.NewValue0(v.Pos, OpThumbMUL, x.Type)
+			v1.AddArg(x)
+			v1.AddArg(y)
+			v0.AddArg(v1)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMNconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMNshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMNshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ADDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (CMNshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbADDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbCMNshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(AND x y)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TST x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbAND {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTST, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ANDconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TSTconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ANDshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TSTshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ANDshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TSTshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(ANDshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TSTshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbANDshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTSTshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XOR x y)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQ x y) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXOR {
+				break
+			}
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQ, types.TypeFlags)
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XORconst [c] x)) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQconst [c] x) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORconst {
+				break
+			}
+			c := l.AuxInt
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQconst, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XORshiftLL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQshiftLL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftLL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftLL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XORshiftRL x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQshiftRL x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRL {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRL, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+		// match: (NE (CMPconst [0] l:(XORshiftRA x y [c])) yes no)
+		// cond: l.Uses==1
+		// result: (NE (TEQshiftRA x y [c]) yes no)
+		for v.Op == OpThumbCMPconst {
+			if v.AuxInt != 0 {
+				break
+			}
+			l := v.Args[0]
+			if l.Op != OpThumbXORshiftRA {
+				break
+			}
+			c := l.AuxInt
+			y := l.Args[1]
+			x := l.Args[0]
+			if !(l.Uses == 1) {
+				break
+			}
+			b.Kind = BlockThumbNE
+			v0 := b.NewValue0(v.Pos, OpThumbTEQshiftRA, types.TypeFlags)
+			v0.AuxInt = c
+			v0.AddArg(x)
+			v0.AddArg(y)
+			b.SetControl(v0)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbUGE:
+		// match: (UGE (FlagEQ) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (UGE (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (UGE (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (UGE (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (UGE (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (UGE (InvertFlags cmp) yes no)
+		// cond:
+		// result: (ULE cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbULE
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbUGT:
+		// match: (UGT (FlagEQ) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (UGT (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (UGT (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (UGT (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (UGT (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (UGT (InvertFlags cmp) yes no)
+		// cond:
+		// result: (ULT cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbULT
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbULE:
+		// match: (ULE (FlagEQ) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (ULE (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (ULE (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (ULE (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (ULE (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (ULE (InvertFlags cmp) yes no)
+		// cond:
+		// result: (UGE cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbUGE
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+	case BlockThumbULT:
+		// match: (ULT (FlagEQ) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagEQ {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (ULT (FlagLT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagLT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (ULT (FlagLT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagLT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (ULT (FlagGT_ULT) yes no)
+		// cond:
+		// result: (First nil yes no)
+		for v.Op == OpThumbFlagGT_ULT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			return true
+		}
+		// match: (ULT (FlagGT_UGT) yes no)
+		// cond:
+		// result: (First nil no yes)
+		for v.Op == OpThumbFlagGT_UGT {
+			b.Kind = BlockFirst
+			b.SetControl(nil)
+			b.Aux = nil
+			b.swapSuccessors()
+			return true
+		}
+		// match: (ULT (InvertFlags cmp) yes no)
+		// cond:
+		// result: (UGT cmp yes no)
+		for v.Op == OpThumbInvertFlags {
+			cmp := v.Args[0]
+			b.Kind = BlockThumbUGT
+			b.SetControl(cmp)
+			b.Aux = nil
+			return true
+		}
+	}
+	return false
+}
diff --git a/src/cmd/compile/internal/ssa/schedule.go b/src/cmd/compile/internal/ssa/schedule.go
index ca0e82953e..64ca16705b 100644
--- a/src/cmd/compile/internal/ssa/schedule.go
+++ b/src/cmd/compile/internal/ssa/schedule.go
@@ -64,9 +64,9 @@ func (h ValHeap) Less(i, j int) bool {
 
 func (op Op) isLoweredGetClosurePtr() bool {
 	switch op {
-	case OpAMD64LoweredGetClosurePtr, OpPPC64LoweredGetClosurePtr, OpARMLoweredGetClosurePtr, OpARM64LoweredGetClosurePtr,
-		Op386LoweredGetClosurePtr, OpMIPS64LoweredGetClosurePtr, OpS390XLoweredGetClosurePtr, OpMIPSLoweredGetClosurePtr,
-		OpWasmLoweredGetClosurePtr:
+	case OpAMD64LoweredGetClosurePtr, OpPPC64LoweredGetClosurePtr, OpARMLoweredGetClosurePtr, OpThumbLoweredGetClosurePtr,
+		OpARM64LoweredGetClosurePtr,	Op386LoweredGetClosurePtr, OpMIPS64LoweredGetClosurePtr, OpS390XLoweredGetClosurePtr,
+		OpMIPSLoweredGetClosurePtr, OpWasmLoweredGetClosurePtr:
 		return true
 	}
 	return false
@@ -112,10 +112,10 @@ func schedule(f *Func) {
 				}
 				score[v.ID] = ScorePhi
 			case v.Op == OpAMD64LoweredNilCheck || v.Op == OpPPC64LoweredNilCheck ||
-				v.Op == OpARMLoweredNilCheck || v.Op == OpARM64LoweredNilCheck ||
-				v.Op == Op386LoweredNilCheck || v.Op == OpMIPS64LoweredNilCheck ||
-				v.Op == OpS390XLoweredNilCheck || v.Op == OpMIPSLoweredNilCheck ||
-				v.Op == OpWasmLoweredNilCheck:
+				v.Op == OpARMLoweredNilCheck || v.Op == OpThumbLoweredNilCheck ||
+				v.Op == OpARM64LoweredNilCheck || v.Op == Op386LoweredNilCheck ||
+				v.Op == OpMIPS64LoweredNilCheck || v.Op == OpS390XLoweredNilCheck ||
+				v.Op == OpMIPSLoweredNilCheck || v.Op == OpWasmLoweredNilCheck:
 				// Nil checks must come before loads from the same address.
 				score[v.ID] = ScoreNilCheck
 			case v.Op == OpPhi:
diff --git a/src/cmd/compile/internal/thumb/galign.go b/src/cmd/compile/internal/thumb/galign.go
new file mode 100644
index 0000000000..cf9a8bdea5
--- /dev/null
+++ b/src/cmd/compile/internal/thumb/galign.go
@@ -0,0 +1,27 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumb
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/compile/internal/ssa"
+	"cmd/internal/obj/thumb"
+	"cmd/internal/objabi"
+)
+
+func Init(arch *gc.Arch) {
+	arch.LinkArch = &thumb.Link
+	arch.REGSP = thumb.REGSP
+	arch.MAXWIDTH = (1 << 32) - 1
+	arch.SoftFloat = objabi.GOARM&0xF != 0xD // TODO: handle GOARM==0x7F (32-bit FPU)
+	arch.ZeroRange = zerorange
+	arch.ZeroAuto = zeroAuto
+	arch.Ginsnop = ginsnop      // used as inline mark
+	arch.Ginsnopdefer = ginsnop // for stack trace to show right line number for deffered calls
+
+	arch.SSAMarkMoves = func(s *gc.SSAGenState, b *ssa.Block) {}
+	arch.SSAGenValue = ssaGenValue
+	arch.SSAGenBlock = ssaGenBlock
+}
diff --git a/src/cmd/compile/internal/thumb/ggen.go b/src/cmd/compile/internal/thumb/ggen.go
new file mode 100644
index 0000000000..66317b9c72
--- /dev/null
+++ b/src/cmd/compile/internal/thumb/ggen.go
@@ -0,0 +1,73 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumb
+
+import (
+	"cmd/compile/internal/gc"
+	"cmd/internal/obj"
+	"cmd/internal/obj/thumb"
+)
+
+func zerorange(pp *gc.Progs, p *obj.Prog, off, cnt int64, r0 *uint32) *obj.Prog {
+	if cnt == 0 {
+		return p
+	}
+	if *r0 == 0 {
+		p = pp.Appendpp(p, thumb.AMOVW, obj.TYPE_CONST, 0, 0, obj.TYPE_REG, thumb.REG_R0, 0)
+		*r0 = 1
+	}
+
+	if cnt < int64(4*gc.Widthptr) {
+		for i := int64(0); i < cnt; i += int64(gc.Widthptr) {
+			p = pp.Appendpp(p, thumb.AMOVW, obj.TYPE_REG, thumb.REG_R0, 0, obj.TYPE_MEM, thumb.REGSP, 4+off+i)
+		}
+	} else if cnt <= int64(128*gc.Widthptr) {
+		p = pp.Appendpp(p, thumb.AADD, obj.TYPE_CONST, 0, 4+off, obj.TYPE_REG, thumb.REG_R1, 0)
+		p.Reg = thumb.REGSP
+		p = pp.Appendpp(p, obj.ADUFFZERO, obj.TYPE_NONE, 0, 0, obj.TYPE_MEM, 0, 0)
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.Duffzero
+		p.To.Offset = 4 * (128 - cnt/int64(gc.Widthptr))
+	} else {
+		p = pp.Appendpp(p, thumb.AADD, obj.TYPE_CONST, 0, 4+off, obj.TYPE_REG, thumb.REG_R1, 0)
+		p.Reg = thumb.REGSP
+		p = pp.Appendpp(p, thumb.AADD, obj.TYPE_CONST, 0, cnt, obj.TYPE_REG, thumb.REG_R2, 0)
+		p.Reg = thumb.REG_R1
+		p = pp.Appendpp(p, thumb.AMOVW, obj.TYPE_REG, thumb.REG_R0, 0, obj.TYPE_MEM, thumb.REG_R1, 4)
+		p1 := p
+		p.Scond |= thumb.C_PBIT
+		p = pp.Appendpp(p, thumb.ACMP, obj.TYPE_REG, thumb.REG_R1, 0, obj.TYPE_NONE, 0, 0)
+		p.Reg = thumb.REG_R2
+		p = pp.Appendpp(p, thumb.ABNE, obj.TYPE_NONE, 0, 0, obj.TYPE_BRANCH, 0, 0)
+		gc.Patch(p, p1)
+	}
+
+	return p
+}
+
+func zeroAuto(pp *gc.Progs, n *gc.Node) {
+	// Note: this code must not clobber any registers.
+	sym := n.Sym.Linksym()
+	size := n.Type.Size()
+	p := pp.Prog(thumb.AMOVW)
+	p.From.Type = obj.TYPE_CONST
+	p.From.Offset = 0
+	p.To.Type = obj.TYPE_REG
+	p.To.Reg = thumb.REGTMP
+	for i := int64(0); i < size; i += 4 {
+		p := pp.Prog(thumb.AMOVW)
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = thumb.REGTMP
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_AUTO
+		p.To.Reg = thumb.REGSP
+		p.To.Offset = n.Xoffset + i
+		p.To.Sym = sym
+	}
+}
+
+func ginsnop(pp *gc.Progs) *obj.Prog {
+	return pp.Prog(thumb.ANOP2)
+}
diff --git a/src/cmd/compile/internal/thumb/ssa.go b/src/cmd/compile/internal/thumb/ssa.go
new file mode 100644
index 0000000000..52ba0fca2b
--- /dev/null
+++ b/src/cmd/compile/internal/thumb/ssa.go
@@ -0,0 +1,903 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumb
+
+import (
+	"fmt"
+	"math"
+	"math/bits"
+
+	"cmd/compile/internal/gc"
+	"cmd/compile/internal/ssa"
+	"cmd/compile/internal/types"
+	"cmd/internal/obj"
+	"cmd/internal/obj/thumb"
+	"cmd/internal/objabi"
+)
+
+// loadByType returns the load instruction of the given type.
+func loadByType(t *types.Type) obj.As {
+	if t.IsFloat() {
+		switch t.Size() {
+		case 4:
+			return thumb.AMOVF
+		case 8:
+			return thumb.AMOVD
+		}
+	} else {
+		switch t.Size() {
+		case 1:
+			if t.IsSigned() {
+				return thumb.AMOVB
+			} else {
+				return thumb.AMOVBU
+			}
+		case 2:
+			if t.IsSigned() {
+				return thumb.AMOVH
+			} else {
+				return thumb.AMOVHU
+			}
+		case 4:
+			return thumb.AMOVW
+		}
+	}
+	panic("bad load type")
+}
+
+// storeByType returns the store instruction of the given type.
+func storeByType(t *types.Type) obj.As {
+	if t.IsFloat() {
+		switch t.Size() {
+		case 4:
+			return thumb.AMOVF
+		case 8:
+			return thumb.AMOVD
+		}
+	} else {
+		switch t.Size() {
+		case 1:
+			return thumb.AMOVB
+		case 2:
+			return thumb.AMOVH
+		case 4:
+			return thumb.AMOVW
+		}
+	}
+	panic("bad store type")
+}
+
+// shift type is used as Offset in obj.TYPE_SHIFT operands to encode shifted register operands
+type shift int64
+
+// copied from ../../../internal/obj/util.go:/TYPE_SHIFT
+func (v shift) String() string {
+	op := "<<>>->@>"[((v>>5)&3)<<1:]
+	if v&(1<<4) != 0 {
+		// register shift
+		return fmt.Sprintf("R%d%c%cR%d", v&15, op[0], op[1], (v>>8)&15)
+	} else {
+		// constant shift
+		return fmt.Sprintf("R%d%c%c%d", v&15, op[0], op[1], (v>>7)&31)
+	}
+}
+
+// makeshift encodes a register shifted by a constant
+func makeshift(reg int16, typ int64, s int64) shift {
+	return shift(int64(reg&0xf) | typ | (s&31)<<7)
+}
+
+// genshift generates a Prog for r = r0 op (r1 shifted by n)
+func genshift(s *gc.SSAGenState, as obj.As, r0, r1, r int16, typ int64, n int64) *obj.Prog {
+	p := s.Prog(as)
+	p.From.Type = obj.TYPE_SHIFT
+	p.From.Offset = int64(makeshift(r1, typ, n))
+	p.Reg = r0
+	if r != 0 {
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	}
+	return p
+}
+
+// makeregshift encodes a register shifted by a register
+func makeregshift(r1 int16, typ int64, r2 int16) shift {
+	return shift(int64(r1&0xf) | typ | int64(r2&0xf)<<8 | 1<<4)
+}
+
+// find a (lsb, width) pair for BFC
+// lsb must be in [0, 31], width must be in [1, 32 - lsb]
+// return (0xffffffff, 0) if v is not a binary like 0...01...10...0
+func getBFC(v uint32) (uint32, uint32) {
+	var m, l uint32
+	// BFC is not applicable with zero
+	if v == 0 {
+		return 0xffffffff, 0
+	}
+	// find the lowest set bit, for example l=2 for 0x3ffffffc
+	l = uint32(bits.TrailingZeros32(v))
+	// m-1 represents the highest set bit index, for example m=30 for 0x3ffffffc
+	m = 32 - uint32(bits.LeadingZeros32(v))
+	// check if v is a binary like 0...01...10...0
+	if (1<<m)-(1<<l) == v {
+		// it must be m > l for non-zero v
+		return l, m - l
+	}
+	// invalid
+	return 0xffffffff, 0
+}
+
+func ssaGenValue(s *gc.SSAGenState, v *ssa.Value) {
+	switch v.Op {
+	case ssa.OpCopy, ssa.OpThumbMOVWreg:
+		if v.Type.IsMemory() {
+			return
+		}
+		x := v.Args[0].Reg()
+		y := v.Reg()
+		if x == y {
+			return
+		}
+		as := thumb.AMOVW
+		if v.Type.IsFloat() {
+			switch v.Type.Size() {
+			case 4:
+				as = thumb.AMOVF
+			case 8:
+				as = thumb.AMOVD
+			default:
+				panic("bad float size")
+			}
+		}
+		p := s.Prog(as)
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = x
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = y
+	case ssa.OpThumbMOVWnop:
+		if v.Reg() != v.Args[0].Reg() {
+			v.Fatalf("input[0] and output not in same register %s", v.LongString())
+		}
+		// nothing to do
+	case ssa.OpLoadReg:
+		if v.Type.IsFlags() {
+			v.Fatalf("load flags not implemented: %v", v.LongString())
+			return
+		}
+		p := s.Prog(loadByType(v.Type))
+		gc.AddrAuto(&p.From, v.Args[0])
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpStoreReg:
+		if v.Type.IsFlags() {
+			v.Fatalf("store flags not implemented: %v", v.LongString())
+			return
+		}
+		p := s.Prog(storeByType(v.Type))
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		gc.AddrAuto(&p.To, v)
+	case ssa.OpThumbADD,
+		ssa.OpThumbADC,
+		ssa.OpThumbSUB,
+		ssa.OpThumbSBC,
+		ssa.OpThumbRSB,
+		ssa.OpThumbAND,
+		ssa.OpThumbOR,
+		ssa.OpThumbXOR,
+		ssa.OpThumbBIC,
+		ssa.OpThumbMUL,
+		ssa.OpThumbDIV,
+		ssa.OpThumbDIVU,
+		ssa.OpThumbADDF,
+		ssa.OpThumbADDD,
+		ssa.OpThumbSUBF,
+		ssa.OpThumbSUBD,
+		ssa.OpThumbSLL,
+		ssa.OpThumbSRL,
+		ssa.OpThumbSRA,
+		ssa.OpThumbMULF,
+		ssa.OpThumbMULD,
+		ssa.OpThumbNMULF,
+		ssa.OpThumbNMULD,
+		ssa.OpThumbDIVF,
+		ssa.OpThumbDIVD:
+		r := v.Reg()
+		r1 := v.Args[0].Reg()
+		r2 := v.Args[1].Reg()
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = r2
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	case ssa.OpThumbMULAF, ssa.OpThumbMULAD, ssa.OpThumbMULSF, ssa.OpThumbMULSD:
+		r := v.Reg()
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+		if r != r0 {
+			v.Fatalf("result and addend are not in the same register: %v", v.LongString())
+		}
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = r2
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	case ssa.OpThumbADDS,
+		ssa.OpThumbSUBS:
+		r := v.Reg0()
+		r1 := v.Args[0].Reg()
+		r2 := v.Args[1].Reg()
+		p := s.Prog(v.Op.Asm())
+		p.Scond = thumb.C_SBIT
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = r2
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	case ssa.OpThumbSRAcond:
+		// Thumb shift instructions uses only the low-order byte of the shift amount
+		// generate conditional instructions to deal with large shifts
+		// flag is already set
+		// SRA.HS	$31, Rarg0, Rdst // shift 31 bits to get the sign bit
+		// SRA.LO	Rarg1, Rarg0, Rdst
+		r := v.Reg()
+		r1 := v.Args[0].Reg()
+		r2 := v.Args[1].Reg()
+		p := s.Prog(thumb.ASRA)
+		p.Scond = thumb.C_SCOND_HS
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 31
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+		p = s.Prog(thumb.ASRA)
+		p.Scond = thumb.C_SCOND_LO
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = r2
+		p.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = r
+	case ssa.OpThumbBFX, ssa.OpThumbBFXU:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt >> 8
+		p.SetFrom3(obj.Addr{Type: obj.TYPE_CONST, Offset: v.AuxInt & 0xff})
+		p.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbANDconst, ssa.OpThumbBICconst:
+		// try to optimize ANDconst and BICconst to BFC, which saves bytes and ticks
+		// BFC is only available on ARMv7, and its result and source are in the same register
+		if objabi.GOARM >= 7 && v.Reg() == v.Args[0].Reg() {
+			var val uint32
+			if v.Op == ssa.OpThumbANDconst {
+				val = ^uint32(v.AuxInt)
+			} else { // BICconst
+				val = uint32(v.AuxInt)
+			}
+			lsb, width := getBFC(val)
+			// omit BFC for ARM's imm12
+			if 8 < width && width < 24 {
+				p := s.Prog(thumb.ABFC)
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = int64(width)
+				p.SetFrom3(obj.Addr{Type: obj.TYPE_CONST, Offset: int64(lsb)})
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = v.Reg()
+				break
+			}
+		}
+		// fall back to ordinary form
+		fallthrough
+	case ssa.OpThumbADDconst,
+		ssa.OpThumbADCconst,
+		ssa.OpThumbSUBconst,
+		ssa.OpThumbSBCconst,
+		ssa.OpThumbRSBconst,
+		ssa.OpThumbORconst,
+		ssa.OpThumbXORconst,
+		ssa.OpThumbSLLconst,
+		ssa.OpThumbSRLconst,
+		ssa.OpThumbSRAconst:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbADDSconst,
+		ssa.OpThumbSUBSconst,
+		ssa.OpThumbRSBSconst:
+		p := s.Prog(v.Op.Asm())
+		p.Scond = thumb.C_SBIT
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg0()
+	case ssa.OpThumbSRRconst:
+		genshift(s, thumb.AMOVW, 0, v.Args[0].Reg(), v.Reg(), thumb.SHIFT_RR, v.AuxInt)
+	case ssa.OpThumbADDshiftLL,
+		ssa.OpThumbADCshiftLL,
+		ssa.OpThumbSUBshiftLL,
+		ssa.OpThumbSBCshiftLL,
+		ssa.OpThumbRSBshiftLL,
+		ssa.OpThumbANDshiftLL,
+		ssa.OpThumbORshiftLL,
+		ssa.OpThumbXORshiftLL,
+		ssa.OpThumbBICshiftLL:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg(), thumb.SHIFT_LL, v.AuxInt)
+	case ssa.OpThumbADDSshiftLL,
+		ssa.OpThumbSUBSshiftLL,
+		ssa.OpThumbRSBSshiftLL:
+		p := genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg0(), thumb.SHIFT_LL, v.AuxInt)
+		p.Scond = thumb.C_SBIT
+	case ssa.OpThumbADDshiftRL,
+		ssa.OpThumbADCshiftRL,
+		ssa.OpThumbSUBshiftRL,
+		ssa.OpThumbSBCshiftRL,
+		ssa.OpThumbRSBshiftRL,
+		ssa.OpThumbANDshiftRL,
+		ssa.OpThumbORshiftRL,
+		ssa.OpThumbXORshiftRL,
+		ssa.OpThumbBICshiftRL:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg(), thumb.SHIFT_LR, v.AuxInt)
+	case ssa.OpThumbADDSshiftRL,
+		ssa.OpThumbSUBSshiftRL,
+		ssa.OpThumbRSBSshiftRL:
+		p := genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg0(), thumb.SHIFT_LR, v.AuxInt)
+		p.Scond = thumb.C_SBIT
+	case ssa.OpThumbADDshiftRA,
+		ssa.OpThumbADCshiftRA,
+		ssa.OpThumbSUBshiftRA,
+		ssa.OpThumbSBCshiftRA,
+		ssa.OpThumbRSBshiftRA,
+		ssa.OpThumbANDshiftRA,
+		ssa.OpThumbORshiftRA,
+		ssa.OpThumbXORshiftRA,
+		ssa.OpThumbBICshiftRA:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg(), thumb.SHIFT_AR, v.AuxInt)
+	case ssa.OpThumbADDSshiftRA,
+		ssa.OpThumbSUBSshiftRA,
+		ssa.OpThumbRSBSshiftRA:
+		p := genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg0(), thumb.SHIFT_AR, v.AuxInt)
+		p.Scond = thumb.C_SBIT
+	case ssa.OpThumbXORshiftRR:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), v.Reg(), thumb.SHIFT_RR, v.AuxInt)
+	case ssa.OpThumbMVNshiftLL:
+		genshift(s, v.Op.Asm(), 0, v.Args[0].Reg(), v.Reg(), thumb.SHIFT_LL, v.AuxInt)
+	case ssa.OpThumbMVNshiftRL:
+		genshift(s, v.Op.Asm(), 0, v.Args[0].Reg(), v.Reg(), thumb.SHIFT_LR, v.AuxInt)
+	case ssa.OpThumbMVNshiftRA:
+		genshift(s, v.Op.Asm(), 0, v.Args[0].Reg(), v.Reg(), thumb.SHIFT_AR, v.AuxInt)
+	case ssa.OpThumbHMUL,
+		ssa.OpThumbHMULU:
+		// 32-bit high multiplication
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.Reg = v.Args[1].Reg()
+		p.To.Type = obj.TYPE_REGREG
+		p.To.Reg = v.Reg()
+		p.To.Offset = thumb.REGTMP // throw away low 32-bit into tmp register
+	case ssa.OpThumbMULLU:
+		// 32-bit multiplication, results 64-bit, high 32-bit in out0, low 32-bit in out1
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.Reg = v.Args[1].Reg()
+		p.To.Type = obj.TYPE_REGREG
+		p.To.Reg = v.Reg0()           // high 32-bit
+		p.To.Offset = int64(v.Reg1()) // low 32-bit
+	case ssa.OpThumbMULA, ssa.OpThumbMULS:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.Reg = v.Args[1].Reg()
+		p.To.Type = obj.TYPE_REGREG2
+		p.To.Reg = v.Reg()                   // result
+		p.To.Offset = int64(v.Args[2].Reg()) // addend
+	case ssa.OpThumbMOVWconst:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbMOVFconst,
+		ssa.OpThumbMOVDconst:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_FCONST
+		p.From.Val = math.Float64frombits(uint64(v.AuxInt))
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbCMP,
+		ssa.OpThumbCMN,
+		ssa.OpThumbTST,
+		ssa.OpThumbTEQ,
+		ssa.OpThumbCMPF,
+		ssa.OpThumbCMPD:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		// Special layout in ARM assembly
+		// Comparing to x86, the operands of ARM's CMP are reversed.
+		p.From.Reg = v.Args[1].Reg()
+		p.Reg = v.Args[0].Reg()
+	case ssa.OpThumbCMPconst,
+		ssa.OpThumbCMNconst,
+		ssa.OpThumbTSTconst,
+		ssa.OpThumbTEQconst:
+		// Special layout in ARM assembly
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.Reg = v.Args[0].Reg()
+	case ssa.OpThumbCMPF0,
+		ssa.OpThumbCMPD0:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+	case ssa.OpThumbCMPshiftLL, ssa.OpThumbCMNshiftLL, ssa.OpThumbTSTshiftLL, ssa.OpThumbTEQshiftLL:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), 0, thumb.SHIFT_LL, v.AuxInt)
+	case ssa.OpThumbCMPshiftRL, ssa.OpThumbCMNshiftRL, ssa.OpThumbTSTshiftRL, ssa.OpThumbTEQshiftRL:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), 0, thumb.SHIFT_LR, v.AuxInt)
+	case ssa.OpThumbCMPshiftRA, ssa.OpThumbCMNshiftRA, ssa.OpThumbTSTshiftRA, ssa.OpThumbTEQshiftRA:
+		genshift(s, v.Op.Asm(), v.Args[0].Reg(), v.Args[1].Reg(), 0, thumb.SHIFT_AR, v.AuxInt)
+	case ssa.OpThumbMOVWaddr:
+		p := s.Prog(thumb.AMOVW)
+		p.From.Type = obj.TYPE_ADDR
+		p.From.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+
+		var wantreg string
+		// MOVW $sym+off(base), R
+		// the assembler expands it as the following:
+		// - base is SP: add constant offset to SP (R13)
+		//               when constant is large, tmp register (R11) may be used
+		// - base is SB: load external address from constant pool (use relocation)
+		switch v.Aux.(type) {
+		default:
+			v.Fatalf("aux is of unknown type %T", v.Aux)
+		case *obj.LSym:
+			wantreg = "SB"
+			gc.AddAux(&p.From, v)
+		case *gc.Node:
+			wantreg = "SP"
+			gc.AddAux(&p.From, v)
+		case nil:
+			// No sym, just MOVW $off(SP), R
+			wantreg = "SP"
+			p.From.Offset = v.AuxInt
+		}
+		if reg := v.Args[0].RegName(); reg != wantreg {
+			v.Fatalf("bad reg %s for symbol type %T, want %s", reg, v.Aux, wantreg)
+		}
+	case ssa.OpThumbMOVWload,
+		ssa.OpThumbMOVHload,
+		ssa.OpThumbMOVHUload,
+		ssa.OpThumbMOVBload,
+		ssa.OpThumbMOVBUload,
+		ssa.OpThumbMOVDload,
+		ssa.OpThumbMOVFload,
+		ssa.OpThumbLoadOnce32,
+		ssa.OpThumbLoadOnce16,
+		ssa.OpThumbLoadOnce8:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = v.Args[0].Reg()
+		gc.AddAux(&p.From, v)
+		p.To.Type = obj.TYPE_REG
+		if _, ok := v.Block.Func.RegAlloc[v.ID].(ssa.LocPair); ok {
+			p.To.Reg = v.Reg0()
+		} else {
+			p.To.Reg = v.Reg()
+		}
+	case ssa.OpThumbMOVWstore,
+		ssa.OpThumbMOVHstore,
+		ssa.OpThumbMOVBstore,
+		ssa.OpThumbMOVDstore,
+		ssa.OpThumbMOVFstore,
+		ssa.OpThumbStoreOnce32,
+		ssa.OpThumbStoreOnce16,
+		ssa.OpThumbStoreOnce8:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[1].Reg()
+		p.To.Type = obj.TYPE_MEM
+		p.To.Reg = v.Args[0].Reg()
+		gc.AddAux(&p.To, v)
+	case ssa.OpThumbMOVWloadidx,
+		ssa.OpThumbMOVHUloadidx,
+		ssa.OpThumbMOVHloadidx,
+		ssa.OpThumbMOVBUloadidx,
+		ssa.OpThumbMOVBloadidx,
+		ssa.OpThumbLoadOnce32idx,
+		ssa.OpThumbLoadOnce16idx,
+		ssa.OpThumbLoadOnce8idx:
+		// this is just shift 0 bits
+		fallthrough
+	case ssa.OpThumbMOVWloadshiftLL,
+		ssa.OpThumbMOVHUloadshiftLL,
+		ssa.OpThumbMOVHloadshiftLL,
+		ssa.OpThumbMOVBUloadshiftLL,
+		ssa.OpThumbMOVBloadshiftLL,
+		ssa.OpThumbLoadOnce32shiftLL,
+		ssa.OpThumbLoadOnce16shiftLL,
+		ssa.OpThumbLoadOnce8shiftLL:
+		var toreg int16
+		if _, ok := v.Block.Func.RegAlloc[v.ID].(ssa.LocPair); ok {
+			toreg = v.Reg0()
+		} else {
+			toreg = v.Reg()
+		}
+		p := genshift(s, v.Op.Asm(), 0, v.Args[1].Reg(), toreg, thumb.SHIFT_LL, v.AuxInt)
+		p.From.Reg = v.Args[0].Reg()
+	case ssa.OpThumbMOVWstoreidx,
+		ssa.OpThumbMOVHstoreidx,
+		ssa.OpThumbMOVBstoreidx,
+		ssa.OpThumbStoreOnce32idx,
+		ssa.OpThumbStoreOnce16idx,
+		ssa.OpThumbStoreOnce8idx:
+		// this is just shift 0 bits
+		fallthrough
+	case ssa.OpThumbMOVWstoreshiftLL,
+		ssa.OpThumbMOVHstoreshiftLL,
+		ssa.OpThumbMOVBstoreshiftLL,
+		ssa.OpThumbStoreOnce32shiftLL,
+		ssa.OpThumbStoreOnce16shiftLL,
+		ssa.OpThumbStoreOnce8shiftLL:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[2].Reg()
+		p.To.Type = obj.TYPE_SHIFT
+		p.To.Reg = v.Args[0].Reg()
+		p.To.Offset = int64(makeshift(v.Args[1].Reg(), thumb.SHIFT_LL, v.AuxInt))
+	case ssa.OpThumbMOVBreg,
+		ssa.OpThumbMOVBUreg,
+		ssa.OpThumbMOVHreg,
+		ssa.OpThumbMOVHUreg:
+		a := v.Args[0]
+		for a.Op == ssa.OpCopy || a.Op == ssa.OpThumbMOVWreg || a.Op == ssa.OpThumbMOVWnop {
+			a = a.Args[0]
+		}
+		if a.Op == ssa.OpLoadReg {
+			t := a.Type
+			switch {
+			case v.Op == ssa.OpThumbMOVBreg && t.Size() == 1 && t.IsSigned(),
+				v.Op == ssa.OpThumbMOVBUreg && t.Size() == 1 && !t.IsSigned(),
+				v.Op == ssa.OpThumbMOVHreg && t.Size() == 2 && t.IsSigned(),
+				v.Op == ssa.OpThumbMOVHUreg && t.Size() == 2 && !t.IsSigned():
+				// arg is a proper-typed load, already zero/sign-extended, don't extend again
+				if v.Reg() == v.Args[0].Reg() {
+					return
+				}
+				p := s.Prog(thumb.AMOVW)
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = v.Args[0].Reg()
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = v.Reg()
+				return
+			default:
+			}
+		}
+		fallthrough
+	case ssa.OpThumbMVN,
+		ssa.OpThumbCLZ,
+		ssa.OpThumbREV,
+		ssa.OpThumbREV16,
+		ssa.OpThumbRBIT,
+		ssa.OpThumbSQRTD,
+		ssa.OpThumbNEGF,
+		ssa.OpThumbNEGD,
+		ssa.OpThumbMOVWF,
+		ssa.OpThumbMOVWD,
+		ssa.OpThumbMOVFW,
+		ssa.OpThumbMOVDW,
+		ssa.OpThumbMOVFD,
+		ssa.OpThumbMOVDF:
+		p := s.Prog(v.Op.Asm())
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbMOVWUF,
+		ssa.OpThumbMOVWUD,
+		ssa.OpThumbMOVFWU,
+		ssa.OpThumbMOVDWU:
+		p := s.Prog(v.Op.Asm())
+		p.Scond = thumb.C_UBIT
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[0].Reg()
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbCMOVWHSconst:
+		p := s.Prog(thumb.AMOVW)
+		p.Scond = thumb.C_SCOND_HS
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbCMOVWLSconst:
+		p := s.Prog(thumb.AMOVW)
+		p.Scond = thumb.C_SCOND_LS
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = v.AuxInt
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbCALLstatic, ssa.OpThumbCALLclosure, ssa.OpThumbCALLinter:
+		s.Call(v)
+	case ssa.OpThumbLoweredWB:
+		p := s.Prog(obj.ACALL)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = v.Aux.(*obj.LSym)
+	case ssa.OpThumbLoweredPanicBoundsA, ssa.OpThumbLoweredPanicBoundsB, ssa.OpThumbLoweredPanicBoundsC:
+		p := s.Prog(obj.ACALL)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.BoundsCheckFunc[v.AuxInt]
+		s.UseArgs(8) // space used in callee args area by assembly stubs
+	case ssa.OpThumbLoweredPanicExtendA, ssa.OpThumbLoweredPanicExtendB, ssa.OpThumbLoweredPanicExtendC:
+		p := s.Prog(obj.ACALL)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.ExtendCheckFunc[v.AuxInt]
+		s.UseArgs(12) // space used in callee args area by assembly stubs
+	case ssa.OpThumbDUFFZERO:
+		p := s.Prog(obj.ADUFFZERO)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.Duffzero
+		p.To.Offset = v.AuxInt
+	case ssa.OpThumbDUFFCOPY:
+		p := s.Prog(obj.ADUFFCOPY)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = gc.Duffcopy
+		p.To.Offset = v.AuxInt
+	case ssa.OpThumbLoweredNilCheck:
+		if objabi.GOOS == "noos" {
+			// BUG: see https://github.com/embeddedgo/go/issues/1
+		} else {
+			// Issue a load which will fault if arg is nil.
+			p := s.Prog(thumb.AMOVBU)
+			p.From.Type = obj.TYPE_MEM
+			p.From.Reg = v.Args[0].Reg()
+			gc.AddAux(&p.From, v)
+			p.To.Type = obj.TYPE_REG
+			p.To.Reg = thumb.REGTMP
+		}
+		if gc.Debug_checknil != 0 && v.Pos.Line() > 1 { // v.Pos.Line()==1 in generated wrappers
+			gc.Warnl(v.Pos, "generated nil check")
+		}
+	case ssa.OpThumbLoweredZero:
+		// MOVW.P	Rarg2, 4(R1)
+		// CMP	Rarg1, R1
+		// BLE	-2(PC)
+		// arg1 is the address of the last element to zero
+		// arg2 is known to be zero
+		// auxint is alignment
+		var sz int64
+		var mov obj.As
+		switch {
+		case v.AuxInt%4 == 0:
+			sz = 4
+			mov = thumb.AMOVW
+		case v.AuxInt%2 == 0:
+			sz = 2
+			mov = thumb.AMOVH
+		default:
+			sz = 1
+			mov = thumb.AMOVB
+		}
+		p := s.Prog(mov)
+		p.Scond = thumb.C_PBIT
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = v.Args[2].Reg()
+		p.To.Type = obj.TYPE_MEM
+		p.To.Reg = thumb.REG_R1
+		p.To.Offset = sz
+		p2 := s.Prog(thumb.ACMP)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = v.Args[1].Reg()
+		p2.Reg = thumb.REG_R1
+		p3 := s.Prog(thumb.ABLE)
+		p3.To.Type = obj.TYPE_BRANCH
+		gc.Patch(p3, p)
+	case ssa.OpThumbLoweredMove:
+		// MOVW.P	4(R1), Rtmp
+		// MOVW.P	Rtmp, 4(R2)
+		// CMP	Rarg2, R1
+		// BLE	-3(PC)
+		// arg2 is the address of the last element of src
+		// auxint is alignment
+		var sz int64
+		var mov obj.As
+		switch {
+		case v.AuxInt%4 == 0:
+			sz = 4
+			mov = thumb.AMOVW
+		case v.AuxInt%2 == 0:
+			sz = 2
+			mov = thumb.AMOVH
+		default:
+			sz = 1
+			mov = thumb.AMOVB
+		}
+		p := s.Prog(mov)
+		p.Scond = thumb.C_PBIT
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = thumb.REG_R1
+		p.From.Offset = sz
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = thumb.REGTMP
+		p2 := s.Prog(mov)
+		p2.Scond = thumb.C_PBIT
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = thumb.REGTMP
+		p2.To.Type = obj.TYPE_MEM
+		p2.To.Reg = thumb.REG_R2
+		p2.To.Offset = sz
+		p3 := s.Prog(thumb.ACMP)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = v.Args[2].Reg()
+		p3.Reg = thumb.REG_R1
+		p4 := s.Prog(thumb.ABLE)
+		p4.To.Type = obj.TYPE_BRANCH
+		gc.Patch(p4, p)
+	case ssa.OpThumbEqual,
+		ssa.OpThumbNotEqual,
+		ssa.OpThumbLessThan,
+		ssa.OpThumbLessEqual,
+		ssa.OpThumbGreaterThan,
+		ssa.OpThumbGreaterEqual,
+		ssa.OpThumbLessThanU,
+		ssa.OpThumbLessEqualU,
+		ssa.OpThumbGreaterThanU,
+		ssa.OpThumbGreaterEqualU:
+		// generate boolean values
+		// use conditional move, preserve flags
+		scond := condBits[v.Op] | thumb.C_PBIT
+		p := s.Prog(thumb.AMOVW)
+		p.Scond = scond ^ 1
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 0
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+		p = s.Prog(thumb.AMOVW)
+		p.Scond = scond
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbLoweredGetClosurePtr:
+		// Closure pointer is R11 (thumb.REGCTXT).
+		gc.CheckLoweredGetClosurePtr(v)
+	case ssa.OpThumbLoweredGetCallerSP:
+		// caller's SP is FixedFrameSize below the address of the first arg
+		p := s.Prog(thumb.AMOVW)
+		p.From.Type = obj.TYPE_ADDR
+		p.From.Offset = -gc.Ctxt.FixedFrameSize()
+		p.From.Name = obj.NAME_PARAM
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbLoweredGetCallerPC:
+		p := s.Prog(obj.AGETCALLERPC)
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = v.Reg()
+	case ssa.OpThumbFlagEQ,
+		ssa.OpThumbFlagLT_ULT,
+		ssa.OpThumbFlagLT_UGT,
+		ssa.OpThumbFlagGT_ULT,
+		ssa.OpThumbFlagGT_UGT:
+		v.Fatalf("Flag* ops should never make it to codegen %v", v.LongString())
+	case ssa.OpThumbInvertFlags:
+		v.Fatalf("InvertFlags should never make it to codegen %v", v.LongString())
+	case ssa.OpClobber:
+		// TODO: implement for clobberdead experiment. Nop is ok for now.
+	case ssa.OpThumbDSB:
+		s.Prog(thumb.ADSB)
+	case ssa.OpThumbDMB_ST:
+		p := s.Prog(thumb.ADMB)
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = thumb.REG_MB_ST
+	default:
+		v.Fatalf("genValue not implemented: %s", v.LongString())
+	}
+}
+
+var condBits = map[ssa.Op]uint8{
+	ssa.OpThumbEqual:         thumb.C_SCOND_EQ,
+	ssa.OpThumbNotEqual:      thumb.C_SCOND_NE,
+	ssa.OpThumbLessThan:      thumb.C_SCOND_LT,
+	ssa.OpThumbLessThanU:     thumb.C_SCOND_LO,
+	ssa.OpThumbLessEqual:     thumb.C_SCOND_LE,
+	ssa.OpThumbLessEqualU:    thumb.C_SCOND_LS,
+	ssa.OpThumbGreaterThan:   thumb.C_SCOND_GT,
+	ssa.OpThumbGreaterThanU:  thumb.C_SCOND_HI,
+	ssa.OpThumbGreaterEqual:  thumb.C_SCOND_GE,
+	ssa.OpThumbGreaterEqualU: thumb.C_SCOND_HS,
+}
+
+var blockJump = map[ssa.BlockKind]struct {
+	asm, invasm obj.As
+}{
+	ssa.BlockThumbEQ:  {thumb.ABEQ, thumb.ABNE},
+	ssa.BlockThumbNE:  {thumb.ABNE, thumb.ABEQ},
+	ssa.BlockThumbLT:  {thumb.ABLT, thumb.ABGE},
+	ssa.BlockThumbGE:  {thumb.ABGE, thumb.ABLT},
+	ssa.BlockThumbLE:  {thumb.ABLE, thumb.ABGT},
+	ssa.BlockThumbGT:  {thumb.ABGT, thumb.ABLE},
+	ssa.BlockThumbULT: {thumb.ABLO, thumb.ABHS},
+	ssa.BlockThumbUGE: {thumb.ABHS, thumb.ABLO},
+	ssa.BlockThumbUGT: {thumb.ABHI, thumb.ABLS},
+	ssa.BlockThumbULE: {thumb.ABLS, thumb.ABHI},
+}
+
+func ssaGenBlock(s *gc.SSAGenState, b, next *ssa.Block) {
+	switch b.Kind {
+	case ssa.BlockPlain:
+		if b.Succs[0].Block() != next {
+			p := s.Prog(obj.AJMP)
+			p.To.Type = obj.TYPE_BRANCH
+			s.Branches = append(s.Branches, gc.Branch{P: p, B: b.Succs[0].Block()})
+		}
+
+	case ssa.BlockDefer:
+		// defer returns in R0:
+		// 0 if we should continue executing
+		// 1 if we should jump to deferreturn call
+		p := s.Prog(thumb.ACMP)
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = 0
+		p.Reg = thumb.REG_R0
+		p = s.Prog(thumb.ABNE)
+		p.To.Type = obj.TYPE_BRANCH
+		s.Branches = append(s.Branches, gc.Branch{P: p, B: b.Succs[1].Block()})
+		if b.Succs[0].Block() != next {
+			p := s.Prog(obj.AJMP)
+			p.To.Type = obj.TYPE_BRANCH
+			s.Branches = append(s.Branches, gc.Branch{P: p, B: b.Succs[0].Block()})
+		}
+
+	case ssa.BlockExit:
+
+	case ssa.BlockRet:
+		s.Prog(obj.ARET)
+
+	case ssa.BlockRetJmp:
+		p := s.Prog(obj.ARET)
+		p.To.Type = obj.TYPE_MEM
+		p.To.Name = obj.NAME_EXTERN
+		p.To.Sym = b.Aux.(*obj.LSym)
+
+	case ssa.BlockThumbEQ, ssa.BlockThumbNE,
+		ssa.BlockThumbLT, ssa.BlockThumbGE,
+		ssa.BlockThumbLE, ssa.BlockThumbGT,
+		ssa.BlockThumbULT, ssa.BlockThumbUGT,
+		ssa.BlockThumbULE, ssa.BlockThumbUGE:
+		jmp := blockJump[b.Kind]
+		switch next {
+		case b.Succs[0].Block():
+			s.Br(jmp.invasm, b.Succs[1].Block())
+		case b.Succs[1].Block():
+			s.Br(jmp.asm, b.Succs[0].Block())
+		default:
+			if b.Likely != ssa.BranchUnlikely {
+				s.Br(jmp.asm, b.Succs[0].Block())
+				s.Br(obj.AJMP, b.Succs[1].Block())
+			} else {
+				s.Br(jmp.invasm, b.Succs[1].Block())
+				s.Br(obj.AJMP, b.Succs[0].Block())
+			}
+		}
+
+	default:
+		b.Fatalf("branch not implemented: %s. Control: %s", b.LongString(), b.Control.LongString())
+	}
+}
diff --git a/src/cmd/compile/main.go b/src/cmd/compile/main.go
index bf4289e8f1..8381491f0b 100644
--- a/src/cmd/compile/main.go
+++ b/src/cmd/compile/main.go
@@ -13,6 +13,7 @@ import (
 	"cmd/compile/internal/mips64"
 	"cmd/compile/internal/ppc64"
 	"cmd/compile/internal/s390x"
+	"cmd/compile/internal/thumb"
 	"cmd/compile/internal/wasm"
 	"cmd/compile/internal/x86"
 	"cmd/internal/objabi"
@@ -34,6 +35,7 @@ var archInits = map[string]func(*gc.Arch){
 	"ppc64":    ppc64.Init,
 	"ppc64le":  ppc64.Init,
 	"s390x":    s390x.Init,
+	"thumb":    thumb.Init,
 	"wasm":     wasm.Init,
 }
 
diff --git a/src/cmd/dist/build.go b/src/cmd/dist/build.go
index 9e503117ae..42fb068ee9 100644
--- a/src/cmd/dist/build.go
+++ b/src/cmd/dist/build.go
@@ -72,6 +72,7 @@ var okgoarch = []string{
 	"ppc64le",
 	"riscv64",
 	"s390x",
+	"thumb",
 	"sparc64",
 	"wasm",
 }
@@ -92,6 +93,7 @@ var okgoos = []string{
 	"plan9",
 	"windows",
 	"aix",
+	"noos",
 }
 
 // find reports the first index of p in l[0:n], or else -1.
@@ -1120,7 +1122,7 @@ func cmdenv() {
 	xprintf(format, "GOROOT", goroot)
 	xprintf(format, "GOTMPDIR", os.Getenv("GOTMPDIR"))
 	xprintf(format, "GOTOOLDIR", tooldir)
-	if goarch == "arm" {
+	if goarch == "arm" || goarch == "thumb" {
 		xprintf(format, "GOARM", goarm)
 	}
 	if goarch == "386" {
@@ -1499,6 +1501,7 @@ var cgoEnabled = map[string]bool{
 	"linux/mips64le":  true,
 	"linux/riscv64":   true,
 	"linux/s390x":     true,
+	"linux/thumb":     false,
 	"linux/sparc64":   true,
 	"android/386":     true,
 	"android/amd64":   true,
@@ -1523,6 +1526,7 @@ var cgoEnabled = map[string]bool{
 	"windows/386":     true,
 	"windows/amd64":   true,
 	"windows/arm":     false,
+	"noos/thumb":      false,
 }
 
 // List of platforms which are supported but not complete yet. These get
diff --git a/src/cmd/dist/buildtool.go b/src/cmd/dist/buildtool.go
index b434d4f60f..f78e416639 100644
--- a/src/cmd/dist/buildtool.go
+++ b/src/cmd/dist/buildtool.go
@@ -48,6 +48,7 @@ var bootstrapDirs = []string{
 	"cmd/compile/internal/s390x",
 	"cmd/compile/internal/ssa",
 	"cmd/compile/internal/syntax",
+	"cmd/compile/internal/thumb",
 	"cmd/compile/internal/x86",
 	"cmd/compile/internal/wasm",
 	"cmd/internal/bio",
@@ -61,6 +62,7 @@ var bootstrapDirs = []string{
 	"cmd/internal/obj/mips",
 	"cmd/internal/obj/ppc64",
 	"cmd/internal/obj/s390x",
+	"cmd/internal/obj/thumb",
 	"cmd/internal/obj/x86",
 	"cmd/internal/obj/wasm",
 	"cmd/internal/src",
@@ -80,6 +82,7 @@ var bootstrapDirs = []string{
 	"cmd/link/internal/ppc64",
 	"cmd/link/internal/s390x",
 	"cmd/link/internal/sym",
+	"cmd/link/internal/thumb",
 	"cmd/link/internal/x86",
 	"compress/flate",
 	"compress/zlib",
@@ -109,6 +112,8 @@ var ignorePrefixes = []string{
 var ignoreSuffixes = []string{
 	"_arm64.s",
 	"_arm64.go",
+	"_thumb.s",
+	"_thumb.go",
 	"_wasm.s",
 	"_wasm.go",
 }
diff --git a/src/cmd/go/internal/cfg/cfg.go b/src/cmd/go/internal/cfg/cfg.go
index a3277a6c3f..65720d9093 100644
--- a/src/cmd/go/internal/cfg/cfg.go
+++ b/src/cmd/go/internal/cfg/cfg.go
@@ -234,7 +234,7 @@ var (
 	GOROOT_FINAL = findGOROOT_FINAL()
 
 	// Used in envcmd.MkEnv and build ID computations.
-	GOARM    = envOr("GOARM", fmt.Sprint(objabi.GOARM))
+	GOARM    = envOr("GOARM", fmt.Sprintf("%x", objabi.GOARM))
 	GO386    = envOr("GO386", objabi.GO386)
 	GOMIPS   = envOr("GOMIPS", objabi.GOMIPS)
 	GOMIPS64 = envOr("GOMIPS64", objabi.GOMIPS64)
diff --git a/src/cmd/internal/goobj/read.go b/src/cmd/internal/goobj/read.go
index dd29bacd04..fa81b4499e 100644
--- a/src/cmd/internal/goobj/read.go
+++ b/src/cmd/internal/goobj/read.go
@@ -630,8 +630,17 @@ func (r *Reloc) String(insnOffset uint64) string {
 	delta := r.Offset - int64(insnOffset)
 	s := fmt.Sprintf("[%d:%d]%s", delta, delta+r.Size, r.Type)
 	if r.Sym.Name != "" {
-		if r.Add != 0 {
-			return fmt.Sprintf("%s:%s+%d", s, r.Sym.Name, r.Add)
+		if add := r.Add; add != 0 {
+			if r.Type == objabi.R_CALLARM {
+				if uint64(add)>>32 == 0 {
+					// arm
+					add = add << (32 + 8) >> (32 + 6)
+				} else {
+					// thumb
+					add = int64(int32(add))
+				}
+			}
+			return fmt.Sprintf("%s:%s%+d", s, r.Sym.Name, add)
 		}
 		return fmt.Sprintf("%s:%s", s, r.Sym.Name)
 	}
diff --git a/src/cmd/internal/obj/link.go b/src/cmd/internal/obj/link.go
index 66748b25d2..b35a1c5e6b 100644
--- a/src/cmd/internal/obj/link.go
+++ b/src/cmd/internal/obj/link.go
@@ -370,6 +370,7 @@ const (
 	ABaseARM64
 	ABaseMIPS
 	ABaseS390X
+	ABaseThumb
 	ABaseWasm
 
 	AllowedOpCodes = 1 << 11            // The number of opcodes available for any given architecture.
@@ -453,7 +454,7 @@ const (
 )
 
 // Attribute is a set of symbol attributes.
-type Attribute uint16
+type Attribute uint32
 
 const (
 	AttrDuplicateOK Attribute = 1 << iota
@@ -494,6 +495,9 @@ const (
 	// keep unwinding beyond this frame.
 	AttrTopFrame
 
+	// Generate an interrupt entry/exit prologue/epilogue.
+	AttrISR
+
 	// attrABIBase is the value at which the ABI is encoded in
 	// Attribute. This must be last; all bits after this are
 	// assumed to be an ABI value.
@@ -517,6 +521,7 @@ func (a Attribute) NoFrame() bool       { return a&AttrNoFrame != 0 }
 func (a Attribute) Static() bool        { return a&AttrStatic != 0 }
 func (a Attribute) WasInlined() bool    { return a&AttrWasInlined != 0 }
 func (a Attribute) TopFrame() bool      { return a&AttrTopFrame != 0 }
+func (a Attribute) ISR() bool           { return a&AttrISR != 0 }
 
 func (a *Attribute) Set(flag Attribute, value bool) {
 	if value {
diff --git a/src/cmd/internal/obj/plist.go b/src/cmd/internal/obj/plist.go
index 303fa469e4..bc96f4aae8 100644
--- a/src/cmd/internal/obj/plist.go
+++ b/src/cmd/internal/obj/plist.go
@@ -133,6 +133,7 @@ func (ctxt *Link) InitTextSym(s *LSym, flag int) {
 	s.Set(AttrNeedCtxt, flag&NEEDCTXT != 0)
 	s.Set(AttrNoFrame, flag&NOFRAME != 0)
 	s.Set(AttrTopFrame, flag&TOPFRAME != 0)
+	s.Set(AttrISR, flag&ISR != 0)
 	s.Type = objabi.STEXT
 	ctxt.Text = append(ctxt.Text, s)
 
diff --git a/src/cmd/internal/obj/sizeof_test.go b/src/cmd/internal/obj/sizeof_test.go
index e70d174637..4865f8e44b 100644
--- a/src/cmd/internal/obj/sizeof_test.go
+++ b/src/cmd/internal/obj/sizeof_test.go
@@ -23,7 +23,7 @@ func TestSizeof(t *testing.T) {
 		_64bit uintptr     // size on 64bit platforms
 	}{
 		{Addr{}, 32, 48},
-		{LSym{}, 56, 104},
+		{LSym{}, 60, 104},
 		{Prog{}, 132, 200},
 	}
 
diff --git a/src/cmd/internal/obj/textflag.go b/src/cmd/internal/obj/textflag.go
index d2cec734b1..1b9975754b 100644
--- a/src/cmd/internal/obj/textflag.go
+++ b/src/cmd/internal/obj/textflag.go
@@ -51,4 +51,7 @@ const (
 	// Function is the top of the call stack. Call stack unwinders should stop
 	// at this function.
 	TOPFRAME = 2048
+
+	// Generate interrupt handler prologue / epilogue.
+	ISR = 4096
 )
diff --git a/src/cmd/internal/obj/thumb/a.out.go b/src/cmd/internal/obj/thumb/a.out.go
new file mode 100644
index 0000000000..1012d10333
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/a.out.go
@@ -0,0 +1,500 @@
+// Inferno utils/5c/5.out.h
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5c/5.out.h
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import "cmd/internal/obj"
+
+const (
+	// must be 16-aligned
+	REG_R0 = obj.RBaseThumb + iota
+	REG_R1
+	REG_R2
+	REG_R3
+	REG_R4
+	REG_R5
+	REG_R6
+	REG_R7
+	REG_R8
+	REG_R9
+	REG_R10
+	REG_R11
+	REG_R12
+	REG_R13
+	REG_R14
+	REG_R15
+
+	// must be 16-aligned
+	REG_F0
+	REG_F1
+	REG_F2
+	REG_F3
+	REG_F4
+	REG_F5
+	REG_F6
+	REG_F7
+	REG_F8
+	REG_F9
+	REG_F10
+	REG_F11
+	REG_F12
+	REG_F13
+	REG_F14
+	REG_F15
+
+	// must be 32-aligned
+	REG_APSR        // 0
+	REG_IAPSR       // 1
+	REG_EAPSR       // 2
+	REG_XPSR        // 3
+	_               // 4
+	REG_IPSR        // 5
+	REG_EPSR        // 6
+	REG_IEPSR       // 7
+	REG_MSP         // 8
+	REG_PSP         // 9
+	_               // 10
+	_               // 11
+	_               // 12
+	_               // 13
+	_               // 14
+	_               // 15
+	REG_PRIMASK     // 16
+	REG_BASEPRI     // 17
+	REG_BASEPRI_MAX // 18
+	REG_FAULTMASK   // 19
+	REG_CONTROL     // 20
+
+	REG_FPSCR
+
+	// Use R7 as REGTMP insetad of R11 to raise the chance to generate 16-bit
+	// instructions. R11 seems to be twice as often used as R7 in arm code:
+	// for i in 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15; do
+	//   echo -n "R$i "
+	//   go tool objdump compile |grep -v 00000000 |grep "R$i[ ,]" |wc -l
+	// done
+	// R0 461185
+	// R1 315395
+	// R2 209245
+	// R3 164614
+	// R4 111672
+	// R5 77637
+	// R6 58855
+	// R7 58084
+	// R8 40492
+	// R9 35562
+	// R10 6508
+	// R11 100948
+	// R12 29360
+	// R13 23314
+	// R14 44902
+	// R15 6227
+	// Drawback: incompatible with arm assembly that use R11.
+
+	REGTMP  = REG_R7  // reserved for compiler and linker
+	REGG    = REG_R10 // pointer to goroutine structure (g)
+	REGCTXT = REG_R11 // closure pointer
+	REGSP   = REG_R13
+	REGLINK = REG_R14
+	REGPC   = REG_R15
+
+	FREGTMP = REG_F15 // reserved for compiler and linker
+)
+
+const pseudoBase = obj.RBaseThumb + 1<<8
+
+const (
+	// Pseudo-registers that encode options for DMB, DSB, ISB (must be 16-aligned).
+	REG_MB_OSHST = pseudoBase + 2
+	REG_MB_OSH   = pseudoBase + 3
+	REG_MB_NSHST = pseudoBase + 6
+	REG_MB_NSH   = pseudoBase + 7
+	REG_MB_ISHST = pseudoBase + 10
+	REG_MB_ISH   = pseudoBase + 11
+	REG_MB_ST    = pseudoBase + 14
+	REG_MB_SY    = pseudoBase + 15
+
+	// Pseudo-registers that encode firstcond for IT (must be 16-aligned).
+	REG_EQ = pseudoBase + 16 // equal (Z==1)
+	REG_NE = pseudoBase + 17 // not equal (Z == 0)
+	REG_HS = pseudoBase + 18 // unsigned higher or same (C == 1)
+	REG_CS                   // carry set (C == 1)
+	REG_LO = pseudoBase + 19 // unsigned lower (C == 0)
+	REG_CC                   // carry clear (C == 0)
+	REG_MI = pseudoBase + 20 // minus/negative (N == 1)
+	REG_PL = pseudoBase + 21 // plus/positive or zero (N == 0)
+	REG_VS = pseudoBase + 22 // overflow (V==1)
+	REG_VC = pseudoBase + 23 // no overflow (V==0)
+	REG_HI = pseudoBase + 24 // unsigned higher (C==1 && Z==0)
+	REG_LS = pseudoBase + 25 // unsigned lower or same (C==0 || Z==1)
+	REG_GE = pseudoBase + 26 // signed greater than or equal (N == V)
+	REG_LT = pseudoBase + 27 // signed less than (N != V)
+	REG_GT = pseudoBase + 28 // signed greater than (Z==0 && N==V)
+	REG_LE = pseudoBase + 29 // signed less than or equal (Z==1 || N!=V)
+	REG_AL = pseudoBase + 30 // always
+
+	MAXREG
+)
+
+type Aclass uint8
+
+//go:generate stringer -type Aclass
+
+const (
+	C_NONE Aclass = iota
+
+	// Do not fragment or reorder these register/shift classes, or match/aclass/oplook will break.
+	C_RLO      // R0-R7
+	C_SP       // R13
+	C_PC       // R15
+	C_REG      // R0-R15
+	C_SHIFTILO // register shift R>>x (R0-R7)
+	C_SHIFTI   // register shift R>>x
+
+	C_REGREG
+	C_REGREG2 // multiply accumulate dest regs
+
+	// Do not fragment or reorder these registerlist classes, or match/aclass/oplook will break.
+	C_LISTLO   // R0-R7
+	C_LISTLOLR // R0-R7,LR
+	C_LISTLOPC // R0-R7,PC
+	C_LIST
+
+	C_SHIFTR   // register shift R>>R
+	C_SHIFTRLO // register shift R>>R (R0-R7)
+	C_FREG
+	C_FCR
+	C_SPEC // special register
+	C_MB   // memory barier option (pseudo-register)
+	C_IT   // firstcond for IT
+
+	// Do not fragment or reorder these C_*CON classes, or match/aclass/oplook will break.
+
+	C_ZCON
+	C_U1CON2
+	C_U6CON2
+	C_U8CON1_4 // not contain C_U8CON2
+	C_U8CON5_8
+	C_U7CON2 // T16 ADD  u7<<2, R13
+	C_U8CON2 // T16 ADD  u8<<2, R13, Rd
+	C_U3CON  // T16 ADD  u3, Rn, Rd
+	C_U8CON  // T16 ADD  u8, Rdn
+	C_U12CON // T32 ADD  u12, Rn, Rd
+	C_U16CON // T32 MOVW u16, Rd
+	C_E32CON // T32 ADD  e32, Rn, Rd
+	C_LCON
+
+	C_ZFCON
+	C_SFCON // T32 MOVF f8, Fd
+	C_LFCON
+
+	// Do not fragment or reorder these C_*BRA classes, or match/aclass/brlook will break.
+	C_U6BRA  // T16 CBZ   Rn, u6<<1
+	C_S8BRA  // T16 Bcond i8<<1
+	C_S11BRA // T16 B     i11<<1
+	C_S20BRA // T32 Bcond ji20<<1
+	C_S24BRA // T32 B     ji24<<1
+
+	// Do not fragment or reorder these C_*OR* classes, or match/aclass/oplook will break.
+	C_BORLO   // T16 MOVB       u5(Rn), Rt
+	C_HORLO   // T16 MOVH       u5<<1(Rn), Rt
+	C_WORLO   // T16 MOVW       u5<<2(Rn), Rt
+	C_WOSP    // T16 MOVW       u8<<2(R13), Rt
+	C_WOPC    // T16 MOVW       u8<<2(PC), Rt
+	C_UOREG   // T32 MOV{B,H,W} u12(Rn), Rt
+	C_SOREG   // T32 MOV{B,H,W} u8(Rn), Rt
+	C_SOPC    // T32 MOVW       u12(R15), Rt
+	C_FOREG   // T32 MOV{F,D}   u8<<2(Rn), Rt (MOVW u8<<2(Rn), (Rta, Rtb))
+	C_ZORLO   // T16 MOVM.IA.W  (Rn), reglist
+	C_ZOSP    // T16 MOVM.IA.W  (R13), reglist
+	C_ZOREG   // T16 JMP        (Rn)
+	C_LOREG   // long offset
+	C_LORLO   // long offset, Rn <= R7
+	C_U0ORLO  // (Rn)        C_BORLO, C_HORLO, C_WORLO, C_UOREG, C_SOREG, C_FOREG, ZORLO, ZOREG
+	C_U3ORLO2 // u3<<2(Rn)   C_BORLO, C_HORLO, C_WORLO, C_UOREG, C_SOREG, C_FOREG
+	C_U4ORLO2 // u4<<2(Rn)   C_HORLO, C_WORLO, C_UOREG, C_SOREG, C_FOREG
+	C_U5ORLO2 // u5<<2(Rn)   C_WORLO, C_UOREG, C_SOREG, C_FOREG
+	C_U4ORLO1 // u4<<1(Rn)   C_BORLO, C_HORLO, C_UOREG, C_SOREG
+	C_U5ORLO1 // u5<<1(Rn)   C_HORLO, C_UOREG, C_SOREG
+	C_U5ORLO  // u5(Rn)      C_BORLO, C_UOREG, C_SOREG
+	C_U0OPC   // (R15)       C_WOPC,  C_SOPC,  C_UOREG, C_SOREG, C_FOREG, ZOREG
+	C_U6OPC2  // u6<<2(R15)  C_WOPC,  C_SOPC,  C_UOREG, C_SOREG, C_FOREG
+	C_U8OPC2  // u8<<2(R15)  C_WOPC,  C_SOPC,  C_UOREG, C_FOREG
+	C_U12OPC  // u12(R15)    C_SOPC,  C_UOREG
+	C_S8OPC   // -u8(R15)    C_SOPC,  C_SOREG
+	C_S8OPC2  // -u8<<2(R15) C_SOPC,  C_FOREG
+	C_S12OPC  // -u12(R15)   C_SOPC
+	C_U0OSP   // (R13)       C_WOSP,  C_UOREG, C_SOREG, C_FOREG, ZOSP, ZOREG
+	C_U6OSP2  // u6<<2(R13)  C_WOSP,  C_UOREG, C_SOREG, C_FOREG
+	C_U8OSP2  // u8<<2(R13)  C_WOSP,  C_UOREG, C_FOREG
+	C_U0OREG  // (Rn)        C_UOREG, C_SOREG, C_FOREG, ZOREG
+	C_U6OREG2 // u6<<2(Rn)   C_UOREG, C_SOREG, C_FOREG
+	C_U8OREG2 // u8<<2(Rn)   C_UOREG, C_FOREG
+	C_U8OREG  // u8(Rn)      C_UOREG, C_SOREG
+	C_U12OREG // u12(Rn)     C_UOREG
+	C_S6OREG2 // -u6<<2(Rn)  C_SOREG, C_FOREG
+	C_S8OREG2 // -u8<<2(Rn)  C_FOREG
+	C_S8OREG  // -u8(Rn)     C_SOREG
+
+	C_ROREG // (Rn)(Rm*x)
+	C_RORLO // (Rn)(Rm)
+
+	C_TEXTSIZE
+
+	C_GOK
+)
+
+//go:generate go run ../stringer.go -i $GOFILE -o anames.go -p thumb
+
+const (
+	AAND = obj.ABaseThumb + obj.A_ARCHSPECIFIC + iota
+	AEOR
+	ASUB
+	ARSB
+	AADD
+	AADC
+	ASBC
+	ATST
+	ATEQ
+	ACMP
+	ACMN
+	AORR
+	ABIC
+
+	AMVN
+	AORN
+
+	AMUL
+	ADIV
+	ADIVU
+
+	AMULL
+	AMULLU
+	AMULAL
+	AMULALU
+	AMULA
+	AMULS
+	AMULAWB
+	AMULAWT
+
+	ACLZ
+	AREV
+	AREV16
+	ARBIT
+	AREVSH
+	ASEL
+	ABFX
+	ABFXU
+	ABFC
+	ABFI
+
+	// Do not reorder or fragment the conditional branch
+	// opcodes, or the predication code will break.
+	ABEQ
+	ABNE
+	ABCS
+	ABHS
+	ABCC
+	ABLO
+	ABMI
+	ABPL
+	ABVS
+	ABVC
+	ABHI
+	ABLS
+	ABGE
+	ABLT
+	ABGT
+	ABLE
+	ACBZ
+	ACBNZ
+
+	ATBB
+	ATBH
+
+	// do not reorder or split AITxyz
+	AITTTT
+	AITTT
+	AITTTE
+	AITT
+	AITTET
+	AITTE
+	AITTEE
+	AIT
+	AITETT
+	AITET
+	AITETE
+	AITE
+	AITEET
+	AITEE
+	AITEEE
+
+	// do not reorder or split NOP-compatible opcodes
+	ANOP2
+	AYIELD
+	AWFE
+	AWFI
+	ASEV
+
+	ACPSID
+	ACPSIE
+
+	// do not reorder or split memory barrier opcodes
+	ADSB
+	ADMB
+	AISB
+
+	AMOVB
+	AMOVBU
+	AMOVH
+	AMOVHU
+	AMOVW
+	AMOVM
+	AMOVT
+
+	ACMPF
+	ACMPD
+
+	ASQRTF
+	ASQRTD
+	AMOVF
+	AMOVD
+
+	ASWI
+	ABKPT
+
+	ALDREX
+	ALDREXB
+	ALDREXH
+	ASTREX
+	ASTREXB
+	ASTREXH
+	ACLREX
+
+	ANOP4
+
+	// do not reorder shift opcodes
+	ASLL
+	ASRL
+	ASRA
+
+	// not implemented
+	AMMULA
+	AMMULS
+	AMULABB
+
+	AADDF
+	AADDD
+	ASUBF
+	ASUBD
+	AMULF
+	AMULD
+	AMULAF
+	AMULAD
+	AMULSF
+	AMULSD
+	ANEGF
+	ANEGD
+	ANMULF
+	ANMULD
+	ADIVF
+	ADIVD
+	AMOVWF
+	AMOVWD
+	AMOVFW
+	AMOVDW
+	AMOVFD
+	AMOVDF
+
+	AWORD
+	AHWORD
+
+	ALAST
+
+	// aliases
+	AB     = obj.AJMP  // B, BX
+	ABL    = obj.ACALL // BL, BLX
+	AMOVBS = AMOVB
+	AMOVHS = AMOVH
+)
+
+// scond byte
+const (
+	C_SCOND = (1 << 4) - 1
+	C_SBIT  = 1 << 4
+	C_PBIT  = 1 << 5
+	C_WBIT  = 1 << 6
+	C_FBIT  = 1 << 7 /* psr flags-only */
+	C_UBIT  = 1 << 7 /* up bit, unsigned bit */
+	C_IA    = C_UBIT
+	C_DB    = C_PBIT
+
+	// These constants are the ARM condition codes encodings,
+	// XORed with 14 so that C_SCOND_NONE has value 0,
+	// so that a zeroed Prog.scond means "always execute".
+	C_SCOND_XOR = 14
+
+	C_SCOND_EQ   = 0 ^ C_SCOND_XOR
+	C_SCOND_NE   = 1 ^ C_SCOND_XOR
+	C_SCOND_HS   = 2 ^ C_SCOND_XOR // CS
+	C_SCOND_LO   = 3 ^ C_SCOND_XOR // CC
+	C_SCOND_MI   = 4 ^ C_SCOND_XOR
+	C_SCOND_PL   = 5 ^ C_SCOND_XOR
+	C_SCOND_VS   = 6 ^ C_SCOND_XOR
+	C_SCOND_VC   = 7 ^ C_SCOND_XOR
+	C_SCOND_HI   = 8 ^ C_SCOND_XOR
+	C_SCOND_LS   = 9 ^ C_SCOND_XOR
+	C_SCOND_GE   = 10 ^ C_SCOND_XOR
+	C_SCOND_LT   = 11 ^ C_SCOND_XOR
+	C_SCOND_GT   = 12 ^ C_SCOND_XOR
+	C_SCOND_LE   = 13 ^ C_SCOND_XOR
+	C_SCOND_NONE = 14 ^ C_SCOND_XOR
+	C_SCOND_NV   = 15 ^ C_SCOND_XOR
+
+	/* D_SHIFT type */
+	SHIFT_LL = 0 << 5
+	SHIFT_LR = 1 << 5
+	SHIFT_AR = 2 << 5
+	SHIFT_RR = 3 << 5
+)
+
+// http://infocenter.arm.com/help/topic/com.arm.doc.ihi0040b/IHI0040B_aadwarf.pdf
+var ARMDWARFRegisters = map[int16]int16{}
+
+func init() {
+	// f assigns dwarfregisters[from:to] = (base):(step*(to-from)+base)
+	f := func(from, to, base, step int16) {
+		for r := int16(from); r <= to; r++ {
+			ARMDWARFRegisters[r] = step*(r-from) + base
+		}
+	}
+	f(REG_R0, REG_R15, 0, 1)
+	f(REG_F0, REG_F15, 64, 2) // Use d0 through D15, aka S0, S2, ..., S30
+}
diff --git a/src/cmd/internal/obj/thumb/aclass_string.go b/src/cmd/internal/obj/thumb/aclass_string.go
new file mode 100644
index 0000000000..a77381a32f
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/aclass_string.go
@@ -0,0 +1,16 @@
+// Code generated by "stringer -type Aclass"; DO NOT EDIT.
+
+package thumb
+
+import "strconv"
+
+const _Aclass_name = "C_NONEC_RLOC_SPC_PCC_REGC_SHIFTILOC_SHIFTIC_REGREGC_REGREG2C_LISTLOC_LISTLOLRC_LISTLOPCC_LISTC_SHIFTRC_SHIFTRLOC_FREGC_FCRC_SPECC_MBC_ITC_ZCONC_U1CON2C_U6CON2C_U8CON1_4C_U8CON5_8C_U7CON2C_U8CON2C_U3CONC_U8CONC_U12CONC_U16CONC_E32CONC_LCONC_ZFCONC_SFCONC_LFCONC_U6BRAC_S8BRAC_S11BRAC_S20BRAC_S24BRAC_BORLOC_HORLOC_WORLOC_WOSPC_WOPCC_UOREGC_SOREGC_SOPCC_FOREGC_ZORLOC_ZOSPC_ZOREGC_LOREGC_LORLOC_U0ORLOC_U3ORLO2C_U4ORLO2C_U5ORLO2C_U4ORLO1C_U5ORLO1C_U5ORLOC_U0OPCC_U6OPC2C_U8OPC2C_U12OPCC_S8OPCC_S8OPC2C_S12OPCC_U0OSPC_U6OSP2C_U8OSP2C_U0OREGC_U6OREG2C_U8OREG2C_U8OREGC_U12OREGC_S6OREG2C_S8OREG2C_S8OREGC_ROREGC_RORLOC_TEXTSIZEC_GOK"
+
+var _Aclass_index = [...]uint16{0, 6, 11, 15, 19, 24, 34, 42, 50, 59, 67, 77, 87, 93, 101, 111, 117, 122, 128, 132, 136, 142, 150, 158, 168, 178, 186, 194, 201, 208, 216, 224, 232, 238, 245, 252, 259, 266, 273, 281, 289, 297, 304, 311, 318, 324, 330, 337, 344, 350, 357, 364, 370, 377, 384, 391, 399, 408, 417, 426, 435, 444, 452, 459, 467, 475, 483, 490, 498, 506, 513, 521, 529, 537, 546, 555, 563, 572, 581, 590, 598, 605, 612, 622, 627}
+
+func (i Aclass) String() string {
+	if i >= Aclass(len(_Aclass_index)-1) {
+		return "Aclass(" + strconv.FormatInt(int64(i), 10) + ")"
+	}
+	return _Aclass_name[_Aclass_index[i]:_Aclass_index[i+1]]
+}
diff --git a/src/cmd/internal/obj/thumb/anames.go b/src/cmd/internal/obj/thumb/anames.go
new file mode 100644
index 0000000000..b4f7c0f4c3
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/anames.go
@@ -0,0 +1,143 @@
+// Code generated by stringer -i a.out.go -o anames.go -p thumb; DO NOT EDIT.
+
+package thumb
+
+import "cmd/internal/obj"
+
+var Anames = []string{
+	obj.A_ARCHSPECIFIC: "AND",
+	"EOR",
+	"SUB",
+	"RSB",
+	"ADD",
+	"ADC",
+	"SBC",
+	"TST",
+	"TEQ",
+	"CMP",
+	"CMN",
+	"ORR",
+	"BIC",
+	"MVN",
+	"ORN",
+	"MUL",
+	"DIV",
+	"DIVU",
+	"MULL",
+	"MULLU",
+	"MULAL",
+	"MULALU",
+	"MULA",
+	"MULS",
+	"MULAWB",
+	"MULAWT",
+	"CLZ",
+	"REV",
+	"REV16",
+	"RBIT",
+	"REVSH",
+	"SEL",
+	"BFX",
+	"BFXU",
+	"BFC",
+	"BFI",
+	"BEQ",
+	"BNE",
+	"BCS",
+	"BHS",
+	"BCC",
+	"BLO",
+	"BMI",
+	"BPL",
+	"BVS",
+	"BVC",
+	"BHI",
+	"BLS",
+	"BGE",
+	"BLT",
+	"BGT",
+	"BLE",
+	"CBZ",
+	"CBNZ",
+	"TBB",
+	"TBH",
+	"ITTTT",
+	"ITTT",
+	"ITTTE",
+	"ITT",
+	"ITTET",
+	"ITTE",
+	"ITTEE",
+	"IT",
+	"ITETT",
+	"ITET",
+	"ITETE",
+	"ITE",
+	"ITEET",
+	"ITEE",
+	"ITEEE",
+	"NOP2",
+	"YIELD",
+	"WFE",
+	"WFI",
+	"SEV",
+	"CPSID",
+	"CPSIE",
+	"DSB",
+	"DMB",
+	"ISB",
+	"MOVB",
+	"MOVBU",
+	"MOVH",
+	"MOVHU",
+	"MOVW",
+	"MOVM",
+	"MOVT",
+	"CMPF",
+	"CMPD",
+	"SQRTF",
+	"SQRTD",
+	"MOVF",
+	"MOVD",
+	"SWI",
+	"BKPT",
+	"LDREX",
+	"LDREXB",
+	"LDREXH",
+	"STREX",
+	"STREXB",
+	"STREXH",
+	"CLREX",
+	"NOP4",
+	"SLL",
+	"SRL",
+	"SRA",
+	"MMULA",
+	"MMULS",
+	"MULABB",
+	"ADDF",
+	"ADDD",
+	"SUBF",
+	"SUBD",
+	"MULF",
+	"MULD",
+	"MULAF",
+	"MULAD",
+	"MULSF",
+	"MULSD",
+	"NEGF",
+	"NEGD",
+	"NMULF",
+	"NMULD",
+	"DIVF",
+	"DIVD",
+	"MOVWF",
+	"MOVWD",
+	"MOVFW",
+	"MOVDW",
+	"MOVFD",
+	"MOVDF",
+	"WORD",
+	"HWORD",
+	"LAST",
+}
diff --git a/src/cmd/internal/obj/thumb/asm.go b/src/cmd/internal/obj/thumb/asm.go
new file mode 100644
index 0000000000..a08160cea2
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/asm.go
@@ -0,0 +1,1160 @@
+// Inferno utils/5l/span.c
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5l/span.c
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"fmt"
+	"math"
+
+	"cmd/internal/obj"
+	"cmd/internal/objabi"
+)
+
+type Ctx struct {
+	ctxt     *obj.Link
+	newprog  obj.ProgAlloc
+	cursym   *obj.LSym
+	blitrl   *obj.Prog
+	elitrl   *obj.Prog
+	it       *obj.Prog
+	beforeit *obj.Prog
+	autosize int // frame size: auto + ret
+	pool     struct {
+		start int
+		size  int
+	}
+}
+
+func regclass(r int16) Aclass {
+	if REG_R0 <= r && r <= REG_R7 {
+		return C_RLO
+	}
+	if REG_R0 <= r && r <= REG_R15 {
+		switch r {
+		case REGSP:
+			return C_SP
+		case REGPC:
+			return C_PC
+		}
+		return C_REG
+	}
+	if REG_F0 <= r && r <= REG_F15 {
+		return C_FREG
+	}
+	if REG_APSR <= r && r <= REG_CONTROL {
+		return C_SPEC
+	}
+	if r == REG_FPSCR {
+		return C_FCR
+	}
+	if REG_MB_OSHST <= r && r <= REG_MB_SY {
+		return C_MB
+	}
+	if REG_EQ <= r && r <= REG_AL {
+		return C_IT
+	}
+	return C_GOK
+}
+
+func rol(x uint32, k int) uint32 {
+	m := uint(k) & 31
+	return x<<m | x>>(32-m)
+}
+
+func mic(u uint32) bool {
+	if u>>8 == 0 {
+		return true // 00000000_00000000_00000000_abcdefgh
+	}
+	for n := 31; n >= 8; n-- {
+		if rol(u, n)&^0xFF == 0 {
+			return true // rotated 00000000_00000000_00000000_1bcdefgh
+		}
+	}
+	if hi, lo := u>>16, u&0xFFFF; hi == lo {
+		hi = lo >> 8
+		lo &= 0xFF
+		switch {
+		case hi == 0:
+			return true // 00000000_abcdefgh_00000000_abcdefgh
+		case lo == 0:
+			return true // abcdefgh_00000000_abcdefgh_00000000
+		case hi == lo:
+			return true // abcdefgh_abcdefgh_abcdefgh_abcdefgh
+		}
+	}
+	return false
+}
+
+func conclass(u uint32) Aclass {
+	if u&3 == 0 {
+		switch {
+		case u == 0:
+			return C_ZCON
+		case u < 1<<3:
+			return C_U1CON2
+		case u < 1<<8:
+			return C_U6CON2
+		case u < 1<<9:
+			return C_U7CON2
+		case u < 1<<10:
+			return C_U8CON2
+		}
+	} else {
+		switch {
+		case u < 1<<3:
+			return C_U3CON
+		case u < 1<<8:
+			return C_U8CON
+		}
+	}
+	for n := uint(1); n <= 8; n++ {
+		if u&(1<<n-1) == 0 && u>>n <= 0xFF {
+			if n <= 4 {
+				return C_U8CON1_4
+			}
+			return C_U8CON5_8
+		}
+	}
+	switch {
+	case u < 1<<12:
+		return C_U12CON
+	case u < 1<<16:
+		return C_U16CON
+	case mic(u):
+		return C_E32CON
+	}
+	return C_LCON
+}
+
+func (c *Ctx) offset(a *obj.Addr) int64 {
+	switch a.Name {
+	case obj.NAME_PARAM:
+		return a.Offset + int64(c.autosize+prasize)
+	case obj.NAME_AUTO:
+		return a.Offset + int64(c.autosize)
+	}
+	return a.Offset
+}
+
+func (c *Ctx) chipzero(e float64) int {
+	switch objabi.GOARM {
+	case 0x7F, 0x7D: // ARMv7-M with floating point extension
+		return 0
+	}
+	return -1
+}
+
+func (c *Ctx) chipfloat(e float64) int {
+	switch objabi.GOARM {
+	case 0x7F, 0x7D: // ARMv7-M with floating point extension
+		break
+	default:
+		return -1
+	}
+	ei := math.Float64bits(e)
+	l := uint32(ei)
+	h := uint32(ei >> 32)
+
+	if l != 0 || h&0xffff != 0 {
+		return -1
+	}
+	h1 := h & 0x7fc00000
+	if h1 != 0x40000000 && h1 != 0x3fc00000 {
+		return -1
+	}
+	n := 0
+
+	// sign bit (a)
+	if h&0x80000000 != 0 {
+		n |= 1 << 7
+	}
+
+	// exp sign bit (b)
+	if h1 == 0x3fc00000 {
+		n |= 1 << 6
+	}
+
+	// rest of exp and mantissa (cd-efgh)
+	n |= int((h >> 16) & 0x3f)
+
+	//print("match %.8lux %.8lux %d\n", l, h, n);
+	return n
+}
+
+func (c *Ctx) aclass(a *obj.Addr, as *obj.As) Aclass {
+	switch a.Type {
+	case obj.TYPE_NONE:
+		return C_NONE
+
+	case obj.TYPE_REG:
+		return regclass(a.Reg)
+
+	case obj.TYPE_REGREG:
+		return C_REGREG
+
+	case obj.TYPE_REGREG2:
+		return C_REGREG2
+
+	case obj.TYPE_REGLIST:
+		if a.Offset&^0xC0FF == 0 {
+			switch a.Offset >> 12 {
+			case 0:
+				return C_LISTLO
+			case 4:
+				return C_LISTLOLR
+			case 8:
+				return C_LISTLOPC
+			}
+		}
+		return C_LIST
+
+	case obj.TYPE_SHIFT:
+		if a.Reg == 0 {
+			rlo := a.Offset&15 <= 7
+			if a.Offset>>4&1 == 0 {
+				// R>>i
+				if rlo && a.Offset>>5&3 != 3 {
+					// rlo and not @>
+					return C_SHIFTILO
+				}
+				return C_SHIFTI
+			} else {
+				// R>>R
+				if rlo && a.Offset>>8&15 <= 7 {
+					return C_SHIFTRLO
+				}
+				return C_SHIFTR
+			}
+		}
+
+	case obj.TYPE_MEM:
+		switch a.Name {
+		case obj.NAME_STATIC, obj.NAME_EXTERN:
+			if a.Sym == nil || a.Sym.Name == "" {
+				return C_GOK
+			}
+			return C_LORLO // MOVW litoffset(PC), R7 and use (R7) as address
+
+		case obj.NAME_NONE, obj.NAME_AUTO, obj.NAME_PARAM:
+			if a.Index != 0 {
+				if a.Reg <= REG_R7 && a.Index <= REG_R7 && uint(a.Scale) <= 1 {
+					return C_RORLO
+				}
+				return C_ROREG
+			}
+			if a.Name == obj.NAME_AUTO || a.Name == obj.NAME_PARAM {
+				a.Reg = REGSP
+			}
+			offset := c.offset(a)
+			switch {
+			case offset > 0:
+				switch {
+				case a.Reg <= REG_R7:
+					switch {
+					case offset&3 != 0:
+						break
+					case offset < 1<<5:
+						return C_U3ORLO2
+					case offset < 1<<6:
+						return C_U4ORLO2
+					case offset < 1<<7:
+						return C_U5ORLO2
+					}
+					switch {
+					case offset&1 != 0:
+						break
+					case offset < 1<<5:
+						return C_U4ORLO1
+					case offset < 1<<6:
+						return C_U5ORLO1
+					}
+					if offset < 1<<5 {
+						return C_U5ORLO
+					}
+				case a.Reg == REGSP:
+					switch {
+					case offset&3 != 0:
+						break
+					case offset < 1<<8:
+						return C_U6OSP2
+					case offset < 1<<10:
+						return C_U8OSP2
+					}
+				case a.Reg == REGPC:
+					switch {
+					case offset&3 != 0:
+						break
+					case offset < 1<<8:
+						return C_U6OPC2
+					case offset < 1<<10:
+						return C_U8OPC2
+					}
+					if offset < 1<<12 {
+						return C_U12OPC
+					}
+					return C_LOREG // there is no wider class than U12
+				}
+				switch {
+				case offset&3 != 0:
+					break
+				case offset < 1<<8:
+					return C_U6OREG2
+				case offset < 1<<10:
+					return C_U8OREG2
+				}
+				switch {
+				case offset < 1<<8:
+					return C_U8OREG
+				case offset < 1<<12:
+					return C_U12OREG
+				}
+			case offset == 0:
+				switch {
+				case a.Reg <= REG_R7:
+					return C_U0ORLO
+				case a.Reg == REG_R13:
+					return C_U0OSP
+				case a.Reg == REG_R15:
+					return C_U0OPC
+				}
+				return C_U0OREG
+			default: // offset < 0
+				offset = -offset
+				if a.Reg == REGPC {
+					switch {
+					case offset&3 != 0:
+						break
+					case offset < 1<<10:
+						return C_S8OPC2
+					}
+					switch {
+					case offset < 1<<8:
+						return C_S8OPC
+					case offset < 1<<12:
+						return C_S12OPC
+					}
+					return C_LOREG // there is no wider class than S12
+				}
+				switch {
+				case offset&3 != 0:
+					break
+				case offset < 1<<8:
+					return C_S6OREG2
+				case offset < 1<<10:
+					return C_S8OREG2
+				}
+				if offset < 1<<8 {
+					return C_S8OREG
+				}
+			}
+			if a.Reg <= REG_R7 {
+				return C_LORLO
+			}
+			return C_LOREG
+		}
+
+	case obj.TYPE_FCONST:
+		if c.chipzero(a.Val.(float64)) >= 0 {
+			return C_ZFCON
+		}
+		if c.chipfloat(a.Val.(float64)) >= 0 {
+			return C_SFCON
+		}
+		return C_LFCON
+
+	case obj.TYPE_TEXTSIZE:
+		return C_TEXTSIZE
+
+	case obj.TYPE_CONST:
+		if a.Offset > 4294967295 || a.Offset < -2147483648 {
+			return C_GOK // doesn't fit in uint32 or int32
+		}
+		con := uint32(a.Offset)
+		cc := conclass(con)
+		if as != nil && cc >= C_E32CON {
+			switch *as {
+			case AADD, ASUB, ACMP, ACMN:
+				con = -con
+				nc := conclass(con)
+				if nc >= cc {
+					break
+				}
+				cc = nc
+				a.Offset = int64(con)
+				switch *as {
+				case AADD:
+					*as = ASUB
+				case ASUB:
+					*as = AADD
+				case ACMP:
+					*as = ACMN
+				default: // ACMN
+					*as = ACMP
+				}
+			case AAND, ABIC, AORR, AORN, AMOVW, AMVN:
+				con = ^con
+				nc := conclass(con)
+				if nc >= cc {
+					break
+				}
+				cc = nc
+				a.Offset = int64(con)
+				switch *as {
+				case AAND:
+					*as = ABIC
+				case ABIC:
+					*as = AAND
+				case AORR:
+					*as = AORN
+				case AORN:
+					*as = AORR
+				case AMOVW:
+					*as = AMVN
+				default: // AMVN
+					*as = AMOVW
+				}
+			}
+		}
+		return cc
+
+	case obj.TYPE_ADDR:
+		switch a.Name {
+		case obj.NAME_STATIC, obj.NAME_EXTERN:
+			if a.Sym == nil || a.Sym.Name == "" {
+				return C_GOK
+			}
+			return C_LCON
+		}
+		// MOV with NAME_NONE, NAME_PARAM, NAME_AUTO is converted to ADD with TYPE_CONST
+
+	case obj.TYPE_BRANCH:
+		// can't determine branch length here so assume some tentative value
+		switch *as {
+		case AB, ABL:
+			return C_S24BRA
+		case ACBZ, ACBNZ:
+			return C_U6BRA
+		default: // Bcond
+			return C_S20BRA
+		}
+	}
+
+	return C_GOK
+}
+
+func itstate(p *obj.Prog) uint16 {
+	mask := int(p.As-AITTTT) + 1
+	// IT      mask = 0b1000
+	// IT T    mask = 0b0100
+	// IT E    mask = 0b1100
+	// IT TT   mask = 0b0010
+	// IT ET   mask = 0b1010
+	// IT TE   mask = 0b0110
+	// IT EE   mask = 0b1110
+	// IT TTT  mask = 0b0001
+	// IT ETT  mask = 0b1001
+	// IT TET  mask = 0b0101
+	// IT EET  mask = 0b1101
+	// IT TTE  mask = 0b0011
+	// IT ETE  mask = 0b1011
+	// IT TEE  mask = 0b0111
+	// IT EEE  mask = 0b1111
+	if fc0 := int(p.Scond) & 1; fc0 != 0 {
+		n := uint(0)
+		for mask>>n&1 == 0 {
+			n++ // count trailing zeros
+		}
+		for n++; n < 4; n++ {
+			mask ^= fc0 << n
+		}
+	}
+	return uint16(mask)
+}
+
+func (c *Ctx) itclose() {
+	c.it.As = AITTTT + obj.As(c.it.Mark) - 1
+	c.it.Mark = uint16(c.it.Scond<<4) | itstate(c.it)
+	c.it = nil
+}
+
+func (c *Ctx) oplook(p *obj.Prog) (ret *Optab) {
+	if k := int(p.Optab) - 1; k >= 0 {
+		return &oprange[p.As&obj.AMask][k]
+	}
+	a1 := Aclass(p.From.Class)
+	if a1 == 0 {
+		a1 = c.aclass(&p.From, &p.As) + 1
+		p.From.Class = int8(a1)
+	}
+	a1--
+	a3 := Aclass(p.To.Class)
+	if a3 == 0 {
+		a3 = c.aclass(&p.To, &p.As)
+		if a1 == C_SHIFTRLO && p.As == AMOVW && int(p.From.Offset)&7 == int(p.To.Reg)&7 {
+			switch a3 {
+			case C_RLO:
+				a3 = C_NONE // special case to generate 16bit `MOVW Rdn<v>Rm, Rdn`
+			case C_NONE:
+				a3 = C_GOK
+			}
+		}
+		a3++
+		p.To.Class = int8(a3)
+	}
+	a3--
+	a2 := C_NONE
+	if p.Reg != 0 {
+		a2 = regclass(p.Reg)
+	}
+
+	//fmt.Printf("\t%-12v %-12v %-12v %04b\n", a1, a2, a3, p.Scond>>4)
+
+	autoit := c.cursym.Func.Text.Mark&AUTOIT != 0
+	cond := p.Scond & 0xF
+again:
+	if autoit {
+		if c.it != nil {
+			if (cond^C_SCOND_XOR)>>1 != c.it.Scond>>1 {
+				// no condition or condition does not match the current it block
+				c.itclose()
+			}
+		}
+		if c.it == nil {
+			if cond != C_SCOND_NONE {
+				if p.As == AB && p.To.Type == obj.TYPE_BRANCH && p.To.Name == obj.NAME_NONE {
+					// convert `B.cond label` to `Bcond label`
+					switch cond {
+					case C_SCOND_EQ:
+						p.As = ABEQ
+					case C_SCOND_NE:
+						p.As = ABNE
+					case C_SCOND_HS:
+						p.As = ABHS
+					case C_SCOND_LO:
+						p.As = ABLO
+					case C_SCOND_MI:
+						p.As = ABMI
+					case C_SCOND_PL:
+						p.As = ABPL
+					case C_SCOND_VS:
+						p.As = ABVS
+					case C_SCOND_VC:
+						p.As = ABVC
+					case C_SCOND_HI:
+						p.As = ABHI
+					case C_SCOND_LS:
+						p.As = ABLS
+					case C_SCOND_GE:
+						p.As = ABGE
+					case C_SCOND_LT:
+						p.As = ABLT
+					case C_SCOND_GT:
+						p.As = ABGT
+					case C_SCOND_LE:
+						p.As = ABLE
+					default:
+						c.ctxt.Diag("invalid flags: %v", p)
+					}
+					p.Scond &^= 0xF
+					a3 = C_S20BRA
+					p.To.Class = int8(C_S20BRA + 1)
+				} else {
+					// insert IT by replacing p to avoid jumping in middle of IT block
+					// BUG: this only works for one instruction IT blocks (jump target check is need)
+					q := c.newprog()
+					*q = *p
+					p.Link = q
+					p.As = AIT
+					p.Spadj = 0
+					p.Scond = cond ^ C_SCOND_XOR
+					p.Mark = 0x10 // invalid (empty IT block)
+					p.Reg = 4     // number of free slots in IT block
+					p.Optab = 1
+					c.it = p
+					return &oprange[AIT&obj.AMask][0]
+				}
+			}
+		}
+	} else {
+		if c.it != nil {
+			switch {
+			case AITTTT <= p.As && p.As <= AITEEE:
+				c.ctxt.Diag("IT in previous IT block: %v", p)
+			case ABEQ <= p.As && p.As <= ACBNZ:
+				c.ctxt.Diag("not allowed in IT block: %v", p)
+			case c.it.RegTo2<<1&0xF != 0 && (p.As == AB || p.As == ABL):
+				// BUG: take into account other cases when PC is modified
+				c.ctxt.Diag("only allowed as the last instruction in IT block: %v", p)
+			case int16(cond^C_SCOND_XOR) != c.it.RegTo2>>4:
+				c.ctxt.Diag("condition code does not match IT code: %v", p)
+			}
+		} else if cond != 0 {
+			c.ctxt.Diag("condition code outisde IT block: %v", p)
+		}
+	}
+	spuw := p.Scond &^ 0xF
+	var spxor uint8
+	if spuw&(C_SBIT|C_PBIT) == 0 {
+		spxor = C_SBIT
+	} else if c.it != nil {
+		spxor = C_SBIT | C_PBIT
+	}
+	ops := oprange[p.As&obj.AMask]
+	for k := range ops {
+		op := &ops[k]
+		if op.flag&NOIT != 0 && c.it != nil {
+			continue
+		}
+		rscond := op.rscond
+		oscond := op.oscond
+		if rscond&C_SBIT != 0 {
+			// 16-bit data processing instruction that sets flags outside IT block
+			rscond ^= spxor
+			oscond ^= spxor
+		}
+		if spuw&rscond == rscond && spuw&oscond == spuw &&
+			(op.a2 == a2 || op.a2 == C_REG && (C_RLO <= a2 && a2 < C_REG)) &&
+			match(op.a1, a1) && match(op.a3, a3) {
+
+			p.Optab = uint16(k + 1)
+			/*fmt.Printf(
+				"\t%-12v %-12v %-12v %04b %04b\n",
+				op.a1, op.a2, op.a3, rscond>>4, oscond>>4,
+			)*/
+			ret = op
+			break
+		}
+	}
+	if ret == nil {
+		c.ctxt.Diag(
+			"illegal combination %v; %v %v %v; from %v %v; to %v %v",
+			p, a1, a2, a3, p.From.Type, p.From.Name, p.To.Type, p.To.Name,
+		)
+		return &Optab{as: obj.AUNDEF}
+	}
+	if autoit {
+		if c.it != nil {
+			n := int(ret.size) >> 4
+			if n > int(c.it.Reg) {
+				// can not fit n instructions in current it block
+				c.itclose()
+				p.Optab = 0
+				ret = nil
+				goto again
+			}
+			cond ^= C_SCOND_XOR
+			for ; n > 0; n-- {
+				c.it.Reg--
+				ito := uint16(1 << uint(c.it.Reg))
+				if cond&1 == c.it.Scond&1 {
+					c.it.Mark -= ito
+				} else {
+					c.it.Mark += ito
+				}
+			}
+			if c.it.Reg == 0 || p.As == AB || p.As == ABL {
+				c.itclose()
+			}
+		}
+	} else {
+		if AITTTT <= p.As && p.As <= AITEEE {
+			p.Scond = uint8(p.From.Reg - REG_EQ)
+			p.Mark = itstate(p)
+			p.RegTo2 = int16(int(p.Scond)<<4 | int(p.Mark))
+			c.it = p
+		} else if c.it != nil {
+			c.it.RegTo2 = c.it.RegTo2&^0x1F | c.it.RegTo2<<(ret.size>>4)&0x1F
+			if c.it.RegTo2&0xF == 0 {
+				c.it = nil
+			}
+		}
+	}
+	return ret
+}
+
+func (c *Ctx) brlook(p *obj.Prog) *Optab {
+	v := (p.Pcond.Pc - p.Pc - 4) >> 1
+	var a3 Aclass
+	switch {
+	case 0 <= v && v < 1<<6:
+		a3 = C_U6BRA
+	case -1<<7 <= v && v < 1<<7:
+		a3 = C_S8BRA
+	case -1<<10 <= v && v < 1<<10:
+		a3 = C_S11BRA
+	case -1<<19 <= v && v < 1<<19:
+		a3 = C_S20BRA
+	case -1<<23 <= v && v < 1<<23:
+		a3 = C_S24BRA
+	default:
+		c.ctxt.Diag("branch too long: %d %v", v*2, p)
+		return &Optab{as: obj.AUNDEF}
+	}
+	p.To.Class = int8(a3) + 1
+	ops := oprange[p.As&obj.AMask]
+	for k := range ops {
+		op := &ops[k]
+		if a3 <= op.a3 && op.a3 <= C_S24BRA {
+			p.Optab = uint16(k + 1)
+			return op
+		}
+	}
+	c.ctxt.Diag("illegal branch combination %v; %v; to %v %v", p, a3, p.To.Type, p.To.Name)
+	return &Optab{as: obj.AUNDEF}
+}
+
+func debug(p *obj.Prog) {
+	fmt.Printf("\n%v\n", p)
+	if p.Reg != 0 {
+		fmt.Printf(
+			"\t%02d %-5v %v(%v), R%d, %v(%v)\n\n",
+			p.Pc, p.As, p.From.Type, p.From.Name, p.Reg-obj.RBaseThumb, p.To.Type, p.To.Name,
+		)
+	} else {
+		fmt.Printf(
+			"\t%02d %-5v %v(%v), %v(%v)\n\n",
+			p.Pc, p.As, p.From.Type, p.From.Name, p.To.Type, p.To.Name,
+		)
+	}
+}
+
+func (c *Ctx) litoffset(p *obj.Prog, reglo bool, pcdiff int) (offset int, short bool) {
+	v := int(p.Pcond.Pc - p.Pc&^3 - 4)
+	if v > 0 {
+		v += pcdiff // forward literal
+	}
+	if v <= -1<<12 || 1<<12 <= v {
+		c.ctxt.Diag("|literal offset| >= 1<<12: %d %v", v, p)
+		return 0, false
+	}
+	return v, reglo && uint(v) < 1<<10
+}
+
+func span(ctxt *obj.Link, cursym *obj.LSym, newprog obj.ProgAlloc) {
+	p := cursym.Func.Text
+	if p == nil || p.Link == nil {
+		return // external functions or ELF section symbol
+	}
+	if oprange[AAND&obj.AMask] == nil {
+		ctxt.Diag("thumb ops not initialized, call thumb.buildop first")
+	}
+	c := &Ctx{ctxt: ctxt, newprog: newprog, cursym: cursym, autosize: int(p.To.Offset) + rasize}
+	p.Pc = 0
+	pc := 0
+	dbg := false
+	if false && p.From.Sym.Name == "\"\".main" {
+		dbg = true
+		fmt.Println("-->", p)
+	}
+	for p.Link != nil {
+		prev := p
+		p = p.Link
+		if p.As == AWORD && pc&3 != 0 {
+			pc += 2
+		}
+		p.Pc = int64(pc)
+		if dbg {
+			debug(p)
+		}
+		o := c.oplook(p)
+		if o.as == AIT {
+			c.beforeit = prev
+		}
+		switch o.flag & (LFROM | LTO) {
+		case LFROM:
+			c.addpool(p, &p.From, pc)
+		case LTO:
+			c.addpool(p, &p.To, pc)
+		}
+		pc += int(o.size) & 0xF
+		if c.blitrl != nil {
+			if p.Link == nil {
+				p, pc = c.flushpool(p, pc, false)
+			} else {
+				p, pc = c.checkpool(p, pc)
+				if dbg && p != prev.Link {
+					if c.it == nil {
+						fmt.Printf("\tpool flushed\n")
+					} else {
+						fmt.Printf("\tpool flushed before last IT - restarting\n")
+					}
+				}
+			}
+		}
+	}
+
+	// determine exect instruction sizes
+	// TODO: jump across functions needs reloc and always uses 32-bit instruction
+
+	for {
+		p = c.cursym.Func.Text
+		pc = 0
+		changed := false
+		for p.Link != nil {
+			p = p.Link
+			if p.As == AWORD && pc&3 != 0 {
+				pc += 2
+			}
+			pcdiff := int(int64(pc) - p.Pc)
+			if pcdiff != 0 {
+				changed = true
+				p.Pc = int64(pc)
+			}
+			var o *Optab
+			var m int
+			if p.To.Type == obj.TYPE_BRANCH && p.Pcond != nil && p.To.Sym == nil {
+				o = c.brlook(p)
+				m = int(o.size) & 0xF
+			} else {
+				o = c.oplook(p)
+				m = int(o.size) & 0xF
+				if o.flag&(LFROM|LTO) != 0 {
+					// in most cases the literal is loaded into R7 (REGTMP)
+					loreg := true
+					// but in the following cases the literal is loaded straight into destination register
+					if p.As == AMOVW || p.As == AMVN {
+						if p.From.Type == obj.TYPE_CONST || p.From.Type == obj.TYPE_ADDR &&
+							(p.From.Name == obj.NAME_STATIC || p.From.Name == obj.NAME_EXTERN) {
+							loreg = p.To.Reg <= REG_R7
+						}
+					}
+					if _, short := c.litoffset(p, loreg, pcdiff); short {
+						// optab assumees 32-bit load literal instruction;
+						// here we are correcting this assumption
+						m -= 2
+						//fmt.Println(-2, loreg)
+					}
+				}
+			}
+			//fmt.Println(m, p)
+			pc += m
+		}
+		if !changed {
+			break
+		}
+	}
+	c.cursym.Size = int64(pc)
+
+	// lay out the code
+
+	c.cursym.Grow(c.cursym.Size)
+	bp := c.cursym.P
+	var out [8]uint16
+	p = c.cursym.Func.Text
+	pc = int(p.Pc)
+	for p.Link != nil {
+		p = p.Link
+		var o *Optab
+		if p.To.Type == obj.TYPE_BRANCH && p.Pcond != nil {
+			o = c.brlook(p)
+		} else {
+			o = c.oplook(p)
+		}
+		if o.asmout == nil {
+			continue
+		}
+		m := o.asmout(c, p, out[:])
+		if m == 0 {
+			return
+		}
+		if int64(pc) > p.Pc {
+			ctxt.Diag("PC padding invalid: want %#d, has %#d: %v", p.Pc, pc, p)
+			return
+		}
+		for int64(pc) != p.Pc {
+			// emit NOP
+			bp[1] = 0xBF
+			bp[0] = 0x00
+			bp = bp[2:]
+			pc += 2
+		}
+		for _, v := range out[:m/2] {
+			bp[0] = byte(v)
+			bp[1] = byte(v >> 8)
+			bp = bp[2:]
+		}
+		pc += m
+		// unset base register in case of SP/FP for better printing
+		var a *obj.Addr
+		if p.From.Type == obj.TYPE_MEM || p.From.Type == obj.TYPE_ADDR {
+			a = &p.From
+		} else if p.To.Type == obj.TYPE_MEM || p.To.Type == obj.TYPE_ADDR {
+			a = &p.To
+		} else {
+			continue
+		}
+		if a.Reg == REGSP && (a.Name == obj.NAME_AUTO || a.Name == obj.NAME_PARAM) {
+			a.Reg = obj.REG_NONE
+		}
+	}
+}
+
+func (c *Ctx) addpool(p *obj.Prog, a *obj.Addr, pc int) {
+	t := c.newprog()
+	t.As = AWORD
+	switch a.Name {
+	case obj.NAME_EXTERN, obj.NAME_STATIC:
+		t.To.Type = a.Type
+		t.To.Name = a.Name
+		t.To.Sym = a.Sym
+		t.To.Offset = a.Offset
+	default:
+		t.To.Type = obj.TYPE_CONST
+		lit := c.offset(a)
+		if p.As == AMVN {
+			lit = ^lit // MVN lit, Rd is implemented as MOVW litoffset(PC), Rd
+		}
+		t.To.Offset = lit
+	}
+	for q := c.blitrl; q != nil; q = q.Link {
+		if q.Rel == nil && q.To == t.To {
+			p.Pcond = q
+			return
+		}
+	}
+	if c.blitrl == nil {
+		c.blitrl = t
+		c.pool.start = pc
+	} else {
+		c.elitrl.Link = t
+	}
+	c.elitrl = t
+	c.pool.size += 4
+	p.Pcond = t // store the link to the pool entry in Pcond
+}
+
+func (c *Ctx) flushpool(p *obj.Prog, pc int, skip bool) (*obj.Prog, int) {
+	if skip {
+		q := c.newprog()
+		q.As = AB
+		q.To.Type = obj.TYPE_BRANCH
+		q.Pcond = p.Link
+		q.Pos = p.Pos
+		q.Link = c.blitrl
+		c.blitrl = q
+	}
+	if pc&3 != 0 {
+		pc += 2
+	}
+	for q := c.blitrl; q != nil; q = q.Link {
+		q.Pos = p.Pos // the line number of the preceding instruction (no deltas in the pcln)
+		q.Pc = int64(pc)
+		pc += 4
+	}
+	c.pool.size = 0
+	c.pool.start = 0
+	c.elitrl.Link = p.Link
+	p.Link = c.blitrl
+	c.blitrl = nil // BUG: should refer back to values until out-of-range
+	return c.elitrl, pc
+}
+
+func (c *Ctx) checkpool(p *obj.Prog, pc int) (*obj.Prog, int) {
+	poolLast := pc
+	skip := !(p.As == AB && p.Scond&0xF == 0)
+	if skip {
+		poolLast += 4 // the AB instruction to jump around the pool
+	}
+	poolLast += c.pool.size - 4 // the offset of the last pool entry
+	refpc := c.pool.start       // PC of the first pool reference
+	v := poolLast - refpc - 4   // PC-relative offset
+	if v >= 0xFF0 {
+		goto flush
+	}
+	if !skip && v >= 0xFD0 {
+		if q := p.Link.Link; q != nil && q.Link != nil {
+			goto flush
+		}
+	}
+	return p, pc
+flush:
+	if c.it == nil {
+		return c.flushpool(p, pc, skip)
+	}
+	// cannot flush literal pool inside IT block
+	q := c.beforeit
+	skip = !(q.As == AB && q.Scond&0xF == 0)
+	return c.flushpool(q, int(c.it.Pc), skip)
+}
+
+var xcmporeg = [...][14]byte{
+	//B H  W  W  W  U  S  S  F  Z  Z  Z  L  L
+	//O O  O  O  O  O  O  O  O  O  O  O  O  O
+	//R R  R  S  P  R  R  P  R  R  S  R  R  R
+	//L L  L  P  C  E  E  C  E  L  P  E  E  L
+	//O O  O        G  G     G  O     G  G  O
+
+	{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1}, // C_LORLO
+	{1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1}, // C_U0ORLO
+	{1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1}, // C_U3ORLO2
+	{0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1}, // C_U4ORLO2
+	{0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1}, // C_U5ORLO2
+	{1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1}, // C_U4ORLO1
+	{0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1}, // C_U5ORLO1
+	{1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1}, // C_U5ORLO
+	{0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0}, // C_U0OPC
+	{0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0}, // C_U6OPC2
+	{0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0}, // C_U8OPC2
+	{0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0}, // C_U12OPC
+	{0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0}, // C_S8OPC
+	{0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0}, // C_S8OPC2
+	{0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0}, // C_S12OPC
+	{0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0}, // C_U0OSP
+	{0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0}, // C_U6OSP2
+	{0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0}, // C_U8OSP2
+	{0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0}, // C_U0OREG
+	{0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0}, // C_U6OREG2
+	{0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0}, // C_U8OREG2
+	{0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0}, // C_U8OREG
+	{0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0}, // C_U12OREG
+	{0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0}, // C_S6OREG2
+	{0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0}, // C_S8OREG2
+	{0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0}, // C_S8OREG
+}
+
+var xcmpcon = [...][8]byte{
+	//U U  U  U  U  U  E  L
+	//7 8  3  8  1  1  3
+	//<<<<       2  6  2
+	//2 2
+
+	{1, 1, 1, 1, 1, 1, 1, 1}, // C_ZCON
+	{1, 1, 1, 1, 1, 1, 1, 1}, // C_U1CON2
+	{1, 1, 0, 1, 1, 1, 1, 1}, // C_U6CON2
+	{0, 0, 0, 0, 1, 1, 1, 1}, // C_U8CON1_4 // not contain C_U8CON2
+	{0, 0, 0, 0, 0, 1, 1, 1}, // C_U8CON5_8
+	{1, 1, 0, 0, 1, 1, 1, 1}, // C_U7CON2
+	{0, 1, 0, 0, 1, 1, 1, 1}, // C_U8CON2
+	{0, 0, 1, 1, 1, 1, 1, 1}, // C_U3CON
+	{0, 0, 0, 1, 1, 1, 1, 1}, // C_U8CON
+	{0, 0, 0, 0, 1, 1, 0, 1}, // C_U12CON
+	{0, 0, 0, 0, 0, 1, 0, 1}, // C_U16CON
+	{0, 0, 0, 0, 0, 0, 1, 1}, // C_E32CON
+}
+
+var xcmpreg = [...][6]byte{
+	//R S  P  R  S  S
+	//L P  C  E  I  I
+	//O       G  L
+
+	{1, 0, 0, 1, 1, 1}, // C_RLO
+	{0, 1, 0, 1, 0, 1}, // C_SP
+	{0, 0, 1, 1, 0, 1}, // C_PC
+	{0, 0, 0, 1, 0, 1}, // C_REG
+	{0, 0, 0, 0, 1, 1}, // C_SIFTILO
+}
+
+func match(op, code Aclass) bool {
+	if op == code {
+		return true
+	}
+	switch {
+	case C_RLO <= code && code <= C_SHIFTILO:
+		if op < C_RLO || C_SHIFTI < op {
+			return false
+		}
+		return xcmpreg[code-C_RLO][op-C_RLO] != 0
+	case C_LORLO <= code && code <= C_S8OREG:
+		if op < C_BORLO || C_LORLO < op {
+			return false
+		}
+		return xcmporeg[code-C_LORLO][op-C_BORLO] != 0
+	case C_ZCON <= code && code <= C_E32CON:
+		if op < C_U7CON2 || C_LCON < op {
+			return false
+		}
+		return xcmpcon[code-C_ZCON][op-C_U7CON2] != 0
+	case op == C_ROREG && code == C_RORLO:
+		return true
+	case op == C_SHIFTR && code == C_SHIFTRLO:
+		return true
+	case op == C_LIST && (C_LISTLO <= code && code <= C_LISTLOPC):
+		return true
+	case (op == C_LISTLOLR || op == C_LISTLOPC) && code == C_LISTLO:
+		return true
+	}
+	return false
+}
+
+var oprange [ALAST & obj.AMask][]Optab
+
+var (
+	deferreturn *obj.LSym
+	symdiv      *obj.LSym
+	symdivu     *obj.LSym
+	symmod      *obj.LSym
+	symmodu     *obj.LSym
+)
+
+func buildop(ctxt *obj.Link) {
+	if oprange[AAND&obj.AMask] != nil {
+		// Already initialized; stop now. This happens in the
+		// cmd/asm tests, each of which re-initializes the arch.
+		return
+	}
+
+	deferreturn = ctxt.LookupABI("runtime.deferreturn", obj.ABIInternal)
+
+	symdiv = ctxt.Lookup("runtime._div")
+	symdivu = ctxt.Lookup("runtime._divu")
+	symmod = ctxt.Lookup("runtime._mod")
+	symmodu = ctxt.Lookup("runtime._modu")
+
+	for i := range optab {
+		oi := &optab[i]
+		if !(oi.size != 0 || oi.a1 != 0 || oi.a2 != 0 || oi.a3 != 0) {
+			continue
+		}
+		oi.oscond |= oi.rscond
+	}
+
+	for i := 0; i < len(optab); {
+		r := optab[i].as
+		if r == obj.AXXX {
+			continue
+		}
+		r0 := r & obj.AMask
+		start := i
+		for i++; i < len(optab) && optab[i].as == r; i++ {
+		}
+		opt := optab[start:i]
+		oprange[r0] = append(oprange[r0], opt...)
+		for i < len(optab) {
+			oi := &optab[i]
+			if oi.size != 0 || oi.a1 != 0 || oi.a2 != 0 || oi.a3 != 0 || oi.flag != 0 {
+				break
+			}
+			op := &oprange[oi.as&obj.AMask]
+			*op = append(*op, opt...)
+			i++
+		}
+	}
+}
diff --git a/src/cmd/internal/obj/thumb/asm.md b/src/cmd/internal/obj/thumb/asm.md
new file mode 100644
index 0000000000..2b4f682f56
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/asm.md
@@ -0,0 +1,34 @@
+The Thumb2 assembler is intended to generate code for ARMv7-M ISA. It can be used for other variants of ARMv7 ISA as long as the access to special registers or coprocessors is not need (however, you can always use WORD, HWORD to generate not supported instructions).
+
+For the rest of this document thumb means the Thumb2 assembler, arm means the ARM assembler.
+
+### Differences in relation to arm
+
+1. The lack of .S suffix does not mean that generated data processing instruction does not modify flags. In such case thumb generates shortest instruction encoding that may (but may not) modify flags. Use .P suffix to preserve flags.
+
+2. Thumb uses R7 as temporary register (REGTMP) to promote shortest encodings (arm uses R11).
+
+3. Thumb requires the hardware division. ARMv7-M ISA supports it, but in case of ARMv7-A the hardware division is optional.
+
+4. Thumb requires support for unalligned memory access by MOVW, MOVH instructions (ARMv7 supports it if ARMv7-A SCTLR.A=0, ARMv7-M CCR.UNALIGN_TRP=0).
+
+5. Thumb does not support .IB (increment before), .DA (decrement after) suffixes.
+
+### Thumb only features
+
+1. As long as a function has not any explicit IT instruction thumb automatically adds IT instructions before group of 1 to 4 instructions with condition suffixes to make them conditional. Any explicit IT instruction disables this auto-IT mode for the entrie function.
+
+2. In auto-IT mode thumb converts `B.cond label` to `Bcond label` (if not the last instruction in IT block).
+
+### Portable code
+
+To write portable code for arm and thumb:
+
+1. Check condition flags immediately after setting them, do not use .P to preserve flags.
+
+2. Be careful when using R7, R11 registers.
+
+3. Do not use IT instruction.
+
+4. Do not use thumb-only instructions like CBZ, CBNZ.
+
diff --git a/src/cmd/internal/obj/thumb/instr_encoding.md b/src/cmd/internal/obj/thumb/instr_encoding.md
new file mode 100644
index 0000000000..8e44386b89
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/instr_encoding.md
@@ -0,0 +1,43 @@
+# Thumb2 assembler, instruction encoding
+
+## Definitions
+
+### Register specifiers
+
+Rm - first register operand, Rn - second register operand, Rd - destination register, Rdm - combined first operand and destination register, Rdn - combined second operand and destination register, Rt - transfered register.
+
+### Shift type
+
+	vv  <v>  Descr
+	-------------------------
+	00  <<   logical left
+	01  >>   logical right
+	10  ->   arithmetic right
+	11  @>   rotate right (u5==0: rotate right with extend)
+	
+### Modified immediate constant (e32)
+
+    xxxx xexx xxxx xxxx  xeee xxxx eeee eeee
+          i               iii      abcd efgh
+    
+    iiiia  const
+    ------------------------------------------
+    0000x  00000000 00000000 00000000 abcdefgh
+    0001x  00000000 abcdefgh 00000000 abcdefgh
+    0010x  abcdefgh 00000000 abcdefgh 00000000
+    0011x  abcdefgh abcdefgh abcdefgh abcdefgh
+    01000  1bcdefgh 00000000 00000000 00000000
+    01001  01bcdefg h0000000 00000000 00000000
+    01010  001bcdef gh000000 00000000 00000000
+    .....  ........ ........ ........ ........
+    11111  00000000 00000000 00000001 bcdefgh0
+	
+## 16 and 32-bit instructions
+
+Bits [15:11] of the least significant half-word determine 16/32-bit instruction. 32-bit instructions have the following format:
+
+	1110 1xxx xxxx xxxx  xxxx xxxx xxxx xxxx
+	1111 0xxx xxxx xxxx  xxxx xxxx xxxx xxxx
+	1111 1xxx xxxx xxxx  xxxx xxxx xxxx xxxx
+	
+Almost all 16 and 32-bit instructions require condition suffix inside IT block. Most 16-bit instructions sets flags outside IT block.
\ No newline at end of file
diff --git a/src/cmd/internal/obj/thumb/instr_group.txt b/src/cmd/internal/obj/thumb/instr_group.txt
new file mode 100644
index 0000000000..bebbfa2797
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/instr_group.txt
@@ -0,0 +1,259 @@
+	0100 0100 dmmm mddd                       .ADD       Rm, Rdn         ; does not set flags
+	1010 1ddd uuuu uuuu                       .ADD       u8<<2, R13, Rd
+	1010 0ddd uuuu uuuu                       .ADD       u8<<2, R15, Rd
+
+	0001 100m mmnn nddd                       .ADD       Rm, Rn, Rd
+	0001 101m mmnn nddd                        SUB       Rm, Rn, Rd
+	1110 1011 000s nnnn  0uuu dddd uuvv mmmm  .ADD.s     Rm<v>u5, Rn, Rd
+	1110 1011 101s nnnn  0uuu dddd uuvv mmmm   SUB.s     Rm<v>u5, Rn, Rd
+	0001 110u uunn nddd                       .ADD       u3, Rn, Rd
+	0001 111u uunn nddd                        SUB       u3, Rn, Rd
+	0011 0ddd uuuu uuuu                       .ADD       u8, Rdn
+	0011 1ddd uuuu uuuu                        SUB       u8, Rdn
+	1011 0000 0uuu uuuu                       .ADD       u7<<2, R13
+	1011 0000 1uuu uuuu                        SUB       u7<<2, R13
+	1111 0u10 0000 nnnn  0uuu dddd uuuu uuuu  .ADD       u12, Rn, Rd
+	1111 0u10 1010 nnnn  0uuu dddd uuuu uuuu   SUB       u12, Rn, Rd
+	1111 0e01 000s nnnn  0eee dddd eeee eeee  .ADD.s     e32, Rn, Rd
+	1111 0e01 101s nnnn  0eee dddd eeee eeee   SUB.s     e32, Rn, Rd
+
+	0100 0000 00mm mddd                       .AND       Rm, Rdn
+	0100 0001 01mm mddd                        ADC       Rm, Rdn
+	0100 0011 10mm mddd                        BIC       Rm, Rdn
+	0100 0000 01mm mddd                        EOR       Rm, Rdn
+	0100 0011 00mm mddd                        ORR       Rm, Rdn
+	0100 0001 10mm mddd                        SBC       Rm, Rdn
+	0100 0011 01mm mddd                        MUL       Rm, Rdn
+	0100 0011 11mm mddd                        MVN       Rm, Rd
+
+	0100 0010 01nn nddd                       .RSB       $0, Rn, Rd
+
+	1110 1010 000s nnnn  0uuu dddd uuvv mmmm  .AND.s     Rm<v>u5, Rn, Rd
+	1110 1011 010s nnnn  0uuu dddd uuvv mmmm   ADC.s     Rm<v>u5, Rn, Rd
+	1110 1010 001s nnnn  0uuu dddd uuvv mmmm   BIC.s     Rm<v>u5, Rn, Rd
+	1110 1010 100s nnnn  0uuu dddd uuvv mmmm   EOR.s     Rm<v>u5, Rn, Rd
+	1110 1010 010s nnnn  0uuu dddd uuvv mmmm   ORR.s     Rm<v>u5, Rn, Rd
+	1110 1011 110s nnnn  0uuu dddd uuvv mmmm   RSB.s     Rm<v>u5, Rn, Rd
+	1110 1011 011s nnnn  0uuu dddd uuvv mmmm   SBC.s     Rm<v>u5, Rn, Rd
+	1110 1010 011s nnnn  0uuu dddd uuvv mmmm   ORN.s     Rm<v>u5, Rn, Rd
+	1111 0e00 000s nnnn  0eee dddd eeee eeee  .AND.s     e32, Rn, Rd
+	1111 0e01 010s nnnn  0eee dddd eeee eeee   ADC.s     e32, Rn, Rd
+	1111 0e00 001s nnnn  0eee dddd eeee eeee   BIC.s     e32, Rn, Rd
+	1111 0e00 100s nnnn  0eee dddd eeee eeee   EOR.s     e32, Rn, Rd
+	1111 0e00 010s nnnn  0eee dddd eeee eeee   ORR.s     e32, Rn, Rd
+	1111 0e01 110s nnnn  0eee dddd eeee eeee   RSB.s     e32, Rn, Rd
+	1111 0e01 011s nnnn  0eee dddd eeee eeee   SBC.s     e32, Rn, Rd
+	1111 0e00 011s nnnn  0eee dddd eeee eeee   ORN.s     e32, Rn, Rd
+
+	0100 0110 dmmm mddd                       .MOVW      Rm, Rd       ; preserves flags
+	0000 0000 00mm mddd                       .MOVW      Rm, Rd       ; not permitted in IT
+	000v vuuu uumm mddd                       .MOVW      Rm<v>u5, Rd  ; vv != 11
+	0100 0000 10mm mddd                       .MOVW      Rdn<<Rm, Rdn
+	0100 0000 11mm mddd                       .MOVW      Rdn>>Rm, Rdn
+	0100 0001 00mm mddd                       .MOVW      Rdn->Rm, Rdn
+	0100 0001 11mm mddd                       .MOVW      Rdn@>Rm, Rdn
+	1111 1010 0vvs nnnn  1111 dddd 0000 mmmm  .MOVW.s    Rn<v>Rm, Rd
+
+	0010 0ddd uuuu uuuu                       .MOVW      u8, Rd
+	1111 0y10 0100 uuuu  0zzz dddd zzzz zzzz  .MOVW      uyz16, Rd
+
+	1110 1010 011s 1111  0uuu dddd uuvv mmmm  .MVN.s     Rm<v>u5, Rd
+	1110 1010 010s 1111  0uuu dddd uuvv mmmm   MOVW.s    Rm<v>u5, Rd
+	1111 0e00 011s 1111  0eee dddd eeee eeee  .MVN.s     e32, Rd
+	1111 0e00 010s 1111  0eee dddd eeee eeee   MOVW.s    e32, Rd
+
+	1111 0y10 1100 uuuu  0zzz dddd zzzz zzzz  .MOVT      uyz16, Rd
+
+	1011 0010 00mm mddd	                      .MOVH      Rm, Rd  ; ARM SXTH
+	1011 0010 01mm mddd                        MOVB      Rm, Rd  ; ARM SXTB
+	1011 0010 10mm mddd                        MOVHU     Rm, Rd  ; ARM UXTH
+	1011 0010 11mm mddd                        MOVBU     Rm, Rd  ; ARM UXTB
+	1111 1010 0000 1111  1111 dddd 10rr mmmm  .MOVH      Rm@>rot, Rd  ; ARM SXTH
+	1111 1010 0100 1111  1111 dddd 10rr mmmm   MOVB      Rm@>rot, Rd  ; ARM SXTB
+	1111 1010 0001 1111  1111 dddd 10rr mmmm   MOVHU     Rm@>rot, Rd  ; ARM UXTH
+	1111 1010 0101 1111  1111 dddd 10rr mmmm   MOVBU     Rm@>rot, Rd  ; ARM UXTB
+
+	1111 0011 1110 1111  1000 dddd mmmm mmmm   MOVW      SYSm, Rd
+	1111 0011 1000 nnnn  1000 mm00 mmmm mmmm   MOVW      Rn, SYSm
+
+	1111 1011 0000 nnnn  1111 dddd 0000 mmmm  .MUL       Rm, Rn, Rd
+	1111 1011 1001 nnnn  1111 dddd 1111 mmmm   DIV       Rm, Rn, Rd
+	1111 1011 1011 nnnn  1111 dddd 1111 mmmm   DIVU      Rm, Rn, Rd
+
+	1111 1011 1000 nnnn  llll hhhh 0000 mmmm  .MULL      Rm, Rn, (Rdh, Rdl)
+	1111 1011 1010 nnnn  llll hhhh 0000 mmmm   MULLU     Rm, Rn, (Rdh, Rdl)
+	1111 1011 1100 nnnn  llll hhhh 0000 mmmm   MULAL     Rm, Rn, (Rdh, Rdl)  ; Rd+=int64(Rn)*int64(Rm)
+	1111 1011 1110 nnnn  llll hhhh 0000 mmmm   MULALU    Rm, Rn, (Rdh, Rdl)  ; Rd+=uint64(Rn)*uint64(Rm)
+
+	1111 1011 0000 nnnn  aaaa dddd 0000 mmmm  .MULA      Rm, Rn, Ra, Rd  ; Rd=Ra+Rn*Rm
+	1111 1011 0000 nnnn  aaaa dddd 0001 mmmm   MULS      Rm, Rn, Ra, Rd  ; Rd=Ra-Rn*Rm
+	1111 1011 0011 nnnn  aaaa dddd 0000 mmmm   MULAWB    Rm, Rn, Ra, Rd
+	1111 1011 0011 nnnn  aaaa dddd 0001 mmmm   MULAWT    Rm, Rn, Ra, Rd
+
+	1011 1010 00mm mddd                       .REV       Rm, Rd
+	1011 1010 01mm mddd                        REV16     Rm, Rd
+	1011 1010 11mm mddd                        REVSH     Rm, Rd
+
+	1111 1010 1011 mmmm  1111 dddd 1000 mmmm  .CLZ       Rm, Rd
+	1111 1010 1001 mmmm  1111 dddd 1000 mmmm   REV       Rm, Rd
+	1111 1010 1001 mmmm  1111 dddd 1001 mmmm   REV16     Rm, Rd
+	1111 1010 1001 mmmm  1111 dddd 1010 mmmm   RBIT      Rm, Rd
+	1111 1010 1001 mmmm  1111 dddd 1011 mmmm   REVSH     Rm, Rd
+
+	1111 1010 1010 nnnn  1111 dddd 1000 mmmm  .SEL       Rm, Rn, Rd  ; DSP extension
+
+	1111 0011 0100 nnnn  0uuu dddd uu0w wwww  .BFX       width, ulsb, Rn, Rd  ; wwww = width-1
+	1111 0011 1100 nnnn  0uuu dddd uu0w wwww   BFXU      width, ulsb, Rn, Rd  ; wwww = width-1
+	1111 0011 0110 nnnn  0uuu dddd uu0k kkkk   BFI       width, ulsb, Rn, Rd  ; kkkk = ulsb+width-1
+	1111 0011 0110 1111  0uuu dddd uu0k kkkk   BFC       width, ulsb, Rd      ; kkkk = ulsb+width-1
+
+	0100 0010 00mm mnnn                       .TST       Rm, Rn
+	0100 0010 10mm mnnn                        CMP       Rm, Rn
+	0100 0010 11mm mnnn                        CMN       Rm, Rn
+
+	0100 0101 nmmm mnnn                       .CMP       Rm, Rn
+	0010 1nnn uuuu uuuu                        CMP       u8, Rn
+
+	1110 1010 0001 nnnn  0uuu 1111 uuvv mmmm  .TST       Rm<v>u5, Rn
+	1110 1010 1001 nnnn  0uuu 1111 uuvv mmmm   TEQ       Rm<v>u5, Rn
+	1110 1011 0001 nnnn  0uuu 1111 uuvv mmmm   CMN       Rm<v>u5, Rn
+	1110 1011 1011 nnnn  0uuu 1111 uuvv mmmm   CMP       Rm<v>u5, Rn
+	1111 0e00 0001 nnnn  0eee 1111 eeee eeee  .TST       e32, Rn
+	1111 0e00 1001 nnnn  0eee 1111 eeee eeee   TEQ       e32, Rn
+	1111 0e01 0001 nnnn  0eee 1111 eeee eeee   CMN       e32, Rn
+	1111 0e01 1011 nnnn  0eee 1111 eeee eeee   CMP       e32, Rn
+
+	0110 1uuu uunn nttt                       .MOVW      u5<<2(Rn), Rt
+	1001 1ttt uuuu uuuu                       .MOVW      u8<<2(R13), Rt
+	0100 1ttt uuuu uuuu                       .MOVW      u8<<2(R15), Rt
+	1000 1uuu uunn nttt                       .MOVHU     u5<<1(Rn), Rt
+	0111 1uuu uunn nttt                       .MOVBU     u5(Rn), Rt
+
+	0110 0uuu uunn nttt                       .MOVW      Rt, u5<<2(Rn)
+	1001 0ttt uuuu uuuu                       .MOVW      Rt, u8<<2(R13)
+	1000 0uuu uunn nttt                       .MOVH      Rt, u5<<1(Rn)
+	0111 0uuu uunn nttt                       .MOVB      Rt, u5(Rn)
+
+	0101 100m mmnn nttt                       .MOVW      (Rn)(Rm), Rt
+	0101 111m mmnn nttt                        MOVH      (Rn)(Rm), Rt
+	0101 101m mmnn nttt                        MOVHU     (Rn)(Rm), Rt
+	0101 011m mmnn nttt                        MOVB      (Rn)(Rm), Rt
+	0101 110m mmnn nttt                        MOVBU     (Rn)(Rm), Rt
+	1111 1000 0101 nnnn  tttt 0000 00uu mmmm  .MOVW      (Rn)(Rm*1<<u2), Rt
+	1111 1001 0011 nnnn  tttt 0000 00uu mmmm   MOVH      (Rn)(Rm*1<<u2), Rt
+	1111 1000 0011 nnnn  tttt 0000 00uu mmmm   MOVHU     (Rn)(Rm*1<<u2), Rt
+	1111 1001 0001 nnnn  tttt 0000 00uu mmmm   MOVB      (Rn)(Rm*1<<u2), Rt
+	1111 1000 0001 nnnn  tttt 0000 00uu mmmm   MOVBU     (Rn)(Rm*1<<u2), Rt
+	1111 1000 101 1111  tttt uuuu uuuu uuuu  .MOVW      u12(R15), Rt
+	1111 1001 011 1111  tttt uuuu uuuu uuuu   MOVH      u12(R15), Rt
+	1111 1000 011 1111  tttt uuuu uuuu uuuu   MOVHU     u12(R15), Rt
+	1111 1001 001 1111  tttt uuuu uuuu uuuu   MOVB      u12(R15), Rt
+	1111 1000 001 1111  tttt uuuu uuuu uuuu   MOVBU     u12(R15), Rt
+	1111 1000 1101 nnnn  tttt uuuu uuuu uuuu  .MOVW      u12(Rn), Rt
+	1111 1001 1011 nnnn  tttt uuuu uuuu uuuu   MOVH      u12(Rn), Rt
+	1111 1000 1011 nnnn  tttt uuuu uuuu uuuu   MOVHU     u12(Rn), Rt
+	1111 1001 1001 nnnn  tttt uuuu uuuu uuuu   MOVB      u12(Rn), Rt
+	1111 1000 1001 nnnn  tttt uuuu uuuu uuuu   MOVBU     u12(Rn), Rt
+	1111 1000 0101 nnnn  tttt 1pw uuuu uuuu  .MOVW.p.w  u8(Rn), Rt
+	1111 1001 0011 nnnn  tttt 1pw uuuu uuuu   MOVH.p.w  u8(Rn), Rt
+	1111 1000 0011 nnnn  tttt 1pw uuuu uuuu   MOVHU.p.w u8(Rn), Rt
+	1111 1001 0001 nnnn  tttt 1pw uuuu uuuu   MOVB.p.w  u8(Rn), Rt
+	1111 1000 0001 nnnn  tttt 1pw uuuu uuuu   MOVBU.p.w u8(Rn), Rt
+
+	0101 000m mmnn nttt                       .MOVW      Rt, (Rn)(Rm)
+	0101 001m mmnn nttt                        MOVH      Rt, (Rn)(Rm)
+	0101 010m mmnn nttt                        MOVB      Rt, (Rn)(Rm)
+	1111 1000 0100 nnnn  tttt 0000 00uu mmmm  .MOVW      Rt, (Rn)(Rm*1<<u2)
+	1111 1000 0010 nnnn  tttt 0000 00uu mmmm   MOVH      Rt, (Rn)(Rm*1<<u2)
+	1111 1000 0000 nnnn  tttt 0000 00uu mmmm   MOVB      Rt, (Rn)(Rm*1<<u2)
+	1111 1000 1100 nnnn  tttt uuuu uuuu uuuu  .MOVW      Rt, u12(Rn)
+	1111 1000 1010 nnnn  tttt uuuu uuuu uuuu   MOVH      Rt, u12(Rn)
+	1111 1000 1000 nnnn  tttt uuuu uuuu uuuu   MOVB      Rt, u12(Rn)
+	1111 1000 0100 nnnn  tttt 1pw uuuu uuuu  .MOVW.p.w  Rt, u8(Rn)
+	1111 1000 0010 nnnn  tttt 1pw uuuu uuuu   MOVH.p.w  Rt, u8(Rn)
+	1111 1000 0000 nnnn  tttt 1pw uuuu uuuu   MOVB.p.w  Rt, u8(Rn)
+
+	1110 1110 1d11 0001  dddd 1010 11m0 mmmm  .SQRTF     Fm, Fd
+	1110 1110 1d11 0001  dddd 1011 11m0 mmmm   SQRTD     Fm, Fd
+
+	1110 1101 d01 nnnn  dddd 1010 uuuu uuuu  .MOVF      u8<<2(Rn), Fd
+	1110 1101 d01 nnnn  dddd 1011 uuuu uuuu   MOVD      u8<<2(Rn), Fd
+	1110 1101 d00 nnnn  dddd 1010 uuuu uuuu  .MOVF      Fd, u8<<2(Rn)
+	1110 1101 d00 nnnn  dddd 1011 uuuu uuuu   MOVD      Fd, u8<<2(Rn)
+	1110 1110 1d11 ffff  dddd 1010 0000 ffff  .MOVF      f8, Fd
+	1110 1110 1d11 ffff  dddd 1011 0000 ffff   MOVD      f8, Fd
+
+	1011 1111 cccc mmmm                       .ITmask    firstcond
+
+	1110 0iii iiii iiii                       .B         i11<<1   ; outside/last in IT
+
+	1011 00u1 uuuu unnn                       .CBZ       Rn, u6<<1  ; outside IT
+	1011 10u1 uuuu unnn                        CBNZ      Rn, u6<<1  ; outside IT
+
+	0100 0111 0mmm m000                       .B         (Rm)  ; ARM: BX Rm
+	0100 0111 1mmm m000                        BL        (Rm)  ; ARM: BLX Rm
+	1111 0jii iiii iiii  10j1 jiii iiii iiii  .B         ji24<<1  ; outside/last in IT
+	1111 0jii iiii iiii  11j1 jiii iiii iiii   BL        ji24<<1  ; outside/last in IT
+
+	1101 cccc iiii iiii                       .Bcond     i8<<1    ; outside IT
+	1111 0jcc ccii iiii  10j0 jiii iiii iiii  .Bcond     ji20<<1  ; outside IT
+
+	1110 1000 1101 nnnn  1111 0000 0000 mmmm  .TBB	     Rm, Rn
+	1110 1000 1101 nnnn  1111 0000 0001 mmmm   TBH	     Rm, Rn
+
+	1101 1111 uuuu uuuu                       .SWI       u8  ; SVC
+	1011 1110 uuuu uuuu                        BKPT      u8
+	1101 1110 uuuu uuuu                        UNDEF     u8
+
+	1011 010r rrrr rrrr                       .MOVM.DB.W reglist, (R13)  ; PUSH [R0-R7,R14]
+	1100 0nnn rrrr rrrr                       .MOVM.IA.W reglist, (Rn)
+	1110 1000 10w0 nnnn  0r0r rrrr rrrr rrrr  .MOVM.IA.w reglist, (Rn)
+	1110 1001 00w0 nnnn  0r0r rrrr rrrr rrrr  .MOVM.DB.w reglist, (Rn)   ; .W,n=13 -> PUSH
+
+	1011 110r rrrr rrrr                       .MOVM.IA.W (R13), reglist  ; POP [R0-R7,R14]
+	1100 1nnn rrrr rrrr                       .MOVM.IA.W (Rn), reglist
+	1110 1000 10w1 nnnn  rr0r rrrr rrrr rrrr  .MOVM.IA.w (Rn), reglist   ; .W,n=13 -> POP
+	1110 1001 00w1 nnnn  rr0r rrrr rrrr rrrr  .MOVM.DB.w (Rn), reglist
+
+	1110 1000 0101 nnnn  tttt 1111 uuuu uuuu  .LDREX     u8<<2(Rn), Rt
+	1110 1000 0100 nnnn  tttt dddd uuuu uuuu  .STREX     Rt, u8<<2(Rn), Rd
+
+	1110 1000 1101 nnnn  tttt 1111 0100 1111  .LDREXB    (Rn), Rt
+	1110 1000 1101 nnnn  tttt 1111 0101 1111   LDREXH    (Rn), Rt
+
+	1110 1000 1100 nnnn  tttt 1111 0100 dddd  .STREXB    Rt, (Rn), Rd
+	1110 1000 1100 nnnn  tttt 1111 0101 dddd   STREXH    Rt, (Rn), Rd
+
+	1011 1111 0000 0000  .NOP2
+	1011 1111 0001 0000   YIELD
+	1011 1111 0010 0000   WFE
+	1011 1111 0011 0000   WFI
+	1011 1111 0100 0000   SEV
+
+	1111 0011 1010 1111  1000 0000 0000 0000  .NOP4
+	1111 0011 1011 1111  1000 1111 0010 1111   CLREX
+
+	1111 0011 1011 1111  1000 1111 0100 oooo  .DSB       opt
+	1111 0011 1011 1111  1000 1111 0101 oooo   DMB       opt
+	1111 0011 1011 1111  1000 1111 0110 oooo   ISB       opt
+
+	1011 0110 0110 0010                        CPSIE     i       ; outside IT
+	1011 0110 0111 0010                        CPSID     i       ; outside IT
+
+--
+
+	1110 100p 1w0 nnnn  aaaa bbbb uuuu uuuu   MOVW.p.w  (Rta, Rtb), u8<<2(Rn)
+	1110 100p 1w1 nnnn  aaaa bbbb uuuu uuuu   MOVW.p.w  u8<<2(Rn), (Rta, Rtb)
+
+	1111 1010 1000 nnnn  1111 dddd 0000 mmmm   ADD8      Rm, Rn, Rd
+	1111 1010 1000 nnnn  1111 dddd 0100 mmmm   ADD8U     Rm, Rn, Rd
+	1111 1010 1001 nnnn  1111 dddd 0000 mmmm   ADD16     Rm, Rn, Rd
+	1111 1010 1001 nnnn  1111 dddd 0100 mmmm   ADD16U    Rm, Rn, Rd
+
+	1111 0011 1010 1111  1000 0000 0000 0000   NOP4
+	1111 0011 1010 1111  1000 0000 0000 0001   YIELD
+	1111 0011 1010 1111  1000 0000 0000 0010   WFE
+	1111 0011 1010 1111  1000 0000 0000 0011   WFI
+	1111 0011 1010 1111  1000 0000 0000 0100   SEV
+
+	1111 0111 1111 uuuu  1010 uuuu uuuu uuuu   UNDEF     u16
diff --git a/src/cmd/internal/obj/thumb/list.go b/src/cmd/internal/obj/thumb/list.go
new file mode 100644
index 0000000000..20fdc2952a
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/list.go
@@ -0,0 +1,176 @@
+// Inferno utils/5c/list.c
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5c/list.c
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"cmd/internal/obj"
+	"fmt"
+)
+
+func init() {
+	obj.RegisterRegister(obj.RBaseThumb, MAXREG, rconv)
+	obj.RegisterOpcode(obj.ABaseThumb, Anames)
+	obj.RegisterRegisterList(obj.RegListARMLo, obj.RegListARMHi, rlconv)
+	obj.RegisterOpSuffix("thumb", cconv)
+}
+
+var condCode = []string{
+	".EQ",
+	".NE",
+	".CS",
+	".CC",
+	".MI",
+	".PL",
+	".VS",
+	".VC",
+	".HI",
+	".LS",
+	".GE",
+	".LT",
+	".GT",
+	".LE",
+	"",
+	".NV",
+}
+
+func cconv(s uint8) string {
+	sc := condCode[(s&C_SCOND)^C_SCOND_XOR]
+	if s&C_SBIT != 0 {
+		sc += ".S"
+	}
+	if s&C_PBIT != 0 {
+		sc += ".P"
+	}
+	if s&C_WBIT != 0 {
+		sc += ".W"
+	}
+	if s&C_UBIT != 0 { /* ambiguous with FBIT */
+		sc += ".U"
+	}
+	return sc
+}
+
+func rconv(r int) string {
+	if r == 0 {
+		return "NONE"
+	}
+	if r == REGG {
+		// Special case.
+		return "g"
+	}
+	if REG_R0 <= r && r <= REG_R15 {
+		return fmt.Sprintf("R%d", r-REG_R0)
+	}
+	if REG_F0 <= r && r <= REG_F15 {
+		return fmt.Sprintf("F%d", r-REG_F0)
+	}
+	if REG_EQ <= r && r <= REG_AL {
+		r = (r - REG_EQ) * 2
+		return "EQNEHSLOMIPLVSVCHILSGELTGTLEAL"[r : r+2]
+	}
+
+	switch r {
+	case REG_APSR:
+		return "APSR"
+	case REG_IAPSR:
+		return "IAPSR"
+	case REG_EAPSR:
+		return "EAPSR"
+	case REG_XPSR:
+		return "XPSR"
+	case REG_IPSR:
+		return "IPSR"
+	case REG_EPSR:
+		return "EPSR"
+	case REG_IEPSR:
+		return "IEPSR"
+
+	case REG_FPSCR:
+		return "FPSCR"
+
+	case REG_MSP:
+		return "MSP"
+	case REG_PSP:
+		return "PSP"
+
+	case REG_PRIMASK:
+		return "PRIMASK"
+	case REG_BASEPRI:
+		return "BASEPRI"
+	case REG_BASEPRI_MAX:
+		return "BASEPRI_MAX"
+	case REG_FAULTMASK:
+		return "FAULTMASK"
+	case REG_CONTROL:
+		return "CONTROL"
+
+	case REG_MB_SY:
+		return "MB_SY"
+	case REG_MB_ST:
+		return "MB_ST"
+	case REG_MB_ISH:
+		return "MB_ISH"
+	case REG_MB_ISHST:
+		return "MB_ISHST"
+	case REG_MB_NSH:
+		return "MB_NSH"
+	case REG_MB_NSHST:
+		return "MB_NSHST"
+	case REG_MB_OSH:
+		return "MB_OSH"
+	case REG_MB_OSHST:
+		return "MB_OSHST"
+	}
+
+	return fmt.Sprintf("Rgok(%d)", r-obj.RBaseThumb)
+}
+
+func rlconv(list int64) string {
+	str := ""
+	for i := 0; i < 16; i++ {
+		if list&(1<<uint(i)) != 0 {
+			if str == "" {
+				str += "["
+			} else {
+				str += ","
+			}
+			// This is ARM-specific; R10 is g.
+			if i == REGG-REG_R0 {
+				str += "g"
+			} else {
+				str += fmt.Sprintf("R%d", i)
+			}
+		}
+	}
+
+	str += "]"
+	return str
+}
diff --git a/src/cmd/internal/obj/thumb/obj.go b/src/cmd/internal/obj/thumb/obj.go
new file mode 100644
index 0000000000..7f68038388
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/obj.go
@@ -0,0 +1,885 @@
+// Derived from Inferno utils/5c/swt.c
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5c/swt.c
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+package thumb
+
+import (
+	"cmd/internal/obj"
+	"cmd/internal/objabi"
+	"cmd/internal/sys"
+)
+
+const (
+	LEAF = 1 << iota
+	AUTOIT
+)
+
+// return address size (on stack frame)
+const (
+	prasize = 4 // parent function
+	rasize  = 4 // current function
+)
+
+func preprocess(ctxt *obj.Link, cursym *obj.LSym, newprog obj.ProgAlloc) {
+	if cursym.Func.Text == nil || cursym.Func.Text.Link == nil {
+		return
+	}
+
+	c := Ctx{ctxt: ctxt, cursym: cursym, newprog: newprog}
+
+	p := c.cursym.Func.Text
+	autoffset := int(p.To.Offset)
+	if autoffset == -4 {
+		// Historical way to mark NOFRAME.
+		p.From.Sym.Set(obj.AttrNoFrame, true)
+		autoffset = 0
+	}
+	if autoffset < 0 || autoffset%4 != 0 {
+		c.ctxt.Diag("frame size %d not 0 or a positive multiple of 4", autoffset)
+	}
+	if p.From.Sym.NoFrame() {
+		if autoffset != 0 {
+			c.ctxt.Diag("NOFRAME functions must have a frame size of 0, not %d", autoffset)
+		}
+	}
+
+	cursym.Func.Locals = int32(autoffset)
+	cursym.Func.Args = p.To.Val.(int32)
+	cursym.Func.Text.Mark = AUTOIT
+	if !cursym.Func.Text.From.Sym.ISR() {
+		cursym.Func.Text.Mark |= LEAF
+	}
+
+	q := cursym.Func.Text
+	for p := q.Link; p != nil; p = p.Link {
+		switch {
+		case p.As == obj.ANOP:
+			q1 := p.Link
+			q.Link = q1 /* q is non-nop */
+			if q1 != nil {
+				q1.Mark |= p.Mark
+			}
+			continue
+
+		case AITTTT <= p.As && p.As <= AITEEE:
+			cursym.Func.Text.Mark &^= AUTOIT
+
+		case p.As == ABL:
+			cursym.Func.Text.Mark &^= LEAF
+			fallthrough
+		case p.As == AB || ABEQ <= p.As && p.As <= ACBNZ:
+			if q1 := p.Pcond; q1 != nil {
+				for q1.As == obj.ANOP {
+					q1 = q1.Link
+					p.Pcond = q1
+				}
+			}
+		}
+		q = p
+	}
+
+	var autosize int
+	for p := cursym.Func.Text; p != nil; p = p.Link {
+		if cursym.Func.Text.Mark&AUTOIT != 0 && p.To.Type == obj.TYPE_BRANCH &&
+			ABEQ <= p.As && p.As <= ABLE {
+			// convert `Bcond label` to `B.cond label`
+			cond := byte(C_SCOND_LE)
+			switch p.As {
+			case ABEQ:
+				cond = C_SCOND_EQ
+			case ABNE:
+				cond = C_SCOND_NE
+			case ABCS, ABHS:
+				cond = C_SCOND_HS
+			case ABCC, ABLO:
+				cond = C_SCOND_LO
+			case ABMI:
+				cond = C_SCOND_MI
+			case ABPL:
+				cond = C_SCOND_PL
+			case ABVS:
+				cond = C_SCOND_VS
+			case ABVC:
+				cond = C_SCOND_VC
+			case ABHI:
+				cond = C_SCOND_HI
+			case ABLS:
+				cond = C_SCOND_LS
+			case ABGE:
+				cond = C_SCOND_GE
+			case ABLT:
+				cond = C_SCOND_LT
+			case ABGT:
+				cond = C_SCOND_GT
+			}
+			p.As = AB
+			p.Scond = p.Scond&^C_SCOND | cond
+		}
+
+		switch p.As {
+		case obj.ATEXT:
+			autosize = autoffset
+
+			if p.Mark&LEAF != 0 && autosize == 0 {
+				// A leaf function with no locals has no frame.
+				p.From.Sym.Set(obj.AttrNoFrame, true)
+			}
+
+			if !p.From.Sym.NoFrame() {
+				// If there is a stack frame at all, it includes
+				// space to save the LR.
+				autosize += rasize
+			}
+
+			if autosize == 0 && cursym.Func.Text.Mark&LEAF == 0 {
+				// A very few functions that do not return to their caller
+				// are not identified as leaves but still have no frame.
+				if ctxt.Debugvlog {
+					ctxt.Logf("save suppressed in: %s\n", cursym.Name)
+				}
+				cursym.Func.Text.Mark |= LEAF
+			}
+
+			c.autosize = autosize
+			p.To.Offset = int64(autosize - rasize) // FP offsets need an updated p.To.Offset.
+
+			if cursym.Func.Text.From.Sym.ISR() {
+				// Emit the interrupt handler prologue:
+				//
+				//  PUSH  [R4-R11]
+				//  TST   $0x10, LR
+				//  BNE   nofpuctx
+				//	MOVW  CONTROL, R0
+				//  CPSID i
+				//  VPUSH [D8-D15]    // sets CONTROL.FPCA
+				//  MOVW  R0, CONTROL // clears FPCA
+				//  CPSIE i
+				// nofpuctx:
+				//
+				// The current Go ABI in the presence of the FPU makes the
+				// exception entry and exit very inefficient. If a thread uses
+				// FPU the above code saves the whole FPU context at exception
+				// entry even if the exception handler does not use the FPU. To
+				// prevent the higher priority exception to save the same FPU
+				// context again the above code clears CONTROL.FPCA just after
+				// VPUSH. An another approach can be disabling the FPU at
+				// exception entry and re-enabling it only if the UsageFault
+				// occurs as descibed in ARM Application Note 298. The undoubted
+				// advantages of the ARMv7-M exception model deffinitely require
+				// to follow AAPCS.
+				//
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVM
+				p.Scond |= C_DB | C_WBIT
+				p.From.Type = obj.TYPE_REGLIST
+				p.From.Offset = 0xFF0
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REGSP
+				p = obj.Appendp(p, newprog)
+				p.As = ATST
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0x10
+				p.Reg = REGLINK
+				bne := obj.Appendp(p, newprog)
+				bne.As = ABNE
+				bne.To.Type = obj.TYPE_BRANCH
+				p = obj.Appendp(bne, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_CONTROL
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R0
+				p = obj.Appendp(p, newprog)
+				p.As = ACPSID
+				p = obj.Appendp(p, newprog)
+				p.As = AHWORD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0xED2D
+				p = obj.Appendp(p, newprog)
+				p.As = AHWORD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0x8B10
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_R0
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_CONTROL
+				p = obj.Appendp(p, newprog)
+				p.As = ACPSIE
+				p = obj.Appendp(p, newprog)
+				p.As = obj.ANOP
+				bne.Pcond = p
+			}
+
+			if cursym.Func.Text.Mark&LEAF != 0 {
+				cursym.Set(obj.AttrLeaf, true)
+				if cursym.Func.Text.From.Sym.NoFrame() {
+					break
+				}
+			}
+
+			if !cursym.Func.Text.From.Sym.NoSplit() {
+				p = c.stacksplit(p, int32(autosize)) // emit split check
+			}
+
+			if autosize <= 255 {
+				// fram size fits into MOVW.W R14,-autosize(SP)
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.Scond |= C_WBIT
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REGLINK
+				p.To.Type = obj.TYPE_MEM
+				p.To.Offset = int64(-autosize)
+				p.To.Reg = REGSP
+				p.Spadj = int32(autosize)
+			} else {
+				// Frame size is too large for a MOVW.W instruction.
+				// Store link register before decrementing SP, so if a signal comes
+				// during the execution of the function prologue, the traceback
+				// code will not see a half-updated stack frame.
+				p = obj.Appendp(p, c.newprog)
+				p.Pos = p.Pos
+				p.As = ASUB
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = int64(autosize)
+				p.Reg = REGSP
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REGTMP
+
+				p = obj.Appendp(p, c.newprog)
+				p.Pos = p.Pos
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REGLINK
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REGTMP
+
+				p = obj.Appendp(p, c.newprog)
+				p.Pos = p.Pos
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REGTMP
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REGSP
+				p.Spadj = int32(autosize)
+			}
+
+			if cursym.Func.Text.From.Sym.ISR() {
+				p = obj.Appendp(p, newprog)
+				p.As = ABL
+				p.To.Type = obj.TYPE_BRANCH
+				p.To.Sym = c.ctxt.Lookup("runtime.identcurcpu")
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_R0
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REGG
+			}
+
+			if cursym.Func.Text.From.Sym.Wrapper() {
+				// if(g->panic != nil && g->panic->argp == FP) g->panic->argp = bottom-of-frame
+				//
+				//	MOVW g_panic(g), R1
+				//	CMP  $0, R1
+				//	BNE checkargp
+				// end:
+				//	NOP
+				// ... function ...
+				// checkargp:
+				//	MOVW panic_argp(R1), R2
+				//	ADD  $(autosize+4), R13, R3
+				//	CMP  R2, R3
+				//	BNE end
+				//	ADD  $4, R13, R4
+				//	MOVW R4, panic_argp(R1)
+				//	B    end
+				//
+				// The NOP is needed to give the jumps somewhere to land.
+				// It is a liblink NOP, not an ARM NOP: it encodes to 0 instruction bytes.
+
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REGG
+				p.From.Offset = 4 * int64(ctxt.Arch.PtrSize) // G.panic
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R1
+
+				p = obj.Appendp(p, newprog)
+				p.As = ACMP
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0
+				p.Reg = REG_R1
+
+				// B.NE checkargp
+				bne := obj.Appendp(p, newprog)
+				bne.As = ABNE
+				bne.To.Type = obj.TYPE_BRANCH
+
+				// end: NOP
+				end := obj.Appendp(bne, newprog)
+				end.As = obj.ANOP
+
+				// find end of function
+				var last *obj.Prog
+				for last = end; last.Link != nil; last = last.Link {
+				}
+
+				// MOVW panic_argp(R1), R2
+				mov := obj.Appendp(last, newprog)
+				mov.As = AMOVW
+				mov.From.Type = obj.TYPE_MEM
+				mov.From.Reg = REG_R1
+				mov.From.Offset = 0 // Panic.argp
+				mov.To.Type = obj.TYPE_REG
+				mov.To.Reg = REG_R2
+
+				// B.NE branch target is MOVW above
+				bne.Pcond = mov
+
+				// ADD $(autosize+4), R13, R3
+				p = obj.Appendp(mov, newprog)
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = int64(autosize) + 4
+				p.Reg = REG_R13
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R3
+
+				// CMP R2, R3
+				p = obj.Appendp(p, newprog)
+				p.As = ACMP
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_R2
+				p.Reg = REG_R3
+
+				// B.NE end
+				p = obj.Appendp(p, newprog)
+				p.As = ABNE
+				p.To.Type = obj.TYPE_BRANCH
+				p.Pcond = end
+
+				// ADD $4, R13, R4
+				p = obj.Appendp(p, newprog)
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 4
+				p.Reg = REG_R13
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R4
+
+				// MOVW R4, panic_argp(R1)
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_R4
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REG_R1
+				p.To.Offset = 0 // Panic.argp
+
+				// B end
+				p = obj.Appendp(p, newprog)
+				p.As = AB
+				p.To.Type = obj.TYPE_BRANCH
+				p.Pcond = end
+
+				// reset for subsequent passes
+				p = end
+			}
+
+		case obj.ARET:
+			nocache(p)
+			if cursym.Func.Text.Mark&LEAF != 0 && autosize == 0 {
+				p.As = AB
+				p.From = obj.Addr{}
+				if p.To.Sym != nil { // retjmp
+					p.To.Type = obj.TYPE_BRANCH
+				} else {
+					p.To.Type = obj.TYPE_MEM
+					p.To.Offset = 0
+					p.To.Reg = REGLINK
+				}
+				break
+			}
+
+			sym := p.To.Sym
+			p.To.Sym = nil
+
+			if autosize <= 255 {
+				// MOVW.P autosize(SP),PC
+				p.As = AMOVW
+				p.Scond |= C_PBIT
+				p.From.Type = obj.TYPE_MEM
+				p.From.Offset = int64(autosize)
+				p.From.Reg = REGSP
+				p.To.Type = obj.TYPE_REG
+				p.To.Offset = 0
+				p.To.Reg = REGPC
+			} else {
+				// MOVW (SP), REGLINK
+				// ADD  autosize, SP
+				// B    (REGLINK)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REGSP
+				p.To.Type = obj.TYPE_REG
+				p.To.Offset = 0
+				p.To.Reg = REGLINK
+				p = obj.Appendp(p, newprog)
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = int64(autosize)
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REGSP
+				p = obj.Appendp(p, newprog)
+				p.As = AB
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REGLINK
+			}
+
+			if cursym.Func.Text.From.Sym.ISR() {
+				if p.As == AMOVW {
+					// MOVW.P autosize(SP),PC -> MOVW.P autosize(SP),LR
+					p.To.Reg = REGLINK
+					p = obj.Appendp(p, newprog)
+				}
+				// emit interrupt handler epilogue:
+				//
+				//  TST  $0x10, LR
+				//  BNE  nofpuctx
+				//  MOVW CONTROL, R0
+				//  TST  $4, R0 // CONTROL.FPCA tells if FPU have been used
+				//  BEQ  nofpuctx
+				//  VPOP [D8-D15]
+				// nofpuctx:
+				//  POP  [R4-R11]
+				//
+				p.As = ATST
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0x10
+				p.Reg = REGLINK
+				bne := obj.Appendp(p, newprog)
+				bne.As = ABNE
+				bne.To.Type = obj.TYPE_BRANCH
+				p = obj.Appendp(bne, newprog)
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REG_CONTROL
+				p.To.Type = obj.TYPE_REG
+				p.To.Reg = REG_R0
+				p = obj.Appendp(p, newprog)
+				p.As = ATST
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 4
+				p.Reg = REG_R0
+				beq := obj.Appendp(p, newprog)
+				beq.As = ABEQ
+				beq.To.Type = obj.TYPE_BRANCH
+				p = obj.Appendp(beq, newprog)
+				p.As = AHWORD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0xECBD
+				p = obj.Appendp(p, newprog)
+				p.As = AHWORD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Offset = 0x8B10
+				p = obj.Appendp(p, newprog)
+				p.As = AMOVM
+				p.Scond |= C_IA | C_WBIT
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REGSP
+				p.To.Type = obj.TYPE_REGLIST
+				p.To.Offset = 0xFF0
+				bne.Pcond = p
+				beq.Pcond = p
+				p = obj.Appendp(p, newprog)
+				p.As = AB
+				p.To.Type = obj.TYPE_MEM
+				p.To.Reg = REGLINK
+			}
+
+			// If there are instructions following this ARET, they come from a
+			// branch with the same stackframe, so no spadj.
+			if p.To.Sym != nil { // retjmp
+				if p.As == AMOVW {
+					// MOVW.P autosize(SP),PC -> MOVW.P autosize(SP),LR
+					p.To.Reg = REGLINK
+					p = obj.Appendp(p, newprog)
+				}
+				p.As = AB
+				p.To.Type = obj.TYPE_BRANCH
+				p.To.Sym = sym
+			}
+
+		case AADD:
+			if p.From.Type == obj.TYPE_CONST && p.From.Reg == 0 && p.To.Type == obj.TYPE_REG && p.To.Reg == REGSP {
+				p.Spadj = int32(-p.From.Offset)
+			}
+
+		case ASUB:
+			if p.From.Type == obj.TYPE_CONST && p.From.Reg == 0 && p.To.Type == obj.TYPE_REG && p.To.Reg == REGSP {
+				p.Spadj = int32(p.From.Offset)
+			}
+
+		case AMOVW:
+			switch {
+			case p.From.Type == obj.TYPE_ADDR && (p.From.Name == obj.NAME_NONE || p.From.Name == obj.NAME_PARAM || p.From.Name == obj.NAME_AUTO):
+				offset := c.offset(&p.From)
+				switch p.From.Name {
+				case obj.NAME_PARAM, obj.NAME_AUTO:
+					p.Reg = REGSP
+				default:
+					p.Reg = p.From.Reg
+				}
+				p.From.Sym = nil
+				p.From.Name = obj.NAME_NONE
+				if offset == 0 {
+					p.From.Type = obj.TYPE_REG
+					p.From.Reg = p.Reg
+					p.From.Offset = 0
+					p.Reg = 0
+					break
+				}
+				p.As = AADD
+				p.From.Type = obj.TYPE_CONST
+				p.From.Reg = 0
+				p.From.Offset = offset
+				if p.Reg == REGSP && p.To.Reg == REGSP {
+					p.Spadj = int32(-offset)
+				}
+				p.Scond = C_PBIT // preserve flags
+			case (p.Scond&C_WBIT != 0) && p.To.Type == obj.TYPE_MEM && p.To.Reg == REGSP:
+				p.Spadj = int32(-p.To.Offset)
+				if p.To.Offset == -4 && (p.From.Reg <= REG_R7 || p.From.Reg == REG_R14) {
+					// T32 MOVW.W Rt, -4(R13) -> T16 MOVM.DB.W [Rt], (R13) == PUSH [Rt]
+					p.As = AMOVM
+					p.To.Offset = 0
+					p.From.Type = obj.TYPE_REGLIST
+					p.From.Offset = 1 << uint(p.From.Reg&15)
+					p.From.Reg = 0
+					p.Scond = C_DB | C_WBIT
+				}
+			case (p.Scond&C_PBIT != 0) && p.From.Type == obj.TYPE_MEM && p.From.Reg == REGSP:
+				p.Spadj = int32(-p.From.Offset)
+				if p.From.Offset == 4 && (p.To.Reg <= REG_R7 || p.To.Reg == REG_R15) {
+					// T32 MOVW.P 4(R13), Rt -> T16 MOVM.IA.W (R13), [Rt] == POP [Rt]
+					p.As = AMOVM
+					p.From.Offset = 0
+					p.To.Type = obj.TYPE_REGLIST
+					p.To.Offset = 1 << uint(p.To.Reg&15)
+					p.To.Reg = 0
+					p.Scond = C_IA | C_WBIT
+				}
+			}
+
+		case AMOVM:
+			// TODO: convert invalid MOVM with one register to MOVW
+
+		case obj.AGETCALLERPC:
+			if cursym.Leaf() {
+				/* MOVW LR, Rd */
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_REG
+				p.From.Reg = REGLINK
+				p.Scond = C_PBIT // preserve flags
+			} else {
+				/* MOVW (RSP), Rd */
+				p.As = AMOVW
+				p.From.Type = obj.TYPE_MEM
+				p.From.Reg = REGSP
+			}
+		}
+	}
+}
+
+func (c *Ctx) stacksplit(p *obj.Prog, framesize int32) *obj.Prog {
+	// MOVW g_stackguard(g), R1
+	p = obj.Appendp(p, c.newprog)
+
+	p.As = AMOVW
+	p.From.Type = obj.TYPE_MEM
+	p.From.Reg = REGG
+	p.From.Offset = 2 * int64(c.ctxt.Arch.PtrSize) // G.stackguard0
+	if c.cursym.CFunc() {
+		p.From.Offset = 3 * int64(c.ctxt.Arch.PtrSize) // G.stackguard1
+	}
+	p.To.Type = obj.TYPE_REG
+	p.To.Reg = REG_R1
+
+	if framesize <= objabi.StackSmall {
+		// small stack: SP < stackguard
+		//	CMP	stackguard, SP
+		p = obj.Appendp(p, c.newprog)
+
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_R1
+		p.Reg = REGSP
+	} else if framesize <= objabi.StackBig {
+		// large stack: SP-framesize < stackguard-StackSmall
+		//	ADD $-(framesize-StackSmall), SP, R2
+		//	CMP stackguard, R2
+		p = obj.Appendp(p, c.newprog)
+
+		p.As = AADD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = -(int64(framesize) - objabi.StackSmall)
+		p.Reg = REGSP
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_R2
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_R1
+		p.Reg = REG_R2
+	} else {
+		// Such a large stack we need to protect against wraparound
+		// if SP is close to zero.
+		//	SP-stackguard+StackGuard < framesize + (StackGuard-StackSmall)
+		// The +StackGuard on both sides is required to keep the left side positive:
+		// SP is allowed to be slightly below stackguard. See stack.h.
+		//	CMP     $StackPreempt, R1
+		//	ADD.P.NE  $StackGuard, SP, R2
+		//	SUB.P.NE  R1, R2
+		//	MOVW.NE $(framesize+(StackGuard-StackSmall)), R3
+		//	CMP.NE  R3, R2
+		p = obj.Appendp(p, c.newprog)
+
+		p.As = ACMP
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(uint32(objabi.StackPreempt & (1<<32 - 1)))
+		p.Reg = REG_R1
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = AADD
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(objabi.StackGuard)
+		p.Reg = REGSP
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_R2
+		p.Scond = C_PBIT | C_SCOND_NE
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = ASUB
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_R1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_R2
+		p.Scond = C_PBIT | C_SCOND_NE
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = AMOVW
+		p.From.Type = obj.TYPE_CONST
+		p.From.Offset = int64(framesize) + int64(objabi.StackGuard) - objabi.StackSmall
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = REG_R3
+		p.Scond = C_SCOND_NE
+
+		p = obj.Appendp(p, c.newprog)
+		p.As = ACMP
+		p.From.Type = obj.TYPE_REG
+		p.From.Reg = REG_R3
+		p.Reg = REG_R2
+		p.Scond = C_SCOND_NE
+	}
+
+	// BLS call-to-morestack
+	bls := obj.Appendp(p, c.newprog)
+	bls.As = ABLS
+	bls.To.Type = obj.TYPE_BRANCH
+
+	var last *obj.Prog
+	for last = c.cursym.Func.Text; last.Link != nil; last = last.Link {
+	}
+
+	// Now we are at the end of the function, but logically
+	// we are still in function prologue. We need to fix the
+	// SP data and PCDATA.
+	spfix := obj.Appendp(last, c.newprog)
+	spfix.As = obj.ANOP
+	spfix.Spadj = -framesize
+
+	pcdata := c.ctxt.EmitEntryLiveness(c.cursym, spfix, c.newprog)
+
+	// MOVW	LR, R3
+	movw := obj.Appendp(pcdata, c.newprog)
+	movw.As = AMOVW
+	movw.From.Type = obj.TYPE_REG
+	movw.From.Reg = REGLINK
+	movw.To.Type = obj.TYPE_REG
+	movw.To.Reg = REG_R3
+
+	bls.Pcond = movw
+
+	// BL runtime.morestack
+	call := obj.Appendp(movw, c.newprog)
+	call.As = obj.ACALL
+	call.To.Type = obj.TYPE_BRANCH
+	morestack := "runtime.morestack"
+	switch {
+	case c.cursym.CFunc():
+		morestack = "runtime.morestackc"
+	case !c.cursym.Func.Text.From.Sym.NeedCtxt():
+		morestack = "runtime.morestack_noctxt"
+	}
+	call.To.Sym = c.ctxt.Lookup(morestack)
+
+	// B start
+	b := obj.Appendp(call, c.newprog)
+	b.As = obj.AJMP
+	b.To.Type = obj.TYPE_BRANCH
+	b.Pcond = c.cursym.Func.Text.Link
+	b.Spadj = +framesize
+
+	return bls
+}
+
+func progedit(ctxt *obj.Link, p *obj.Prog, newprog obj.ProgAlloc) {
+	p.From.Class = 0
+	p.To.Class = 0
+
+	c := &Ctx{ctxt: ctxt, newprog: newprog}
+
+	// Rewrite RSB Rm, Rn, Rd to SUB Rn, Rm, Rd to allow T16 encoding
+	if p.As == ARSB && p.From.Type == obj.TYPE_REG {
+		p.As = ASUB
+		if p.Reg == 0 {
+			p.Reg = p.To.Reg
+		}
+		p.From.Reg, p.Reg = p.Reg, p.From.Reg
+	}
+	// Rewrite 3-arg to 2-arg
+	if p.Reg != 0 && p.To.Type == obj.TYPE_REG && p.To.Reg == p.Reg {
+		p.Reg = 0
+	}
+	// Rewrite TYPE_SHIFT R<<i(R) to TYPE_MEM
+	if p.From.Type == obj.TYPE_SHIFT && p.From.Reg != 0 {
+		if !shifttomem(&p.From) {
+			c.ctxt.Diag("bad src addr mode: %v", p)
+		}
+	} else if p.To.Type == obj.TYPE_SHIFT && p.To.Reg != 0 {
+		if !shifttomem(&p.To) {
+			c.ctxt.Diag("bad dst addr mode: %v", p)
+		}
+	}
+	if p.As == obj.ADUFFZERO || p.As == obj.ADUFFCOPY {
+		// as long we do not support dynamic linking they are identical to BL
+		p.As = ABL
+	}
+	// Rewrite B/BL to symbol as TYPE_BRANCH.
+	if p.As == AB || p.As == ABL || ABEQ <= p.As && p.As <= ACBNZ {
+		if p.To.Type == obj.TYPE_MEM && (p.To.Name == obj.NAME_EXTERN || p.To.Name == obj.NAME_STATIC) && p.To.Sym != nil {
+			p.To.Type = obj.TYPE_BRANCH
+		}
+		return
+	}
+	// Rewrite SRA/SRL/SLL to MOVW
+	if ASLL <= p.As && p.As <= ASRA {
+		Rn := int(p.To.Reg)
+		if p.Reg != 0 {
+			Rn = int(p.Reg)
+			p.Reg = 0
+		}
+		typ := int(p.As - ASLL)
+		p.As = AMOVW
+		if p.From.Type == obj.TYPE_REG {
+			p.From.Offset = int64(int(p.From.Reg)&15<<8 | typ<<5 | 1<<4 | Rn&15)
+		} else {
+			p.From.Offset = int64(int(p.From.Offset)<<7 | typ<<5 | Rn&15)
+		}
+		p.From.Type = obj.TYPE_SHIFT
+		p.From.Reg = 0
+		return
+	}
+	switch p.As {
+	// Rewrite float constants to values stored in memory.
+	case AMOVF:
+		if p.From.Type == obj.TYPE_FCONST && c.chipfloat(p.From.Val.(float64)) < 0 && (c.chipzero(p.From.Val.(float64)) < 0 || p.Scond&C_SCOND != C_SCOND_NONE) {
+			f32 := float32(p.From.Val.(float64))
+			p.From.Type = obj.TYPE_MEM
+			p.From.Sym = ctxt.Float32Sym(f32)
+			p.From.Name = obj.NAME_EXTERN
+			p.From.Offset = 0
+		}
+	case AMOVD:
+		if p.From.Type == obj.TYPE_FCONST && c.chipfloat(p.From.Val.(float64)) < 0 && (c.chipzero(p.From.Val.(float64)) < 0 || p.Scond&C_SCOND != C_SCOND_NONE) {
+			p.From.Type = obj.TYPE_MEM
+			p.From.Sym = ctxt.Float64Sym(p.From.Val.(float64))
+			p.From.Name = obj.NAME_EXTERN
+			p.From.Offset = 0
+		}
+	}
+}
+
+func nocache(p *obj.Prog) {
+	p.Optab = 0
+	p.From.Class = 0
+	if p.GetFrom3() != nil {
+		p.GetFrom3().Class = 0
+	}
+	p.To.Class = 0
+}
+
+func onesCount(u uint) int {
+	n := 0
+	for u != 0 {
+		n += int(u & 1)
+		u >>= 1
+	}
+	return n
+}
+
+func shifttomem(a *obj.Addr) bool {
+	if a.Offset>>4&3 != 0 {
+		return false // Thumb2 supports only LSL
+	}
+	shift := uint(a.Offset) >> 7 & 31
+	if shift > 3 {
+		return false // Thumb2 supports only 2-bit shift
+	}
+	a.Type = obj.TYPE_MEM
+	a.Index = REG_R0 + int16(a.Offset)&15
+	if shift != 0 {
+		a.Scale = 1 << shift
+	}
+	return true
+}
+
+var unaryDst = map[obj.As]bool{
+	AWORD:      true,
+	AB:         true,
+	ABL:        true,
+	ASWI:       true,
+	ABKPT:      true,
+	obj.AUNDEF: true,
+}
+
+var Link = obj.LinkArch{
+	Arch:           sys.ArchThumb,
+	Init:           buildop,
+	Preprocess:     preprocess,
+	Assemble:       span,
+	Progedit:       progedit,
+	UnaryDst:       unaryDst,
+	DWARFRegisters: ARMDWARFRegisters,
+}
diff --git a/src/cmd/internal/obj/thumb/optab.go b/src/cmd/internal/obj/thumb/optab.go
new file mode 100644
index 0000000000..b1efabac86
--- /dev/null
+++ b/src/cmd/internal/obj/thumb/optab.go
@@ -0,0 +1,1632 @@
+// Inferno utils/5l/span.c
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5l/span.c
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"cmd/internal/obj"
+	"cmd/internal/objabi"
+	"log"
+)
+
+type asmoutFunc func(c *Ctx, p *obj.Prog, out []uint16) int
+
+type Optab struct {
+	as     obj.As
+	a1     Aclass
+	a2     Aclass
+	a3     Aclass
+	size   uint8 // instr_num<<4 | code_size
+	flag   uint8
+	rscond uint8 // flags required by the instruction
+	oscond uint8 // optional flags accepted by the instruction
+	asmout asmoutFunc
+}
+
+const (
+	LFROM = 1 << iota
+	LTO
+	VALID
+	NOIT
+)
+
+const pcoff = 4 // in Thumb mode PC points 4 bytes forward
+
+// optab contains description of Thumb2 instruction set in slightly compressed form. buildop
+// decompress it to oprange keeping the order of instruction variants unchanged. oplook
+// returns the first matched description so the order matters. Current order ensures ISA
+// requirements and prefers 16-bit variants.
+//
+// optab is manual translation of instr_group.txt - please keep them in sync.
+// TODO: automatic translation of instr_group.txt.
+var optab = [...]Optab{
+	// as, a1, a2, a3, size, flag, rscond, oscond, asmout
+
+	{obj.ATEXT, C_LOREG, C_NONE, C_TEXTSIZE, 0, 0, 0, 0, nil},
+	{obj.AFUNCDATA, C_LCON, C_NONE, C_LOREG, 0, 0, 0, 0, nil},
+	{obj.APCDATA, C_LCON, C_NONE, C_LCON, 0, 0, 0, 0, nil},
+	{obj.ANOP, C_NONE, C_NONE, C_NONE, 0, VALID, 0, 0, nil},
+
+	{AWORD, C_NONE, C_NONE, C_LCON, 0x14, 0, 0, 0, _WORD__u32},
+	{AWORD, C_NONE, C_NONE, C_LOREG, 0x14, 0, 0, 0, _WORD__u32},
+
+	{AHWORD, C_U16CON, C_NONE, C_NONE, 0x12, 0, 0, 0, _HWORD__u16},
+
+	{AADD, C_REG, C_NONE, C_REG, 0x12, 0, 0, C_PBIT, _ADD__Rm__Rdn},        // ADD Rm, Rdn
+	{AADD, C_U8CON2, C_SP, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u8_2__R13__Rd}, // ADD u8<<2, R13, Rd
+	{AADD, C_U8CON2, C_PC, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u8_2__R13__Rd}, // ADD u8<<2, R15, Rd
+	{as: obj.AXXX},
+
+	{AADD, C_RLO, C_RLO, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__Rm__Rn__Rd},                    // ADD   Rm, Rn, Rd
+	{AADD, C_RLO, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__Rm__Rn__Rd},                   // ADD   Rm, Rdn
+	{AADD, C_SHIFTI, C_REG, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd},  // ADD.s Rm<v>u5, Rn, Rd
+	{AADD, C_SHIFTI, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd}, // ADD.s Rm<v>u5, Rdn
+	{AADD, C_U3CON, C_RLO, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u3__Rn__Rd},                  // ADD   u3, Rn, Rd
+	{AADD, C_U8CON, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u8__Rdn},                    // ADD   u8, Rdn
+	{AADD, C_U7CON2, C_NONE, C_SP, 0x12, 0, C_SBIT, 0, _ADD__u7_2__SP},                   // ADD   u7<<2, R13
+	{AADD, C_U12CON, C_REG, C_REG, 0x14, 0, 0, C_PBIT, _ADD__u12__Rn__Rd},                // ADD   u12, Rn, Rd
+	{AADD, C_U12CON, C_NONE, C_REG, 0x14, 0, 0, C_PBIT, _ADD__u12__Rn__Rd},               // ADD   u12, Rdn
+	{AADD, C_E32CON, C_REG, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},      // ADD.s e32, Rn, Rd
+	{AADD, C_E32CON, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},     // ADD.s e32, Rdn
+	{as: ASUB},
+
+	{AADD, C_LCON, C_NONE, C_REG, 0x26, LFROM, 0, C_PBIT, _ADD__lit__Rdn}, // ADD   lit, Rdn
+	{as: obj.AXXX},
+
+	{AADD, C_LCON, C_RLO, C_RLO, 0x26, LFROM, C_SBIT, 0, _ADD__lit__Rn__Rd},           // ADD   lit, Rn, Rd
+	{AADD, C_LCON, C_NONE, C_RLO, 0x26, LFROM, C_SBIT, 0, _ADD__lit__Rn__Rd},          // ADD   lit, Rdn
+	{AADD, C_LCON, C_REG, C_REG, 0x28, LFROM, 0, C_SBIT | C_PBIT, _ADD__lit__Rn__Rd},  // ADD.s lit, Rn, Rd
+	{AADD, C_LCON, C_NONE, C_REG, 0x28, LFROM, 0, C_SBIT | C_PBIT, _ADD__lit__Rn__Rd}, // ADD.s lit, Rdn
+	{as: ASUB},
+
+	{AAND, C_RLO, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _AND__Rm__Rdn}, // AND Rm, Rdn
+	{as: AADC},
+	{as: ABIC},
+	{as: AEOR},
+	{as: AORR},
+	{as: ASBC},
+	{as: AMUL},
+	{as: AMVN},
+
+	{ARSB, C_ZCON, C_RLO, C_RLO, 0x12, 0, C_SBIT, 0, _AND__Rm__Rdn}, // RSB $0, Rn, Rd ; old NEG
+
+	{AAND, C_SHIFTI, C_REG, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd},  // AND.s Rm<v>u5, Rn, Rd
+	{AAND, C_SHIFTI, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd}, // AND.s Rm<v>u5, Rdn
+	{AAND, C_E32CON, C_REG, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},      // AND.s e32, Rn, Rd
+	{AAND, C_E32CON, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},     // AND.s e32, Rdn
+	{as: AADC},
+	{as: ABIC},
+	{as: AEOR},
+	{as: AORR},
+	{as: ARSB},
+	{as: ASBC},
+	{as: AORN},
+
+	{AAND, C_LCON, C_NONE, C_RLO, 0x26, LFROM, C_SBIT, 0, _ADD__lit__Rdn}, // AND lit, Rdn
+	{as: AADC},
+	{as: ABIC},
+	{as: AEOR},
+	{as: AORR},
+	{as: ASBC},
+
+	{AAND, C_LCON, C_REG, C_REG, 0x28, LFROM, 0, C_SBIT | C_PBIT, _ADD__lit__Rn__Rd},  // AND.s lit, Rn, Rd
+	{AAND, C_LCON, C_NONE, C_REG, 0x28, LFROM, 0, C_SBIT | C_PBIT, _ADD__lit__Rn__Rd}, // AND.s lit, Rdn
+	{as: AADC},
+	{as: ABIC},
+	{as: AEOR},
+	{as: AORR},
+	{as: ARSB},
+	{as: ASBC},
+	{as: AORN},
+
+	{AMOVW, C_REG, C_NONE, C_REG, 0x12, 0, 0, C_PBIT, _MOVW__Rm__Rd},                    // MOVW Rm, Rd
+	{AMOVW, C_RLO, C_NONE, C_RLO, 0x12, NOIT, C_SBIT, 0, _MOVW__Rm__Rd},                 // MOVW Rm, Rd
+	{AMOVW, C_SHIFTILO, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _MOVW__Rm_v_u5__Rd},          // MOVW Rm<v>u5, Rd ; vv != 11
+	{AMOVW, C_SHIFTRLO, C_NONE, C_NONE, 0x12, 0, C_SBIT, 0, _MOVW__Rdn_v_Rm__Rdn},       // MOVW Rdn<v>Rm, Rdn
+	{AMOVW, C_SHIFTR, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _MOVWs__Rn_v_Rm__Rd},  // MOVW.s Rn<v>Rm, Rd
+	{AMOVW, C_SHIFTR, C_NONE, C_NONE, 0x14, 0, 0, C_SBIT | C_PBIT, _MOVWs__Rn_v_Rm__Rd}, // MOVW.s Rn<v>Rm, Rd
+
+	{AMOVW, C_U8CON, C_NONE, C_RLO, 0x12, 0, C_SBIT, 0, _ADD__u8__Rdn},     // MOVW u8, Rd
+	{AMOVW, C_U16CON, C_NONE, C_REG, 0x14, 0, 0, C_PBIT, _MOVW__uyz16__Rd}, // MOVW uyz16, Rd
+	{AMOVT, C_U16CON, C_NONE, C_REG, 0x14, 0, 0, C_PBIT, _MOVW__uyz16__Rd}, // MOVT uyz16, Rd
+
+	{AMOVW, C_SHIFTI, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__Rm_v_u5__Rn__Rd}, // MOVW.s Rm<v>u5, Rd
+	{AMOVW, C_E32CON, C_NONE, C_REG, 0x14, 0, 0, C_SBIT | C_PBIT, _ADDs__e32__Rn__Rd},     // MOVW.s e32, Rd
+	{AMOVW, C_LCON, C_NONE, C_REG, 0x14, LFROM, 0, C_PBIT, _MOVW__lit__Rd},                // MOVW lit, Rd
+	{as: AMVN}, // MVN lit, Rd is converted to MOVW -lit, Rd
+
+	{AMOVH, C_RLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVH__Rm__Rd},        // MOVH Rm, Rd
+	{AMOVH, C_SHIFTI, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVH__Rm_rot__Rd}, // MOVH Rm@>rot, Rd
+	{as: AMOVB},
+	{as: AMOVHU},
+	{as: AMOVBU},
+
+	{AMOVW, C_SPEC, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__SYSm__Rd}, // MOVW SYSm, Rd
+	{AMOVW, C_REG, C_NONE, C_SPEC, 0x14, 0, 0, 0, _MOVW__SYSm__Rd}, // MOVW Rn, SYSm
+
+	{AMUL, C_REG, C_REG, C_REG, 0x14, 0, 0, C_PBIT, _MUL__Rm__Rn__Rd},  // MUL Rm, Rn, Rd
+	{AMUL, C_REG, C_NONE, C_REG, 0x14, 0, 0, C_PBIT, _MUL__Rm__Rn__Rd}, // MUL Rm, Rdn
+	{as: ADIV},
+	{as: ADIVU},
+
+	{AMULL, C_REG, C_REG, C_REGREG, 0x14, 0, 0, 0, _MULL__Rm__Rn__Rdh_Rdl}, // MULL Rm, Rn, (Rdh, Rdl)
+	{as: AMULLU},
+	{as: AMULAL},
+	{as: AMULALU},
+
+	{AMULA, C_REG, C_REG, C_REGREG2, 0x14, 0, 0, 0, _MULA__Rm__Rn__Ra__Rd}, // MULA Rm, Rn, Ra, Rd
+	{as: AMULS},
+	{as: AMULAWB},
+	{as: AMULAWT},
+
+	{AREV, C_RLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _REV__Rm__Rd}, // REV Rm, Rd
+	{as: AREV16},
+	{as: AREVSH},
+
+	{ACLZ, C_REG, C_NONE, C_REG, 0x14, 0, 0, 0, _CLZ__Rm__Rd}, // CLZ Rm, Rd
+	{as: AREV},
+	{as: AREV16},
+	{as: ARBIT},
+	{as: AREVSH},
+
+	{ASEL, C_REG, C_REG, C_REG, 0x14, 0, 0, 0, _CLZ__Rm__Rd},  // SEL Rm, Rn, Rd ; DSP extension
+	{ASEL, C_REG, C_NONE, C_REG, 0x14, 0, 0, 0, _CLZ__Rm__Rd}, // SEL Rm, Rd     ; DSP extension
+
+	{ABFX, C_LCON, C_REG, C_REG, 0x14, 0, 0, 0, _BFX__width__ulsb__Rn__Rd},  // BFX width, ulsb, Rn, Rd
+	{ABFX, C_LCON, C_NONE, C_REG, 0x14, 0, 0, 0, _BFX__width__ulsb__Rn__Rd}, // BFX width, ulsb, Rdn
+	{as: ABFXU},
+	{as: ABFI},
+
+	{ABFC, C_LCON, C_NONE, C_REG, 0x14, 0, 0, 0, _BFX__width__ulsb__Rn__Rd}, // BFC width, ulsb, Rd
+
+	{ATST, C_RLO, C_RLO, C_NONE, 0x12, 0, 0, 0, _TST__Rm__Rn}, // TST Rm, Rn
+	{as: ACMP},
+	{as: ACMN},
+
+	{ACMP, C_REG, C_REG, C_NONE, 0x12, 0, 0, 0, _CMP__Rm__Rn},   // CMP Rm, Rn
+	{ACMP, C_U8CON, C_RLO, C_NONE, 0x12, 0, 0, 0, _CMP__u8__Rn}, // CMP u8, Rn
+
+	{ATST, C_SHIFTI, C_REG, C_NONE, 0x14, 0, 0, 0, _TST__Rm_v_u5__Rn}, // TST Rm<v>u5, Rn
+	{ATST, C_E32CON, C_REG, C_NONE, 0x14, 0, 0, 0, _TST__e32__Rn},     // TST e32, Rn
+	{as: ATEQ},
+	{as: ACMN},
+	{as: ACMP},
+
+	{ACMN, C_LCON, C_RLO, C_NONE, 0x26, LFROM, 0, 0, _CMN__lit__Rn}, // CMN lit, Rn
+	{as: ATST},
+	{as: ACMP},
+
+	{ACMP, C_LCON, C_REG, C_NONE, 0x26, LFROM, 0, 0, _CMP__lit__Rn}, // CMP lit, Rn
+
+	{ATST, C_LCON, C_REG, C_NONE, 0x28, LFROM, 0, 0, _TST__lit__Rn}, // TST lit, Rn
+	{as: ATEQ},
+	{as: ACMN},
+
+	{AMOVW, C_WORLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt},  // MOVW  u5<<2(Rn), Rt
+	{AMOVW, C_WOSP, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u8_2_R13__Rt},  // MOVW  u8<<2(R13), Rt
+	{AMOVW, C_WOPC, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u8_2_R13__Rt},  // MOVW  u8<<2(R15), Rt
+	{AMOVHU, C_HORLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVHU u5<<1(Rn), Rt
+	{AMOVBU, C_BORLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVBU u5(Rn), Rt
+
+	{AMOVW, C_RLO, C_NONE, C_WORLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVW Rt, u5<<2(Rn)
+	{AMOVW, C_RLO, C_NONE, C_WOSP, 0x12, 0, 0, 0, _MOVW__u8_2_R13__Rt}, // MOVW Rt, u8<<2(R13)
+	{AMOVH, C_RLO, C_NONE, C_HORLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVH Rt, u5<<1(Rn)
+	{as: AMOVHU},
+
+	{AMOVB, C_RLO, C_NONE, C_BORLO, 0x12, 0, 0, 0, _MOVW__u5_2_Rn__Rt}, // MOVB Rt, u5(Rn)
+	{as: AMOVBU},
+
+	{AMOVW, C_RORLO, C_NONE, C_RLO, 0x12, 0, 0, 0, _MOVW__Rn_Rm__Rt},                 // MOVW (Rn)(Rm), Rt
+	{AMOVW, C_ROREG, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__Rn_Rm_1_u2__Rt},            // MOVW (Rn)(Rm*1<<u2), Rt
+	{AMOVW, C_SOPC, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__s12_Rn__Rt},                 // MOVW u12(R15), Rt
+	{AMOVW, C_UOREG, C_NONE, C_REG, 0x14, 0, 0, 0, _MOVW__s12_Rn__Rt},                // MOVW u12(Rn), Rt
+	{AMOVW, C_SOREG, C_NONE, C_REG, 0x14, 0, 0, C_PBIT | C_WBIT, _MOVWpw__s8_Rn__Rt}, // MOVW.p.w u8(Rn), Rt
+	{as: AMOVH},
+	{as: AMOVHU},
+	{as: AMOVB},
+	{as: AMOVBU},
+
+	{AMOVW, C_LORLO, C_NONE, C_RLO, 0x26, LFROM, 0, 0, _MOVW__lit_Rn__Rt}, // MOVW lit(Rn), Rt
+	{as: AMOVHU},
+	{as: AMOVBU},
+
+	{AMOVW, C_LOREG, C_NONE, C_REG, 0x28, LFROM, 0, 0, _MOVW__lit_Rn__Rt}, // MOVW lit(Rn), Rt
+	{as: AMOVH},
+	{as: AMOVHU},
+	{as: AMOVB},
+	{as: AMOVBU},
+
+	{AMOVW, C_RLO, C_NONE, C_RORLO, 0x12, 0, 0, 0, _MOVW__Rn_Rm__Rt},                 // MOVW Rt, (Rn)(Rm)
+	{AMOVW, C_REG, C_NONE, C_ROREG, 0x14, 0, 0, 0, _MOVW__Rn_Rm_1_u2__Rt},            // MOVW Rt, (Rn)(Rm*1<<u2)
+	{AMOVW, C_REG, C_NONE, C_UOREG, 0x14, 0, 0, 0, _MOVW__s12_Rn__Rt},                // MOVW Rt, u12(Rn)
+	{AMOVW, C_REG, C_NONE, C_SOREG, 0x14, 0, 0, C_PBIT | C_WBIT, _MOVWpw__s8_Rn__Rt}, // MOVW.p.w Rt, u8(Rn)
+	{AMOVW, C_RLO, C_NONE, C_LORLO, 0x26, LTO, 0, 0, _MOVW__lit_Rn__Rt},              // MOVW Rt, lit(Rn)
+	{AMOVW, C_REG, C_NONE, C_LOREG, 0x28, LTO, 0, 0, _MOVW__lit_Rn__Rt},              // MOVW Rt, lit(Rn)
+	{as: AMOVH},
+	{as: AMOVHU},
+	{as: AMOVB},
+	{as: AMOVBU},
+
+	{ASQRTF, C_FREG, C_NONE, C_FREG, 0x14, 0, 0, 0, _SQRTF__Fm__Fd}, // SQRTF Fm, Fd
+	{as: ASQRTD},
+
+	{AMOVF, C_FOREG, C_NONE, C_FREG, 0x14, 0, 0, 0, _MOVF__s8_2_Rn__Fd}, // MOVF u8<<2(Rn), Fd
+	{AMOVF, C_FREG, C_NONE, C_FOREG, 0x14, 0, 0, 0, _MOVF__s8_2_Rn__Fd}, // MOVF Fd, u8<<2(Rn)
+	{AMOVF, C_SFCON, C_NONE, C_FREG, 0x14, 0, 0, 0, _MOVF__f8__Fd},      // MOVF f8, Fd
+	{as: AMOVD},
+
+	{AIT, C_IT, C_NONE, C_NONE, 0x12, 0, 0, 0, _ITmask__firstcond}, // ITmask firstcond
+	{as: AITT},
+	{as: AITE},
+	{as: AITTT},
+	{as: AITET},
+	{as: AITTE},
+	{as: AITEE},
+	{as: AITTTT},
+	{as: AITETT},
+	{as: AITTET},
+	{as: AITEET},
+	{as: AITTTE},
+	{as: AITETE},
+	{as: AITTEE},
+	{as: AITEEE},
+
+	{AB, C_NONE, C_NONE, C_S11BRA, 0x12, 0, 0, 0, _B__i11_1}, // B i11<<1
+
+	{ACBZ, C_RLO, C_NONE, C_U6BRA, 0x12, 0, 0, 0, _CBZ__Rn__u6_1}, // CBZ Rn, u6<<1
+	{as: ACBNZ},
+
+	{AB, C_NONE, C_NONE, C_ZOREG, 0x12, 0, 0, 0, _B__Rm},      // B (Rm)  ; ARM: BX Rm
+	{AB, C_NONE, C_NONE, C_S24BRA, 0x14, 0, 0, 0, _B__ji24_1}, // B ji24<<1
+	{as: ABL},
+
+	{ABEQ, C_NONE, C_NONE, C_S8BRA, 0x12, 0, 0, 0, _Bcond__i8_1},    // Bcond i8<<1
+	{ABEQ, C_NONE, C_NONE, C_S20BRA, 0x14, 0, 0, 0, _Bcond__ji20_1}, // Bcond ji20<<1
+	{as: ABNE},
+	{as: ABCS},
+	{as: ABHS},
+	{as: ABCC},
+	{as: ABLO},
+	{as: ABMI},
+	{as: ABPL},
+	{as: ABVS},
+	{as: ABVC},
+	{as: ABHI},
+	{as: ABLS},
+	{as: ABGE},
+	{as: ABLT},
+	{as: ABGT},
+	{as: ABLE},
+
+	{ATBB, C_REG, C_REG, C_NONE, 0x14, 0, 0, 0, _TBB__Rm__Rn}, // TBB Rm, Rn
+	{as: ATBH},
+
+	{ASWI, C_NONE, C_NONE, C_U8CON, 0x12, 0, 0, 0, _SWI__u8}, // SWI u8
+	{ASWI, C_NONE, C_NONE, C_NONE, 0x12, 0, 0, 0, _SWI__u8},  // SWI
+	{as: ABKPT},
+	{as: obj.AUNDEF},
+
+	{AMOVM, C_LISTLOLR, C_NONE, C_ZOSP, 0x12, 0, C_DB | C_WBIT, 0, _PUSH__reglist}, // PUSH reglist (MOVM.DB.W)
+	{AMOVM, C_LISTLO, C_NONE, C_ZORLO, 0x12, 0, C_IA | C_WBIT, 0, _MOVM_IAW},       // MOVM.IA.W reglist, (Rn)
+	{AMOVM, C_LIST, C_NONE, C_LOREG, 0x14, 0, C_IA, C_WBIT, _MOVM_IAw},             // MOVM.IA.w reglist, (Rn)
+	{AMOVM, C_LIST, C_NONE, C_LOREG, 0x14, 0, C_DB, C_WBIT, _MOVM_IAw},             // MOVM.DB.w reglist, (Rn)
+
+	{AMOVM, C_ZOSP, C_NONE, C_LISTLOPC, 0x12, 0, C_IA | C_WBIT, 0, _PUSH__reglist}, // POP reglist (MOVM.IA.W)
+	{AMOVM, C_ZORLO, C_NONE, C_LISTLO, 0x12, 0, C_IA, C_WBIT, _MOVM_IAW},           // MOVM.IA.W (Rn), reglist
+	{AMOVM, C_LOREG, C_NONE, C_LIST, 0x14, 0, C_IA, C_WBIT, _MOVM_IAw},             // MOVM.IA.w (Rn), reglist
+	{AMOVM, C_LOREG, C_NONE, C_LIST, 0x14, 0, C_DB, C_WBIT, _MOVM_IAw},             // MOVM.DB.w (Rn), reglist
+
+	{ALDREX, C_LOREG, C_NONE, C_REG, 0x14, 0, 0, 0, _LDREX__u8_2_Rn__Rt}, // LDREX u8<<2(Rn), Rt
+	{ASTREX, C_LOREG, C_REG, C_REG, 0x14, 0, 0, 0, _LDREX__u8_2_Rn__Rt},  // STREX Rt, u8<<2(Rn), Rd
+
+	{ALDREXB, C_LOREG, C_NONE, C_REG, 0x14, 0, 0, 0, _LDREXB__Rn__Rt}, // LDREXB (Rn), Rt
+	{as: ALDREXH},
+
+	{ASTREXB, C_LOREG, C_REG, C_REG, 0x14, 0, 0, 0, _LDREXB__Rn__Rt}, // STREXB Rt, (Rn), Rd
+	{as: ASTREXH},
+
+	{ANOP2, C_NONE, C_NONE, C_NONE, 0x12, 0, 0, 0, _NOP2},
+	{as: AYIELD},
+	{as: AWFE},
+	{as: AWFI},
+	{as: ASEV},
+
+	{ACPSID, C_NONE, C_NONE, C_NONE, 0x12, 0, 0, 0, _CPSID},
+	{as: ACPSIE},
+	
+	{ANOP4, C_NONE, C_NONE, C_NONE, 0x14, 0, 0, 0, _NOP4},
+	{as: ACLREX},
+
+	{ADSB, C_NONE, C_NONE, C_NONE, 0x14, 0, 0, 0, _DSB}, // DSB
+	{ADSB, C_MB, C_NONE, C_NONE, 0x14, 0, 0, 0, _DSB},   // DSB opt
+	{as: ADMB},
+	{as: AISB},
+}
+
+// 16-bit instructions
+
+// 0100 0100 dmmm mddd
+func _ADD__Rm__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	out[0] = uint16(0x44<<8 | Rd&8<<4 | Rm&15<<3 | Rd&7)
+	return 2
+}
+
+// 0001 10xm mmnn nddd
+func _ADD__Rm__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x1800
+	if p.As == ASUB {
+		o1 |= 0x0200
+	}
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg)
+	}
+	out[0] = uint16(o1 | Rm&7<<6 | Rn&7<<3 | Rd&7)
+	return 2
+}
+
+// 0001 11xu uunn nddd
+func _ADD__u3__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x1C00
+	if p.As == ASUB {
+		o1 |= 0x0200
+	}
+	u3 := int(p.From.Offset)
+	Rn := int(p.Reg)
+	Rd := int(p.To.Reg)
+	out[0] = uint16(o1 | u3<<6 | Rn&7<<3 | Rd&7)
+	return 2
+}
+
+// 1011 0000 xuuu uuuu
+func _ADD__u7_2__SP(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xB000
+	if p.As == ASUB {
+		o1 |= 0x0080
+	}
+	u7 := int(p.From.Offset) >> 2
+	out[0] = uint16(o1 | u7)
+	return 2
+}
+
+// 001x xddd uuuu uuuu
+func _ADD__u8__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x3000 // AADD
+	switch p.As {
+	case AMOVW:
+		o1 = 0x2000
+	case ASUB:
+		o1 = 0x3800
+	}
+	Rd := int(p.To.Reg)
+	u8 := int(p.From.Offset)
+	out[0] = uint16(o1 | Rd&7<<8 | u8)
+	return 2
+}
+
+// 1010 xddd uuuu uuuu
+func _ADD__u8_2__R13__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xA000
+	if p.Reg == REGSP {
+		o1 |= 0x0800
+	}
+	Rd := int(p.To.Reg)
+	u8 := int(p.From.Offset) >> 2
+	out[0] = uint16(o1 | Rd&7<<8 | u8)
+	return 2
+}
+
+// 0100 00xx xxmm mddd
+func _AND__Rm__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	var o1 int
+	switch p.As {
+	case AAND:
+		o1 = 0x4000
+	case AADC:
+		o1 = 0x4140
+	case ABIC:
+		o1 = 0x4380
+	case AEOR:
+		o1 = 0x4040
+	case AORR:
+		o1 = 0x4300
+	case ARSB:
+		o1 = 0x4240
+		Rm = int(p.Reg)
+	case ASBC:
+		o1 = 0x4180
+	case AMUL:
+		o1 = 0x4340
+	default: // AMVN
+		o1 = 0x43C0
+	}
+	out[0] = uint16(o1 | Rm&7<<3 | Rd&7)
+	return 2
+}
+
+// 1110 0iii iiii iiii
+func _B__i11_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	v := int(p.Pcond.Pc-p.Pc-pcoff) >> 1
+	out[0] = uint16(0xE000 | v&0x7FF)
+	return 2
+}
+
+// 0100 0111 xmmm m000
+func _B__Rm(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x4700 | int(p.To.Reg)&15<<3
+	if p.As == ABL {
+		o1 |= 0x0080
+		rel := obj.Addrel(c.cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 0
+		rel.Type = objabi.R_CALLIND
+	}
+	out[0] = uint16(o1)
+	return 2
+}
+
+// 1101 cccc iiii iiii
+func _Bcond__i8_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	v := int(p.Pcond.Pc-p.Pc-pcoff) >> 1
+	out[0] = uint16(0xD000 | obcond(p.As)<<8 | v&0xFF)
+	return 2
+}
+
+// 1011 x0u1 uuuu unnn
+func _CBZ__Rn__u6_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xB100
+	if p.As == ACBNZ {
+		o1 |= 0x0800
+	}
+	Rn := int(p.From.Reg & 7)
+	v := int(p.Pcond.Pc-p.Pc-4) >> 1
+	out[0] = uint16(o1 | v&0x20<<4 | v&0x1F<<3 | Rn)
+	return 2
+}
+
+// 0100 0101 nmmm mnnn
+func _CMP__Rm__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	out[0] = uint16(0x4500 | Rn&8<<4 | Rm&15<<3 | Rn&7)
+	return 2
+}
+
+// 0010 1nnn uuuu uuuu
+func _CMP__u8__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0x2800
+	Rn := int(p.Reg)
+	u8 := int(p.From.Offset)
+	out[0] = uint16(o1 | Rn&7<<8 | u8)
+	return 2
+}
+
+// 1011 1111 cccc mmmm
+func _ITmask__firstcond(c *Ctx, p *obj.Prog, out []uint16) int {
+	out[0] = uint16(0xBF00 | int(p.Scond)<<4 | int(p.Mark))
+	return 2
+}
+
+// 1011 0010 xxmm mddd
+func _MOVH__Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xB200 // AMOVH
+	switch p.As {
+	case AMOVB:
+		o1 |= 0x0040
+	case AMOVHU:
+		o1 |= 0x0080
+	case AMOVBU:
+		o1 |= 0x00C0
+	}
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	out[0] = uint16(o1 | Rm&7<<3 | Rd&7)
+	return 2
+}
+
+// 1100 xnnn rrrr rrrr
+func _MOVM_IAW(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xC000
+	rlist, Rn := int(p.From.Offset), int(p.To.Reg)
+	if p.To.Type == obj.TYPE_REGLIST {
+		if !c.checkldm(p) {
+			return 0
+		}
+		rlist, Rn = int(p.To.Offset), int(p.From.Reg)
+		o1 |= 0x0800
+	}
+	out[0] = uint16(o1 | Rn&15<<8 | rlist)
+	return 2
+}
+
+// 0100 000x xxmm mddd (xxx => vv)
+func _MOVW__Rdn_v_Rm__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rdn, typ, Rm := shiftr(int(p.From.Offset))
+	var o1 int
+	switch typ {
+	case 0: // <<
+		o1 = 0x4080
+	case 1: // >>
+		o1 = 0x40C0
+	case 2: // ->
+		o1 = 0x4100
+	default: // @>
+		o1 = 0x41C0
+	}
+	out[0] = uint16(o1 | Rm<<3 | Rdn)
+	return 2
+}
+
+// 0100 0110 dmmm mddd
+// 0000 0000 00mm mddd
+func _MOVW__Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	if p.Scond&C_SBIT == 0 {
+		out[0] = uint16(0x4600 | Rd&8<<4 | Rm&15<<3 | Rd&7)
+	} else {
+		out[0] = uint16(0x0000 | Rm&7<<3 | Rd&7)
+	}
+	return 2
+}
+
+// 000v vuuu uumm mddd (vv != 11)
+func _MOVW__Rm_v_u5__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	Rm, typ, count := shifti(int(p.From.Offset))
+	Rd := int(p.To.Reg) & 7
+	out[0] = uint16(typ<<11 | count<<6 | Rm<<3 | Rd)
+	return 2
+}
+
+// 0101 xxxm mmnn nttt
+func _MOVW__Rn_Rm__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1 int
+	mem, r := &p.From, &p.To
+	if r.Type == obj.TYPE_REG {
+		switch p.As {
+		case AMOVW: // LDR
+			o1 = 0x5800
+		case AMOVB: // LDRSB
+			o1 = 0x5600
+		case AMOVBU: // LDRB
+			o1 = 0x5C00
+		case AMOVH: // LDRSH
+			o1 = 0x5E00
+		default: // AMOVHU (LDRH)
+			o1 = 0x5A00
+		}
+	} else {
+		mem, r = r, mem
+		switch p.As {
+		case AMOVW: // STR
+			o1 = 0x5000
+		case AMOVB, AMOVBU: // STRB
+			o1 = 0x5400
+		default: // AMOVH, AMOVHU (STRH)
+			o1 = 0x5200
+		}
+	}
+	Rn := int(mem.Reg)
+	Rm := int(mem.Index)
+	Rt := int(r.Reg)
+	out[0] = uint16(o1 | Rm&7<<6 | Rn&7<<3 | Rt&7)
+	return 2
+}
+
+// xxxx xuuu uunn nttt
+func _MOVW__u5_2_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1 int
+	var shift uint
+	mem, r := &p.From, &p.To
+	if r.Type == obj.TYPE_REG {
+		switch p.As {
+		case AMOVW: // LDR
+			o1 = 0x6800
+			shift = 2
+		case AMOVBU: // LDRB
+			o1 = 0x7800
+		default: // AMOVHU (LDRH)
+			o1 = 0x8800
+			shift = 1
+		}
+	} else {
+		mem, r = r, mem
+		switch p.As {
+		case AMOVW: // STR
+			o1 = 0x6000
+			shift = 2
+		case AMOVB, AMOVBU: // STRB
+			o1 = 0x7000
+		default: // AMOVH, AMOVHU (STRH)
+			o1 = 0x8000
+			shift = 1
+		}
+	}
+	Rn := int(mem.Reg)
+	u5 := int(c.offset(mem)) >> shift
+	Rt := int(r.Reg)
+	out[0] = uint16(o1 | u5<<6 | Rn&7<<3 | Rt&7)
+	return 2
+}
+
+// xx0x xttt uuuu uuuu
+func _MOVW__u8_2_R13__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	var (
+		x uint16
+		r int16
+		a *obj.Addr
+	)
+	fr, to := &p.From, &p.To
+	switch {
+	case fr.Type == obj.TYPE_REG: // MOVW Rt, u8<<2(R13)
+		x = 0x9000
+		r = fr.Reg
+		a = to
+	case fr.Reg == REGSP: // MOVW u8<<2(R13), Rt
+		x = 0x9800
+		r = to.Reg
+		a = fr
+	default: // MOVW u8<<2(R15), Rt
+		x = 0x4800
+		r = to.Reg
+		a = fr
+	}
+	out[0] = x | uint16(r&7)<<8 | uint16(c.offset(a)>>2)
+	return 2
+}
+
+// 1011 1111 0xxx 0000
+func _NOP2(c *Ctx, p *obj.Prog, out []uint16) int {
+	out[0] = uint16(0xBF00 + int(p.As-ANOP2)<<4)
+	return 2
+}
+
+// 1011 0110 011x 0010
+func _CPSID(c *Ctx, p *obj.Prog, out []uint16) int {
+	o := 0xB662
+	if p.As == ACPSID {
+		o |= 0x10
+	}
+	out[0] = uint16(o)
+	return 2
+}
+
+// 1011 x10r rrrr rrrr
+func _PUSH__reglist(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xB400
+	rlist := int(p.From.Offset)
+	if p.To.Type == obj.TYPE_REGLIST {
+		rlist = int(p.To.Offset)
+		o1 |= 0x0800 | rlist>>7&0x100 // T16 POP supports PC
+	} else {
+		o1 |= rlist >> 6 & 0x100 // T16 PUSH supports LR
+	}
+	out[0] = uint16(o1 | rlist&0xFF)
+	return 2
+}
+
+// 1011 1010 xxmm mddd
+func _REV__Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xBA00 // AREV
+	switch p.As {
+	case AREV16:
+		o1 |= 0x0040
+	case AREVSH:
+		o1 |= 0x00C0
+	}
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	out[0] = uint16(o1 | Rm&7<<3 | Rd&7)
+	return 2
+}
+
+// 1xx1 111x uuuu uuuu
+func _SWI__u8(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xDF00 // ASWI
+	switch p.As {
+	case obj.AUNDEF:
+		o1 = 0xDE00
+	case ABKPT:
+		o1 = 0xBE00
+	}
+	u8 := int(p.To.Offset)
+	out[0] = uint16(o1 | u8)
+	return 2
+}
+
+// 0100 0010 xxmm mnnn
+func _TST__Rm__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1 int
+	switch p.As {
+	case ATST:
+		o1 = 0x4200
+	case ACMP:
+		o1 = 0x4280
+	default: // ACMN
+		o1 = 0x42C0
+	}
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	out[0] = uint16(o1 | Rm&7<<3 | Rn&7)
+	return 2
+}
+
+// 32-bit instructions
+
+// 1111 0exx xxxs nnnn  0eee dddd eeee eeee
+func _ADDs__e32__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := oadd32(p)
+	e1, e2 := encodeMIC(uint32(p.From.Offset))
+	out[0] = 0xF000 | o1 | e1
+	out[1] = o2 | e2
+	return 4
+}
+
+// 1110 101x xxxs nnnn  0uuu dddd uuvv mmmm
+func _ADDs__Rm_v_u5__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := oadd32(p)
+	out[0] = 0xEA00 | o1
+	out[1] = o2 | oshifti32(&p.From)
+	return 4
+}
+
+// 1111 0u10 x0x0 nnnn  0uuu dddd uuuu uuuu
+func _ADD__u12__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	imm := int(p.From.Offset)
+	Rd := int(p.To.Reg) & 15
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg) & 15
+	}
+	o1 := imm>>1&0x400 | Rn
+	o2 := imm&0x700<<4 | Rd<<8 | imm&0xFF
+	switch p.As {
+	case AADD:
+		o1 |= 0xF200
+	default: // ASUB
+		o1 |= 0xF2A0
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	return 4
+}
+
+// 1111 0jii iiii iiii  1xj1 jiii iiii iiii
+func _B__ji24_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xF000
+	o2 := 0x9000
+	if p.As == ABL {
+		o2 |= 0x4000
+	}
+	v := c.boffsetrel(p, o1, o2)
+	s := v >> 23 & 1
+	j1 := ^(v>>22 ^ s) & 1
+	j2 := ^(v>>21 ^ s) & 1
+	imm10 := v >> 11 & 0x3FF
+	imm11 := v & 0x7FF
+	out[0] = uint16(o1 | s<<10 | imm10)
+	out[1] = uint16(o2 | j1<<13 | j2<<11 | imm11)
+	return 4
+}
+
+// 1111 0jcc ccii iiii  10j0 jiii iiii iiii
+func _Bcond__ji20_1(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xF000 | obcond(p.As)<<6
+	o2 := 0x8000
+	v := c.boffsetrel(p, o1, o2)
+	out[0] = uint16(o1 | v>>6&0x400 | v>>11&0x3F)
+	out[1] = uint16(o2 | v>>4&0x2000 | v>>7&0x800 | v&0x7FF)
+	return 4
+}
+
+//1110 1000 1101 nnnn  1111 0000 000h mmmm
+func _TBB__Rm__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xE8D0, 0xF000
+	if p.As == ATBH {
+		o1 |= 0x0010
+	}
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Rm&15)
+	return 4
+}
+
+// 1111 0011 x1x0 nnnn  0uuu dddd uu0w wwww
+func _BFX__width__ulsb__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	width := int(p.From.Offset)
+	if len(p.RestArgs) == 0 {
+		c.ctxt.Diag("missing LSB: %v", p)
+		return 0
+	}
+	lsb := int(p.RestArgs[0].Offset)
+	if uint(lsb) > 31 || width <= 0 || lsb+width > 32 {
+		c.ctxt.Diag("wrong width or LSB: %v", p)
+		return 0
+	}
+	Rd := int(p.To.Reg)
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg)
+	}
+	var o1, o2 int
+	switch p.As {
+	case ABFX, ABFXU:
+		o1 = 0xF340
+		if p.As == ABFXU {
+			o1 |= 0x0080
+		}
+		o2 = width - 1
+	default: // ABFI, ABFC
+		o1 = 0xF360
+		if p.As == ABFC {
+			o1 |= 0x000F
+		}
+		o2 = lsb + width - 1
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | lsb&0x1C<<10 | Rd&15<<8 | lsb&3<<6)
+	return 4
+}
+
+// 1111 1010 10xx nnnn  1111 dddd 10xx mmmm
+func _CLZ__Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xFA90, 0xF080 // AREV
+	Rm := int(p.From.Reg)
+	Rn := Rm
+	Rd := int(p.To.Reg)
+	switch p.As {
+	case ACLZ:
+		o1 += 0x0020
+	case AREV16:
+		o2 |= 0x0010
+	case ARBIT:
+		o2 |= 0x0020
+	case AREVSH:
+		o2 |= 0x0030
+	case ASEL:
+		o1 += 0x0010
+		if p.Reg != 0 {
+			Rn = int(p.Reg)
+		} else {
+			Rn = Rd
+		}
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Rd&15<<8 | Rm&15)
+	return 4
+}
+
+func _DSB(c *Ctx, p *obj.Prog, out []uint16) int {
+	opt := int(REG_MB_SY)
+	if p.From.Reg != 0 {
+		opt = int(p.From.Reg)
+	}
+	out[0] = 0xF3BF
+	out[1] = uint16(0x8F40 + int(p.As-ADSB)<<4 + opt&15)
+	return 4
+}
+
+// 1110 1000 010x nnnn  tttt dddd uuuu uuuu
+func _LDREX__u8_2_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xE840
+	if p.As == ALDREX {
+		o1 |= 0x0010
+	}
+	offset := uint64(c.offset(&p.From))
+	if offset >= 1<<10 || offset&3 != 0 {
+		c.ctxt.Diag("bad offset: %v", p)
+		return 0
+	}
+	u8 := int(offset) >> 2
+	Rn := int(p.From.Reg)
+	Rt := int(p.To.Reg)
+	Rd := int(REGPC)
+	if p.Reg != 0 {
+		Rd = Rt
+		Rt = int(p.Reg)
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(Rt&15<<12 | Rd&15<<8 | u8)
+	return 4
+}
+
+// 1110 1000 110x nnnn  tttt 1111 010x dddd
+func _LDREXB__Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	if c.offset(&p.From) != 0 {
+		c.ctxt.Diag("offset not supported: %v", p)
+		return 0
+	}
+	o1, o2 := 0xE8C0, 0x0F40 // STREXB
+	switch p.As {
+	case ALDREXB:
+		o1 |= 0x0010
+	case ASTREXH:
+		o2 |= 0x0010
+	case ALDREXH:
+		o1 |= 0x0010
+		o2 |= 0x0010
+	}
+	Rn := int(p.From.Reg)
+	Rt := int(p.To.Reg)
+	Rd := int(REGPC)
+	if p.Reg != 0 {
+		Rd = Rt
+		Rt = int(p.Reg)
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Rt&15<<12 | Rd&15)
+	return 4
+}
+
+// 1111 1010 0x0x 1111  1111 dddd 10rr mmmm
+func _MOVH__Rm_rot__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xFA0F, 0xF080 // AMOVH
+	switch p.As {
+	case AMOVB:
+		o1 |= 0x0040
+	case AMOVHU:
+		o1 |= 0x0010
+	case AMOVBU:
+		o1 |= 0x0050
+	}
+	Rm := int(p.From.Reg) & 15
+	rot := 0
+	if p.From.Type == obj.TYPE_SHIFT {
+		var typ, count int
+		Rm, typ, count = shifti(int(p.From.Offset))
+		switch count {
+		case 0:
+			rot = 0
+			typ = 3
+		case 8:
+			rot = 1
+		case 16:
+			rot = 2
+		case 24:
+			rot = 3
+		default:
+			rot = -1
+		}
+		if typ != 3 || rot < 0 {
+			c.ctxt.Diag("only right rotation by 0,8,16,24 is supported: %v", p)
+			return 0
+		}
+	}
+	Rd := int(p.To.Reg) & 15
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | Rd<<8 | rot<<4 | Rm)
+	return 4
+}
+
+// 1110 100x x0wx nnnn  rr0r rrrr rrrr rrrr
+func _MOVM_IAw(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xE800
+	rlist, mem := uint(p.From.Offset), &p.To
+	if mem.Type == obj.TYPE_REGLIST {
+		if !c.checkldm(p) {
+			return 0
+		}
+		rlist, mem = uint(p.To.Offset), &p.From
+		if rlist&0xC000 == 0xC000 {
+			c.ctxt.Diag("both R14 and R15 in reglist: %v", p)
+			return 0
+		}
+		o1 |= 0x0010
+	} else if rlist&0x8000 != 0 {
+		c.ctxt.Diag("R15 in reglist: %v", p)
+		return 0
+	}
+	if rlist&0x2000 != 0 {
+		c.ctxt.Diag("R13 in reglist: %v", p)
+		return 0
+	}
+	if c.offset(mem) != 0 {
+		c.ctxt.Diag("offset not supported: %v", p)
+		return 0
+	}
+	if onesCount(rlist) < 2 {
+		c.ctxt.Diag("to few registers in reglist: %v", p)
+		return 0
+	}
+	if p.Scond&C_DB != 0 {
+		o1 |= 0x0100
+	} else {
+		o1 |= 0x0080
+	}
+	if p.Scond&C_WBIT != 0 {
+		o1 |= 0x0020
+	}
+	out[0] = uint16(o1 | int(mem.Reg)&15)
+	out[1] = uint16(rlist)
+	return 4
+}
+
+// 1111 100x 0xxx nnnn  tttt 0000 00uu mmmm
+func _MOVW__Rn_Rm_1_u2__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2, a := oldrstr32(p)
+	var u2 int
+	switch a.Scale {
+	case 0, 1:
+		break
+	case 2:
+		u2 = 1
+	case 4:
+		u2 = 2
+	case 8:
+		u2 = 3
+	default:
+		log.Fatal("bad scale")
+	}
+	Rm := int(a.Index)
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | u2<<4 | Rm&15)
+	return 4
+}
+
+// 1111 100x 1xxx nnnn  tttt uuuu uuuu uuuu
+// 1111 100x xxx 1111  tttt uuuu uuuu uuuu
+func _MOVW__s12_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2, a := oldrstr32(p)
+	imm12 := int(c.offset(a))
+	if imm12 < 0 {
+		imm12 = -imm12
+	} else {
+		o1 |= 0x0080
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | imm12&0xFFF)
+	return 4
+}
+
+// 1111 0011 1110 1111  1000 dddd mmmm mmmm
+// 1111 0011 1000 nnnn  1000 kk00 mmmm mmmm
+func _MOVW__SYSm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	if p.From.Reg >= REG_APSR && p.From.Reg <= REG_CONTROL {
+		SYSm := int(p.From.Reg)
+		Rd := int(p.To.Reg)
+		out[0] = 0xF3EF
+		out[1] = uint16(0x8000 | Rd&15<<8 | SYSm&31)
+	} else {
+		// TODO: support kk != 0b10 (nzcvq)
+		Rn := int(p.From.Reg)
+		SYSm := int(p.To.Reg)
+		out[0] = uint16(0xF380 | Rn&15)
+		out[1] = uint16(0x8800 | SYSm&31)
+	}
+	return 4
+}
+
+// 1111 0y10 x100 uuuu  0zzz dddd zzzz zzzz
+func _MOVW__uyz16__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := uint(0xF240)
+	if p.As == AMOVT {
+		o1 |= 0x80
+	}
+	imm := uint(p.From.Offset)
+	out[0] = uint16(o1 | imm>>12 | imm>>1&0x400)
+	out[1] = uint16(imm&0x700<<4 | uint(p.To.Reg)&15<<8 | imm&0xFF)
+	return 4
+}
+
+// 1111 100x 0xxx nnnn  tttt 1pw uuuu uuuu
+func _MOVWpw__s8_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2, a := oldrstr32(p)
+	imm8 := int(c.offset(a))
+	if imm8 < 0 {
+		o2 |= 0x0800
+		imm8 = -imm8
+	} else {
+		o2 |= 0x0A00
+	}
+	switch p.Scond & (C_PBIT | C_WBIT) {
+	case 0:
+		o2 |= 1 << 10 // offset addressing (set p)
+	case C_PBIT:
+		o2 |= 1 << 8 // post-indexed addressing (set w)
+	case C_WBIT:
+		o2 |= 1<<10 | 1<<8 // pre-indexed addressing (set p,w)
+	default: // C_PBIT|C_WBIT
+		c.ctxt.Diag("invalid .P/.W suffix: %v", p)
+	}
+	out[0] = uint16(o1)
+	out[1] = uint16(o2 | imm8&0xFF)
+	return 4
+}
+
+// 1111 1010 0vvs nnnn  1111 dddd 0000 mmmm
+func _MOVWs__Rn_v_Rm__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xFA00
+	if p.Scond&C_SBIT != 0 {
+		o1 |= 0x0010
+	}
+	Rn, typ, Rm := shiftr(int(p.From.Offset))
+	Rd := int(p.To.Reg) & 15
+	out[0] = uint16(o1 | typ<<5 | Rn)
+	out[1] = uint16(0xF000 | Rd<<8 | Rm)
+	return 4
+}
+
+// 1111 1011 xxxx nnnn  1111 dddd xxxx mmmm
+func _MUL__Rm__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xFB00, 0xF000 // MUL
+	switch p.As {
+	case ADIV:
+		o1 |= 0x0090
+		o2 |= 0x00F0
+	case ADIVU:
+		o1 |= 0x00B0
+		o2 |= 0x00F0
+	}
+	Rm := int(p.From.Reg)
+	Rd := int(p.To.Reg)
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg)
+	}
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Rd&15<<8 | Rm&15)
+	return 4
+}
+
+// 1111 1011 00xx nnnn  aaaa dddd 000x mmmm
+func _MULA__Rm__Rn__Ra__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := 0xFB00, 0x0000 // AMULA
+	switch p.As {
+	case AMULS:
+		o2 |= 0x0010
+	case AMULAWB:
+		o1 |= 0x0030
+	case AMULAWT:
+		o1 |= 0x0030
+		o2 |= 0x0010
+	}
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	Rd := int(p.To.Reg)
+	Ra := int(p.To.Offset)
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(o2 | Ra&15<<12 | Rd&15<<8 | Rm&15)
+	return 4
+}
+
+// 1111 1011 10x0 nnnn  llll hhhh 0000 mmmm
+func _MULL__Rm__Rn__Rdh_Rdl(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xFB00
+	switch p.As {
+	case AMULL:
+		o1 |= 0x0080
+	case AMULLU:
+		o1 |= 0x00A0
+	case AMULAL:
+		o1 |= 0x00C0
+	default: // AMULALU
+		o1 |= 0x00E0
+	}
+	Rm := int(p.From.Reg)
+	Rn := int(p.Reg)
+	Rdh := int(p.To.Reg)
+	Rdl := int(p.To.Offset)
+	out[0] = uint16(o1 | Rn&15)
+	out[1] = uint16(Rdl&15<<12 | Rdh&15<<8 | Rm&15)
+	return 4
+}
+
+// 1111 0011 101x 1111  1000 xxxx 00x0 xxxx
+func _NOP4(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1, o2 uint16
+	switch p.As {
+	case ANOP4:
+		o1, o2 = 0xF3AF, 0x8000
+	default: // ACLREX
+		o1, o2 = 0xF3BF, 0x8F2F
+	}
+	out[0] = o1
+	out[1] = o2
+	return 4
+}
+
+// 1111 0e0x x0x1 nnnn  0eee 1111 eeee eeee
+func _TST__e32__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1, o2 := encodeMIC(uint32(p.From.Offset))
+	switch p.As {
+	case ACMP:
+		o1 |= 0xF1B0
+	case ATST:
+		o1 |= 0xF010
+	case ACMN:
+		o1 |= 0xF110
+	default: // ATEQ
+		o1 |= 0xF090
+	}
+	out[0] = o1 | uint16(p.Reg&15)
+	out[1] = o2 | 15<<8
+	return 4
+}
+
+// 1110 101x x0x1 nnnn  0uuu 1111 uuvv mmmm
+func _TST__Rm_v_u5__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	var o1 int
+	switch p.As {
+	case ACMP:
+		o1 = 0xEBB0
+	case ATST:
+		o1 = 0xEA10
+	case ACMN:
+		o1 = 0xEB10
+	default: // ATEQ
+		o1 = 0xEA90
+	}
+	out[0] = uint16(o1 | int(p.Reg&15))
+	out[1] = 0x0F00 | oshifti32(&p.From)
+	return 4
+}
+
+// Pseudoinstructions (they must not change the number of instructions declared in optab)
+
+func _ADD__lit__Rdn(c *Ctx, p *obj.Prog, out []uint16) int {
+	q := *p
+	q.As = AMOVW
+	q.To.Reg = REGTMP
+	n := _MOVW__lit__Rd(c, &q, out)
+	out = out[n/2:]
+	q.As = p.As
+	q.From = q.To
+	q.To = p.To
+	if p.As == AADD {
+		return n + _ADD__Rm__Rdn(c, &q, out)
+	}
+	return n + _AND__Rm__Rdn(c, &q, out)
+}
+
+func _ADD__lit__Rn__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	q := *p
+	q.As = AMOVW
+	q.To.Reg = REGTMP
+	n := _MOVW__lit__Rd(c, &q, out)
+	out = out[n/2:]
+	q.As = p.As
+	q.From = q.To
+	q.To = p.To
+	switch q.As {
+	case AADD, ASUB:
+		if q.Reg <= REG_R7 && q.To.Reg <= REG_R7 {
+			return n + _ADD__Rm__Rn__Rd(c, &q, out)
+		}
+	}
+	return n + _ADDs__Rm_v_u5__Rn__Rd(c, &q, out)
+}
+
+func _CMN__lit__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	return c.cmp__lit__Rn(_TST__Rm__Rn, p, out)
+}
+
+func _CMP__lit__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	return c.cmp__lit__Rn(_CMP__Rm__Rn, p, out)
+}
+
+func _MOVW__lit__Rd(c *Ctx, p *obj.Prog, out []uint16) int {
+	v, short := c.litoffset(p, p.To.Reg <= REG_R7, 0)
+	var q obj.Prog
+	q.As = AMOVW
+	q.To = p.To
+	q.From.Type = obj.TYPE_MEM
+	q.From.Name = obj.NAME_NONE
+	q.From.Reg = REGPC
+	q.From.Offset = int64(v)
+	if short {
+		return _MOVW__u8_2_R13__Rt(c, &q, out)
+	}
+	return _MOVW__s12_Rn__Rt(c, &q, out)
+}
+
+func _MOVW__lit_Rn__Rt(c *Ctx, p *obj.Prog, out []uint16) int {
+	q := *p
+	if q.To.Type == obj.TYPE_MEM {
+		q.From, q.To.Type = q.To, q.From.Type
+	}
+	q.As = AMOVW
+	q.To.Reg = REGTMP
+	n := _MOVW__lit__Rd(c, &q, out)
+	out = out[n/2:]
+	q = *p
+	mem, reg := &q.From, &q.To
+	if q.To.Type == obj.TYPE_MEM {
+		mem, reg = &q.To, &q.From
+	}
+	mem.Offset = 0
+	switch mem.Name {
+	case obj.NAME_STATIC, obj.NAME_EXTERN:
+		mem.Reg = REGTMP
+		if (q.To.Type == obj.TYPE_MEM || q.As != AMOVB && q.As != AMOVH) && reg.Reg <= REG_R7 {
+			return n + _MOVW__u5_2_Rn__Rt(c, &q, out)
+		}
+		return n + _MOVW__s12_Rn__Rt(c, &q, out)
+	default:
+		mem.Index = REGTMP
+		if mem.Reg <= REG_R7 && reg.Reg <= REG_R7 {
+			return n + _MOVW__Rn_Rm__Rt(c, &q, out)
+		}
+		return n + _MOVW__Rn_Rm_1_u2__Rt(c, &q, out)
+	}
+}
+
+func _TST__lit__Rn(c *Ctx, p *obj.Prog, out []uint16) int {
+	return c.cmp__lit__Rn(_TST__Rm_v_u5__Rn, p, out)
+}
+
+func _WORD__u32(c *Ctx, p *obj.Prog, out []uint16) int {
+	if p.To.Sym != nil {
+		rel := obj.Addrel(c.cursym)
+		rel.Off = int32(p.Pc)
+		rel.Siz = 4
+		rel.Sym = p.To.Sym
+		rel.Add = p.To.Offset
+		rel.Type = objabi.R_ADDR
+		out[0] = 0
+		out[1] = 0
+	} else {
+		offset := int(p.To.Offset)
+		out[0] = uint16(offset)
+		out[1] = uint16(offset >> 16)
+	}
+	return 4
+}
+
+func _HWORD__u16(c *Ctx, p *obj.Prog, out []uint16) int {
+	out[0] = uint16(p.From.Offset)
+	return 2
+}
+
+// 1110 1101 d0x nnnn  dddd 101x uuuu uuuu
+func _MOVF__s8_2_Rn__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	o1 := 0xED00
+	reg, mem := p.From, p.To
+	if mem.Type == obj.TYPE_REG {
+		reg, mem = mem, reg
+		o1 |= 0x0010
+	}
+	o2 := 0x0A00 | int(reg.Reg)&15<<12
+	if p.As == AMOVD {
+		o2 |= 1 << 8
+	}
+	o1 |= int(mem.Reg & 15)
+	offset := int(mem.Offset)
+	if offset >= 0 {
+		o1 |= 0x0080
+	} else {
+		offset = -offset
+	}
+	o2 |= offset
+	out[0] = uint16(o1)
+	out[1] = uint16(o2)
+	return 4
+}
+
+// 1110 1110 1d11 0001  dddd 101x 11m0 mmmm
+func _SQRTF__Fm__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	Fm := int(p.From.Reg)
+	Fd := int(p.To.Reg)
+	o2 := 0x0AC0 | Fd&15<<12 | Fm&15
+	if p.As == ASQRTD {
+		o2 |= 1 << 8
+	}
+	out[0] = 0xEEB1
+	out[1] = uint16(o2)
+	return 4
+}
+
+// ********
+
+// 1110 1110 1d11 ffff  dddd 101x 0000 ffff
+func _MOVF__f8__Fd(c *Ctx, p *obj.Prog, out []uint16) int {
+	log.Fatal("not implemented ", p)
+	return 4
+}
+
+func oldrstr32(p *obj.Prog) (o1, o2 int, mem *obj.Addr) {
+	var r *obj.Addr
+	mem, r = &p.From, &p.To
+	if r.Type == obj.TYPE_REG {
+		switch p.As {
+		case AMOVW: // LDR
+			o1 = 0xF850
+		case AMOVB: // LDRSB
+			o1 = 0xF910
+		case AMOVBU: // LDRB
+			o1 = 0xF810
+		case AMOVH: // LDRSH
+			o1 = 0xF930
+		default: // AMOVHU (LDRH)
+			o1 = 0xF830
+		}
+	} else {
+		mem, r = r, mem
+		switch p.As {
+		case AMOVW: // STR
+			o1 = 0xF840
+		case AMOVB, AMOVBU: // STRB
+			o1 = 0xF800
+		default: // AMOVH, AMOVHU (STRH)
+			o1 = 0xF820
+		}
+	}
+	o1 |= int(mem.Reg) & 15
+	o2 = int(r.Reg) & 15 << 12
+	return
+}
+
+// u must be representable as Modified Immediate Constant (checked by mic)
+func encodeMIC(u uint32) (o1, o2 uint16) {
+	if u>>8 == 0 {
+		// fast path for 00000000_00000000_00000000_abcdefgh
+		return 0, uint16(u)
+	}
+	for n := 31; n >= 8; n-- {
+		if v := rol(u, n); v&^0xFF == 0 {
+			o1 = uint16(n & 0x10 << 6)
+			o2 = uint16(n&0x0E<<11) | uint16(n&1<<7) | uint16(v&0x7F)
+			return
+		}
+	}
+	switch uint32(0) {
+	case u >> 24:
+		// 00000000_abcdefgh_00000000_abcdefgh
+		return 0, 0x1000 | uint16(u&0xFF)
+	case u << 24:
+		// abcdefgh_00000000_abcdefgh_00000000
+		return 0, 0x2000 | uint16(u>>8&0xFF)
+	}
+	// abcdefgh_abcdefgh_abcdefgh_abcdefgh
+	return 0, 0x3000 | uint16(u&0xFF)
+}
+
+func oadd32(p *obj.Prog) (o1, o2 uint16) {
+	switch p.As {
+	case AADD:
+		o1 = 0x100
+	case ASUB:
+		o1 = 0x1A0
+	case AAND:
+		o1 = 0x000
+	case AORR:
+		o1 = 0x040
+	case ABIC:
+		o1 = 0x020
+	case AORN:
+		o1 = 0x060
+	case AEOR:
+		o1 = 0x080
+	case AADC:
+		o1 = 0x140
+	case ARSB:
+		o1 = 0x1C0
+	case ASBC:
+		o1 = 0x160
+	case AMOVW:
+		o1 = 0x04F
+	case AMVN:
+		o1 = 0x06F
+	default:
+		log.Fatalf("not a data processing instruction")
+	}
+	if p.Scond&C_SBIT != 0 {
+		o1 |= 0x10
+	}
+	Rd := int(p.To.Reg) & 15
+	Rn := Rd
+	if p.Reg != 0 {
+		Rn = int(p.Reg) & 15
+	}
+	return o1 | uint16(Rn), uint16(Rd) << 8
+}
+
+func shiftr(offset int) (Rn, typ, Rm int) {
+	Rn = offset & 15
+	typ = offset >> 5 & 3
+	Rm = offset >> 8 & 15
+	return
+}
+
+func shifti(offset int) (Rm, typ, count int) {
+	Rm = offset & 15
+	typ = offset >> 5 & 3
+	count = offset >> 7 & 31
+	return
+}
+
+func oshifti32(a *obj.Addr) uint16 {
+	Rm := int(a.Reg) & 15
+	var typ, count int
+	if a.Type == obj.TYPE_SHIFT {
+		Rm, typ, count = shifti(int(a.Offset))
+	}
+	return uint16(count&0x1C<<10 | count&3<<6 | typ<<4 | Rm)
+}
+
+func (c *Ctx) checkldm(p *obj.Prog) bool {
+	rinlist := 1<<uint(p.From.Reg&15)&p.To.Offset != 0 // true if Rn in reglist
+	w := p.Scond&C_WBIT != 0
+	if rinlist && w {
+		c.ctxt.Diag("Rn in reglist: %v", p)
+		return false
+	}
+	return true
+}
+
+func obcond(as obj.As) int {
+	switch {
+	case as <= ABCS:
+		return int(as - ABEQ)
+	case as == ABHS:
+		return 2
+	case as <= ABLO:
+		return 3
+	default: //  ABMI <= as:
+		return 4 + int(as-ABMI)
+	}
+}
+
+func (c *Ctx) boffsetrel(p *obj.Prog, o1, o2 int) int {
+	v := -pcoff
+	if p.To.Sym == nil {
+		if p.Pcond != nil {
+			v += int(p.Pcond.Pc - p.Pc)
+		}
+		return v >> 1
+	}
+	rel := obj.Addrel(c.cursym)
+	rel.Off = int32(p.Pc)
+	rel.Siz = 4
+	rel.Sym = p.To.Sym
+	v += int(p.To.Offset)
+	rel.Add = int64(o2)<<48 | int64(o1)<<32 | int64(uint32(v))
+	rel.Type = objabi.R_CALLARM
+	return 0
+}
+
+func (c *Ctx) cmp__lit__Rn(cmp asmoutFunc, p *obj.Prog, out []uint16) int {
+	q := *p
+	q.As = AMOVW
+	q.To.Reg = REGTMP
+	q.To.Type = obj.TYPE_REG
+	n := _MOVW__lit__Rd(c, &q, out)
+	out = out[n/2:]
+	q.As = p.As
+	q.From.Type = obj.TYPE_REG
+	q.From.Reg = REGTMP
+	return n + cmp(c, &q, out)
+}
diff --git a/src/cmd/internal/obj/util.go b/src/cmd/internal/obj/util.go
index f1517d3d5d..8b89858eea 100644
--- a/src/cmd/internal/obj/util.go
+++ b/src/cmd/internal/obj/util.go
@@ -282,7 +282,7 @@ func Dconv(p *Prog, a *Addr) string {
 		v := int(a.Offset)
 		ops := "<<>>->@>"
 		switch objabi.GOARCH {
-		case "arm":
+		case "arm", "thumb":
 			op := ops[((v>>5)&3)<<1:]
 			if v&(1<<4) != 0 {
 				str = fmt.Sprintf("R%d%c%cR%d", v&15, op[0], op[1], (v>>8)&15)
@@ -457,6 +457,7 @@ const (
 	RBaseARM64 = 8 * 1024  // range [8k, 13k)
 	RBaseMIPS  = 13 * 1024 // range [13k, 14k)
 	RBaseS390X = 14 * 1024 // range [14k, 15k)
+	RBaseThumb = 15 * 1024
 	RBaseWasm  = 16 * 1024
 )
 
diff --git a/src/cmd/internal/objabi/head.go b/src/cmd/internal/objabi/head.go
index 0a54228228..747a02631d 100644
--- a/src/cmd/internal/objabi/head.go
+++ b/src/cmd/internal/objabi/head.go
@@ -49,6 +49,7 @@ const (
 	Hsolaris
 	Hwindows
 	Haix
+	Hnoos
 )
 
 func (h *HeadType) Set(s string) error {
@@ -69,6 +70,8 @@ func (h *HeadType) Set(s string) error {
 		*h = Hnacl
 	case "netbsd":
 		*h = Hnetbsd
+	case "noos":
+		*h = Hnoos
 	case "openbsd":
 		*h = Hopenbsd
 	case "plan9":
@@ -101,6 +104,8 @@ func (h *HeadType) String() string {
 		return "nacl"
 	case Hnetbsd:
 		return "netbsd"
+	case Hnoos:
+		return "noos"
 	case Hopenbsd:
 		return "openbsd"
 	case Hplan9:
diff --git a/src/cmd/internal/objabi/stack.go b/src/cmd/internal/objabi/stack.go
index 62ab0398a6..fc9b57598f 100644
--- a/src/cmd/internal/objabi/stack.go
+++ b/src/cmd/internal/objabi/stack.go
@@ -8,7 +8,6 @@ package objabi
 
 const (
 	STACKSYSTEM = 0
-	StackSystem = STACKSYSTEM
 	StackBig    = 4096
 	StackSmall  = 128
 )
@@ -17,9 +16,19 @@ const (
 	StackPreempt = -1314 // 0xfff...fade
 )
 
+var StackGuard, StackLimit, StackSystem int
+
 // Initialize StackGuard and StackLimit according to target system.
-var StackGuard = 880*stackGuardMultiplier() + StackSystem
-var StackLimit = StackGuard - StackSystem - StackSmall
+func init() {
+	if GOOS == "noos" && GOARCH == "thumb" {
+		StackSystem = 27 * 4
+		StackGuard = 440 + StackSystem
+	} else {
+		StackSystem = STACKSYSTEM
+		StackGuard = 880*stackGuardMultiplier() + StackSystem
+	}
+	StackLimit = StackGuard - StackSystem - StackSmall
+}
 
 // stackGuardMultiplier returns a multiplier to apply to the default
 // stack guard size. Larger multipliers are used for non-optimized
diff --git a/src/cmd/internal/objabi/util.go b/src/cmd/internal/objabi/util.go
index 9e41b87aa4..885752b6a2 100644
--- a/src/cmd/internal/objabi/util.go
+++ b/src/cmd/internal/objabi/util.go
@@ -47,6 +47,10 @@ func goarm() int {
 		return 6
 	case "7":
 		return 7
+	case "7f", "7F":
+		return 0x7F
+	case "7d", "7D":
+		return 0x7D
 	}
 	// Fail here, rather than validate at multiple call sites.
 	log.Fatalf("Invalid GOARM value. Must be 5, 6, or 7.")
diff --git a/src/cmd/internal/objfile/disasm.go b/src/cmd/internal/objfile/disasm.go
index 50fc51be87..d52bf338f5 100644
--- a/src/cmd/internal/objfile/disasm.go
+++ b/src/cmd/internal/objfile/disasm.go
@@ -24,6 +24,7 @@ import (
 	"golang.org/x/arch/arm/armasm"
 	"golang.org/x/arch/arm64/arm64asm"
 	"golang.org/x/arch/ppc64/ppc64asm"
+	"golang.org/x/arch/thumb/thumbasm"
 	"golang.org/x/arch/x86/x86asm"
 )
 
@@ -37,6 +38,7 @@ type Disasm struct {
 	goarch    string           // GOARCH string
 	disasm    disasmFunc       // disassembler function for goarch
 	byteOrder binary.ByteOrder // byte order for goarch
+	gofile    bool
 }
 
 // Disasm returns a disassembler for the file f.
@@ -83,6 +85,7 @@ func (e *Entry) Disasm() (*Disasm, error) {
 		goarch:    goarch,
 		disasm:    disasm,
 		byteOrder: byteOrder,
+		gofile:    e.gofile,
 	}
 
 	return d, nil
@@ -197,7 +200,7 @@ func (d *Disasm) Print(w io.Writer, filter *regexp.Regexp, start, end uint64, pr
 		fc = NewFileCache(8)
 	}
 
-	tw := tabwriter.NewWriter(bw, 18, 8, 1, '\t', tabwriter.StripEscape)
+	tw := tabwriter.NewWriter(bw, 18, 8, 1, ' ', tabwriter.StripEscape)
 	for _, sym := range d.syms {
 		symStart := sym.Addr
 		symEnd := sym.Addr + uint64(sym.Size)
@@ -241,7 +244,12 @@ func (d *Disasm) Print(w io.Writer, filter *regexp.Regexp, start, end uint64, pr
 				fmt.Fprintf(tw, "  %s:%d\t%#x\t", base(file), line, pc)
 			}
 
-			if size%4 != 0 || d.goarch == "386" || d.goarch == "amd64" || d.goarch == "amd64p32" {
+			if d.goarch == "thumb" {
+				fmt.Fprintf(tw, "%04x", d.byteOrder.Uint16(code[i:i+2]))
+				if size == 4 {
+					fmt.Fprintf(tw, " %04x", d.byteOrder.Uint16(code[i+2:]))
+				}
+			} else if size%4 != 0 || d.goarch == "386" || d.goarch == "amd64" || d.goarch == "amd64p32" {
 				// Print instruction as bytes.
 				fmt.Fprintf(tw, "%x", code[i:i+size])
 			} else {
@@ -268,19 +276,24 @@ func (d *Disasm) Decode(start, end uint64, relocs []Reloc, f func(pc, size uint6
 	if end > d.textEnd {
 		end = d.textEnd
 	}
+	var pctoa uint64 // TODO(md) if more such cases this can be a function
+	if !d.gofile && d.goarch == "thumb" {
+		pctoa = 1
+	}
 	code := d.text[:end-d.textStart]
 	lookup := d.lookup
 	for pc := start; pc < end; {
-		i := pc - d.textStart
-		text, size := d.disasm(code[i:], pc, lookup, d.byteOrder)
+		addr := pc &^ pctoa
+		i := addr - d.textStart
+		text, size := d.disasm(code[i:], addr, lookup, d.byteOrder)
 		file, line, _ := d.pcln.PCToLine(pc)
 		sep := "\t"
 		for len(relocs) > 0 && relocs[0].Addr < i+uint64(size) {
-			text += sep + relocs[0].Stringer.String(pc-start)
+			text += sep + relocs[0].Stringer.String(addr-start)
 			sep = " "
 			relocs = relocs[1:]
 		}
-		f(pc, uint64(size), file, line, text)
+		f(addr, uint64(size), file, line, text)
 		pc += uint64(size)
 	}
 }
@@ -366,6 +379,19 @@ func disasm_ppc64(code []byte, pc uint64, lookup lookupFunc, byteOrder binary.By
 	return text, size
 }
 
+func disasm_thumb(code []byte, pc uint64, lookup lookupFunc, _ binary.ByteOrder) (string, int) {
+	inst, err := thumbasm.Decode(code)
+	var text string
+	size := inst.Len
+	if err != nil || size == 0 || inst.Op == 0 {
+		size = 2
+		text = "?"
+	} else {
+		text = thumbasm.GoSyntax(inst, pc, lookup, textReader{code, pc})
+	}
+	return text, size
+}
+
 var disasms = map[string]disasmFunc{
 	"386":      disasm_386,
 	"amd64":    disasm_amd64,
@@ -374,6 +400,7 @@ var disasms = map[string]disasmFunc{
 	"arm64":    disasm_arm64,
 	"ppc64":    disasm_ppc64,
 	"ppc64le":  disasm_ppc64,
+	"thumb":    disasm_thumb,
 }
 
 var byteOrders = map[string]binary.ByteOrder{
@@ -385,6 +412,7 @@ var byteOrders = map[string]binary.ByteOrder{
 	"ppc64":    binary.BigEndian,
 	"ppc64le":  binary.LittleEndian,
 	"s390x":    binary.BigEndian,
+	"thumb":    binary.LittleEndian,
 }
 
 type Liner interface {
diff --git a/src/cmd/internal/objfile/elf.go b/src/cmd/internal/objfile/elf.go
index a48a9df5d6..3c10b2433e 100644
--- a/src/cmd/internal/objfile/elf.go
+++ b/src/cmd/internal/objfile/elf.go
@@ -98,6 +98,9 @@ func (f *elfFile) goarch() string {
 	case elf.EM_X86_64:
 		return "amd64"
 	case elf.EM_ARM:
+		if f.elf.Entry&1 != 0 {
+			return "thumb" // BUG(md): should be per function
+		}
 		return "arm"
 	case elf.EM_AARCH64:
 		return "arm64"
diff --git a/src/cmd/internal/objfile/goobj.go b/src/cmd/internal/objfile/goobj.go
index 473e773ec2..ea9c4b1439 100644
--- a/src/cmd/internal/objfile/goobj.go
+++ b/src/cmd/internal/objfile/goobj.go
@@ -29,7 +29,7 @@ func openGoFile(r *os.File) (*File, error) {
 	}
 	rf := &goobjFile{goobj: f, f: r}
 	if len(f.Native) == 0 {
-		return &File{r, []*Entry{&Entry{raw: rf}}}, nil
+		return &File{r, []*Entry{&Entry{raw: rf, gofile: true}}}, nil
 	}
 	entries := make([]*Entry, len(f.Native)+1)
 	entries[0] = &Entry{
diff --git a/src/cmd/internal/objfile/objfile.go b/src/cmd/internal/objfile/objfile.go
index 41c5d9b9f5..426c94f0b7 100644
--- a/src/cmd/internal/objfile/objfile.go
+++ b/src/cmd/internal/objfile/objfile.go
@@ -30,8 +30,9 @@ type File struct {
 }
 
 type Entry struct {
-	name string
-	raw  rawFile
+	name   string
+	raw    rawFile
+	gofile bool
 }
 
 // A Sym is a symbol defined in an executable file.
diff --git a/src/cmd/internal/sys/arch.go b/src/cmd/internal/sys/arch.go
index 487c9260e8..ba15812ba9 100644
--- a/src/cmd/internal/sys/arch.go
+++ b/src/cmd/internal/sys/arch.go
@@ -21,6 +21,7 @@ const (
 	MIPS64
 	PPC64
 	S390X
+	Thumb
 	Wasm
 )
 
@@ -161,6 +162,16 @@ var ArchS390X = &Arch{
 	MinLC:     2,
 }
 
+var ArchThumb = &Arch{
+	Name:      "thumb",
+	Family:    Thumb,
+	ByteOrder: binary.LittleEndian,
+	PtrSize:   4,
+	RegSize:   4,
+	MinLC:     2,
+}
+
+
 var ArchWasm = &Arch{
 	Name:      "wasm",
 	Family:    Wasm,
@@ -183,5 +194,6 @@ var Archs = [...]*Arch{
 	ArchPPC64,
 	ArchPPC64LE,
 	ArchS390X,
+	ArchThumb,
 	ArchWasm,
 }
diff --git a/src/cmd/link/internal/ld/data.go b/src/cmd/link/internal/ld/data.go
index 8e35f5c9dc..869ce832d9 100644
--- a/src/cmd/link/internal/ld/data.go
+++ b/src/cmd/link/internal/ld/data.go
@@ -126,7 +126,7 @@ func trampoline(ctxt *Link, s *sym.Symbol) {
 //
 // This is a performance-critical function for the linker; be careful
 // to avoid introducing unnecessary allocations in the main loop.
-func relocsym(ctxt *Link, s *sym.Symbol) {
+func relocsym(ctxt *Link, s *sym.Symbol, dwarf bool) {
 	if len(s.R) == 0 {
 		return
 	}
@@ -340,6 +340,10 @@ func relocsym(ctxt *Link, s *sym.Symbol) {
 
 			o = Symaddr(r.Sym) + r.Add
 
+			if !dwarf && ctxt.Arch.Family == sys.Thumb && r.Sym.Type == sym.STEXT {
+				o |= 1 // thumb function call address
+			}
+
 			// On amd64, 4-byte offsets will be sign-extended, so it is impossible to
 			// access more than 2GB of static data; fail at link time is better than
 			// fail at runtime. See https://golang.org/issue/7980.
@@ -562,15 +566,14 @@ func (ctxt *Link) reloc() {
 	if ctxt.Debugvlog != 0 {
 		ctxt.Logf("%5.2f reloc\n", Cputime())
 	}
-
 	for _, s := range ctxt.Textp {
-		relocsym(ctxt, s)
+		relocsym(ctxt, s, false)
 	}
 	for _, s := range datap {
-		relocsym(ctxt, s)
+		relocsym(ctxt, s, false)
 	}
 	for _, s := range dwarfp {
-		relocsym(ctxt, s)
+		relocsym(ctxt, s, true)
 	}
 }
 
@@ -1208,6 +1211,46 @@ func (ctxt *Link) dodata() {
 		}
 	}
 
+	if ctxt.HeadType == objabi.Hnoos {
+		// leave some read-only variables in Flash (hack to save some RAM)
+		for _, s := range ctxt.Syms.Allsym {
+			switch s.Name {
+			case "embedded/rtos.errorsByNumber",
+				"math.mPi4", "math._tanP", "math._tanQ", "math._lgamA",
+				"math._lgamR", "math._lgamS", "math._lgamT", "math._lgamU",
+				"math._lgamV", "math._lgamW", "math._sin", "math._cos",
+				"math.pow10tab", "math.pow10postab32", "math.pow10negtab32",
+				"math.tanhP", "math.tanhQ", "math._gamP", "math._gamQ",
+				"math._gamS",
+				"math/big.pow5tab", "math/big._Accuracy_index",
+				"math/big._RoundingMode_index",
+				"math/rand.rngCooked", "math/rand.ke", "math/rand.we",
+				"math/rand.fe", "math/rand.kn", "math/rand.wn", "math/rand.fn",
+				"runtime.zeroVal", "runtime.staticbytes",
+				"runtime.fastlog2Table", "runtime.class_to_size",
+				"runtime.class_to_allocnpages", "runtime.class_to_divmagic",
+				"runtime.size_to_class8", "runtime.size_to_class128",
+				"runtime.waitReasonStrings", "runtime.boundsErrorFmts",
+				"runtime.boundsNegErrorFmts", "runtime.finalizer1",
+				"runtime.gcMarkWorkerModeStrings", "runtime.gStatusStrings",
+				"runtime.emptymspan",
+				"runtime/internal/sys.ntz8tab",
+				"strconv.smallPowersOfTen", "strconv.powersOfTen",
+				"strconv.uint64pow10", "strconv.leftcheats",
+				"strconv.isPrint32", "strconv.isPrint16",
+				"strconv.isNotPrint32", "strconv.isNotPrint16",
+				"strconv.isGraphic", "strconv.float64info",
+				"strconv.float32info",
+				"syscall.errors",
+				"time.std0x", "time.months", "time.days", "time.daysBefore",
+				"time.utcLoc",
+				"unicode/utf8.first", "unicode/utf8.acceptRanges":
+
+				s.Type = sym.SRODATA
+			}
+		}
+	}
+
 	// Collect data symbols by type into data.
 	var data [sym.SXREF][]*sym.Symbol
 	for _, s := range ctxt.Syms.Allsym {
@@ -1544,6 +1587,13 @@ func (ctxt *Link) dodata() {
 		ctxt.Syms.Lookup("runtime.types", 0).Sect = sect
 		ctxt.Syms.Lookup("runtime.etypes", 0).Sect = sect
 	}
+	if ctxt.HeadType == objabi.Hnoos {
+		ctxt.Syms.Lookup("runtime.ramstart", 0).Sect = sect
+		ctxt.Syms.Lookup("runtime.ramend", 0).Sect = sect
+		ctxt.Syms.Lookup("runtime.romdata", 0).Sect = sect
+		ctxt.Syms.Lookup("runtime.nodmastart", 0).Sect = sect
+		ctxt.Syms.Lookup("runtime.nodmaend", 0).Sect = sect
+	}
 	for _, symn := range sym.ReadOnly {
 		align := dataMaxAlign[symn]
 		if sect.Align < align {
@@ -2160,9 +2210,11 @@ func (ctxt *Link) address() []*sym.Segment {
 	order = append(order, &Segtext)
 	Segtext.Rwx = 05
 	Segtext.Vaddr = va
+	Segtext.Laddr = va
 	for _, s := range Segtext.Sections {
 		va = uint64(Rnd(int64(va), int64(s.Align)))
 		s.Vaddr = va
+		s.Laddr = va
 		va += s.Length
 	}
 
@@ -2190,9 +2242,11 @@ func (ctxt *Link) address() []*sym.Segment {
 		order = append(order, &Segrodata)
 		Segrodata.Rwx = 04
 		Segrodata.Vaddr = va
+		Segrodata.Laddr = va
 		for _, s := range Segrodata.Sections {
 			va = uint64(Rnd(int64(va), int64(s.Align)))
 			s.Vaddr = va
+			s.Laddr = va
 			va += s.Length
 		}
 
@@ -2210,9 +2264,11 @@ func (ctxt *Link) address() []*sym.Segment {
 		order = append(order, &Segrelrodata)
 		Segrelrodata.Rwx = 06
 		Segrelrodata.Vaddr = va
+		Segrelrodata.Laddr = va
 		for _, s := range Segrelrodata.Sections {
 			va = uint64(Rnd(int64(va), int64(s.Align)))
 			s.Vaddr = va
+			s.Laddr = va
 			va += s.Length
 		}
 
@@ -2220,15 +2276,30 @@ func (ctxt *Link) address() []*sym.Segment {
 	}
 
 	va = uint64(Rnd(int64(va), int64(*FlagRound)))
-	if ctxt.HeadType == objabi.Haix && len(Segrelrodata.Sections) == 0 {
-		// Data sections are moved to an unreachable segment
-		// to ensure that they are position-independent.
-		// Already done if relro sections exist.
-		va += uint64(XCOFFDATABASE) - uint64(XCOFFTEXTBASE)
+	la := va
+	switch ctxt.HeadType {
+	case objabi.Haix:
+		if len(Segrelrodata.Sections) == 0 {
+			// Data sections are moved to an unreachable segment
+			// to ensure that they are position-independent.
+			// Already done if relro sections exist.
+			va += uint64(XCOFFDATABASE) - uint64(XCOFFTEXTBASE)
+			la = va
+		}
+	case objabi.Hnoos:
+		va = RAM.Base
+		switch ctxt.Arch {
+		case sys.ArchThumb:
+			// Main stack on the lowest addresses so overflows can be detected
+			// even without MPU. Segdata.Laddr is set to main stack size (see
+			// ../thumb/asm.go:/Laddr = /)
+			va += Segdata.Laddr //
+		}
 	}
 	order = append(order, &Segdata)
 	Segdata.Rwx = 06
 	Segdata.Vaddr = va
+	Segdata.Laddr = la
 	var data *sym.Section
 	var noptr *sym.Section
 	var bss *sym.Section
@@ -2242,13 +2313,16 @@ func (ctxt *Link) address() []*sym.Segment {
 			vlen = int64(Segdata.Sections[i+1].Vaddr - s.Vaddr)
 		}
 		s.Vaddr = va
+		s.Laddr = la
 		va += uint64(vlen)
 		Segdata.Length = va - Segdata.Vaddr
 		if s.Name == ".data" {
 			data = s
+			la += uint64(vlen)
 		}
 		if s.Name == ".noptrdata" {
 			noptr = s
+			la += uint64(vlen)
 		}
 		if s.Name == ".bss" {
 			bss = s
@@ -2263,18 +2337,23 @@ func (ctxt *Link) address() []*sym.Segment {
 	Segdata.Filelen = bss.Vaddr - Segdata.Vaddr
 
 	va = uint64(Rnd(int64(va), int64(*FlagRound)))
+	la = uint64(Rnd(int64(la), int64(*FlagRound)))
 	order = append(order, &Segdwarf)
 	Segdwarf.Rwx = 06
 	Segdwarf.Vaddr = va
+	Segdwarf.Vaddr = la
 	for i, s := range Segdwarf.Sections {
 		vlen := int64(s.Length)
 		if i+1 < len(Segdwarf.Sections) {
 			vlen = int64(Segdwarf.Sections[i+1].Vaddr - s.Vaddr)
 		}
 		s.Vaddr = va
+		s.Laddr = la
 		va += uint64(vlen)
+		la += uint64(vlen)
 		if ctxt.HeadType == objabi.Hwindows {
 			va = uint64(Rnd(int64(va), PEFILEALIGN))
+			la = uint64(Rnd(int64(la), PEFILEALIGN))
 		}
 		Segdwarf.Length = va - Segdwarf.Vaddr
 	}
@@ -2370,6 +2449,14 @@ func (ctxt *Link) address() []*sym.Segment {
 	ctxt.xdefine("runtime.enoptrbss", sym.SNOPTRBSS, int64(noptrbss.Vaddr+noptrbss.Length))
 	ctxt.xdefine("runtime.end", sym.SBSS, int64(Segdata.Vaddr+Segdata.Length))
 
+	if ctxt.HeadType == objabi.Hnoos {
+		ctxt.xdefine("runtime.ramstart", sym.SRODATA, int64(RAM.Base))
+		ctxt.xdefine("runtime.ramend", sym.SRODATA, int64(RAM.Base+RAM.Size))
+		ctxt.xdefine("runtime.romdata", sym.SRODATA, int64(Segdata.Laddr))
+		ctxt.xdefine("runtime.nodmastart", sym.SRODATA, int64(NoDMA.Base))
+		ctxt.xdefine("runtime.nodmaend", sym.SRODATA, int64(NoDMA.Base+NoDMA.Size))
+	}
+
 	return order
 }
 
@@ -2386,7 +2473,7 @@ func (ctxt *Link) layout(order []*sym.Segment) uint64 {
 				// Assuming the previous segment was
 				// aligned, the following rounding
 				// should ensure that this segment's
-				// VA  Fileoff mod FlagRound.
+				// VA  Fileoff mod FlagRound.
 				seg.Fileoff = uint64(Rnd(int64(prev.Fileoff+prev.Filelen), int64(*FlagRound)))
 				if seg.Vaddr%uint64(*FlagRound) != seg.Fileoff%uint64(*FlagRound) {
 					Exitf("bad segment rounding (Vaddr=%#x Fileoff=%#x FlagRound=%#x)", seg.Vaddr, seg.Fileoff, *FlagRound)
@@ -2450,7 +2537,7 @@ func compressSyms(ctxt *Link, syms []*sym.Symbol) []byte {
 			s.P = ctxt.relocbuf
 			s.Attr.Set(sym.AttrReadOnly, false)
 		}
-		relocsym(ctxt, s)
+		relocsym(ctxt, s, true)
 		if _, err := z.Write(s.P); err != nil {
 			log.Fatalf("compression failed: %s", err)
 		}
diff --git a/src/cmd/link/internal/ld/deadcode.go b/src/cmd/link/internal/ld/deadcode.go
index f9a0ee0f96..88dd72f42c 100644
--- a/src/cmd/link/internal/ld/deadcode.go
+++ b/src/cmd/link/internal/ld/deadcode.go
@@ -9,6 +9,7 @@ import (
 	"cmd/internal/sys"
 	"cmd/link/internal/sym"
 	"fmt"
+	"strconv"
 	"strings"
 	"unicode"
 )
@@ -16,7 +17,7 @@ import (
 // deadcode marks all reachable symbols.
 //
 // The basis of the dead code elimination is a flood fill of symbols,
-// following their relocations, beginning at *flagEntrySymbol.
+// following their relocations, beginning at *FlagEntrySymbol.
 //
 // This flood fill is wrapped in logic for pruning unused methods.
 // All methods are mentioned by relocations on their receiver's *rtype.
@@ -56,7 +57,7 @@ func deadcode(ctxt *Link) {
 	}
 
 	// First, flood fill any symbols directly reachable in the call
-	// graph from *flagEntrySymbol. Ignore all methods not directly called.
+	// graph from *FlagEntrySymbol. Ignore all methods not directly called.
 	d.init()
 	d.flood()
 
@@ -204,11 +205,43 @@ func (d *deadcodepass) markMethod(m methodref) {
 	}
 }
 
+var cortexmSystemHandlers = [...]string{
+	"runtime.nmiHandler",
+	"runtime.hardfaultHandler",
+	"runtime.memmanageHandler",
+	"runtime.busfaultHandler",
+	"runtime.usagefaultHandler",
+	"runtime.securefaultHandler",
+	"runtime.reservedHandler",
+	"runtime.reservedHandler",
+	"runtime.reservedHandler",
+	"runtime.svcallHandler",
+	"runtime.debugmonHandler",
+	"runtime.reservedHandler",
+	"runtime.pendsvHandler",
+	"SysTick_Handler",
+}
+
+func CortexmHandler(irqn int) string {
+	if irqn < 0 {
+		return cortexmSystemHandlers[irqn+14]
+	}
+	return "IRQ" + strconv.Itoa(irqn) + "_Handler"
+}
+
 // init marks all initial symbols as reachable.
-// In a typical binary, this is *flagEntrySymbol.
+// In a typical binary, this is *FlagEntrySymbol.
 func (d *deadcodepass) init() {
 	var names []string
 
+	if d.ctxt.HeadType == objabi.Hnoos {
+		if d.ctxt.Arch.Family == sys.Thumb {
+			// mark exception handlers
+			for irqn := -14; irqn < 480; irqn++ {
+				names = append(names, CortexmHandler(irqn))
+			}
+		}
+	}
 	if d.ctxt.BuildMode == BuildModeShared {
 		// Mark all symbols defined in this library as reachable when
 		// building a shared library.
@@ -227,12 +260,12 @@ func (d *deadcodepass) init() {
 			// The external linker refers main symbol directly.
 			if d.ctxt.LinkMode == LinkExternal && (d.ctxt.BuildMode == BuildModeExe || d.ctxt.BuildMode == BuildModePIE) {
 				if d.ctxt.HeadType == objabi.Hwindows && d.ctxt.Arch.Family == sys.I386 {
-					*flagEntrySymbol = "_main"
+					*FlagEntrySymbol = "_main"
 				} else {
-					*flagEntrySymbol = "main"
+					*FlagEntrySymbol = "main"
 				}
 			}
-			names = append(names, *flagEntrySymbol)
+			names = append(names, *FlagEntrySymbol)
 			if d.ctxt.BuildMode == BuildModePlugin {
 				names = append(names, objabi.PathToPrefix(*flagPluginPath)+"..inittask", objabi.PathToPrefix(*flagPluginPath)+".main", "go.plugin.tabs")
 
diff --git a/src/cmd/link/internal/ld/elf.go b/src/cmd/link/internal/ld/elf.go
index 5a3098ce85..5dc250fc04 100644
--- a/src/cmd/link/internal/ld/elf.go
+++ b/src/cmd/link/internal/ld/elf.go
@@ -513,10 +513,10 @@ func Elfinit(ctxt *Link) {
 		ehdr.shentsize = ELF64SHDRSIZE /* Must be ELF64SHDRSIZE */
 
 	// 32-bit architectures
-	case sys.ARM, sys.MIPS:
-		if ctxt.Arch.Family == sys.ARM {
+	case sys.ARM, sys.Thumb, sys.MIPS:
+		if ctxt.Arch.Family == sys.ARM || ctxt.Arch.Family == sys.Thumb {
 			// we use EABI on linux/arm, freebsd/arm, netbsd/arm.
-			if ctxt.HeadType == objabi.Hlinux || ctxt.HeadType == objabi.Hfreebsd || ctxt.HeadType == objabi.Hnetbsd {
+			if ctxt.HeadType == objabi.Hlinux || ctxt.HeadType == objabi.Hfreebsd || ctxt.HeadType == objabi.Hnetbsd || ctxt.HeadType == objabi.Hnoos {
 				// We set a value here that makes no indication of which
 				// float ABI the object uses, because this is information
 				// used by the dynamic linker to compare executables and
@@ -1152,7 +1152,7 @@ func elfphload(seg *sym.Segment) *ElfPhdr {
 		ph.flags |= PF_X
 	}
 	ph.vaddr = seg.Vaddr
-	ph.paddr = seg.Vaddr
+	ph.paddr = seg.Laddr
 	ph.memsz = seg.Length
 	ph.off = seg.Fileoff
 	ph.filesz = seg.Filelen
@@ -1165,7 +1165,7 @@ func elfphrelro(seg *sym.Segment) {
 	ph := newElfPhdr()
 	ph.type_ = PT_GNU_RELRO
 	ph.vaddr = seg.Vaddr
-	ph.paddr = seg.Vaddr
+	ph.paddr = seg.Laddr
 	ph.memsz = seg.Length
 	ph.off = seg.Fileoff
 	ph.filesz = seg.Filelen
@@ -1747,7 +1747,7 @@ func Asmbelf(ctxt *Link, symo int64) {
 		Exitf("unknown architecture in asmbelf: %v", ctxt.Arch.Family)
 	case sys.MIPS, sys.MIPS64:
 		eh.machine = EM_MIPS
-	case sys.ARM:
+	case sys.ARM, sys.Thumb:
 		eh.machine = EM_ARM
 	case sys.AMD64:
 		eh.machine = EM_X86_64
@@ -1810,27 +1810,28 @@ func Asmbelf(ctxt *Link, symo int64) {
 	}
 
 	/* program header info */
-	pph = newElfPhdr()
-
-	pph.type_ = PT_PHDR
-	pph.flags = PF_R
-	pph.off = uint64(eh.ehsize)
-	pph.vaddr = uint64(*FlagTextAddr) - uint64(HEADR) + pph.off
-	pph.paddr = uint64(*FlagTextAddr) - uint64(HEADR) + pph.off
-	pph.align = uint64(*FlagRound)
-
-	/*
-	 * PHDR must be in a loaded segment. Adjust the text
-	 * segment boundaries downwards to include it.
-	 * Except on NaCl where it must not be loaded.
-	 */
-	if ctxt.HeadType != objabi.Hnacl {
-		o := int64(Segtext.Vaddr - pph.vaddr)
-		Segtext.Vaddr -= uint64(o)
-		Segtext.Length += uint64(o)
-		o = int64(Segtext.Fileoff - pph.off)
-		Segtext.Fileoff -= uint64(o)
-		Segtext.Filelen += uint64(o)
+	if ctxt.HeadType != objabi.Hnoos {
+		pph = newElfPhdr()
+		pph.type_ = PT_PHDR
+		pph.flags = PF_R
+		pph.off = uint64(eh.ehsize)
+		pph.vaddr = uint64(*FlagTextAddr) - uint64(HEADR) + pph.off
+		pph.paddr = uint64(*FlagTextAddr) - uint64(HEADR) + pph.off
+		pph.align = uint64(*FlagRound)
+
+		/*
+		 * PHDR must be in a loaded segment. Adjust the text
+		 * segment boundaries downwards to include it.
+		 * Except on NaCl where it must not be loaded.
+		 */
+		if ctxt.HeadType != objabi.Hnacl {
+			o := int64(Segtext.Vaddr - pph.vaddr)
+			Segtext.Vaddr -= uint64(o)
+			Segtext.Length += uint64(o)
+			o = int64(Segtext.Fileoff - pph.off)
+			Segtext.Fileoff -= uint64(o)
+			Segtext.Filelen += uint64(o)
+		}
 	}
 
 	if !*FlagD { /* -d suppresses dynamic loader format */
@@ -1907,7 +1908,7 @@ func Asmbelf(ctxt *Link, symo int64) {
 		phsh(pnote, sh)
 	}
 
-	if *flagBuildid != "" {
+	if *flagBuildid != "" && ctxt.HeadType != objabi.Hnoos {
 		sh := elfshname(".note.go.buildid")
 		resoff -= int64(elfgobuildid(sh, uint64(startva), uint64(resoff)))
 
@@ -2242,7 +2243,7 @@ elfobj:
 		if len(buildinfo) > 0 {
 			a += int64(elfwritebuildinfo(ctxt.Out))
 		}
-		if *flagBuildid != "" {
+		if *flagBuildid != "" && ctxt.HeadType != objabi.Hnoos {
 			a += int64(elfwritegobuildid(ctxt.Out))
 		}
 	}
diff --git a/src/cmd/link/internal/ld/lib.go b/src/cmd/link/internal/ld/lib.go
index 3fa258d275..f0bbd47811 100644
--- a/src/cmd/link/internal/ld/lib.go
+++ b/src/cmd/link/internal/ld/lib.go
@@ -276,16 +276,16 @@ func libinit(ctxt *Link) {
 	ctxt.Out.w = bufio.NewWriter(f)
 	ctxt.Out.f = f
 
-	if *flagEntrySymbol == "" {
+	if *FlagEntrySymbol == "" {
 		switch ctxt.BuildMode {
 		case BuildModeCShared, BuildModeCArchive:
-			*flagEntrySymbol = fmt.Sprintf("_rt0_%s_%s_lib", objabi.GOARCH, objabi.GOOS)
+			*FlagEntrySymbol = fmt.Sprintf("_rt0_%s_%s_lib", objabi.GOARCH, objabi.GOOS)
 		case BuildModeExe, BuildModePIE:
-			*flagEntrySymbol = fmt.Sprintf("_rt0_%s_%s", objabi.GOARCH, objabi.GOOS)
+			*FlagEntrySymbol = fmt.Sprintf("_rt0_%s_%s", objabi.GOARCH, objabi.GOOS)
 		case BuildModeShared, BuildModePlugin:
-			// No *flagEntrySymbol for -buildmode=shared and plugin
+			// No *FlagEntrySymbol for -buildmode=shared and plugin
 		default:
-			Errorf(nil, "unknown *flagEntrySymbol for buildmode %v", ctxt.BuildMode)
+			Errorf(nil, "unknown *FlagEntrySymbol for buildmode %v", ctxt.BuildMode)
 		}
 	}
 }
@@ -502,7 +502,7 @@ func (ctxt *Link) loadlib() {
 
 		// In addition, on ARM, the runtime depends on the linker
 		// recording the value of GOARM.
-		if ctxt.Arch.Family == sys.ARM {
+		if ctxt.Arch.Family == sys.ARM || ctxt.Arch.Family == sys.Thumb {
 			s := ctxt.Syms.Lookup("runtime.goarm", 0)
 			s.Type = sym.SDATA
 			s.Size = 0
@@ -2430,7 +2430,7 @@ func datoff(s *sym.Symbol, addr int64) int64 {
 }
 
 func Entryvalue(ctxt *Link) int64 {
-	a := *flagEntrySymbol
+	a := *FlagEntrySymbol
 	if a[0] >= '0' && a[0] <= '9' {
 		return atolwhex(a)
 	}
@@ -2441,6 +2441,9 @@ func Entryvalue(ctxt *Link) int64 {
 	if ctxt.HeadType != objabi.Haix && s.Type != sym.STEXT {
 		Errorf(s, "entry not text")
 	}
+	if ctxt.Arch.Family == sys.Thumb {
+		return s.Value | 1
+	}
 	return s.Value
 }
 
diff --git a/src/cmd/link/internal/ld/main.go b/src/cmd/link/internal/ld/main.go
index 67e5ef9392..78ca521795 100644
--- a/src/cmd/link/internal/ld/main.go
+++ b/src/cmd/link/internal/ld/main.go
@@ -40,12 +40,15 @@ import (
 	"os"
 	"runtime"
 	"runtime/pprof"
+	"strconv"
 	"strings"
 )
 
 var (
 	pkglistfornote []byte
 	windowsgui     bool // writes a "GUI binary" instead of a "console binary"
+	RAM            MemBlock
+	NoDMA          MemBlock
 )
 
 func init() {
@@ -89,13 +92,47 @@ var (
 
 	FlagRound       = flag.Int("R", -1, "set address rounding `quantum`")
 	FlagTextAddr    = flag.Int64("T", -1, "set text segment `address`")
-	flagEntrySymbol = flag.String("E", "", "set `entry` symbol name")
+	FlagEntrySymbol = flag.String("E", "", "set `entry` symbol name")
 
 	cpuprofile     = flag.String("cpuprofile", "", "write cpu profile to `file`")
 	memprofile     = flag.String("memprofile", "", "write memory profile to `file`")
 	memprofilerate = flag.Int64("memprofilerate", 0, "set runtime.MemProfileRate to `rate`")
 )
 
+type MemBlock struct {
+	Base, Size uint64
+}
+
+func (mb *MemBlock) init(descr string) {
+	i := strings.IndexByte(descr, ':')
+	if i < 0 {
+		Exitf("memory layout (-M): no BASE:SIZE separator: %s", descr)
+	}
+	var err error
+	mb.Base, err = strconv.ParseUint(descr[:i], 0, 64)
+	if err != nil {
+		Exitf("memory layout (-M): bad BASE address: %v", err)
+	}
+	size := descr[i+1:]
+	scale := uint64(1)
+	switch size[len(size)-1] {
+	case 'K':
+		scale = 1024
+	case 'M':
+		scale = 1024 * 1024
+	case 'G':
+		scale = 1024 * 1024 * 1024
+	}
+	if scale != 1 {
+		size = size[:len(size)-1]
+	}
+	mb.Size, err = strconv.ParseUint(size, 0, 64)
+	if err != nil {
+		Exitf("memory layout (-M): bad SIZE: %v", err)
+	}
+	mb.Size *= scale
+}
+
 // Main is the main entry point for the linker code.
 func Main(arch *sys.Arch, theArch Arch) {
 	thearch = theArch
@@ -130,9 +167,29 @@ func Main(arch *sys.Arch, theArch Arch) {
 	objabi.Flagfn1("X", "add string value `definition` of the form importpath.name=value", func(s string) { addstrdata1(ctxt, s) })
 	objabi.Flagcount("v", "print link trace", &ctxt.Debugvlog)
 	objabi.Flagfn1("importcfg", "read import configuration from `file`", ctxt.readImportCfg)
+	var flagMemory string
+	if objabi.GOOS == "noos" {
+		flag.StringVar(&flagMemory, "M", "", "set memory layout: BASE1:SIZE1[,BASE2:SIZE2]")
+	}
 
 	objabi.Flagparse(usage)
 
+	if objabi.GOOS == "noos" {
+		descr := strings.Split(flagMemory, ",")
+		if len(descr) == 0 {
+			Exitf("memory layout (-M) not specified")
+		}
+		if len(descr) > 0 {
+			RAM.init(descr[0])
+		}
+		if len(descr) > 1 {
+			NoDMA.init(descr[1])
+		}
+		if len(descr) > 2 {
+			Exitf("-M describes more than two memory blocks")
+		}
+	}
+
 	switch *flagHeadType {
 	case "":
 	case "windowsgui":
diff --git a/src/cmd/link/internal/ld/pe.go b/src/cmd/link/internal/ld/pe.go
index 032968f983..5f6418b3ce 100644
--- a/src/cmd/link/internal/ld/pe.go
+++ b/src/cmd/link/internal/ld/pe.go
@@ -499,7 +499,7 @@ func (f *peFile) addInitArray(ctxt *Link) *peSection {
 	ctxt.Out.SeekSet(int64(sect.pointerToRawData))
 	sect.checkOffset(ctxt.Out.Offset())
 
-	init_entry := ctxt.Syms.Lookup(*flagEntrySymbol, 0)
+	init_entry := ctxt.Syms.Lookup(*FlagEntrySymbol, 0)
 	addr := uint64(init_entry.Value) - init_entry.Sect.Vaddr
 	switch objabi.GOARCH {
 	case "386", "arm":
diff --git a/src/cmd/link/internal/ld/sym.go b/src/cmd/link/internal/ld/sym.go
index bf7a56aff2..f2e31bb569 100644
--- a/src/cmd/link/internal/ld/sym.go
+++ b/src/cmd/link/internal/ld/sym.go
@@ -80,6 +80,7 @@ func (ctxt *Link) computeTLSOffset() {
 		objabi.Hnetbsd,
 		objabi.Hopenbsd,
 		objabi.Hdragonfly,
+		objabi.Hnoos,
 		objabi.Hsolaris:
 		ctxt.Tlsoffset = -1 * ctxt.Arch.PtrSize
 
diff --git a/src/cmd/link/internal/ld/symtab.go b/src/cmd/link/internal/ld/symtab.go
index d686a8a476..77d24f12fc 100644
--- a/src/cmd/link/internal/ld/symtab.go
+++ b/src/cmd/link/internal/ld/symtab.go
@@ -144,6 +144,9 @@ func putelfsym(ctxt *Link, x *sym.Symbol, s string, t SymbolType, addr int64, go
 	if ctxt.LinkMode == LinkExternal && elfshnum != SHN_UNDEF {
 		addr -= int64(xo.Sect.Vaddr)
 	}
+	if ctxt.Arch.Family == sys.Thumb && typ == STT_FUNC {
+		addr |= 1
+	}
 	other := STV_DEFAULT
 	if x.Attr.VisibilityHidden() {
 		// TODO(mwhudson): We only set AttrVisibilityHidden in ldelf, i.e. when
@@ -331,7 +334,7 @@ func (ctxt *Link) symtab() {
 		for _, s := range ctxt.Syms.Allsym {
 			// Create a new entry in the .init_array section that points to the
 			// library initializer function.
-			if s.Name == *flagEntrySymbol && ctxt.HeadType != objabi.Haix {
+			if s.Name == *FlagEntrySymbol && ctxt.HeadType != objabi.Haix {
 				addinitarrdata(ctxt, s)
 			}
 		}
diff --git a/src/cmd/link/internal/ld/xcoff.go b/src/cmd/link/internal/ld/xcoff.go
index 67e558a475..78bd90409f 100644
--- a/src/cmd/link/internal/ld/xcoff.go
+++ b/src/cmd/link/internal/ld/xcoff.go
@@ -1195,7 +1195,7 @@ func (ctxt *Link) doxcoff() {
 	}
 
 	// Add entry point to .loader symbols.
-	ep := ctxt.Syms.ROLookup(*flagEntrySymbol, 0)
+	ep := ctxt.Syms.ROLookup(*FlagEntrySymbol, 0)
 	if !ep.Attr.Reachable() {
 		Exitf("wrong entry point")
 	}
@@ -1298,7 +1298,7 @@ func (f *xcoffFile) writeLdrScn(ctxt *Link, globalOff uint64) {
 	off := hdr.Lrldoff                                // current offset is the same of reloc offset
 
 	/* Reloc */
-	ep := ctxt.Syms.ROLookup(*flagEntrySymbol, 0)
+	ep := ctxt.Syms.ROLookup(*FlagEntrySymbol, 0)
 	ldr := &XcoffLdRel64{
 		Lvaddr:  uint64(ep.Value),
 		Lrtype:  0x3F00,
@@ -1461,7 +1461,7 @@ func (f *xcoffFile) writeFileHeader(ctxt *Link) {
 		f.xahdr.Ovstamp = 1 // based on dump -o
 		f.xahdr.Omagic = 0x10b
 		copy(f.xahdr.Omodtype[:], "1L")
-		entry := ctxt.Syms.ROLookup(*flagEntrySymbol, 0)
+		entry := ctxt.Syms.ROLookup(*FlagEntrySymbol, 0)
 		f.xahdr.Oentry = uint64(entry.Value)
 		f.xahdr.Osnentry = f.getXCOFFscnum(entry.Sect)
 		toc := ctxt.Syms.ROLookup("TOC", 0)
diff --git a/src/cmd/link/internal/sym/segment.go b/src/cmd/link/internal/sym/segment.go
index d5255bf142..04d07acc51 100644
--- a/src/cmd/link/internal/sym/segment.go
+++ b/src/cmd/link/internal/sym/segment.go
@@ -38,6 +38,7 @@ package sym
 type Segment struct {
 	Rwx      uint8  // permission as usual unix bits (5 = r-x etc)
 	Vaddr    uint64 // virtual address
+	Laddr    uint64 // load address
 	Length   uint64 // length in memory
 	Fileoff  uint64 // file offset
 	Filelen  uint64 // length on disk
@@ -50,6 +51,7 @@ type Section struct {
 	Align   int32
 	Name    string
 	Vaddr   uint64
+	Laddr   uint64 
 	Length  uint64
 	Seg     *Segment
 	Elfsect interface{} // an *ld.ElfShdr
diff --git a/src/cmd/link/internal/thumb/asm.go b/src/cmd/link/internal/thumb/asm.go
new file mode 100644
index 0000000000..da0f66656d
--- /dev/null
+++ b/src/cmd/link/internal/thumb/asm.go
@@ -0,0 +1,419 @@
+// Inferno utils/5l/asm.c
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5l/asm.c
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"cmd/internal/objabi"
+	"cmd/internal/sys"
+	"cmd/link/internal/ld"
+	"cmd/link/internal/sym"
+	"debug/elf"
+	"fmt"
+	"log"
+)
+
+const pcoff = 4 // in Thumb mode PC points 4 bytes forward
+
+func lookupFuncSym(syms *sym.Symbols, name string) *sym.Symbol {
+	if s := syms.ROLookup(name, sym.SymVerABI0); s != nil && s.FuncInfo != nil {
+		return s
+	}
+	if s := syms.ROLookup(name, sym.SymVerABIInternal); s != nil && s.FuncInfo != nil {
+		return s
+	}
+	return nil
+}
+
+func gentext(ctxt *ld.Link) {
+	if ctxt.HeadType != objabi.Hnoos {
+		return
+	}
+	vectors := ctxt.Syms.Lookup("runtime.vectors", 0)
+	vectors.Type = sym.STEXT
+	vectors.Attr |= sym.AttrReachable
+	vectors.Align = 4
+
+	vectorsAdd := func(symname string) {
+		if s := lookupFuncSym(ctxt.Syms, symname); s != nil {
+			s.Attr |= sym.AttrReachable
+			rel := vectors.AddRel()
+			rel.Off = int32(len(vectors.P))
+			rel.Siz = 4
+			rel.Type = objabi.R_ADDR
+			rel.Sym = s
+		}
+		vectors.AddUint32(ctxt.Arch, 0)
+	}
+
+	ld.Segdata.Laddr = 2048 // communicate the main stack size to Link.address()
+	msp := uint32(ld.RAM.Base + ld.Segdata.Laddr)
+	vectors.AddUint32(ctxt.Arch, msp) // Main Stack Pointer after reset
+	vectorsAdd(*ld.FlagEntrySymbol)   // Reset
+
+	// system exception handlers
+	for irqn := -14; irqn < 0; irqn++ {
+		vectorsAdd(ld.CortexmHandler(irqn))
+	}
+
+	// search for user defined ISRs: //go:linkname functionName IRQ%d_Handler
+	var irqHandlers [480]*sym.Symbol
+	irqNum := 0
+	for i := range irqHandlers {
+		s := lookupFuncSym(ctxt.Syms, ld.CortexmHandler(i))
+		if s != nil {
+			irqHandlers[i] = s
+			irqNum = i + 1
+		}
+	}
+	for _, s := range irqHandlers[:irqNum] {
+		if s != nil {
+			s.Attr |= sym.AttrReachable
+			rel := vectors.AddRel()
+			rel.Off = int32(len(vectors.P))
+			rel.Siz = 4
+			rel.Type = objabi.R_ADDR
+			rel.Sym = s
+		}
+		vectors.AddUint32(ctxt.Arch, 0)
+	}
+	ctxt.Textp = append(ctxt.Textp, nil)
+	copy(ctxt.Textp[1:], ctxt.Textp)
+	ctxt.Textp[0] = vectors
+}
+
+func elfreloc1(ctxt *ld.Link, r *sym.Reloc, sectoff int64) bool {
+	ctxt.Out.Write32(uint32(sectoff))
+
+	elfsym := r.Xsym.ElfsymForReloc()
+	switch r.Type {
+	default:
+		return false
+	case objabi.R_ADDR:
+		if r.Siz == 4 {
+			ctxt.Out.Write32(uint32(elf.R_ARM_ABS32) | uint32(elfsym)<<8)
+		} else {
+			return false
+		}
+	case objabi.R_PCREL:
+		if r.Siz == 4 {
+			ctxt.Out.Write32(uint32(elf.R_ARM_REL32) | uint32(elfsym)<<8)
+		} else {
+			return false
+		}
+	case objabi.R_CALLARM:
+		if r.Siz != 4 {
+			return false
+		}
+		// r.Add contains branch opcode and initial addend
+		op := uint32(r.Add >> 32)
+		switch op & 0xD000F800 {
+		case 0x9000F000: // B
+			ctxt.Out.Write32(uint32(elf.R_ARM_THM_JUMP24) | uint32(elfsym)<<8)
+		case 0xD000F000: // BL
+			ctxt.Out.Write32(uint32(elf.R_ARM_THM_PC22) | uint32(elfsym)<<8) // R_ARM_THM_CALL
+		case 0x8000F000: // Bcond
+			ctxt.Out.Write32(uint32(elf.R_ARM_THM_JUMP19) | uint32(elfsym)<<8)
+		default:
+			return false
+		}
+	}
+	return true
+}
+
+// Convert the direct jump relocation r to refer to a trampoline if the target is too far
+func trampoline(ctxt *ld.Link, r *sym.Reloc, s *sym.Symbol) {
+	switch r.Type {
+	case objabi.R_CALLARM:
+		var maxoffset int64
+		switch uint32(r.Add>>32) & 0x9000F800 {
+		case 0x9000F000: // B/BL imm24
+			maxoffset = 1 << 24
+		case 0x8000F000: // Bcond imm20
+			maxoffset = 1 << 20
+		default:
+			ld.Errorf(s, "bad branch opcode")
+		}
+		t := (ld.Symaddr(r.Sym) + int64(int32(r.Add)) - (s.Value + int64(r.Off)))
+		if -maxoffset <= t && t < maxoffset {
+			return
+		}
+		// Direct call too far, need to insert trampoline.
+		// Look up existing trampolines first. If we found one within the range
+		// of direct call, we can reuse it. Otherwise create a new one.
+		offset := t + pcoff
+		var tramp *sym.Symbol
+		for i := 0; ; i++ {
+			name := r.Sym.Name + fmt.Sprintf("%+d-tramp%d", offset, i)
+			tramp = ctxt.Syms.Lookup(name, int(r.Sym.Version))
+			if tramp.Type == sym.SDYNIMPORT {
+				// don't reuse trampoline defined in other module
+				continue
+			}
+			if tramp.Value == 0 {
+				// Either the trampoline does not exist -- we need to create one,
+				// or found one the address which is not assigned -- this will be
+				// laid down immediately after the current function. Use this one.
+				break
+			}
+			t = (ld.Symaddr(tramp) - pcoff - (s.Value + int64(r.Off)))
+			if -maxoffset <= t && t < maxoffset {
+				// found an existing trampoline that is not too far
+				// we can just use it
+				break
+			}
+		}
+		if tramp.Type == 0 {
+			// trampoline does not exist, create one
+			ctxt.AddTramp(tramp)
+			gentramp(ctxt.Arch, ctxt.LinkMode, tramp, r.Sym, int64(offset))
+		}
+		// modify reloc to point to tramp, which will be resolved later
+		r.Sym = tramp
+		r.Add = r.Add&^0xFFFFFFFF | int64(-pcoff)&0xFFFFFFFF // clear the offset embedded in the instruction
+		r.Done = false
+	default:
+		ld.Errorf(s, "trampoline called with non-jump reloc: %d (%s)", r.Type, sym.RelocName(ctxt.Arch, r.Type))
+	}
+}
+
+// generate a trampoline to target+offset
+func gentramp(arch *sys.Arch, linkmode ld.LinkMode, tramp, target *sym.Symbol, offset int64) {
+	t := ld.Symaddr(target) + offset
+
+	o1 := uint16(0x4F00) // MOVW (R15), R7 // R15 is actual pc+4 (points to o3)
+	o2 := uint16(0x4738) // B  (R7)
+	o3 := uint32(t) | 1  // WORD $(target|1)
+
+	tramp.Size = 8
+	tramp.P = make([]byte, tramp.Size)
+	arch.ByteOrder.PutUint16(tramp.P, o1)
+	arch.ByteOrder.PutUint16(tramp.P[2:], o2)
+	arch.ByteOrder.PutUint32(tramp.P[4:], o3)
+}
+
+func archreloc(ctxt *ld.Link, r *sym.Reloc, s *sym.Symbol, val int64) (int64, bool) {
+	if ctxt.LinkMode == ld.LinkExternal {
+		log.Fatalf("BUGL: external linking not supported")
+		return val, false
+	}
+
+	switch r.Type {
+	case objabi.R_CONST:
+		return r.Add, true
+	case objabi.R_GOTOFF:
+		return ld.Symaddr(r.Sym) + r.Add - ld.Symaddr(ctxt.Syms.Lookup(".got", 0)), true
+	case objabi.R_PLT0, objabi.R_PLT1, objabi.R_PLT2:
+		log.Fatalf("BUGL: PLT not supported")
+	case objabi.R_CALLARM: // B, BL
+		// r.Add contains branch opcode and initial addend
+		op := uint32(r.Add >> 32)
+		t := (ld.Symaddr(r.Sym) + int64(int32(r.Add)) - (s.Value + int64(r.Off)))
+		switch op & 0x9000F800 {
+		case 0x9000F000: // B/BL imm24
+			if t < -1<<24 || 1<<24 <= t {
+				break
+			}
+			v := uint32(t >> 1)
+			s := v >> 23 & 1
+			j1 := ^(v>>22 ^ s) & 1
+			j2 := ^(v>>21 ^ s) & 1
+			imm10 := v >> 11 & 0x3FF
+			imm11 := v & 0x7FF
+			return int64(op | j1<<29 | j2<<27 | imm11<<16 | s<<10 | imm10), true
+		case 0x8000F000: // Bcond imm20
+			if t < -1<<20 || 1<<20 <= t {
+				break
+			}
+			v := uint32(t >> 1)
+			s := v >> 19 & 1
+			j2 := v >> 18 & 1
+			j1 := v >> 17 & 1
+			imm6 := v >> 11 & 0x3F
+			imm11 := v & 0x7FF
+			return int64(op | j1<<29 | j2<<27 | imm11<<16 | s<<10 | imm6), true
+		default:
+			ld.Errorf(s, "bad branch opcode")
+		}
+		ld.Errorf(s, "direct call too far: %s %x", r.Sym.Name, t)
+	}
+	return val, false
+}
+
+func archrelocvariant(ctxt *ld.Link, r *sym.Reloc, s *sym.Symbol, t int64) int64 {
+	log.Fatalf("unexpected relocation variant")
+	return t
+}
+
+func asmb(ctxt *ld.Link) {
+	if ctxt.Debugvlog != 0 {
+		ctxt.Logf("%5.2f asmb\n", ld.Cputime())
+	}
+
+	if ctxt.IsELF {
+		ld.Asmbelfsetup()
+	}
+
+	sect := ld.Segtext.Sections[0]
+	ctxt.Out.SeekSet(int64(sect.Vaddr - ld.Segtext.Vaddr + ld.Segtext.Fileoff))
+	ld.Codeblk(ctxt, int64(sect.Vaddr), int64(sect.Length))
+	for _, sect = range ld.Segtext.Sections[1:] {
+		ctxt.Out.SeekSet(int64(sect.Vaddr - ld.Segtext.Vaddr + ld.Segtext.Fileoff))
+		ld.Datblk(ctxt, int64(sect.Vaddr), int64(sect.Length))
+	}
+
+	if ld.Segrodata.Filelen > 0 {
+		if ctxt.Debugvlog != 0 {
+			ctxt.Logf("%5.2f rodatblk\n", ld.Cputime())
+		}
+		ctxt.Out.SeekSet(int64(ld.Segrodata.Fileoff))
+		ld.Datblk(ctxt, int64(ld.Segrodata.Vaddr), int64(ld.Segrodata.Filelen))
+	}
+	if ld.Segrelrodata.Filelen > 0 {
+		if ctxt.Debugvlog != 0 {
+			ctxt.Logf("%5.2f relrodatblk\n", ld.Cputime())
+		}
+		ctxt.Out.SeekSet(int64(ld.Segrelrodata.Fileoff))
+		ld.Datblk(ctxt, int64(ld.Segrelrodata.Vaddr), int64(ld.Segrelrodata.Filelen))
+	}
+
+	if ctxt.Debugvlog != 0 {
+		ctxt.Logf("%5.2f datblk\n", ld.Cputime())
+	}
+
+	ctxt.Out.SeekSet(int64(ld.Segdata.Fileoff))
+	ld.Datblk(ctxt, int64(ld.Segdata.Vaddr), int64(ld.Segdata.Filelen))
+
+	ctxt.Out.SeekSet(int64(ld.Segdwarf.Fileoff))
+	ld.Dwarfblk(ctxt, int64(ld.Segdwarf.Vaddr), int64(ld.Segdwarf.Filelen))
+}
+
+func asmb2(ctxt *ld.Link) {
+	machlink := uint32(0)
+	if ctxt.HeadType == objabi.Hdarwin {
+		machlink = uint32(ld.Domacholink(ctxt))
+	}
+
+	/* output symbol table */
+	ld.Symsize = 0
+
+	ld.Lcsize = 0
+	symo := uint32(0)
+	if !*ld.FlagS {
+		// TODO: rationalize
+		if ctxt.Debugvlog != 0 {
+			ctxt.Logf("%5.2f sym\n", ld.Cputime())
+		}
+		switch ctxt.HeadType {
+		default:
+			if ctxt.IsELF {
+				symo = uint32(ld.Segdwarf.Fileoff + ld.Segdwarf.Filelen)
+				symo = uint32(ld.Rnd(int64(symo), int64(*ld.FlagRound)))
+			}
+
+		case objabi.Hplan9:
+			symo = uint32(ld.Segdata.Fileoff + ld.Segdata.Filelen)
+
+		case objabi.Hdarwin:
+			symo = uint32(ld.Segdwarf.Fileoff + uint64(ld.Rnd(int64(ld.Segdwarf.Filelen), int64(*ld.FlagRound))) + uint64(machlink))
+		}
+
+		ctxt.Out.SeekSet(int64(symo))
+		switch ctxt.HeadType {
+		default:
+			if ctxt.IsELF {
+				if ctxt.Debugvlog != 0 {
+					ctxt.Logf("%5.2f elfsym\n", ld.Cputime())
+				}
+				ld.Asmelfsym(ctxt)
+				ctxt.Out.Flush()
+				ctxt.Out.Write(ld.Elfstrdat)
+
+				if ctxt.LinkMode == ld.LinkExternal {
+					ld.Elfemitreloc(ctxt)
+				}
+			}
+
+		case objabi.Hplan9:
+			ld.Asmplan9sym(ctxt)
+			ctxt.Out.Flush()
+
+			sym := ctxt.Syms.Lookup("pclntab", 0)
+			if sym != nil {
+				ld.Lcsize = int32(len(sym.P))
+				ctxt.Out.Write(sym.P)
+				ctxt.Out.Flush()
+			}
+
+		case objabi.Hdarwin:
+			if ctxt.LinkMode == ld.LinkExternal {
+				ld.Machoemitreloc(ctxt)
+			}
+		}
+	}
+
+	if ctxt.Debugvlog != 0 {
+		ctxt.Logf("%5.2f header\n", ld.Cputime())
+	}
+	ctxt.Out.SeekSet(0)
+	switch ctxt.HeadType {
+	default:
+	case objabi.Hplan9: /* plan 9 */
+		ctxt.Out.Write32b(0x647)                      /* magic */
+		ctxt.Out.Write32b(uint32(ld.Segtext.Filelen)) /* sizes */
+		ctxt.Out.Write32b(uint32(ld.Segdata.Filelen))
+		ctxt.Out.Write32b(uint32(ld.Segdata.Length - ld.Segdata.Filelen))
+		ctxt.Out.Write32b(uint32(ld.Symsize))          /* nsyms */
+		ctxt.Out.Write32b(uint32(ld.Entryvalue(ctxt))) /* va of entry */
+		ctxt.Out.Write32b(0)
+		ctxt.Out.Write32b(uint32(ld.Lcsize))
+
+	case objabi.Hlinux,
+		objabi.Hfreebsd,
+		objabi.Hnetbsd,
+		objabi.Hopenbsd,
+		objabi.Hnacl,
+		objabi.Hnoos:
+		ld.Asmbelf(ctxt, int64(symo))
+
+	case objabi.Hdarwin:
+		ld.Asmbmacho(ctxt)
+	}
+
+	ctxt.Out.Flush()
+	if *ld.FlagC {
+		fmt.Printf("textsize=%d\n", ld.Segtext.Filelen)
+		fmt.Printf("datsize=%d\n", ld.Segdata.Filelen)
+		fmt.Printf("bsssize=%d\n", ld.Segdata.Length-ld.Segdata.Filelen)
+		fmt.Printf("symsize=%d\n", ld.Symsize)
+		fmt.Printf("lcsize=%d\n", ld.Lcsize)
+		fmt.Printf("total=%d\n", ld.Segtext.Filelen+ld.Segdata.Length+uint64(ld.Symsize)+uint64(ld.Lcsize))
+	}
+}
diff --git a/src/cmd/link/internal/thumb/l.go b/src/cmd/link/internal/thumb/l.go
new file mode 100644
index 0000000000..451019b8e4
--- /dev/null
+++ b/src/cmd/link/internal/thumb/l.go
@@ -0,0 +1,75 @@
+// Inferno utils/5l/asm.c
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5l/asm.c
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+// Writing object files.
+
+// Inferno utils/5l/l.h
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5l/l.h
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+const (
+	maxAlign  = 8 // max data alignment
+	minAlign  = 1 // min data alignment
+	funcAlign = 4 // literal alignment
+)
+
+/* Used by ../internal/ld/dwarf.go */
+const (
+	dwarfRegSP = 13
+	dwarfRegLR = 14
+)
diff --git a/src/cmd/link/internal/thumb/obj.go b/src/cmd/link/internal/thumb/obj.go
new file mode 100644
index 0000000000..3d5eb295e1
--- /dev/null
+++ b/src/cmd/link/internal/thumb/obj.go
@@ -0,0 +1,91 @@
+// Inferno utils/5l/obj.c
+// https://bitbucket.org/inferno-os/inferno-os/src/default/utils/5l/obj.c
+//
+//	Copyright  1994-1999 Lucent Technologies Inc.  All rights reserved.
+//	Portions Copyright  1995-1997 C H Forsyth (forsyth@terzarima.net)
+//	Portions Copyright  1997-1999 Vita Nuova Limited
+//	Portions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com)
+//	Portions Copyright  2004,2006 Bruce Ellis
+//	Portions Copyright  2005-2007 C H Forsyth (forsyth@terzarima.net)
+//	Revisions Copyright  2000-2007 Lucent Technologies Inc. and others
+//	Portions Copyright  2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+package thumb
+
+import (
+	"cmd/internal/objabi"
+	"cmd/internal/sys"
+	"cmd/link/internal/ld"
+)
+
+func Init() (*sys.Arch, ld.Arch) {
+	arch := sys.ArchThumb
+	theArch := ld.Arch{
+		Funcalign:        funcAlign,
+		Maxalign:         maxAlign,
+		Minalign:         minAlign,
+		Dwarfregsp:       dwarfRegSP,
+		Dwarfreglr:       dwarfRegLR,
+		Archinit:         archinit,
+		Archreloc:        archreloc,
+		Archrelocvariant: archrelocvariant,
+		Trampoline:       trampoline,
+		Asmb:             asmb,
+		Asmb2:            asmb2,
+		Elfreloc1:        elfreloc1,
+		Gentext:          gentext,
+
+		//Adddynrel:      adddynrel,
+		//Elfsetupplt:    elfsetupplt,
+		//Machoreloc1:    machoreloc1,
+		//PEreloc1:       pereloc1,
+	}
+	return arch, theArch
+}
+
+func archinit(ctxt *ld.Link) {
+	switch ctxt.HeadType {
+	default:
+		ld.Exitf("unknown -H option: %v", ctxt.HeadType)
+
+	case objabi.Hlinux:
+		*ld.FlagD = true // force static linking
+		ld.Elfinit(ctxt)
+		ld.HEADR = ld.ELFRESERVE
+		if *ld.FlagTextAddr == -1 {
+			*ld.FlagTextAddr = 0x10000 + int64(ld.HEADR)
+		}
+		if *ld.FlagRound == -1 {
+			*ld.FlagRound = 0x10000
+		}
+
+	case objabi.Hnoos:
+		*ld.FlagD = true
+		ld.Elfinit(ctxt)
+		ld.HEADR = ld.ELFRESERVE
+		if *ld.FlagTextAddr == -1 {
+			*ld.FlagTextAddr = 0
+		}
+		if *ld.FlagRound == -1 {
+			*ld.FlagRound = 8
+		}
+	}
+}
diff --git a/src/cmd/link/main.go b/src/cmd/link/main.go
index 4b8df11451..80c0d75a3a 100644
--- a/src/cmd/link/main.go
+++ b/src/cmd/link/main.go
@@ -15,6 +15,7 @@ import (
 	"cmd/link/internal/mips64"
 	"cmd/link/internal/ppc64"
 	"cmd/link/internal/s390x"
+	"cmd/link/internal/thumb"
 	"cmd/link/internal/wasm"
 	"cmd/link/internal/x86"
 	"fmt"
@@ -59,6 +60,8 @@ func main() {
 		arch, theArch = ppc64.Init()
 	case "s390x":
 		arch, theArch = s390x.Init()
+	case "thumb":
+		arch, theArch = thumb.Init()
 	case "wasm":
 		arch, theArch = wasm.Init()
 	}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/decode.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/decode.go
new file mode 100644
index 0000000000..8ea2b13164
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/decode.go
@@ -0,0 +1,504 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumbasm
+
+import (
+	"encoding/binary"
+	"fmt"
+)
+
+var (
+	errShort   = fmt.Errorf("truncated instruction")
+	errUnknown = fmt.Errorf("unknown instruction")
+	errBad     = fmt.Errorf("instruction")
+)
+
+// Decode decodes the leading bytes in src as a single instruction.
+func Decode(src []byte) (Inst, error) {
+	if len(src) < 2 {
+		return Inst{}, errShort
+	}
+	if src[1]>>3 > 0x1C {
+		if len(src) < 4 {
+			return Inst{}, errShort
+		}
+		enc := binary.LittleEndian.Uint32(src)
+		x := enc>>16 | enc<<16 // ARM documentation artifact
+		for i := range inst32formats {
+			f := &inst32formats[i]
+			if f.mask&x != f.value {
+				continue
+			}
+			var args Args
+			if f.args != nil {
+				args = f.args(x)
+				if args[0] == nil {
+					return Inst{}, errBad
+				}
+			}
+			return Inst{Op: f.op, Enc: enc, Len: 4, Args: args}, nil
+		}
+		return Inst{}, errUnknown
+	}
+	x := binary.LittleEndian.Uint16(src)
+	for i := range inst16formats {
+		f := &inst16formats[i]
+		if f.mask&x != f.value {
+			continue
+		}
+		var args Args
+		if f.args != nil {
+			args = f.args(x)
+			if args[0] == nil {
+				return Inst{}, errBad
+			}
+		}
+		return Inst{Op: f.op, Enc: uint32(x), Len: 2, Args: args}, nil
+	}
+	return Inst{}, errUnknown
+}
+
+type inst16format struct {
+	mask  uint16
+	value uint16
+	op    Op
+	args  func(enc uint16) Args
+}
+
+type inst32format struct {
+	mask  uint32
+	value uint32
+	op    Op
+	args  func(enc uint32) Args
+}
+
+// 16-bit instructions
+
+// 0100 01x0 dmmm mddd
+func _ADD__Rm__Rdn(enc uint16) Args {
+	return Args{Reg(enc>>4&8 | enc&7), Reg(enc >> 3 & 15)} // Rd, Rm
+}
+
+// 0001 10xm mmnn nddd
+func _ADD__Rm__Rn__Rd(enc uint16) Args {
+	return Args{Reg(enc & 7), Reg(enc >> 3 & 7), Reg(enc >> 6 & 7)}
+}
+
+// 0001 11xu uunn nddd
+func _ADD__u3__Rn__Rd(enc uint16) Args {
+	return Args{Reg(enc & 7), Reg(enc >> 3 & 7), Imm(enc >> 6 & 7)}
+}
+
+// 1011 0000 xuuu uuuu
+func _ADD__u7_2__R13(enc uint16) Args {
+	return Args{R13, Imm(enc & 0x7F << 2)} // R13, u7<<2
+}
+
+// 1010 xddd uuuu uuuu
+func _ADD__u8_2__R13__Rd(enc uint16) Args {
+	Rn := R15
+	if enc&0x800 != 0 {
+		Rn = R13
+	}
+	return Args{Reg(enc >> 8 & 7), Rn, Imm(enc & 0xFF << 2)}
+}
+
+// 0100 xxxx xxmm mddd
+func _AND__Rm__Rdn(enc uint16) Args {
+	args := Args{Reg(enc & 7), Reg(enc >> 3 & 7)} // Rdn, Rm
+	if enc&0xFFC0 == 0x4240 {
+		args[2] = Imm(0)
+	}
+	return args
+}
+
+// 1110 0iii iiii iiii
+func _B__i11_1(enc uint16) Args {
+	return Args{PCRel(enc) << 21 >> 20}
+}
+
+// 0100 0111 xmmm m000
+func _B__Rm(enc uint16) Args {
+	return Args{Reg(enc >> 3 & 15)} // Rm
+}
+
+// 1101 cccc iiii iiii
+func _Bcond__i8_1(enc uint16) Args {
+	return Args{PCRel(enc) << 24 >> 23}
+}
+
+// 1011 x0u1 uuuu unnn
+func _CBZ__Rn__u6_1(enc uint16) Args {
+	return Args{Reg(enc & 7), PCRel(enc>>4&0x20 | enc>>3&0x1F)}
+}
+
+// 1011 0110 011x 00if
+func _CPSIE(enc uint16) Args {
+	var s string
+	switch enc & 3 {
+	case 1:
+		s = "f"
+	case 2:
+		s = "i"
+	case 3:
+		s = "if"
+	}
+	return Args{Str(s)}
+}
+
+// 1011 1111 cccc mmmm
+func _ITmask__firstcond(enc uint16) Args {
+	return Args{Cond(enc >> 4 & 0xF)}
+}
+
+// 1100 xnnn rrrr rrrr
+func _MOVM_IAW(enc uint16) Args {
+	rlist := RegList(enc & 0xFF)
+	Rn := Reg(enc >> 8 & 7)
+	mode := AddrLDM_WB
+	if enc&0x800 != 0 && 1<<Rn&rlist != 0 {
+		mode = AddrLDM
+	}
+	return Args{Mem{Base: Rn, Mode: mode}, rlist}
+}
+
+// 0101 xxxm mmnn nttt
+func _MOVW__Rn_Rm__Rt(enc uint16) Args {
+	return Args{
+		Reg(enc & 7),
+		Mem{Base: Reg(enc >> 3 & 7), Mode: AddrOffset, Sign: 1, Index: Reg(enc >> 6 & 7)},
+	}
+}
+
+// 000v vuuu uumm mddd (vv != 11)
+func _MOVW__Rm_v_u5__Rd(enc uint16) Args {
+	return Args{Reg(enc & 7), Reg(enc >> 3 & 7), Imm(enc >> 6 & 0x1F)}
+}
+
+// xxxx xuuu uunn nttt
+func _MOVW__u5_2_Rn__Rt(enc uint16) Args {
+	Rt := Reg(enc & 7)
+	Rn := Reg(enc >> 3 & 7)
+	offset := uint(enc >> 6 & 0x1F)
+	switch enc >> 12 {
+	case 6:
+		offset <<= 2
+	case 8:
+		offset <<= 1
+	}
+	return Args{Rt, Mem{Base: Rn, Offset: int16(offset)}}
+}
+
+// 001x xddd uuuu uuuu
+func _MOVW__u8__Rd(enc uint16) Args {
+	return Args{Reg(enc >> 8 & 7), Imm(enc & 0xFF)}
+}
+
+// xx0x xttt uuuu uuuu
+func _MOVW__u8_2_R13__Rt(enc uint16) Args {
+	Rt := Reg(enc >> 8 & 7)
+	mem := Mem{Mode: AddrOffset, Offset: int16(enc & 0xFF << 2)}
+	switch enc >> 11 {
+	case 0x09: // Rt, [R15, u8<<2]
+		mem.Base = R15
+	case 0x12, 0x13: // Rt, [R13, u8<<2]
+		mem.Base = R13
+	}
+	return Args{Rt, mem}
+}
+
+// 1011 x10r rrrr rrrr
+func _PUSH__reglist(enc uint16) Args {
+	rlist := enc & 0xFF
+	lrpc := enc & 0x100
+	if enc&0x800 == 0 {
+		rlist |= lrpc << 6 // PUSH
+	} else {
+		rlist |= lrpc << 7 // POP
+	}
+	return Args{RegList(rlist)}
+}
+
+// 1xx1 111x uuuu uuuu
+func _UDF__u8(enc uint16) Args {
+	return Args{Imm(enc & 0xFF)}
+}
+
+// 32-bit instructions
+
+// 1111 0u10 x0x0 nnnn  0uuu dddd uuuu uuuu
+func _ADD__u12__Rn__Rd(enc uint32) Args {
+	return Args{
+		Reg(enc >> 8 & 15),
+		Reg(enc >> 16 & 15),
+		Imm(enc>>15&0x800 | enc>>4&0x700 | enc&0xFF),
+	}
+}
+
+// 1111 0e0x xxxs nnnn  0eee dddd eeee eeee   RSB.s     e32, Rn, Rd
+func _ANDs__e32__Rn__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), Reg(enc >> 16 & 15), decodeMIC(enc)}
+}
+
+// 1110 101x xxxs nnnn  0uuu dddd uuvv mmmm
+func _ANDs__Rm_v_u5__Rn__Rd(enc uint32) Args {
+	a := _TST__Rm_v_u5__Rn(enc)
+	return Args{Reg(enc >> 8 & 15), a[0], a[1]}
+}
+
+// 1111 0jii iiii iiii  1xj1 jiii iiii iiii
+func _B__ji24_1(enc uint32) Args {
+	s := enc >> 26 & 1
+	imm10 := enc >> 16 & 0x3FF
+	i1 := ^(enc>>13 ^ s) & 1
+	i2 := ^(enc>>11 ^ s) & 1
+	imm11 := enc & 0x7FF
+	enc = s<<23 | i1<<22 | i2<<21 | imm10<<11 | imm11
+	return Args{PCRel(enc) << 8 >> 7}
+}
+
+// 1111 0011 0110 nnnn  0uuu dddd uu0k kkkk
+func _BFC__width__ulsb__Rd(enc uint32) Args {
+	a := _BFX__width__ulsb__Rn__Rd(enc)
+	Rd := a[0]
+	Rn := a[1]
+	lsb := a[2]
+	width := a[3].(Imm) - lsb.(Imm)
+	if Rn.(Reg) == 15 {
+		return Args{Rd, lsb, width} // BFC
+	}
+	return Args{Rd, Rn, lsb, width} // BFI
+}
+
+// 1111 0011 x100 nnnn  0uuu dddd uu0w wwww
+func _BFX__width__ulsb__Rn__Rd(enc uint32) Args {
+	width := Imm(enc&0x1F) + 1
+	lsb := Imm(enc>>10&0x1C | enc>>6&0x3)
+	Rn := Reg(enc >> 16 & 15)
+	Rd := Reg(enc >> 8 & 15)
+	return Args{Rd, Rn, lsb, width}
+}
+
+// 1111 1010 10x1 mmmm  1111 dddd 10xx mmmm
+func _CLZ__Rm__Rd(enc uint32) Args {
+	Rn := Reg(enc >> 16 & 15)
+	Rd := Reg(enc >> 8 & 15)
+	Rm := Reg(enc & 15)
+	if Rn != Rm {
+		return Args{Rd, ArgErr(fmt.Sprintf("{!inconsistent Rm: %v/%v}", Rm, Rn))}
+	}
+	return Args{Rd, Rm}
+}
+
+// 1111 0011 1011 1111  1000 1111 0100 oooo
+func _DSB__opt(enc uint32) Args {
+	return Args{Imm(enc & 15)}
+}
+
+// 1110 1000 0101 nnnn  tttt 1111 uuuu uuuu
+func _LDREX__u8_2_Rn__Rt(enc uint32) Args {
+	return Args{
+		Reg(enc >> 12 & 15),
+		Mem{Base: Reg(enc >> 16 & 15), Mode: AddrOffset, Offset: int16(enc & 0xFF << 2)},
+	}
+}
+
+// 1110 1000 1101 nnnn  tttt 1111 010x 1111
+func _LDREXB__Rn__Rt(enc uint32) Args {
+	return Args{
+		Reg(enc >> 12 & 15),
+		Mem{Base: Reg(enc >> 16 & 15), Mode: AddrOffset},
+	}
+}
+
+// 1111 1010 0x0x 1111  1111 dddd 10rr mmmm
+func _MOVH__Rm_rot__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), RegShift{Reg(enc & 15), RotateRight, uint8(enc >> 1 & 0x18)}}
+}
+
+// 1110 100x x0wx nnnn  rr0r rrrr rrrr rrrr
+func _MOVM_IAw(enc uint32) Args {
+	mode := AddrLDM
+	if enc&0x200000 != 0 {
+		mode = AddrLDM_WB
+	}
+	Rn := Reg(enc >> 16 & 15)
+	return Args{Mem{Base: Rn, Mode: mode}, RegList(enc)}
+}
+
+// 1111 100x 0xxx nnnn  tttt 0000 00uu mmmm
+func _MOVW__Rn_Rm_1_u2__Rt(enc uint32) Args {
+	return Args{
+		Reg(enc >> 12 & 15),
+		Mem{
+			Base:  Reg(enc >> 16 & 15),
+			Mode:  AddrOffset,
+			Sign:  1,
+			Index: Reg(enc & 15),
+			Shift: ShiftLeft,
+			Count: uint8(enc >> 4 & 3),
+		},
+	}
+}
+
+// 1111 100x 1xxx nnnn  tttt uuuu uuuu uuuu
+// 1111 100x xxx 1111  tttt uuuu uuuu uuuu
+func _MOVW__s12_Rn__Rt(enc uint32) Args {
+	Rn := Reg(enc >> 16 & 15)
+	Rt := Reg(enc >> 12 & 15)
+	offset := int16(enc & 0xFFF)
+	if enc&0x800000 == 0 {
+		offset = -offset
+	}
+	return Args{Rt, Mem{Base: Rn, Offset: offset}}
+}
+
+// 1111 0y10 x100 uuuu  0zzz dddd zzzz zzzz
+func _MOVW__uyz16__Rd(enc uint32) Args {
+	Rd := Reg(enc >> 8 & 15)
+	imm := Imm(enc>>4&0xF000 | enc>>15&0x800 | enc>>4&0x700 | enc&0xFF)
+	return Args{Rd, imm}
+}
+
+// 1111 100x 0xxx nnnn  tttt 1pw uuuu uuuu
+func _MOVWpw__s8_Rn__Rt(enc uint32) Args {
+	Rn := Reg(enc >> 16 & 15)
+	Rt := Reg(enc >> 12 & 15)
+	offset := int16(enc & 0xFF)
+	if enc&0x200 == 0 {
+		offset = -offset
+	}
+	p0w := enc >> 8 & 5
+	var mode AddrMode
+	switch p0w {
+	case 1:
+		mode = AddrPostIndex
+	case 4:
+		mode = AddrOffset
+	case 5:
+		mode = AddrPreIndex
+	default:
+		return Args{} // undefined
+	}
+	return Args{Rt, Mem{Base: Rn, Mode: mode, Offset: offset}}
+}
+
+// 1111 0e00 01xs 1111  0eee dddd eeee eeee
+func _MOVWs__e32__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), decodeMIC(enc)}
+}
+
+// 1111 1010 0vvs nnnn  1111 dddd 0000 mmmm
+func _MOVWs__Rn_v_Rm__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), Reg(enc >> 16 & 15), Reg(enc & 15)}
+}
+
+// 1111 0011 1110 1111  1000 dddd mmmm mmmm
+// 1111 0011 1000 nnnn  1000 kk00 mmmm mmmm
+func _MOVW__SYSm__Rd(enc uint32) Args {
+	SYSm := APSR + Reg(enc&0xFF)
+	if enc>>20 == 0xF3E {
+		// MRS
+		return Args{Reg(enc >> 8 & 15), SYSm}
+	} else {
+		// MSR
+		return Args{SYSm, Reg(enc >> 16 & 15)}
+	}
+}
+
+// 1111 1011 xxxx nnnn  1111 dddd xxxx mmmm
+func _MUL__Rm__Rn__Rd(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), Reg(enc >> 16 & 15), Reg(enc & 15)}
+}
+
+// 1111 1011 xxxx nnnn  llll hhhh 000x mmmm
+func _MULL__Rm__Rn__Rdh_Rdl(enc uint32) Args {
+	return Args{Reg(enc >> 12 & 15), Reg(enc>>8) & 15, Reg(enc >> 16 & 15), Reg(enc & 15)}
+}
+
+// 1111 1000 010x 1101 tttt 1xx1 0000 0100
+func _PUSH__Rt(enc uint32) Args {
+	return Args{RegList(1 << (enc >> 12 & 0xF))}
+}
+
+// 1110 1000 1101 nnnn  1111 0000 000h mmmm
+func _TBB__Rm__Rn(enc uint32) Args {
+	return Args{Reg(enc >> 16 & 15), Reg(enc & 15)}
+}
+
+// 1111 0e0x x0x1 nnnn  0eee 1111 eeee eeee
+func _TST__e32__Rn(enc uint32) Args {
+	return Args{Reg(enc >> 16 & 15), decodeMIC(enc)}
+}
+
+// 1110 101x x0x1 nnnn  0uuu 1111 uuvv mmmm
+func _TST__Rm_v_u5__Rn(enc uint32) Args {
+	return Args{Reg(enc >> 16 & 15), decodeShiftI(enc)}
+}
+
+// 1110 1010 01xs 1111  0uuu dddd uuvv mmmm
+func _MOVWs__Rm_v_u5__Rn(enc uint32) Args {
+	return Args{Reg(enc >> 8 & 15), decodeShiftI(enc)}
+}
+
+// 1110 1000 0100 nnnn  tttt dddd uuuu uuuu
+func _STREX__Rt__u8_2_Rn__Rd(enc uint32) Args {
+	a := _LDREX__u8_2_Rn__Rt(enc)
+	return Args{Reg(enc >> 8 & 15), a[0], a[1]}
+}
+
+// 1110 1000 1100 nnnn  tttt 1111 010x dddd
+func _STREXB__Rn__Rt(enc uint32) Args {
+	a := _LDREXB__Rn__Rt(enc)
+	return Args{Reg(enc & 15), a[0], a[1]}
+}
+
+// 1110 1101 d0x nnnn  dddd 101x uuuu uuuu
+func _MOVF__s8_2_Rn__Fd(enc uint32) Args {
+	var Vd Reg
+	if (enc>>8)&1 != 0 {
+		Vd = Reg(int(D0) + int((enc>>18)&16|(enc>>12)&15))
+	} else {
+		Vd = Reg(int(S0) + int((enc>>11)&30|(enc>>22)&1))
+	}
+	offset := int(enc) & 0xFF
+	if (enc>>23)&1 == 0 {
+		offset = -offset
+	}
+	return Args{Vd, Mem{Mode: AddrOffset, Base: Reg(enc >> 16 & 15), Offset: int16(offset)}}
+}
+
+// 1110 1110 1d11 0001  dddd 101x 11m0 mmmm
+func _SQRTF__Fm__Fd(enc uint32) Args {
+	var Vm, Vd Reg
+	if (enc>>8)&1 != 0 {
+		Vm = Reg(int(D0) + int((enc>>1)&16|enc&15))
+		Vd = Reg(int(D0) + int((enc>>18)&16|(enc>>12)&15))
+	} else {
+		Vm = Reg(int(S0) + int((enc<<1)&30|(enc>>5)&1))
+		Vd = Reg(int(S0) + int((enc>>11)&30|(enc>>22)&1))
+	}
+	return Args{Vd, Vm}
+}
+
+func decodeMIC(enc uint32) ImmAlt {
+	rot := int(enc>>22&0x10 | enc>>11&0x0E | enc>>7&0x01)
+	if rot < 8 {
+		enc &= 0xFF
+	} else {
+		enc = enc&0x7F | 0x80
+	}
+	return ImmAlt{uint8(enc), uint8(rot)}
+}
+
+func decodeShiftI(enc uint32) RegShift {
+	Rm := Reg(enc & 15)
+	shift := Shift(enc >> 4 & 3)
+	count := uint8(enc>>10&0x1C | enc>>6&3)
+	if shift == RotateRight && count == 0 {
+		shift = RotateRightExt
+	}
+	return RegShift{Rm, shift, count}
+}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/inst.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/inst.go
new file mode 100644
index 0000000000..ae0ff54ecb
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/inst.go
@@ -0,0 +1,486 @@
+// Copyright 2018 The Go Authors.  All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumbasm
+
+import (
+	"bytes"
+	"fmt"
+)
+
+type Op uint16
+
+// An Inst is a single instruction.
+type Inst struct {
+	Op   Op     // Opcode mnemonic
+	Enc  uint32 // Raw encoding bits.
+	Len  int    // Length of encoding in bytes.
+	Args Args   // Instruction arguments, in ARM manual order.
+}
+
+func (i Inst) String() string {
+	var buf bytes.Buffer
+	buf.WriteString(i.Op.String())
+	for j, arg := range i.Args {
+		if arg == nil {
+			break
+		}
+		if j == 0 {
+			buf.WriteString(" ")
+		} else {
+			buf.WriteString(", ")
+		}
+		buf.WriteString(arg.String())
+	}
+	return buf.String()
+}
+
+// An Args holds the instruction arguments.
+// If an instruction has fewer than 4 arguments,
+// the final elements in the array are nil.
+type Args [4]Arg
+
+// An Arg is a single instruction argument, one of these types:
+// Endian, Imm, Mem, PCRel, Reg, RegList, RegShift, RegShiftReg.
+type Arg interface {
+	IsArg()
+	String() string
+}
+
+type Float32Imm float32
+
+func (Float32Imm) IsArg() {}
+
+func (f Float32Imm) String() string {
+	return fmt.Sprintf("#%v", float32(f))
+}
+
+type Float64Imm float32
+
+func (Float64Imm) IsArg() {}
+
+func (f Float64Imm) String() string {
+	return fmt.Sprintf("#%v", float64(f))
+}
+
+// An Imm is an integer constant.
+type Imm uint32
+
+func (Imm) IsArg() {}
+
+func (i Imm) String() string {
+	return fmt.Sprintf("#%#x", uint32(i))
+}
+
+// An ImmAlt is an alternate encoding of an integer constant.
+type ImmAlt struct {
+	Val uint8
+	Rot uint8
+}
+
+func (ImmAlt) IsArg() {}
+
+func (i ImmAlt) Imm() Imm {
+	v := uint32(i.Val)
+	r := uint(i.Rot)
+	return Imm(v>>r | v<<(32-r))
+}
+
+func ror(x uint32, shift int) uint32 {
+	m := uint(shift) & 31
+	return x>>m | x<<(32-m)
+}
+
+func (i ImmAlt) String() string {
+	val, rot := uint32(i.Val), int(i.Rot)
+	var u uint32
+	var s string
+	if rot > 7 {
+		u = ror(val, rot)
+		s = "@>"
+	} else {
+		rot >>= 1
+		switch {
+		case rot == 0:
+			u = val
+		case val == 0:
+			return "UNPREDICTABLE!"
+		case rot == 1:
+			u = val<<16 | val
+		case rot == 2:
+			u = val<<24 | val<<8
+		default: // rot == 3
+			u = val<<24 | val<<16 | val<<8 | val
+		}
+		s = ":"
+	}
+	return fmt.Sprintf("#%#x{%#x%s%d}", u, val, s, rot)
+}
+
+// A Label is a text (code) address.
+type Label uint32
+
+func (Label) IsArg() {}
+
+func (i Label) String() string {
+	return fmt.Sprintf("%#x", uint32(i))
+}
+
+// A Reg is a single register.
+// The zero value denotes R0, not the absence of a register.
+type Reg uint8
+
+const (
+	R0 Reg = iota
+	R1
+	R2
+	R3
+	R4
+	R5
+	R6
+	R7
+	R8
+	R9
+	R10
+	R11
+	R12
+	R13
+	R14
+	R15
+
+	S0
+	S1
+	S2
+	S3
+	S4
+	S5
+	S6
+	S7
+	S8
+	S9
+	S10
+	S11
+	S12
+	S13
+	S14
+	S15
+	S16
+	S17
+	S18
+	S19
+	S20
+	S21
+	S22
+	S23
+	S24
+	S25
+	S26
+	S27
+	S28
+	S29
+	S30
+	S31
+
+	D0
+	D1
+	D2
+	D3
+	D4
+	D5
+	D6
+	D7
+	D8
+	D9
+	D10
+	D11
+	D12
+	D13
+	D14
+	D15
+	D16
+	D17
+	D18
+	D19
+	D20
+	D21
+	D22
+	D23
+	D24
+	D25
+	D26
+	D27
+	D28
+	D29
+	D30
+	D31
+
+	FPSCR
+
+	APSR        // 0
+	IAPSR       // 1
+	EAPSR       // 2
+	XPSR        // 3
+	_           // 4
+	IPSR        // 5
+	EPSR        // 6
+	IEPSR       // 7
+	MSP         // 8
+	PSP         // 9
+	_           // 10
+	_           // 11
+	_           // 12
+	_           // 13
+	_           // 14
+	_           // 15
+	PRIMASK     // 16
+	BASEPRI     // 17
+	BASEPRI_MAX // 18
+	FAULTMASK   // 19
+	CONTROL     // 20
+
+	SP = R13
+	LR = R14
+	PC = R15
+)
+
+func (Reg) IsArg() {}
+
+func (r Reg) String() string {
+	if R0 <= r && r <= R15 {
+		return fmt.Sprintf("R%d", int(r-R0))
+	}
+	if D0 <= r && r <= D15 {
+		return fmt.Sprintf("D%d", int(r-D0))
+	}
+	if S0 <= r && r <= S31 {
+		return fmt.Sprintf("S%d", int(r-S0))
+	}
+	switch r {
+	case APSR:
+		return "APSR"
+	case IAPSR:
+		return "IAPSR"
+	case EAPSR:
+		return "EAPSR"
+	case XPSR:
+		return "XPSR"
+	case IPSR:
+		return "IPSR"
+	case EPSR:
+		return "EPSR"
+	case IEPSR:
+		return "IEPSR"
+
+	case FPSCR:
+		return "FPSCR"
+
+	case MSP:
+		return "MSP"
+	case PSP:
+		return "PSP"
+
+	case PRIMASK:
+		return "PRIMASK"
+	case BASEPRI:
+		return "BASEPRI"
+	case BASEPRI_MAX:
+		return "BASEPRI_MAX"
+	case FAULTMASK:
+		return "FAULTMASK"
+	case CONTROL:
+		return "CONTROL"
+	}
+	return fmt.Sprintf("Reg(%d)", int(r))
+}
+
+// A RegX represents a fraction of a multi-value register.
+// The Index field specifies the index number,
+// but the size of the fraction is not specified.
+// It must be inferred from the instruction and the register type.
+// For example, in a VMOV instruction, RegX{D5, 1} represents
+// the top 32 bits of the 64-bit D5 register.
+type RegX struct {
+	Reg   Reg
+	Index int
+}
+
+func (RegX) IsArg() {}
+
+func (r RegX) String() string {
+	return fmt.Sprintf("%s[%d]", r.Reg, r.Index)
+}
+
+// A RegList is a register list.
+// Bits at indexes x = 0 through 15 indicate whether the corresponding Rx register is in the list.
+type RegList uint16
+
+func (RegList) IsArg() {}
+
+func (r RegList) String() string {
+	var buf bytes.Buffer
+	fmt.Fprintf(&buf, "{")
+	sep := ""
+	for i := 0; i < 16; i++ {
+		if r&(1<<uint(i)) != 0 {
+			fmt.Fprintf(&buf, "%s%s", sep, Reg(i).String())
+			sep = ","
+		}
+	}
+	fmt.Fprintf(&buf, "}")
+	return buf.String()
+}
+
+// An Endian is the argument to the SETEND instruction.
+type Endian uint8
+
+const (
+	LittleEndian Endian = 0
+	BigEndian    Endian = 1
+)
+
+func (Endian) IsArg() {}
+
+func (e Endian) String() string {
+	if e != 0 {
+		return "BE"
+	}
+	return "LE"
+}
+
+// A Shift describes an ARM shift operation.
+type Shift uint8
+
+const (
+	ShiftLeft        Shift = 0 // left shift
+	ShiftRight       Shift = 1 // logical (unsigned) right shift
+	ShiftRightSigned Shift = 2 // arithmetic (signed) right shift
+	RotateRight      Shift = 3 // right rotate
+	RotateRightExt   Shift = 4 // right rotate through carry (Count will always be 1)
+)
+
+var shiftName = [...]string{
+	"LSL", "LSR", "ASR", "ROR", "RRX",
+}
+
+func (s Shift) String() string {
+	if s < 5 {
+		return shiftName[s]
+	}
+	return fmt.Sprintf("Shift(%d)", int(s))
+}
+
+// A RegShift is a register shifted by a constant.
+type RegShift struct {
+	Reg   Reg
+	Shift Shift
+	Count uint8
+}
+
+func (RegShift) IsArg() {}
+
+func (r RegShift) String() string {
+	return fmt.Sprintf("%s %s #%d", r.Reg, r.Shift, r.Count)
+}
+
+// A PCRel describes a memory address (usually a code label)
+// as a distance relative to the PC+4.
+type PCRel int32
+
+func (PCRel) IsArg() {}
+
+func (r PCRel) String() string {
+	return fmt.Sprintf("PC%+#x", int32(r))
+}
+
+// An AddrMode is an ARM addressing mode.
+type AddrMode uint8
+
+const (
+	_             AddrMode = iota
+	AddrPostIndex          // [R], X  use address R, set R = R + X
+	AddrPreIndex           // [R, X]!  use address R + X, set R = R + X
+	AddrOffset             // [R, X]  use address R + X
+	AddrLDM                // R  [R] but formats as R, for LDM/STM only
+	AddrLDM_WB             // R! - [R], X where X is instruction-specific amount, for LDM/STM only
+)
+
+// A Mem is a memory reference made up of a base R and index expression X.
+// The effective memory address is R or R+X depending on AddrMode.
+// The index expression is X = Sign*(Index Shift Count) + Offset,
+// but in any instruction either Sign = 0 or Offset = 0.
+type Mem struct {
+	Base   Reg
+	Mode   AddrMode
+	Sign   int8
+	Index  Reg
+	Shift  Shift
+	Count  uint8
+	Offset int16
+}
+
+func (Mem) IsArg() {}
+
+func (m Mem) String() string {
+	R := m.Base.String()
+	X := ""
+	if m.Sign != 0 {
+		X = "+"
+		if m.Sign < 0 {
+			X = "-"
+		}
+		X += m.Index.String()
+		if m.Shift != ShiftLeft || m.Count != 0 {
+			X += fmt.Sprintf(", %s #%d", m.Shift, m.Count)
+		}
+	} else {
+		X = fmt.Sprintf("#%d", m.Offset)
+	}
+
+	switch m.Mode {
+	case AddrOffset:
+		if X == "#0" {
+			return fmt.Sprintf("[%s]", R)
+		}
+		return fmt.Sprintf("[%s, %s]", R, X)
+	case AddrPreIndex:
+		return fmt.Sprintf("[%s, %s]!", R, X)
+	case AddrPostIndex:
+		return fmt.Sprintf("[%s], %s", R, X)
+	case AddrLDM:
+		if X == "#0" {
+			return R
+		}
+	case AddrLDM_WB:
+		if X == "#0" {
+			return R + "!"
+		}
+	}
+	return fmt.Sprintf("[%s Mode(%d) %s]", R, int(m.Mode), X)
+}
+
+// A Cond is an argument of the IT instruction.
+type Cond uint8
+
+func (c Cond) IsArg() {}
+
+func (c Cond) String() string {
+	if c > 14 {
+		return "??"
+	}
+	i := int(c) * 2
+	return "EQNEHSLOMIPLVSVCHILSGELTGTLEAL"[i : i+2]
+}
+
+type Str string
+
+func (s Str) IsArg()         {}
+func (s Str) String() string { return string(s) }
+
+type ArgErr string
+
+func (e ArgErr) IsArg() {}
+
+func (e ArgErr) String() string {
+	return string(e)
+}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/op_string.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/op_string.go
new file mode 100644
index 0000000000..933419f6da
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/op_string.go
@@ -0,0 +1,17 @@
+// Code generated by "stringer -type Op"; DO NOT EDIT.
+
+package thumbasm
+
+import "strconv"
+
+const _Op_name = "ADDADD_SADCADC_SANDAND_SASRASR_SBBCCBCSBEQBFCBFIBGEBGTBHIBICBIC_SBKPTBLBLEBLSBLTBLXBMIBNEBPLBVCBVSBXCBNZCBZCLREXCLZCMNCMPCPSIDCPSIEDMBDSBEOREOR_SISBITITEITEEITEEEITEETITETITETEITETTITTITTEITTEEITTETITTTITTTEITTTTLDMIALDMDBLDRLDRBLDRDLDREXLDREXBLDREXHLDRHLDRSBLDRSHLSLLSL_SLSRLSR_SMLAMLSMOVMOV_SMOVTMOVWMRSMSRMVNMVN_SMULNOPORNORN_SORRORR_SPLDPLIPOPPUSHRBITREVREV16REVSHRORROR_SRSBRSB_SSBCSBC_SSBFXSDIVSELSEVSMLABBSMLABTSMLADSMLADXSMLALSMLATBSMLATTSMLAWBSMLAWTSMLSDSMLSDXSMMLASMMLSSMULLSTMIASTMDBSTRSTRBSTRDSTREXSTREXBSTREXHSTRHSUBSUB_SSVCSXTBSXTHTBBTBHTSTTEQUBFXUDFUDIVUMLALUMULLUXTBUXTHVABS_F32VABS_F64VADD_F32VADD_F64VCMP_F32VCMP_F64VCMPE_F32VCMPE_F64VCVT_F32_FXS16VCVT_F32_FXS32VCVT_F32_FXU16VCVT_F32_FXU32VCVT_F64_FXS16VCVT_F64_FXS32VCVT_F64_FXU16VCVT_F64_FXU32VCVT_F32_U32VCVT_F32_S32VCVT_F64_U32VCVT_F64_S32VCVT_F64_F32VCVT_F32_F64VCVT_FXS16_F32VCVT_FXS16_F64VCVT_FXS32_F32VCVT_FXS32_F64VCVT_FXU16_F32VCVT_FXU16_F64VCVT_FXU32_F32VCVT_FXU32_F64VCVTB_F32_F16VCVTB_F16_F32VCVTT_F32_F16VCVTT_F16_F32VCVTR_U32_F32VCVTR_U32_F64VCVTR_S32_F32VCVTR_S32_F64VCVT_U32_F32VCVT_U32_F64VCVT_S32_F32VCVT_S32_F64VDIV_F32VDIV_F64VLDRVMLA_F32VMLA_F64VMLS_F32VMLS_F64VMOVVMOV_32VMOV_F32VMOV_F64VMRSVMSRVMUL_F32VMUL_F64VNEG_F32VNEG_F64VNMLS_F32VNMLS_F64VNMLA_F32VNMLA_F64VNMUL_F32VNMUL_F64VSQRT_F32VSQRT_F64VSTRVSUB_F32VSUB_F64WFEWFIYIELD"
+
+var _Op_index = [...]uint16{0, 3, 8, 11, 16, 19, 24, 27, 32, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 65, 69, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 100, 104, 107, 112, 115, 118, 121, 126, 131, 134, 137, 140, 145, 148, 150, 153, 157, 162, 167, 171, 176, 181, 184, 188, 193, 198, 202, 207, 212, 217, 222, 225, 229, 233, 238, 244, 250, 254, 259, 264, 267, 272, 275, 280, 283, 286, 289, 294, 298, 302, 305, 308, 311, 316, 319, 322, 325, 330, 333, 338, 341, 344, 347, 351, 355, 358, 363, 368, 371, 376, 379, 384, 387, 392, 396, 400, 403, 406, 412, 418, 423, 429, 434, 440, 446, 452, 458, 463, 469, 474, 479, 484, 489, 494, 497, 501, 505, 510, 516, 522, 526, 529, 534, 537, 541, 545, 548, 551, 554, 557, 561, 564, 568, 573, 578, 582, 586, 594, 602, 610, 618, 626, 634, 643, 652, 666, 680, 694, 708, 722, 736, 750, 764, 776, 788, 800, 812, 824, 836, 850, 864, 878, 892, 906, 920, 934, 948, 961, 974, 987, 1000, 1013, 1026, 1039, 1052, 1064, 1076, 1088, 1100, 1108, 1116, 1120, 1128, 1136, 1144, 1152, 1156, 1163, 1171, 1179, 1183, 1187, 1195, 1203, 1211, 1219, 1228, 1237, 1246, 1255, 1264, 1273, 1282, 1291, 1295, 1303, 1311, 1314, 1317, 1322}
+
+func (i Op) String() string {
+	i -= 1
+	if i >= Op(len(_Op_index)-1) {
+		return "Op(" + strconv.FormatInt(int64(i+1), 10) + ")"
+	}
+	return _Op_name[_Op_index[i]:_Op_index[i+1]]
+}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/plan9x.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/plan9x.go
new file mode 100644
index 0000000000..f7f614ccbc
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/plan9x.go
@@ -0,0 +1,412 @@
+// Copyright 2014 The Go Authors.  All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package thumbasm
+
+import (
+	"bytes"
+	"encoding/binary"
+	"fmt"
+	"io"
+	"math"
+	"strings"
+)
+
+// GoSyntax returns the Go assembler syntax for the instruction.
+// The syntax was originally defined by Plan 9.
+// The pc is the program counter of the instruction, used for expanding
+// PC-relative addresses into absolute ones.
+// The symname function queries the symbol table for the program
+// being disassembled. Given a target address it returns the name and base
+// address of the symbol containing the target, if any; otherwise it returns "", 0.
+// The reader r should read from the text segment using text addresses
+// as offsets; it is used to display pc-relative loads as constant loads.
+func GoSyntax(inst Inst, pc uint64, symname func(uint64) (string, uint64), text io.ReaderAt) string {
+	if symname == nil {
+		symname = func(uint64) (string, uint64) { return "", 0 }
+	}
+
+	//switch inst.Op {
+	//case BX, BLX:
+	//	if inst.Op == BX {
+	//		inst.Op = B
+	//	} else {
+	//		inst.Op = BL
+	//	}
+	//	inst.Args[0] = Mem{Base: inst.Args[0].(Reg), Mode: AddrOffset}
+	//}
+
+	var args []string
+	for _, a := range inst.Args {
+		if a == nil {
+			break
+		}
+		args = append(args, plan9Arg(&inst, pc, symname, a))
+	}
+
+	op := inst.Op.String()
+
+	switch inst.Op {
+	case LDR, LDRB, LDRH, LDRSB, LDRSH, VLDR:
+		// Check for RET
+		reg, _ := inst.Args[0].(Reg)
+		mem, _ := inst.Args[1].(Mem)
+		if inst.Op == LDR && reg == R15 && mem.Base == SP && mem.Sign == 0 && mem.Mode == AddrPostIndex {
+			return fmt.Sprintf("RET%s #%d", op[3:], mem.Offset)
+		}
+
+		// Check for PC-relative load.
+		if mem.Base == PC && mem.Sign == 0 && mem.Mode == AddrOffset && text != nil {
+			addr := uint32(pc) + 4 + uint32(mem.Offset)
+			buf := make([]byte, 8)
+			switch inst.Op {
+			case LDRB, LDRSB:
+				if _, err := text.ReadAt(buf[:1], int64(addr)); err != nil {
+					break
+				}
+				args[1] = fmt.Sprintf("$%#x", buf[0])
+
+			case LDRH, LDRSH:
+				if _, err := text.ReadAt(buf[:2], int64(addr)); err != nil {
+					break
+				}
+				args[1] = fmt.Sprintf("$%#x", binary.LittleEndian.Uint16(buf))
+
+			case LDR:
+				if _, err := text.ReadAt(buf[:4], int64(addr)); err != nil {
+					break
+				}
+				x := binary.LittleEndian.Uint32(buf)
+				if s, base := symname(uint64(x)); s != "" && uint64(x) == base {
+					args[1] = fmt.Sprintf("$%s(SB)", s)
+				} else {
+					args[1] = fmt.Sprintf("$%#x", x)
+				}
+
+			case VLDR:
+				switch {
+				case strings.HasPrefix(args[0], "D"): // VLDR.F64
+					if _, err := text.ReadAt(buf, int64(addr)); err != nil {
+						break
+					}
+					args[1] = fmt.Sprintf("$%f", math.Float64frombits(binary.LittleEndian.Uint64(buf)))
+				case strings.HasPrefix(args[0], "S"): // VLDR.F32
+					if _, err := text.ReadAt(buf[:4], int64(addr)); err != nil {
+						break
+					}
+					args[1] = fmt.Sprintf("$%f", math.Float32frombits(binary.LittleEndian.Uint32(buf)))
+				default:
+					panic(fmt.Sprintf("wrong FP register: %v", inst))
+				}
+			}
+		}
+	}
+
+	// Move addressing mode into opcode suffix.
+	suffix := ""
+	switch inst.Op {
+	case PLD, PLI:
+		if mem, ok := inst.Args[0].(Mem); ok {
+			args[0], suffix = memOpTrans(mem)
+		} else {
+			panic(fmt.Sprintf("illegal instruction: %v", inst))
+		}
+	case LDR, LDRB, LDRSB, LDRH, LDRSH, STR, STRB, STRH, VLDR, VSTR, LDREX, LDREXH, LDREXB:
+		if mem, ok := inst.Args[1].(Mem); ok {
+			args[1], suffix = memOpTrans(mem)
+		} else {
+			panic(fmt.Sprintf("illegal instruction: %v", inst))
+		}
+	case STREX, STREXB, STREXH:
+		if mem, ok := inst.Args[2].(Mem); ok {
+			args[2], suffix = memOpTrans(mem)
+		} else {
+			panic(fmt.Sprintf("illegal instruction: %v", inst))
+		}
+	}
+
+	switch inst.Op {
+	case STR, STRB, STRH, CBZ, CBNZ:
+		break
+	default:
+		// Reverse args, placing dest last.
+		for i, j := 0, len(args)-1; i < j; i, j = i+1, j-1 {
+			args[i], args[j] = args[j], args[i]
+		}
+	}
+
+	// For MLA-like instructions, the addend is the third operand.
+	switch inst.Op {
+	case SMLAWT, SMLAWB, MLA, MLS, SMMLA, SMMLS, SMLABB, SMLATB, SMLABT, SMLATT, SMLAD, SMLADX, SMLSD, SMLSDX:
+		args = []string{args[1], args[2], args[0], args[3]}
+	}
+	// For STREX like instructions, the memory operands comes first.
+	switch inst.Op {
+	case STREX, STREXB, STREXH:
+		args = []string{args[1], args[0], args[2]}
+	}
+
+	// special process for FP instructions
+	op, args = fpTrans(&inst, op, args)
+
+	// LDR/STR like instructions -> MOV like
+	switch inst.Op {
+	case MOV:
+		op = "MOVW" + op[3:]
+	case LDR, MSR, MRS:
+		op = "MOVW" + op[3:] + suffix
+	case VMRS, VMSR:
+		op = "MOVW" + op[4:] + suffix
+	case LDRB, UXTB:
+		op = "MOVBU" + op[4:] + suffix
+	case LDRSB:
+		op = "MOVBS" + op[5:] + suffix
+	case SXTB:
+		op = "MOVBS" + op[4:] + suffix
+	case LDRH, UXTH:
+		op = "MOVHU" + op[4:] + suffix
+	case LDRSH:
+		op = "MOVHS" + op[5:] + suffix
+	case SXTH:
+		op = "MOVHS" + op[4:] + suffix
+	case STR:
+		op = "MOVW" + op[3:] + suffix
+	case STRB:
+		op = "MOVB" + op[4:] + suffix
+	case STRH:
+		op = "MOVH" + op[4:] + suffix
+	case VSTR:
+		args[0], args[1] = args[1], args[0]
+	default:
+		op = op + suffix
+	}
+
+	if args != nil {
+		op += " " + strings.Join(args, ", ")
+	}
+
+	return op
+}
+
+// assembler syntax for the various shifts.
+// @x> is a lie; the assembler uses @> 0
+// instead of @x> 1, but i wanted to be clear that it
+// was a different operation (rotate right extended, not rotate right).
+var plan9Shift = []string{"<<", ">>", "->", "@>", "@x>"}
+
+func plan9Arg(inst *Inst, pc uint64, symname func(uint64) (string, uint64), arg Arg) string {
+	switch a := arg.(type) {
+	case Endian:
+
+	case Imm:
+		return fmt.Sprintf("$%d", uint32(a))
+
+	case Mem:
+		if a.Mode == AddrOffset && a.Sign == 0 && a.Offset == 0 {
+			return fmt.Sprintf("(R%d)", a.Base)
+		}
+
+	case PCRel:
+		addr := uint64(int64(pc) + 4 + int64(a))
+		if s, base := symname(addr); s != "" && addr == base {
+			return fmt.Sprintf("%s(SB)", s)
+		}
+		return fmt.Sprintf("%#x", addr)
+
+	case Reg:
+		if a < 16 {
+			return fmt.Sprintf("R%d", int(a))
+		}
+
+	case RegList:
+		var buf bytes.Buffer
+		start := -2
+		end := -2
+		fmt.Fprintf(&buf, "[")
+		flush := func() {
+			if start >= 0 {
+				if buf.Len() > 1 {
+					fmt.Fprintf(&buf, ",")
+				}
+				if start == end {
+					fmt.Fprintf(&buf, "R%d", start)
+				} else {
+					fmt.Fprintf(&buf, "R%d-R%d", start, end)
+				}
+				start = -2
+				end = -2
+			}
+		}
+		for i := 0; i < 16; i++ {
+			if a&(1<<uint(i)) != 0 {
+				if i == end+1 {
+					end++
+					continue
+				}
+				start = i
+				end = i
+			} else {
+				flush()
+			}
+		}
+		flush()
+		fmt.Fprintf(&buf, "]")
+		return buf.String()
+
+	case RegShift:
+		return fmt.Sprintf("R%d%s$%d", int(a.Reg), plan9Shift[a.Shift], int(a.Count))
+
+	}
+	return arg.String()
+}
+
+// convert memory operand from GNU syntax to Plan 9 syntax, for example,
+// [r5] -> (R5)
+// [r6, #4080] -> 0xff0(R6)
+// [r2, r0, ror #1] -> (R2)(R0@>1)
+// inst [r2, -r0, ror #1] -> INST.U (R2)(R0@>1)
+// input:
+//   a memory operand
+// return values:
+//   corresponding memory operand in Plan 9 syntax
+//   .W/.P/.U suffix
+func memOpTrans(mem Mem) (string, string) {
+	suffix := ""
+	switch mem.Mode {
+	case AddrOffset, AddrLDM:
+		// no suffix
+	case AddrPreIndex, AddrLDM_WB:
+		suffix = ".W"
+	case AddrPostIndex:
+		suffix = ".P"
+	}
+	off := ""
+	if mem.Offset != 0 {
+		off = fmt.Sprintf("%#x", mem.Offset)
+	}
+	base := fmt.Sprintf("(R%d)", int(mem.Base))
+	index := ""
+	if mem.Sign != 0 {
+		sign := ""
+		if mem.Sign < 0 {
+			suffix += ".U"
+		}
+		shift := ""
+		if mem.Count != 0 {
+			shift = fmt.Sprintf("%s%d", plan9Shift[mem.Shift], mem.Count)
+		}
+		index = fmt.Sprintf("(%sR%d%s)", sign, int(mem.Index), shift)
+	}
+	return off + base + index, suffix
+}
+
+type goFPInfo struct {
+	op        Op
+	transArgs []int  // indexes of arguments which need transformation
+	gnuName   string // instruction name in GNU syntax
+	goName    string // instruction name in Plan 9 syntax
+}
+
+var fpInst []goFPInfo = []goFPInfo{
+	{VADD_F32, []int{2, 1, 0}, "VADD", "ADDF"},
+	{VADD_F64, []int{2, 1, 0}, "VADD", "ADDD"},
+	{VSUB_F32, []int{2, 1, 0}, "VSUB", "SUBF"},
+	{VSUB_F64, []int{2, 1, 0}, "VSUB", "SUBD"},
+	{VMUL_F32, []int{2, 1, 0}, "VMUL", "MULF"},
+	{VMUL_F64, []int{2, 1, 0}, "VMUL", "MULD"},
+	{VNMUL_F32, []int{2, 1, 0}, "VNMUL", "NMULF"},
+	{VNMUL_F64, []int{2, 1, 0}, "VNMUL", "NMULD"},
+	{VMLA_F32, []int{2, 1, 0}, "VMLA", "MULAF"},
+	{VMLA_F64, []int{2, 1, 0}, "VMLA", "MULAD"},
+	{VMLS_F32, []int{2, 1, 0}, "VMLS", "MULSF"},
+	{VMLS_F64, []int{2, 1, 0}, "VMLS", "MULSD"},
+	{VNMLA_F32, []int{2, 1, 0}, "VNMLA", "NMULAF"},
+	{VNMLA_F64, []int{2, 1, 0}, "VNMLA", "NMULAD"},
+	{VNMLS_F32, []int{2, 1, 0}, "VNMLS", "NMULSF"},
+	{VNMLS_F64, []int{2, 1, 0}, "VNMLS", "NMULSD"},
+	{VDIV_F32, []int{2, 1, 0}, "VDIV", "DIVF"},
+	{VDIV_F64, []int{2, 1, 0}, "VDIV", "DIVD"},
+	{VNEG_F32, []int{1, 0}, "VNEG", "NEGF"},
+	{VNEG_F64, []int{1, 0}, "VNEG", "NEGD"},
+	{VABS_F32, []int{1, 0}, "VABS", "ABSF"},
+	{VABS_F64, []int{1, 0}, "VABS", "ABSD"},
+	{VSQRT_F32, []int{1, 0}, "VSQRT", "SQRTF"},
+	{VSQRT_F64, []int{1, 0}, "VSQRT", "SQRTD"},
+	{VCMP_F32, []int{1, 0}, "VCMP", "CMPF"},
+	{VCMP_F64, []int{1, 0}, "VCMP", "CMPD"},
+	{VCMPE_F32, []int{1, 0}, "VCMPE", "CMPF"},
+	{VCMPE_F64, []int{1, 0}, "VCMPE", "CMPD"},
+	{VLDR, []int{1}, "VLDR", "MOV"},
+	{VSTR, []int{1}, "VSTR", "MOV"},
+	{VMOV_F32, []int{1, 0}, "VMOV", "MOVF"},
+	{VMOV_F64, []int{1, 0}, "VMOV", "MOVD"},
+	{VMOV_32, []int{1, 0}, "VMOV", "MOVW"},
+	{VMOV, []int{1, 0}, "VMOV", "MOVW"},
+	{VCVT_F64_F32, []int{1, 0}, "VCVT", "MOVFD"},
+	{VCVT_F32_F64, []int{1, 0}, "VCVT", "MOVDF"},
+	{VCVT_F32_U32, []int{1, 0}, "VCVT", "MOVWF.U"},
+	{VCVT_F32_S32, []int{1, 0}, "VCVT", "MOVWF"},
+	{VCVT_S32_F32, []int{1, 0}, "VCVT", "MOVFW"},
+	{VCVT_U32_F32, []int{1, 0}, "VCVT", "MOVFW.U"},
+	{VCVT_F64_U32, []int{1, 0}, "VCVT", "MOVWD.U"},
+	{VCVT_F64_S32, []int{1, 0}, "VCVT", "MOVWD"},
+	{VCVT_S32_F64, []int{1, 0}, "VCVT", "MOVDW"},
+	{VCVT_U32_F64, []int{1, 0}, "VCVT", "MOVDW.U"},
+}
+
+// convert FP instructions from GNU syntax to Plan 9 syntax, for example,
+// vadd.f32 s0, s3, s4 -> ADDF F0, S3, F2
+// vsub.f64 d0, d2, d4 -> SUBD F0, F2, F4
+// vldr s2, [r11] -> MOVF (R11), F1
+// inputs: instruction name and arguments in GNU syntax
+// return values: corresponding instruction name and arguments in Plan 9 syntax
+func fpTrans(inst *Inst, op string, args []string) (string, []string) {
+	for _, fp := range fpInst {
+		if inst.Op == fp.op {
+			// remove gnu syntax suffixes
+			op = strings.Replace(op, ".F32", "", -1)
+			op = strings.Replace(op, ".F64", "", -1)
+			op = strings.Replace(op, ".S32", "", -1)
+			op = strings.Replace(op, ".U32", "", -1)
+			op = strings.Replace(op, ".32", "", -1)
+			// compose op name
+			if fp.op == VLDR || fp.op == VSTR {
+				switch {
+				case strings.HasPrefix(args[fp.transArgs[0]], "D"):
+					op = "MOVD" + op[len(fp.gnuName):]
+				case strings.HasPrefix(args[fp.transArgs[0]], "S"):
+					op = "MOVF" + op[len(fp.gnuName):]
+				default:
+					panic(fmt.Sprintf("wrong FP register: %v", inst))
+				}
+			} else {
+				op = fp.goName + op[len(fp.gnuName):]
+			}
+			// transform registers
+			for ix, ri := range fp.transArgs {
+				switch {
+				case strings.HasSuffix(args[ri], "[1]"): // MOVW Rx, Dy[1]
+					break
+				case strings.HasSuffix(args[ri], "[0]"): // Dx[0] -> Fx
+					args[ri] = strings.Replace(args[ri], "[0]", "", -1)
+					fallthrough
+				case strings.HasPrefix(args[ri], "D"): // Dx -> Fx
+					args[ri] = "F" + args[ri][1:]
+				case strings.HasPrefix(args[ri], "S"):
+					if inst.Args[ix].(Reg)&1 == 0 { // Sx -> Fy, y = x/2, if x is even
+						args[ri] = fmt.Sprintf("F%d", (inst.Args[ix].(Reg)-S0)/2)
+					}
+				case strings.HasPrefix(args[ri], "$"): // CMPF/CMPD $0, Fx
+					break
+				case strings.HasPrefix(args[ri], "R"): // MOVW Rx, Dy[1]
+					break
+				default:
+					panic(fmt.Sprintf("wrong FP register: %v", inst))
+				}
+			}
+			break
+		}
+	}
+	return op, args
+}
diff --git a/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/tables.go b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/tables.go
new file mode 100644
index 0000000000..a703569b77
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/arch/thumb/thumbasm/tables.go
@@ -0,0 +1,554 @@
+package thumbasm
+
+//go:generate stringer -type Op
+
+const (
+	_ Op = iota
+	ADD
+	ADD_S
+	ADC
+	ADC_S
+	AND
+	AND_S
+	ASR
+	ASR_S
+	B
+	BCC
+	BCS
+	BEQ
+	BFC
+	BFI
+	BGE
+	BGT
+	BHI
+	BIC
+	BIC_S
+	BKPT
+	BL
+	BLE
+	BLS
+	BLT
+	BLX
+	BMI
+	BNE
+	BPL
+	BVC
+	BVS
+	BX
+	CBNZ
+	CBZ
+	CLREX
+	CLZ
+	CMN
+	CMP
+	CPSID
+	CPSIE
+	DMB
+	DSB
+	EOR
+	EOR_S
+	ISB
+	IT
+	ITE
+	ITEE
+	ITEEE
+	ITEET
+	ITET
+	ITETE
+	ITETT
+	ITT
+	ITTE
+	ITTEE
+	ITTET
+	ITTT
+	ITTTE
+	ITTTT
+	LDMIA
+	LDMDB
+	LDR
+	LDRB
+	LDRD
+	LDREX
+	LDREXB
+	LDREXH
+	LDRH
+	LDRSB
+	LDRSH
+	LSL
+	LSL_S
+	LSR
+	LSR_S
+	MLA
+	MLS
+	MOV
+	MOV_S
+	MOVT
+	MOVW
+	MRS
+	MSR
+	MVN
+	MVN_S
+	MUL
+	NOP
+	ORN
+	ORN_S
+	ORR
+	ORR_S
+	PLD
+	PLI
+	POP
+	PUSH
+	RBIT
+	REV
+	REV16
+	REVSH
+	ROR
+	ROR_S
+	RSB
+	RSB_S
+	SBC
+	SBC_S
+	SBFX
+	SDIV
+	SEL
+	SEV
+	SMLABB
+	SMLABT
+	SMLAD
+	SMLADX
+	SMLAL
+	SMLATB
+	SMLATT
+	SMLAWB
+	SMLAWT
+	SMLSD
+	SMLSDX
+	SMMLA
+	SMMLS
+	SMULL
+	STMIA
+	STMDB
+	STR
+	STRB
+	STRD
+	STREX
+	STREXB
+	STREXH
+	STRH
+	SUB
+	SUB_S
+	SVC
+	SXTB
+	SXTH
+	TBB
+	TBH
+	TST
+	TEQ
+	UBFX
+	UDF
+	UDIV
+	UMLAL
+	UMULL
+	UXTB
+	UXTH
+
+	VABS_F32
+	VABS_F64
+	VADD_F32
+	VADD_F64
+	VCMP_F32
+	VCMP_F64
+	VCMPE_F32
+	VCMPE_F64
+	VCVT_F32_FXS16
+	VCVT_F32_FXS32
+	VCVT_F32_FXU16
+	VCVT_F32_FXU32
+	VCVT_F64_FXS16
+	VCVT_F64_FXS32
+	VCVT_F64_FXU16
+	VCVT_F64_FXU32
+	VCVT_F32_U32
+	VCVT_F32_S32
+	VCVT_F64_U32
+	VCVT_F64_S32
+	VCVT_F64_F32
+	VCVT_F32_F64
+	VCVT_FXS16_F32
+	VCVT_FXS16_F64
+	VCVT_FXS32_F32
+	VCVT_FXS32_F64
+	VCVT_FXU16_F32
+	VCVT_FXU16_F64
+	VCVT_FXU32_F32
+	VCVT_FXU32_F64
+	VCVTB_F32_F16
+	VCVTB_F16_F32
+	VCVTT_F32_F16
+	VCVTT_F16_F32
+	VCVTR_U32_F32
+	VCVTR_U32_F64
+	VCVTR_S32_F32
+	VCVTR_S32_F64
+	VCVT_U32_F32
+	VCVT_U32_F64
+	VCVT_S32_F32
+	VCVT_S32_F64
+	VDIV_F32
+	VDIV_F64
+	VLDR
+	VMLA_F32
+	VMLA_F64
+	VMLS_F32
+	VMLS_F64
+	VMOV
+	VMOV_32
+	VMOV_F32
+	VMOV_F64
+	VMRS
+	VMSR
+	VMUL_F32
+	VMUL_F64
+	VNEG_F32
+	VNEG_F64
+	VNMLS_F32
+	VNMLS_F64
+	VNMLA_F32
+	VNMLA_F64
+	VNMUL_F32
+	VNMUL_F64
+	VSQRT_F32
+	VSQRT_F64
+	VSTR
+	VSUB_F32
+	VSUB_F64
+
+	WFE
+	WFI
+	YIELD
+)
+
+var inst16formats = [...]inst16format{
+	{0xFFFF, 0xBF00, NOP, nil},
+	{0xFFFF, 0xBF10, YIELD, nil},
+	{0xFFFF, 0xBF20, WFE, nil},
+	{0xFFFF, 0xBF30, WFI, nil},
+	{0xFFFF, 0xBF40, SEV, nil},
+
+	{0xFFFC, 0xB660, CPSIE, _CPSIE},
+	{0xFFFC, 0xB670, CPSID, _CPSIE},
+
+	{0xFF0F, 0xBF00 | 8, IT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 4, ITT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 4, ITE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 0 | 2, ITTT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 0 | 2, ITET, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 4 | 2, ITTE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 4 | 2, ITEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 0 | 0 | 1, ITTTT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 0 | 0 | 1, ITETT, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 4 | 0 | 1, ITTET, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 4 | 0 | 1, ITEET, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 0 | 2 | 1, ITTTE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 0 | 2 | 1, ITETE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 0 | 4 | 2 | 1, ITTEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF00 | 8 | 4 | 2 | 1, ITEEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 4, ITE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 4, ITT, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 0 | 2, ITEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 0 | 2, ITTE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 4 | 2, ITET, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 4 | 2, ITTT, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 0 | 0 | 1, ITEEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 0 | 0 | 1, ITTEE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 4 | 0 | 1, ITETE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 4 | 0 | 1, ITTTE, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 0 | 2 | 1, ITEET, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 0 | 2 | 1, ITTET, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 0 | 4 | 2 | 1, ITETT, _ITmask__firstcond},
+	{0xFF1F, 0xBF10 | 8 | 4 | 2 | 1, ITTTT, _ITmask__firstcond},
+
+	{0xFF87, 0x4700, BX, _B__Rm},
+	{0xFF87, 0x4780, BLX, _B__Rm},
+
+	{0xFFC0, 0x0000, MOV_S, _AND__Rm__Rdn},
+	{0xFFC0, 0x4000, AND, _AND__Rm__Rdn},
+	{0xFFC0, 0x4040, EOR, _AND__Rm__Rdn},
+	{0xFFC0, 0x4140, ADC, _AND__Rm__Rdn},
+	{0xFFC0, 0x4180, SBC, _AND__Rm__Rdn},
+	{0xFFC0, 0x4200, TST, _AND__Rm__Rdn},
+	{0xFFC0, 0x4240, RSB, _AND__Rm__Rdn},
+	{0xFFC0, 0x4280, CMP, _AND__Rm__Rdn},
+	{0xFFC0, 0x42C0, CMN, _AND__Rm__Rdn},
+	{0xFFC0, 0x4300, ORR, _AND__Rm__Rdn},
+	{0xFFC0, 0x4340, MUL, _AND__Rm__Rdn},
+	{0xFFC0, 0x4380, BIC, _AND__Rm__Rdn},
+	{0xFFC0, 0x43C0, MVN, _AND__Rm__Rdn},
+	{0xFFC0, 0x4080, LSL, _AND__Rm__Rdn},
+	{0xFFC0, 0x40C0, LSR, _AND__Rm__Rdn},
+	{0xFFC0, 0x4100, ASR, _AND__Rm__Rdn},
+	{0xFFC0, 0x41C0, ROR, _AND__Rm__Rdn},
+	{0xFFC0, 0xBA00, REV, _AND__Rm__Rdn},
+	{0xFFC0, 0xBA40, REV16, _AND__Rm__Rdn},
+	{0xFFC0, 0xBAC0, REVSH, _AND__Rm__Rdn},
+	{0xFFC0, 0xB200, SXTH, _AND__Rm__Rdn},
+	{0xFFC0, 0xB240, SXTB, _AND__Rm__Rdn},
+	{0xFFC0, 0xB280, UXTH, _AND__Rm__Rdn},
+	{0xFFC0, 0xB2C0, UXTB, _AND__Rm__Rdn},
+
+	{0xFF80, 0xB000, ADD, _ADD__u7_2__R13},
+	{0xFF80, 0xB080, SUB, _ADD__u7_2__R13},
+
+	{0xFF00, 0x4400, ADD, _ADD__Rm__Rdn},
+	{0xFF00, 0x4600, MOV, _ADD__Rm__Rdn},
+	{0xFF00, 0x4500, CMP, _ADD__Rm__Rdn},
+
+	{0xFF00, 0xD000, BEQ, _Bcond__i8_1},
+	{0xFF00, 0xD100, BNE, _Bcond__i8_1},
+	{0xFF00, 0xD200, BCS, _Bcond__i8_1},
+	{0xFF00, 0xD300, BCC, _Bcond__i8_1},
+	{0xFF00, 0xD400, BMI, _Bcond__i8_1},
+	{0xFF00, 0xD500, BPL, _Bcond__i8_1},
+	{0xFF00, 0xD600, BVS, _Bcond__i8_1},
+	{0xFF00, 0xD700, BVC, _Bcond__i8_1},
+	{0xFF00, 0xD800, BHI, _Bcond__i8_1},
+	{0xFF00, 0xD900, BLS, _Bcond__i8_1},
+	{0xFF00, 0xDA00, BGE, _Bcond__i8_1},
+	{0xFF00, 0xDB00, BLT, _Bcond__i8_1},
+	{0xFF00, 0xDC00, BGT, _Bcond__i8_1},
+	{0xFF00, 0xDD00, BLE, _Bcond__i8_1},
+
+	{0xFF00, 0xDE00, UDF, _UDF__u8},
+	{0xFF00, 0xDF00, SVC, _UDF__u8},
+	{0xFF00, 0xBE00, BKPT, _UDF__u8},
+
+	{0xFE00, 0x1800, ADD, _ADD__Rm__Rn__Rd},
+	{0xFE00, 0x1A00, SUB, _ADD__Rm__Rn__Rd},
+
+	{0xFE00, 0xB400, PUSH, _PUSH__reglist},
+	{0xFE00, 0xBC00, POP, _PUSH__reglist},
+
+	{0xFE00, 0x1C00, ADD, _ADD__u3__Rn__Rd},
+	{0xFE00, 0x1E00, SUB, _ADD__u3__Rn__Rd},
+
+	{0xFD00, 0xB100, CBZ, _CBZ__Rn__u6_1},
+	{0xFD00, 0xB900, CBNZ, _CBZ__Rn__u6_1},
+
+	{0xF800, 0x0000, LSL, _MOVW__Rm_v_u5__Rd},
+	{0xF800, 0x0800, LSR, _MOVW__Rm_v_u5__Rd},
+	{0xF800, 0x1000, ASR, _MOVW__Rm_v_u5__Rd},
+
+	{0xF800, 0x2000, MOV, _MOVW__u8__Rd},
+	{0xF800, 0x2800, CMP, _MOVW__u8__Rd},
+	{0xF800, 0x3000, ADD, _MOVW__u8__Rd},
+	{0xF800, 0x3800, SUB, _MOVW__u8__Rd},
+
+	{0xF800, 0x4800, LDR, _MOVW__u8_2_R13__Rt},
+	{0xF800, 0x9000, STR, _MOVW__u8_2_R13__Rt},
+	{0xF800, 0x9800, LDR, _MOVW__u8_2_R13__Rt},
+
+	{0xF800, 0x6000, STR, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x7000, STRB, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x8000, STRH, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x6800, LDR, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x7800, LDRB, _MOVW__u5_2_Rn__Rt},
+	{0xF800, 0x8800, LDRH, _MOVW__u5_2_Rn__Rt},
+
+	{0xF800, 0xE000, B, _B__i11_1},
+
+	{0xF800, 0xC000, STMIA, _MOVM_IAW},
+	{0xF800, 0xC800, LDMIA, _MOVM_IAW},
+
+	{0xF000, 0xA000, ADD, _ADD__u8_2__R13__Rd},
+
+	{0xFE00, 0x5800, LDR, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5E00, LDRSH, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5A00, LDRH, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5600, LDRSB, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5C00, LDRB, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5000, STR, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5200, STRH, _MOVW__Rn_Rm__Rt},
+	{0xFE00, 0x5400, STRB, _MOVW__Rn_Rm__Rt},
+}
+
+var inst32formats = [...]inst32format{
+	{0xFFFFFFFF, 0xF3AF8000, NOP, nil},
+	{0xFFFFFFFF, 0xF3BF8F2F, CLREX, nil},
+
+	{0xFFFF0FFF, 0xF84D0D04, PUSH, _PUSH__Rt},
+	{0xFFFF0FFF, 0xF85D0B04, POP, _PUSH__Rt},
+
+	{0xFFFFFFF0, 0xF3BF8F40, DSB, _DSB__opt},
+	{0xFFFFFFF0, 0xF3BF8F50, DMB, _DSB__opt},
+	{0xFFFFFFF0, 0xF3BF8F60, ISB, _DSB__opt},
+
+	{0xFFF0FFF0, 0xE8D0F000, TBB, _TBB__Rm__Rn},
+	{0xFFF0FFF0, 0xE8D0F010, TBH, _TBB__Rm__Rn},
+
+	{0xFFF0F0F0, 0xFB00F000, MUL, _MUL__Rm__Rn__Rd},
+	{0xFFF0F0F0, 0xFB90F0F0, SDIV, _MUL__Rm__Rn__Rd},
+	{0xFFF0F0F0, 0xFBB0F0F0, UDIV, _MUL__Rm__Rn__Rd},
+	{0xFFF0F0F0, 0xFAA0F080, SEL, _MUL__Rm__Rn__Rd},
+
+	{0xFFF0F0F0, 0xFAB0F080, CLZ, _CLZ__Rm__Rd},
+	{0xFFF0F0F0, 0xFA90F080, REV, _CLZ__Rm__Rd},
+	{0xFFF0F0F0, 0xFA90F090, REV16, _CLZ__Rm__Rd},
+	{0xFFF0F0F0, 0xFA90F0A0, RBIT, _CLZ__Rm__Rd},
+	{0xFFF0F0F0, 0xFA90F0B0, REVSH, _CLZ__Rm__Rd},
+
+	{0xFFF0F0F0, 0xFA00F000, LSL, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA10F000, LSL_S, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA20F000, LSR, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA30F000, LSR_S, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA40F000, ASR, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA50F000, ASR_S, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA60F000, ROR, _MOVWs__Rn_v_Rm__Rd},
+	{0xFFF0F0F0, 0xFA70F000, ROR_S, _MOVWs__Rn_v_Rm__Rd},
+
+	{0xFFF00FFF, 0xE8D00F4F, LDREXB, _LDREXB__Rn__Rt},
+	{0xFFF00FFF, 0xE8D00F5F, LDREXB, _LDREXB__Rn__Rt},
+
+	{0xFFF00FF0, 0xE8C00F40, STREXB, _STREXB__Rn__Rt},
+	{0xFFF00FF0, 0xE8C00F50, STREXH, _STREXB__Rn__Rt},
+
+	{0xFFFFF0C0, 0xFA0FF080, SXTH, _MOVH__Rm_rot__Rd},
+	{0xFFFFF0C0, 0xFA4FF080, SXTB, _MOVH__Rm_rot__Rd},
+	{0xFFFFF0C0, 0xFA1FF080, UXTH, _MOVH__Rm_rot__Rd},
+	{0xFFFFF0C0, 0xFA5FF080, UXTB, _MOVH__Rm_rot__Rd},
+
+	{0xFFFFF000, 0xF3EF8000, MRS, _MOVW__SYSm__Rd},
+	{0xFFF0FF00, 0xF3808800, MSR, _MOVW__SYSm__Rd},
+
+	{0xFFF08F00, 0xEA100F00, TST, _TST__Rm_v_u5__Rn},
+	{0xFFF08F00, 0xEA900F00, TEQ, _TST__Rm_v_u5__Rn},
+	{0xFFF08F00, 0xEB100F00, CMN, _TST__Rm_v_u5__Rn},
+	{0xFFF08F00, 0xEBB00F00, CMP, _TST__Rm_v_u5__Rn},
+
+	{0xFFFF8000, 0xEA4F0000, MOV, _MOVWs__Rm_v_u5__Rn},
+	{0xFFFF8000, 0xEA5F0000, MOV_S, _MOVWs__Rm_v_u5__Rn},
+	{0xFFFF8000, 0xEA6F0000, MVN, _MOVWs__Rm_v_u5__Rn},
+	{0xFFFF8000, 0xEA7F0000, MVN_S, _MOVWs__Rm_v_u5__Rn},
+
+	{0xFFF000F0, 0xFB800000, SMULL, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFBA00000, UMULL, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFBC00000, SMLAL, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFBE00000, UMLAL, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFB000000, MLA, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFB000010, MLS, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFB300000, SMLAWB, _MULL__Rm__Rn__Rdh_Rdl},
+	{0xFFF000F0, 0xFB300010, SMLAWT, _MULL__Rm__Rn__Rdh_Rdl},
+
+	{0xFF7F0000, 0xF81F0000, LDRB, _MOVW__s12_Rn__Rt},
+	{0xFF7F0000, 0xF83F0000, LDRH, _MOVW__s12_Rn__Rt},
+	{0xFF7F0000, 0xF85F0000, LDR, _MOVW__s12_Rn__Rt},
+	{0xFF7F0000, 0xF91F0000, LDRSB, _MOVW__s12_Rn__Rt},
+	{0xFF7F0000, 0xF93F0000, LDRSH, _MOVW__s12_Rn__Rt},
+
+	{0xFFF00FC0, 0xF8500000, LDR, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF9300000, LDRSH, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8300000, LDRH, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF9100000, LDRSB, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8100000, LDRB, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8400000, STR, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8200000, STRH, _MOVW__Rn_Rm_1_u2__Rt},
+	{0xFFF00FC0, 0xF8000000, STRB, _MOVW__Rn_Rm_1_u2__Rt},
+
+	{0xFBFF8000, 0xF04F0000, MOV, _MOVWs__e32__Rd},
+	{0xFBFF8000, 0xF05F0000, MOV_S, _MOVWs__e32__Rd},
+	{0xFBFF8000, 0xF06F0000, MVN, _MOVWs__e32__Rd},
+	{0xFBFF8000, 0xF07F0000, MVN_S, _MOVWs__e32__Rd},
+
+	{0xFFF00800, 0xF8000800, STRB, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8100800, LDRB, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8200800, STRH, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8300800, LDRH, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8400800, STR, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF8500800, LDR, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF9100800, LDRSB, _MOVWpw__s8_Rn__Rt},
+	{0xFFF00800, 0xF9300800, LDRSH, _MOVWpw__s8_Rn__Rt},
+
+	{0xFFF08000, 0xEA000000, AND, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA100000, AND_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA200000, BIC, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA300000, BIC_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA400000, ORR, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA500000, ORR_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA600000, ORN, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA700000, ORN_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA800000, EOR, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEA900000, EOR_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB000000, ADD, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB100000, ADD_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB400000, ADC, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB500000, ADC_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB600000, SBC, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEB700000, SBC_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEBA00000, SUB, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEBB00000, SUB_S, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEBC00000, RSB, _ANDs__Rm_v_u5__Rn__Rd},
+	{0xFFF08000, 0xEBD00000, RSB_S, _ANDs__Rm_v_u5__Rn__Rd},
+
+	{0xFFF00000, 0xF8900000, LDRB, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8800000, STRB, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8B00000, LDRH, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8A00000, STRH, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8D00000, LDR, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF8C00000, STR, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF9900000, LDRSB, _MOVW__s12_Rn__Rt},
+	{0xFFF00000, 0xF9B00000, LDRSH, _MOVW__s12_Rn__Rt},
+
+	{0xFFD02000, 0xE8900000, LDMIA, _MOVM_IAw},
+	{0xFFD02000, 0xE9100000, LDMDB, _MOVM_IAw},
+	{0xFFD02000, 0xE8800000, STMIA, _MOVM_IAw},
+	{0xFFD02000, 0xE9000000, STMDB, _MOVM_IAw},
+
+	{0xFBF08F00, 0xF0100F00, TST, _TST__e32__Rn},
+	{0xFBF08F00, 0xF0900F00, TEQ, _TST__e32__Rn},
+	{0xFBF08F00, 0xF1100F00, CMN, _TST__e32__Rn},
+	{0xFBF08F00, 0xF1B00F00, CMP, _TST__e32__Rn},
+
+	{0xFBF08000, 0xF0000000, AND, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0100000, AND_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0200000, BIC, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0300000, BIC_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0400000, ORR, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0500000, ORR_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0600000, ORN, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0700000, ORN_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0800000, EOR, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF0900000, EOR_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1000000, ADD, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1100000, ADD_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1400000, ADC, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1500000, ADC_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1600000, SBC, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1700000, SBC_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1A00000, SUB, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1B00000, SUB_S, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1C00000, RSB, _ANDs__e32__Rn__Rd},
+	{0xFBF08000, 0xF1D00000, RSB_S, _ANDs__e32__Rn__Rd},
+
+	{0xFBF08000, 0xF2400000, MOVW, _MOVW__uyz16__Rd},
+	{0xFBF08000, 0xF2C00000, MOVT, _MOVW__uyz16__Rd},
+
+	{0xFBF08000, 0xF2000000, ADD, _ADD__u12__Rn__Rd},
+	{0xFBF08000, 0xF2A00000, SUB, _ADD__u12__Rn__Rd},
+
+	{0xF800D000, 0xF0009000, B, _B__ji24_1},
+	{0xF800D000, 0xF000D000, BL, _B__ji24_1},
+
+	{0xFFF00F00, 0xE8500F00, LDREX, _LDREX__u8_2_Rn__Rt},
+	{0xFFF00000, 0xE8400000, STREX, _STREX__Rt__u8_2_Rn__Rd},
+
+	{0xFFF08020, 0xF3400000, SBFX, _BFX__width__ulsb__Rn__Rd},
+	{0xFFF08020, 0xF3C00000, UBFX, _BFX__width__ulsb__Rn__Rd},
+
+	{0xFFFF8020, 0xF36F0000, BFC, _BFC__width__ulsb__Rd},
+	{0xFFF08020, 0xF3600000, BFI, _BFC__width__ulsb__Rd},
+
+	{0xFF300E00, 0xED100A00, VLDR, _MOVF__s8_2_Rn__Fd},
+	{0xFF300E00, 0xED000A00, VSTR, _MOVF__s8_2_Rn__Fd},
+
+	{0xFFBF0FD0, 0xEEB10AC0, VSQRT_F32, _SQRTF__Fm__Fd},
+	{0xFFBF0FD0, 0xEEB10BC0, VSQRT_F64, _SQRTF__Fm__Fd},
+}
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/asm_linux_thumb.s b/src/cmd/vendor/golang.org/x/sys/unix/asm_linux_thumb.s
new file mode 100644
index 0000000000..cf0f3575c1
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/asm_linux_thumb.s
@@ -0,0 +1,56 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !gccgo
+
+#include "textflag.h"
+
+//
+// System calls for arm, Linux
+//
+
+// Just jump to package syscall's implementation for all these functions.
+// The runtime may know about them.
+
+TEXT Syscall(SB),NOSPLIT,$0-28
+	B	syscallSyscall(SB)
+
+TEXT Syscall6(SB),NOSPLIT,$0-40
+	B	syscallSyscall6(SB)
+
+TEXT SyscallNoError(SB),NOSPLIT,$0-24
+	BL	runtimeentersyscall(SB)
+	MOVW	trap+0(FP), R7
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	MOVW	$0, R3
+	MOVW	$0, R4
+	MOVW	$0, R5
+	SWI	$0
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	BL	runtimeexitsyscall(SB)
+	RET
+
+TEXT RawSyscall(SB),NOSPLIT,$0-28
+	B	syscallRawSyscall(SB)
+
+TEXT RawSyscall6(SB),NOSPLIT,$0-40
+	B	syscallRawSyscall6(SB)
+
+TEXT RawSyscallNoError(SB),NOSPLIT,$0-24
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	SWI	$0
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	RET
+
+TEXT seek(SB),NOSPLIT,$0-28
+	B	syscallseek(SB)
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/fcntl_linux_32bit.go b/src/cmd/vendor/golang.org/x/sys/unix/fcntl_linux_32bit.go
index fc0e50e037..94c5a61c25 100644
--- a/src/cmd/vendor/golang.org/x/sys/unix/fcntl_linux_32bit.go
+++ b/src/cmd/vendor/golang.org/x/sys/unix/fcntl_linux_32bit.go
@@ -1,4 +1,4 @@
-// +build linux,386 linux,arm linux,mips linux,mipsle
+// +build linux,386 linux,arm linux,thumb linux,mips linux,mipsle
 
 // Copyright 2014 The Go Authors. All rights reserved.
 // Use of this source code is governed by a BSD-style
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_arm.go b/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_armt.go
similarity index 99%
rename from src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_arm.go
rename to src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_armt.go
index 3a3c37b4c8..af1f628ef2 100644
--- a/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_arm.go
+++ b/src/cmd/vendor/golang.org/x/sys/unix/syscall_linux_armt.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build arm,linux
+// +build arm,linux thumb,linux
 
 package unix
 
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/zerrors_linux_thumb.go b/src/cmd/vendor/golang.org/x/sys/unix/zerrors_linux_thumb.go
new file mode 100644
index 0000000000..3622e38699
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/zerrors_linux_thumb.go
@@ -0,0 +1,2845 @@
+// mkerrors.sh -Wall -Werror -static -I/tmp/include
+// Code generated by the command above; see README.md. DO NOT EDIT.
+
+// +build thumb,linux
+
+// Code generated by cmd/cgo -godefs; DO NOT EDIT.
+// cgo -godefs -- -Wall -Werror -static -I/tmp/include _const.go
+
+package unix
+
+import "syscall"
+
+const (
+	AAFS_MAGIC                           = 0x5a3c69f0
+	ADFS_SUPER_MAGIC                     = 0xadf5
+	AFFS_SUPER_MAGIC                     = 0xadff
+	AFS_FS_MAGIC                         = 0x6b414653
+	AFS_SUPER_MAGIC                      = 0x5346414f
+	AF_ALG                               = 0x26
+	AF_APPLETALK                         = 0x5
+	AF_ASH                               = 0x12
+	AF_ATMPVC                            = 0x8
+	AF_ATMSVC                            = 0x14
+	AF_AX25                              = 0x3
+	AF_BLUETOOTH                         = 0x1f
+	AF_BRIDGE                            = 0x7
+	AF_CAIF                              = 0x25
+	AF_CAN                               = 0x1d
+	AF_DECnet                            = 0xc
+	AF_ECONET                            = 0x13
+	AF_FILE                              = 0x1
+	AF_IB                                = 0x1b
+	AF_IEEE802154                        = 0x24
+	AF_INET                              = 0x2
+	AF_INET6                             = 0xa
+	AF_IPX                               = 0x4
+	AF_IRDA                              = 0x17
+	AF_ISDN                              = 0x22
+	AF_IUCV                              = 0x20
+	AF_KCM                               = 0x29
+	AF_KEY                               = 0xf
+	AF_LLC                               = 0x1a
+	AF_LOCAL                             = 0x1
+	AF_MAX                               = 0x2d
+	AF_MPLS                              = 0x1c
+	AF_NETBEUI                           = 0xd
+	AF_NETLINK                           = 0x10
+	AF_NETROM                            = 0x6
+	AF_NFC                               = 0x27
+	AF_PACKET                            = 0x11
+	AF_PHONET                            = 0x23
+	AF_PPPOX                             = 0x18
+	AF_QIPCRTR                           = 0x2a
+	AF_RDS                               = 0x15
+	AF_ROSE                              = 0xb
+	AF_ROUTE                             = 0x10
+	AF_RXRPC                             = 0x21
+	AF_SECURITY                          = 0xe
+	AF_SMC                               = 0x2b
+	AF_SNA                               = 0x16
+	AF_TIPC                              = 0x1e
+	AF_UNIX                              = 0x1
+	AF_UNSPEC                            = 0x0
+	AF_VSOCK                             = 0x28
+	AF_WANPIPE                           = 0x19
+	AF_X25                               = 0x9
+	AF_XDP                               = 0x2c
+	ALG_OP_DECRYPT                       = 0x0
+	ALG_OP_ENCRYPT                       = 0x1
+	ALG_SET_AEAD_ASSOCLEN                = 0x4
+	ALG_SET_AEAD_AUTHSIZE                = 0x5
+	ALG_SET_IV                           = 0x2
+	ALG_SET_KEY                          = 0x1
+	ALG_SET_OP                           = 0x3
+	ANON_INODE_FS_MAGIC                  = 0x9041934
+	ARPHRD_6LOWPAN                       = 0x339
+	ARPHRD_ADAPT                         = 0x108
+	ARPHRD_APPLETLK                      = 0x8
+	ARPHRD_ARCNET                        = 0x7
+	ARPHRD_ASH                           = 0x30d
+	ARPHRD_ATM                           = 0x13
+	ARPHRD_AX25                          = 0x3
+	ARPHRD_BIF                           = 0x307
+	ARPHRD_CAIF                          = 0x336
+	ARPHRD_CAN                           = 0x118
+	ARPHRD_CHAOS                         = 0x5
+	ARPHRD_CISCO                         = 0x201
+	ARPHRD_CSLIP                         = 0x101
+	ARPHRD_CSLIP6                        = 0x103
+	ARPHRD_DDCMP                         = 0x205
+	ARPHRD_DLCI                          = 0xf
+	ARPHRD_ECONET                        = 0x30e
+	ARPHRD_EETHER                        = 0x2
+	ARPHRD_ETHER                         = 0x1
+	ARPHRD_EUI64                         = 0x1b
+	ARPHRD_FCAL                          = 0x311
+	ARPHRD_FCFABRIC                      = 0x313
+	ARPHRD_FCPL                          = 0x312
+	ARPHRD_FCPP                          = 0x310
+	ARPHRD_FDDI                          = 0x306
+	ARPHRD_FRAD                          = 0x302
+	ARPHRD_HDLC                          = 0x201
+	ARPHRD_HIPPI                         = 0x30c
+	ARPHRD_HWX25                         = 0x110
+	ARPHRD_IEEE1394                      = 0x18
+	ARPHRD_IEEE802                       = 0x6
+	ARPHRD_IEEE80211                     = 0x321
+	ARPHRD_IEEE80211_PRISM               = 0x322
+	ARPHRD_IEEE80211_RADIOTAP            = 0x323
+	ARPHRD_IEEE802154                    = 0x324
+	ARPHRD_IEEE802154_MONITOR            = 0x325
+	ARPHRD_IEEE802_TR                    = 0x320
+	ARPHRD_INFINIBAND                    = 0x20
+	ARPHRD_IP6GRE                        = 0x337
+	ARPHRD_IPDDP                         = 0x309
+	ARPHRD_IPGRE                         = 0x30a
+	ARPHRD_IRDA                          = 0x30f
+	ARPHRD_LAPB                          = 0x204
+	ARPHRD_LOCALTLK                      = 0x305
+	ARPHRD_LOOPBACK                      = 0x304
+	ARPHRD_METRICOM                      = 0x17
+	ARPHRD_NETLINK                       = 0x338
+	ARPHRD_NETROM                        = 0x0
+	ARPHRD_NONE                          = 0xfffe
+	ARPHRD_PHONET                        = 0x334
+	ARPHRD_PHONET_PIPE                   = 0x335
+	ARPHRD_PIMREG                        = 0x30b
+	ARPHRD_PPP                           = 0x200
+	ARPHRD_PRONET                        = 0x4
+	ARPHRD_RAWHDLC                       = 0x206
+	ARPHRD_RAWIP                         = 0x207
+	ARPHRD_ROSE                          = 0x10e
+	ARPHRD_RSRVD                         = 0x104
+	ARPHRD_SIT                           = 0x308
+	ARPHRD_SKIP                          = 0x303
+	ARPHRD_SLIP                          = 0x100
+	ARPHRD_SLIP6                         = 0x102
+	ARPHRD_TUNNEL                        = 0x300
+	ARPHRD_TUNNEL6                       = 0x301
+	ARPHRD_VOID                          = 0xffff
+	ARPHRD_VSOCKMON                      = 0x33a
+	ARPHRD_X25                           = 0x10f
+	AUTOFS_SUPER_MAGIC                   = 0x187
+	B0                                   = 0x0
+	B1000000                             = 0x1008
+	B110                                 = 0x3
+	B115200                              = 0x1002
+	B1152000                             = 0x1009
+	B1200                                = 0x9
+	B134                                 = 0x4
+	B150                                 = 0x5
+	B1500000                             = 0x100a
+	B1800                                = 0xa
+	B19200                               = 0xe
+	B200                                 = 0x6
+	B2000000                             = 0x100b
+	B230400                              = 0x1003
+	B2400                                = 0xb
+	B2500000                             = 0x100c
+	B300                                 = 0x7
+	B3000000                             = 0x100d
+	B3500000                             = 0x100e
+	B38400                               = 0xf
+	B4000000                             = 0x100f
+	B460800                              = 0x1004
+	B4800                                = 0xc
+	B50                                  = 0x1
+	B500000                              = 0x1005
+	B57600                               = 0x1001
+	B576000                              = 0x1006
+	B600                                 = 0x8
+	B75                                  = 0x2
+	B921600                              = 0x1007
+	B9600                                = 0xd
+	BALLOON_KVM_MAGIC                    = 0x13661366
+	BDEVFS_MAGIC                         = 0x62646576
+	BINDERFS_SUPER_MAGIC                 = 0x6c6f6f70
+	BINFMTFS_MAGIC                       = 0x42494e4d
+	BLKBSZGET                            = 0x80041270
+	BLKBSZSET                            = 0x40041271
+	BLKFLSBUF                            = 0x1261
+	BLKFRAGET                            = 0x1265
+	BLKFRASET                            = 0x1264
+	BLKGETSIZE                           = 0x1260
+	BLKGETSIZE64                         = 0x80041272
+	BLKPBSZGET                           = 0x127b
+	BLKRAGET                             = 0x1263
+	BLKRASET                             = 0x1262
+	BLKROGET                             = 0x125e
+	BLKROSET                             = 0x125d
+	BLKRRPART                            = 0x125f
+	BLKSECTGET                           = 0x1267
+	BLKSECTSET                           = 0x1266
+	BLKSSZGET                            = 0x1268
+	BOTHER                               = 0x1000
+	BPF_A                                = 0x10
+	BPF_ABS                              = 0x20
+	BPF_ADD                              = 0x0
+	BPF_ALU                              = 0x4
+	BPF_AND                              = 0x50
+	BPF_B                                = 0x10
+	BPF_DIV                              = 0x30
+	BPF_FS_MAGIC                         = 0xcafe4a11
+	BPF_H                                = 0x8
+	BPF_IMM                              = 0x0
+	BPF_IND                              = 0x40
+	BPF_JA                               = 0x0
+	BPF_JEQ                              = 0x10
+	BPF_JGE                              = 0x30
+	BPF_JGT                              = 0x20
+	BPF_JMP                              = 0x5
+	BPF_JSET                             = 0x40
+	BPF_K                                = 0x0
+	BPF_LD                               = 0x0
+	BPF_LDX                              = 0x1
+	BPF_LEN                              = 0x80
+	BPF_LL_OFF                           = -0x200000
+	BPF_LSH                              = 0x60
+	BPF_MAJOR_VERSION                    = 0x1
+	BPF_MAXINSNS                         = 0x1000
+	BPF_MEM                              = 0x60
+	BPF_MEMWORDS                         = 0x10
+	BPF_MINOR_VERSION                    = 0x1
+	BPF_MISC                             = 0x7
+	BPF_MOD                              = 0x90
+	BPF_MSH                              = 0xa0
+	BPF_MUL                              = 0x20
+	BPF_NEG                              = 0x80
+	BPF_NET_OFF                          = -0x100000
+	BPF_OR                               = 0x40
+	BPF_RET                              = 0x6
+	BPF_RSH                              = 0x70
+	BPF_ST                               = 0x2
+	BPF_STX                              = 0x3
+	BPF_SUB                              = 0x10
+	BPF_TAX                              = 0x0
+	BPF_TXA                              = 0x80
+	BPF_W                                = 0x0
+	BPF_X                                = 0x8
+	BPF_XOR                              = 0xa0
+	BRKINT                               = 0x2
+	BS0                                  = 0x0
+	BS1                                  = 0x2000
+	BSDLY                                = 0x2000
+	BTRFS_SUPER_MAGIC                    = 0x9123683e
+	BTRFS_TEST_MAGIC                     = 0x73727279
+	CAN_BCM                              = 0x2
+	CAN_EFF_FLAG                         = 0x80000000
+	CAN_EFF_ID_BITS                      = 0x1d
+	CAN_EFF_MASK                         = 0x1fffffff
+	CAN_ERR_FLAG                         = 0x20000000
+	CAN_ERR_MASK                         = 0x1fffffff
+	CAN_INV_FILTER                       = 0x20000000
+	CAN_ISOTP                            = 0x6
+	CAN_MAX_DLC                          = 0x8
+	CAN_MAX_DLEN                         = 0x8
+	CAN_MCNET                            = 0x5
+	CAN_MTU                              = 0x10
+	CAN_NPROTO                           = 0x7
+	CAN_RAW                              = 0x1
+	CAN_RAW_FILTER_MAX                   = 0x200
+	CAN_RTR_FLAG                         = 0x40000000
+	CAN_SFF_ID_BITS                      = 0xb
+	CAN_SFF_MASK                         = 0x7ff
+	CAN_TP16                             = 0x3
+	CAN_TP20                             = 0x4
+	CBAUD                                = 0x100f
+	CBAUDEX                              = 0x1000
+	CFLUSH                               = 0xf
+	CGROUP2_SUPER_MAGIC                  = 0x63677270
+	CGROUP_SUPER_MAGIC                   = 0x27e0eb
+	CIBAUD                               = 0x100f0000
+	CLOCAL                               = 0x800
+	CLOCK_BOOTTIME                       = 0x7
+	CLOCK_BOOTTIME_ALARM                 = 0x9
+	CLOCK_DEFAULT                        = 0x0
+	CLOCK_EXT                            = 0x1
+	CLOCK_INT                            = 0x2
+	CLOCK_MONOTONIC                      = 0x1
+	CLOCK_MONOTONIC_COARSE               = 0x6
+	CLOCK_MONOTONIC_RAW                  = 0x4
+	CLOCK_PROCESS_CPUTIME_ID             = 0x2
+	CLOCK_REALTIME                       = 0x0
+	CLOCK_REALTIME_ALARM                 = 0x8
+	CLOCK_REALTIME_COARSE                = 0x5
+	CLOCK_TAI                            = 0xb
+	CLOCK_THREAD_CPUTIME_ID              = 0x3
+	CLOCK_TXFROMRX                       = 0x4
+	CLOCK_TXINT                          = 0x3
+	CLONE_CHILD_CLEARTID                 = 0x200000
+	CLONE_CHILD_SETTID                   = 0x1000000
+	CLONE_DETACHED                       = 0x400000
+	CLONE_FILES                          = 0x400
+	CLONE_FS                             = 0x200
+	CLONE_IO                             = 0x80000000
+	CLONE_NEWCGROUP                      = 0x2000000
+	CLONE_NEWIPC                         = 0x8000000
+	CLONE_NEWNET                         = 0x40000000
+	CLONE_NEWNS                          = 0x20000
+	CLONE_NEWPID                         = 0x20000000
+	CLONE_NEWUSER                        = 0x10000000
+	CLONE_NEWUTS                         = 0x4000000
+	CLONE_PARENT                         = 0x8000
+	CLONE_PARENT_SETTID                  = 0x100000
+	CLONE_PTRACE                         = 0x2000
+	CLONE_SETTLS                         = 0x80000
+	CLONE_SIGHAND                        = 0x800
+	CLONE_SYSVSEM                        = 0x40000
+	CLONE_THREAD                         = 0x10000
+	CLONE_UNTRACED                       = 0x800000
+	CLONE_VFORK                          = 0x4000
+	CLONE_VM                             = 0x100
+	CMSPAR                               = 0x40000000
+	CODA_SUPER_MAGIC                     = 0x73757245
+	CR0                                  = 0x0
+	CR1                                  = 0x200
+	CR2                                  = 0x400
+	CR3                                  = 0x600
+	CRAMFS_MAGIC                         = 0x28cd3d45
+	CRDLY                                = 0x600
+	CREAD                                = 0x80
+	CRTSCTS                              = 0x80000000
+	CRYPTO_MAX_NAME                      = 0x40
+	CRYPTO_MSG_MAX                       = 0x15
+	CRYPTO_NR_MSGTYPES                   = 0x6
+	CRYPTO_REPORT_MAXSIZE                = 0x160
+	CS5                                  = 0x0
+	CS6                                  = 0x10
+	CS7                                  = 0x20
+	CS8                                  = 0x30
+	CSIGNAL                              = 0xff
+	CSIZE                                = 0x30
+	CSTART                               = 0x11
+	CSTATUS                              = 0x0
+	CSTOP                                = 0x13
+	CSTOPB                               = 0x40
+	CSUSP                                = 0x1a
+	DAXFS_MAGIC                          = 0x64646178
+	DEBUGFS_MAGIC                        = 0x64626720
+	DEVPTS_SUPER_MAGIC                   = 0x1cd1
+	DT_BLK                               = 0x6
+	DT_CHR                               = 0x2
+	DT_DIR                               = 0x4
+	DT_FIFO                              = 0x1
+	DT_LNK                               = 0xa
+	DT_REG                               = 0x8
+	DT_SOCK                              = 0xc
+	DT_UNKNOWN                           = 0x0
+	DT_WHT                               = 0xe
+	ECHO                                 = 0x8
+	ECHOCTL                              = 0x200
+	ECHOE                                = 0x10
+	ECHOK                                = 0x20
+	ECHOKE                               = 0x800
+	ECHONL                               = 0x40
+	ECHOPRT                              = 0x400
+	ECRYPTFS_SUPER_MAGIC                 = 0xf15f
+	EFD_CLOEXEC                          = 0x80000
+	EFD_NONBLOCK                         = 0x800
+	EFD_SEMAPHORE                        = 0x1
+	EFIVARFS_MAGIC                       = 0xde5e81e4
+	EFS_SUPER_MAGIC                      = 0x414a53
+	ENCODING_DEFAULT                     = 0x0
+	ENCODING_FM_MARK                     = 0x3
+	ENCODING_FM_SPACE                    = 0x4
+	ENCODING_MANCHESTER                  = 0x5
+	ENCODING_NRZ                         = 0x1
+	ENCODING_NRZI                        = 0x2
+	EPOLLERR                             = 0x8
+	EPOLLET                              = 0x80000000
+	EPOLLEXCLUSIVE                       = 0x10000000
+	EPOLLHUP                             = 0x10
+	EPOLLIN                              = 0x1
+	EPOLLMSG                             = 0x400
+	EPOLLONESHOT                         = 0x40000000
+	EPOLLOUT                             = 0x4
+	EPOLLPRI                             = 0x2
+	EPOLLRDBAND                          = 0x80
+	EPOLLRDHUP                           = 0x2000
+	EPOLLRDNORM                          = 0x40
+	EPOLLWAKEUP                          = 0x20000000
+	EPOLLWRBAND                          = 0x200
+	EPOLLWRNORM                          = 0x100
+	EPOLL_CLOEXEC                        = 0x80000
+	EPOLL_CTL_ADD                        = 0x1
+	EPOLL_CTL_DEL                        = 0x2
+	EPOLL_CTL_MOD                        = 0x3
+	ETH_P_1588                           = 0x88f7
+	ETH_P_8021AD                         = 0x88a8
+	ETH_P_8021AH                         = 0x88e7
+	ETH_P_8021Q                          = 0x8100
+	ETH_P_80221                          = 0x8917
+	ETH_P_802_2                          = 0x4
+	ETH_P_802_3                          = 0x1
+	ETH_P_802_3_MIN                      = 0x600
+	ETH_P_802_EX1                        = 0x88b5
+	ETH_P_AARP                           = 0x80f3
+	ETH_P_AF_IUCV                        = 0xfbfb
+	ETH_P_ALL                            = 0x3
+	ETH_P_AOE                            = 0x88a2
+	ETH_P_ARCNET                         = 0x1a
+	ETH_P_ARP                            = 0x806
+	ETH_P_ATALK                          = 0x809b
+	ETH_P_ATMFATE                        = 0x8884
+	ETH_P_ATMMPOA                        = 0x884c
+	ETH_P_AX25                           = 0x2
+	ETH_P_BATMAN                         = 0x4305
+	ETH_P_BPQ                            = 0x8ff
+	ETH_P_CAIF                           = 0xf7
+	ETH_P_CAN                            = 0xc
+	ETH_P_CANFD                          = 0xd
+	ETH_P_CONTROL                        = 0x16
+	ETH_P_CUST                           = 0x6006
+	ETH_P_DDCMP                          = 0x6
+	ETH_P_DEC                            = 0x6000
+	ETH_P_DIAG                           = 0x6005
+	ETH_P_DNA_DL                         = 0x6001
+	ETH_P_DNA_RC                         = 0x6002
+	ETH_P_DNA_RT                         = 0x6003
+	ETH_P_DSA                            = 0x1b
+	ETH_P_ECONET                         = 0x18
+	ETH_P_EDSA                           = 0xdada
+	ETH_P_ERSPAN                         = 0x88be
+	ETH_P_ERSPAN2                        = 0x22eb
+	ETH_P_FCOE                           = 0x8906
+	ETH_P_FIP                            = 0x8914
+	ETH_P_HDLC                           = 0x19
+	ETH_P_HSR                            = 0x892f
+	ETH_P_IBOE                           = 0x8915
+	ETH_P_IEEE802154                     = 0xf6
+	ETH_P_IEEEPUP                        = 0xa00
+	ETH_P_IEEEPUPAT                      = 0xa01
+	ETH_P_IFE                            = 0xed3e
+	ETH_P_IP                             = 0x800
+	ETH_P_IPV6                           = 0x86dd
+	ETH_P_IPX                            = 0x8137
+	ETH_P_IRDA                           = 0x17
+	ETH_P_LAT                            = 0x6004
+	ETH_P_LINK_CTL                       = 0x886c
+	ETH_P_LOCALTALK                      = 0x9
+	ETH_P_LOOP                           = 0x60
+	ETH_P_LOOPBACK                       = 0x9000
+	ETH_P_MACSEC                         = 0x88e5
+	ETH_P_MAP                            = 0xf9
+	ETH_P_MOBITEX                        = 0x15
+	ETH_P_MPLS_MC                        = 0x8848
+	ETH_P_MPLS_UC                        = 0x8847
+	ETH_P_MVRP                           = 0x88f5
+	ETH_P_NCSI                           = 0x88f8
+	ETH_P_NSH                            = 0x894f
+	ETH_P_PAE                            = 0x888e
+	ETH_P_PAUSE                          = 0x8808
+	ETH_P_PHONET                         = 0xf5
+	ETH_P_PPPTALK                        = 0x10
+	ETH_P_PPP_DISC                       = 0x8863
+	ETH_P_PPP_MP                         = 0x8
+	ETH_P_PPP_SES                        = 0x8864
+	ETH_P_PREAUTH                        = 0x88c7
+	ETH_P_PRP                            = 0x88fb
+	ETH_P_PUP                            = 0x200
+	ETH_P_PUPAT                          = 0x201
+	ETH_P_QINQ1                          = 0x9100
+	ETH_P_QINQ2                          = 0x9200
+	ETH_P_QINQ3                          = 0x9300
+	ETH_P_RARP                           = 0x8035
+	ETH_P_SCA                            = 0x6007
+	ETH_P_SLOW                           = 0x8809
+	ETH_P_SNAP                           = 0x5
+	ETH_P_TDLS                           = 0x890d
+	ETH_P_TEB                            = 0x6558
+	ETH_P_TIPC                           = 0x88ca
+	ETH_P_TRAILER                        = 0x1c
+	ETH_P_TR_802_2                       = 0x11
+	ETH_P_TSN                            = 0x22f0
+	ETH_P_WAN_PPP                        = 0x7
+	ETH_P_WCCP                           = 0x883e
+	ETH_P_X25                            = 0x805
+	ETH_P_XDSA                           = 0xf8
+	EXABYTE_ENABLE_NEST                  = 0xf0
+	EXT2_SUPER_MAGIC                     = 0xef53
+	EXT3_SUPER_MAGIC                     = 0xef53
+	EXT4_SUPER_MAGIC                     = 0xef53
+	EXTA                                 = 0xe
+	EXTB                                 = 0xf
+	EXTPROC                              = 0x10000
+	F2FS_SUPER_MAGIC                     = 0xf2f52010
+	FALLOC_FL_COLLAPSE_RANGE             = 0x8
+	FALLOC_FL_INSERT_RANGE               = 0x20
+	FALLOC_FL_KEEP_SIZE                  = 0x1
+	FALLOC_FL_NO_HIDE_STALE              = 0x4
+	FALLOC_FL_PUNCH_HOLE                 = 0x2
+	FALLOC_FL_UNSHARE_RANGE              = 0x40
+	FALLOC_FL_ZERO_RANGE                 = 0x10
+	FANOTIFY_METADATA_VERSION            = 0x3
+	FAN_ACCESS                           = 0x1
+	FAN_ACCESS_PERM                      = 0x20000
+	FAN_ALLOW                            = 0x1
+	FAN_ALL_CLASS_BITS                   = 0xc
+	FAN_ALL_EVENTS                       = 0x3b
+	FAN_ALL_INIT_FLAGS                   = 0x3f
+	FAN_ALL_MARK_FLAGS                   = 0xff
+	FAN_ALL_OUTGOING_EVENTS              = 0x3403b
+	FAN_ALL_PERM_EVENTS                  = 0x30000
+	FAN_AUDIT                            = 0x10
+	FAN_CLASS_CONTENT                    = 0x4
+	FAN_CLASS_NOTIF                      = 0x0
+	FAN_CLASS_PRE_CONTENT                = 0x8
+	FAN_CLOEXEC                          = 0x1
+	FAN_CLOSE                            = 0x18
+	FAN_CLOSE_NOWRITE                    = 0x10
+	FAN_CLOSE_WRITE                      = 0x8
+	FAN_DENY                             = 0x2
+	FAN_ENABLE_AUDIT                     = 0x40
+	FAN_EVENT_METADATA_LEN               = 0x18
+	FAN_EVENT_ON_CHILD                   = 0x8000000
+	FAN_MARK_ADD                         = 0x1
+	FAN_MARK_DONT_FOLLOW                 = 0x4
+	FAN_MARK_FILESYSTEM                  = 0x100
+	FAN_MARK_FLUSH                       = 0x80
+	FAN_MARK_IGNORED_MASK                = 0x20
+	FAN_MARK_IGNORED_SURV_MODIFY         = 0x40
+	FAN_MARK_INODE                       = 0x0
+	FAN_MARK_MOUNT                       = 0x10
+	FAN_MARK_ONLYDIR                     = 0x8
+	FAN_MARK_REMOVE                      = 0x2
+	FAN_MODIFY                           = 0x2
+	FAN_NOFD                             = -0x1
+	FAN_NONBLOCK                         = 0x2
+	FAN_ONDIR                            = 0x40000000
+	FAN_OPEN                             = 0x20
+	FAN_OPEN_EXEC                        = 0x1000
+	FAN_OPEN_EXEC_PERM                   = 0x40000
+	FAN_OPEN_PERM                        = 0x10000
+	FAN_Q_OVERFLOW                       = 0x4000
+	FAN_REPORT_TID                       = 0x100
+	FAN_UNLIMITED_MARKS                  = 0x20
+	FAN_UNLIMITED_QUEUE                  = 0x10
+	FD_CLOEXEC                           = 0x1
+	FD_SETSIZE                           = 0x400
+	FF0                                  = 0x0
+	FF1                                  = 0x8000
+	FFDLY                                = 0x8000
+	FLUSHO                               = 0x1000
+	FS_ENCRYPTION_MODE_ADIANTUM          = 0x9
+	FS_ENCRYPTION_MODE_AES_128_CBC       = 0x5
+	FS_ENCRYPTION_MODE_AES_128_CTS       = 0x6
+	FS_ENCRYPTION_MODE_AES_256_CBC       = 0x3
+	FS_ENCRYPTION_MODE_AES_256_CTS       = 0x4
+	FS_ENCRYPTION_MODE_AES_256_GCM       = 0x2
+	FS_ENCRYPTION_MODE_AES_256_XTS       = 0x1
+	FS_ENCRYPTION_MODE_INVALID           = 0x0
+	FS_ENCRYPTION_MODE_SPECK128_256_CTS  = 0x8
+	FS_ENCRYPTION_MODE_SPECK128_256_XTS  = 0x7
+	FS_IOC_GET_ENCRYPTION_POLICY         = 0x400c6615
+	FS_IOC_GET_ENCRYPTION_PWSALT         = 0x40106614
+	FS_IOC_SET_ENCRYPTION_POLICY         = 0x800c6613
+	FS_KEY_DESCRIPTOR_SIZE               = 0x8
+	FS_KEY_DESC_PREFIX                   = "fscrypt:"
+	FS_KEY_DESC_PREFIX_SIZE              = 0x8
+	FS_MAX_KEY_SIZE                      = 0x40
+	FS_POLICY_FLAGS_PAD_16               = 0x2
+	FS_POLICY_FLAGS_PAD_32               = 0x3
+	FS_POLICY_FLAGS_PAD_4                = 0x0
+	FS_POLICY_FLAGS_PAD_8                = 0x1
+	FS_POLICY_FLAGS_PAD_MASK             = 0x3
+	FS_POLICY_FLAGS_VALID                = 0x7
+	FUTEXFS_SUPER_MAGIC                  = 0xbad1dea
+	F_ADD_SEALS                          = 0x409
+	F_DUPFD                              = 0x0
+	F_DUPFD_CLOEXEC                      = 0x406
+	F_EXLCK                              = 0x4
+	F_GETFD                              = 0x1
+	F_GETFL                              = 0x3
+	F_GETLEASE                           = 0x401
+	F_GETLK                              = 0xc
+	F_GETLK64                            = 0xc
+	F_GETOWN                             = 0x9
+	F_GETOWN_EX                          = 0x10
+	F_GETPIPE_SZ                         = 0x408
+	F_GETSIG                             = 0xb
+	F_GET_FILE_RW_HINT                   = 0x40d
+	F_GET_RW_HINT                        = 0x40b
+	F_GET_SEALS                          = 0x40a
+	F_LOCK                               = 0x1
+	F_NOTIFY                             = 0x402
+	F_OFD_GETLK                          = 0x24
+	F_OFD_SETLK                          = 0x25
+	F_OFD_SETLKW                         = 0x26
+	F_OK                                 = 0x0
+	F_RDLCK                              = 0x0
+	F_SEAL_GROW                          = 0x4
+	F_SEAL_SEAL                          = 0x1
+	F_SEAL_SHRINK                        = 0x2
+	F_SEAL_WRITE                         = 0x8
+	F_SETFD                              = 0x2
+	F_SETFL                              = 0x4
+	F_SETLEASE                           = 0x400
+	F_SETLK                              = 0xd
+	F_SETLK64                            = 0xd
+	F_SETLKW                             = 0xe
+	F_SETLKW64                           = 0xe
+	F_SETOWN                             = 0x8
+	F_SETOWN_EX                          = 0xf
+	F_SETPIPE_SZ                         = 0x407
+	F_SETSIG                             = 0xa
+	F_SET_FILE_RW_HINT                   = 0x40e
+	F_SET_RW_HINT                        = 0x40c
+	F_SHLCK                              = 0x8
+	F_TEST                               = 0x3
+	F_TLOCK                              = 0x2
+	F_ULOCK                              = 0x0
+	F_UNLCK                              = 0x2
+	F_WRLCK                              = 0x1
+	GENL_ADMIN_PERM                      = 0x1
+	GENL_CMD_CAP_DO                      = 0x2
+	GENL_CMD_CAP_DUMP                    = 0x4
+	GENL_CMD_CAP_HASPOL                  = 0x8
+	GENL_HDRLEN                          = 0x4
+	GENL_ID_CTRL                         = 0x10
+	GENL_ID_PMCRAID                      = 0x12
+	GENL_ID_VFS_DQUOT                    = 0x11
+	GENL_MAX_ID                          = 0x3ff
+	GENL_MIN_ID                          = 0x10
+	GENL_NAMSIZ                          = 0x10
+	GENL_START_ALLOC                     = 0x13
+	GENL_UNS_ADMIN_PERM                  = 0x10
+	GRND_NONBLOCK                        = 0x1
+	GRND_RANDOM                          = 0x2
+	HDIO_DRIVE_CMD                       = 0x31f
+	HDIO_DRIVE_CMD_AEB                   = 0x31e
+	HDIO_DRIVE_CMD_HDR_SIZE              = 0x4
+	HDIO_DRIVE_HOB_HDR_SIZE              = 0x8
+	HDIO_DRIVE_RESET                     = 0x31c
+	HDIO_DRIVE_TASK                      = 0x31e
+	HDIO_DRIVE_TASKFILE                  = 0x31d
+	HDIO_DRIVE_TASK_HDR_SIZE             = 0x8
+	HDIO_GETGEO                          = 0x301
+	HDIO_GET_32BIT                       = 0x309
+	HDIO_GET_ACOUSTIC                    = 0x30f
+	HDIO_GET_ADDRESS                     = 0x310
+	HDIO_GET_BUSSTATE                    = 0x31a
+	HDIO_GET_DMA                         = 0x30b
+	HDIO_GET_IDENTITY                    = 0x30d
+	HDIO_GET_KEEPSETTINGS                = 0x308
+	HDIO_GET_MULTCOUNT                   = 0x304
+	HDIO_GET_NICE                        = 0x30c
+	HDIO_GET_NOWERR                      = 0x30a
+	HDIO_GET_QDMA                        = 0x305
+	HDIO_GET_UNMASKINTR                  = 0x302
+	HDIO_GET_WCACHE                      = 0x30e
+	HDIO_OBSOLETE_IDENTITY               = 0x307
+	HDIO_SCAN_HWIF                       = 0x328
+	HDIO_SET_32BIT                       = 0x324
+	HDIO_SET_ACOUSTIC                    = 0x32c
+	HDIO_SET_ADDRESS                     = 0x32f
+	HDIO_SET_BUSSTATE                    = 0x32d
+	HDIO_SET_DMA                         = 0x326
+	HDIO_SET_KEEPSETTINGS                = 0x323
+	HDIO_SET_MULTCOUNT                   = 0x321
+	HDIO_SET_NICE                        = 0x329
+	HDIO_SET_NOWERR                      = 0x325
+	HDIO_SET_PIO_MODE                    = 0x327
+	HDIO_SET_QDMA                        = 0x32e
+	HDIO_SET_UNMASKINTR                  = 0x322
+	HDIO_SET_WCACHE                      = 0x32b
+	HDIO_SET_XFER                        = 0x306
+	HDIO_TRISTATE_HWIF                   = 0x31b
+	HDIO_UNREGISTER_HWIF                 = 0x32a
+	HOSTFS_SUPER_MAGIC                   = 0xc0ffee
+	HPFS_SUPER_MAGIC                     = 0xf995e849
+	HUGETLBFS_MAGIC                      = 0x958458f6
+	HUPCL                                = 0x400
+	IBSHIFT                              = 0x10
+	ICANON                               = 0x2
+	ICMPV6_FILTER                        = 0x1
+	ICRNL                                = 0x100
+	IEXTEN                               = 0x8000
+	IFA_F_DADFAILED                      = 0x8
+	IFA_F_DEPRECATED                     = 0x20
+	IFA_F_HOMEADDRESS                    = 0x10
+	IFA_F_MANAGETEMPADDR                 = 0x100
+	IFA_F_MCAUTOJOIN                     = 0x400
+	IFA_F_NODAD                          = 0x2
+	IFA_F_NOPREFIXROUTE                  = 0x200
+	IFA_F_OPTIMISTIC                     = 0x4
+	IFA_F_PERMANENT                      = 0x80
+	IFA_F_SECONDARY                      = 0x1
+	IFA_F_STABLE_PRIVACY                 = 0x800
+	IFA_F_TEMPORARY                      = 0x1
+	IFA_F_TENTATIVE                      = 0x40
+	IFA_MAX                              = 0xa
+	IFF_ALLMULTI                         = 0x200
+	IFF_ATTACH_QUEUE                     = 0x200
+	IFF_AUTOMEDIA                        = 0x4000
+	IFF_BROADCAST                        = 0x2
+	IFF_DEBUG                            = 0x4
+	IFF_DETACH_QUEUE                     = 0x400
+	IFF_DORMANT                          = 0x20000
+	IFF_DYNAMIC                          = 0x8000
+	IFF_ECHO                             = 0x40000
+	IFF_LOOPBACK                         = 0x8
+	IFF_LOWER_UP                         = 0x10000
+	IFF_MASTER                           = 0x400
+	IFF_MULTICAST                        = 0x1000
+	IFF_MULTI_QUEUE                      = 0x100
+	IFF_NAPI                             = 0x10
+	IFF_NAPI_FRAGS                       = 0x20
+	IFF_NOARP                            = 0x80
+	IFF_NOFILTER                         = 0x1000
+	IFF_NOTRAILERS                       = 0x20
+	IFF_NO_PI                            = 0x1000
+	IFF_ONE_QUEUE                        = 0x2000
+	IFF_PERSIST                          = 0x800
+	IFF_POINTOPOINT                      = 0x10
+	IFF_PORTSEL                          = 0x2000
+	IFF_PROMISC                          = 0x100
+	IFF_RUNNING                          = 0x40
+	IFF_SLAVE                            = 0x800
+	IFF_TAP                              = 0x2
+	IFF_TUN                              = 0x1
+	IFF_TUN_EXCL                         = 0x8000
+	IFF_UP                               = 0x1
+	IFF_VNET_HDR                         = 0x4000
+	IFF_VOLATILE                         = 0x70c5a
+	IFNAMSIZ                             = 0x10
+	IGNBRK                               = 0x1
+	IGNCR                                = 0x80
+	IGNPAR                               = 0x4
+	IMAXBEL                              = 0x2000
+	INLCR                                = 0x40
+	INPCK                                = 0x10
+	IN_ACCESS                            = 0x1
+	IN_ALL_EVENTS                        = 0xfff
+	IN_ATTRIB                            = 0x4
+	IN_CLASSA_HOST                       = 0xffffff
+	IN_CLASSA_MAX                        = 0x80
+	IN_CLASSA_NET                        = 0xff000000
+	IN_CLASSA_NSHIFT                     = 0x18
+	IN_CLASSB_HOST                       = 0xffff
+	IN_CLASSB_MAX                        = 0x10000
+	IN_CLASSB_NET                        = 0xffff0000
+	IN_CLASSB_NSHIFT                     = 0x10
+	IN_CLASSC_HOST                       = 0xff
+	IN_CLASSC_NET                        = 0xffffff00
+	IN_CLASSC_NSHIFT                     = 0x8
+	IN_CLOEXEC                           = 0x80000
+	IN_CLOSE                             = 0x18
+	IN_CLOSE_NOWRITE                     = 0x10
+	IN_CLOSE_WRITE                       = 0x8
+	IN_CREATE                            = 0x100
+	IN_DELETE                            = 0x200
+	IN_DELETE_SELF                       = 0x400
+	IN_DONT_FOLLOW                       = 0x2000000
+	IN_EXCL_UNLINK                       = 0x4000000
+	IN_IGNORED                           = 0x8000
+	IN_ISDIR                             = 0x40000000
+	IN_LOOPBACKNET                       = 0x7f
+	IN_MASK_ADD                          = 0x20000000
+	IN_MASK_CREATE                       = 0x10000000
+	IN_MODIFY                            = 0x2
+	IN_MOVE                              = 0xc0
+	IN_MOVED_FROM                        = 0x40
+	IN_MOVED_TO                          = 0x80
+	IN_MOVE_SELF                         = 0x800
+	IN_NONBLOCK                          = 0x800
+	IN_ONESHOT                           = 0x80000000
+	IN_ONLYDIR                           = 0x1000000
+	IN_OPEN                              = 0x20
+	IN_Q_OVERFLOW                        = 0x4000
+	IN_UNMOUNT                           = 0x2000
+	IOCTL_VM_SOCKETS_GET_LOCAL_CID       = 0x7b9
+	IPPROTO_AH                           = 0x33
+	IPPROTO_BEETPH                       = 0x5e
+	IPPROTO_COMP                         = 0x6c
+	IPPROTO_DCCP                         = 0x21
+	IPPROTO_DSTOPTS                      = 0x3c
+	IPPROTO_EGP                          = 0x8
+	IPPROTO_ENCAP                        = 0x62
+	IPPROTO_ESP                          = 0x32
+	IPPROTO_FRAGMENT                     = 0x2c
+	IPPROTO_GRE                          = 0x2f
+	IPPROTO_HOPOPTS                      = 0x0
+	IPPROTO_ICMP                         = 0x1
+	IPPROTO_ICMPV6                       = 0x3a
+	IPPROTO_IDP                          = 0x16
+	IPPROTO_IGMP                         = 0x2
+	IPPROTO_IP                           = 0x0
+	IPPROTO_IPIP                         = 0x4
+	IPPROTO_IPV6                         = 0x29
+	IPPROTO_MH                           = 0x87
+	IPPROTO_MPLS                         = 0x89
+	IPPROTO_MTP                          = 0x5c
+	IPPROTO_NONE                         = 0x3b
+	IPPROTO_PIM                          = 0x67
+	IPPROTO_PUP                          = 0xc
+	IPPROTO_RAW                          = 0xff
+	IPPROTO_ROUTING                      = 0x2b
+	IPPROTO_RSVP                         = 0x2e
+	IPPROTO_SCTP                         = 0x84
+	IPPROTO_TCP                          = 0x6
+	IPPROTO_TP                           = 0x1d
+	IPPROTO_UDP                          = 0x11
+	IPPROTO_UDPLITE                      = 0x88
+	IPV6_2292DSTOPTS                     = 0x4
+	IPV6_2292HOPLIMIT                    = 0x8
+	IPV6_2292HOPOPTS                     = 0x3
+	IPV6_2292PKTINFO                     = 0x2
+	IPV6_2292PKTOPTIONS                  = 0x6
+	IPV6_2292RTHDR                       = 0x5
+	IPV6_ADDRFORM                        = 0x1
+	IPV6_ADDR_PREFERENCES                = 0x48
+	IPV6_ADD_MEMBERSHIP                  = 0x14
+	IPV6_AUTHHDR                         = 0xa
+	IPV6_AUTOFLOWLABEL                   = 0x46
+	IPV6_CHECKSUM                        = 0x7
+	IPV6_DONTFRAG                        = 0x3e
+	IPV6_DROP_MEMBERSHIP                 = 0x15
+	IPV6_DSTOPTS                         = 0x3b
+	IPV6_FREEBIND                        = 0x4e
+	IPV6_HDRINCL                         = 0x24
+	IPV6_HOPLIMIT                        = 0x34
+	IPV6_HOPOPTS                         = 0x36
+	IPV6_IPSEC_POLICY                    = 0x22
+	IPV6_JOIN_ANYCAST                    = 0x1b
+	IPV6_JOIN_GROUP                      = 0x14
+	IPV6_LEAVE_ANYCAST                   = 0x1c
+	IPV6_LEAVE_GROUP                     = 0x15
+	IPV6_MINHOPCOUNT                     = 0x49
+	IPV6_MTU                             = 0x18
+	IPV6_MTU_DISCOVER                    = 0x17
+	IPV6_MULTICAST_ALL                   = 0x1d
+	IPV6_MULTICAST_HOPS                  = 0x12
+	IPV6_MULTICAST_IF                    = 0x11
+	IPV6_MULTICAST_LOOP                  = 0x13
+	IPV6_NEXTHOP                         = 0x9
+	IPV6_ORIGDSTADDR                     = 0x4a
+	IPV6_PATHMTU                         = 0x3d
+	IPV6_PKTINFO                         = 0x32
+	IPV6_PMTUDISC_DO                     = 0x2
+	IPV6_PMTUDISC_DONT                   = 0x0
+	IPV6_PMTUDISC_INTERFACE              = 0x4
+	IPV6_PMTUDISC_OMIT                   = 0x5
+	IPV6_PMTUDISC_PROBE                  = 0x3
+	IPV6_PMTUDISC_WANT                   = 0x1
+	IPV6_RECVDSTOPTS                     = 0x3a
+	IPV6_RECVERR                         = 0x19
+	IPV6_RECVFRAGSIZE                    = 0x4d
+	IPV6_RECVHOPLIMIT                    = 0x33
+	IPV6_RECVHOPOPTS                     = 0x35
+	IPV6_RECVORIGDSTADDR                 = 0x4a
+	IPV6_RECVPATHMTU                     = 0x3c
+	IPV6_RECVPKTINFO                     = 0x31
+	IPV6_RECVRTHDR                       = 0x38
+	IPV6_RECVTCLASS                      = 0x42
+	IPV6_ROUTER_ALERT                    = 0x16
+	IPV6_RTHDR                           = 0x39
+	IPV6_RTHDRDSTOPTS                    = 0x37
+	IPV6_RTHDR_LOOSE                     = 0x0
+	IPV6_RTHDR_STRICT                    = 0x1
+	IPV6_RTHDR_TYPE_0                    = 0x0
+	IPV6_RXDSTOPTS                       = 0x3b
+	IPV6_RXHOPOPTS                       = 0x36
+	IPV6_TCLASS                          = 0x43
+	IPV6_TRANSPARENT                     = 0x4b
+	IPV6_UNICAST_HOPS                    = 0x10
+	IPV6_UNICAST_IF                      = 0x4c
+	IPV6_V6ONLY                          = 0x1a
+	IPV6_XFRM_POLICY                     = 0x23
+	IP_ADD_MEMBERSHIP                    = 0x23
+	IP_ADD_SOURCE_MEMBERSHIP             = 0x27
+	IP_BIND_ADDRESS_NO_PORT              = 0x18
+	IP_BLOCK_SOURCE                      = 0x26
+	IP_CHECKSUM                          = 0x17
+	IP_DEFAULT_MULTICAST_LOOP            = 0x1
+	IP_DEFAULT_MULTICAST_TTL             = 0x1
+	IP_DF                                = 0x4000
+	IP_DROP_MEMBERSHIP                   = 0x24
+	IP_DROP_SOURCE_MEMBERSHIP            = 0x28
+	IP_FREEBIND                          = 0xf
+	IP_HDRINCL                           = 0x3
+	IP_IPSEC_POLICY                      = 0x10
+	IP_MAXPACKET                         = 0xffff
+	IP_MAX_MEMBERSHIPS                   = 0x14
+	IP_MF                                = 0x2000
+	IP_MINTTL                            = 0x15
+	IP_MSFILTER                          = 0x29
+	IP_MSS                               = 0x240
+	IP_MTU                               = 0xe
+	IP_MTU_DISCOVER                      = 0xa
+	IP_MULTICAST_ALL                     = 0x31
+	IP_MULTICAST_IF                      = 0x20
+	IP_MULTICAST_LOOP                    = 0x22
+	IP_MULTICAST_TTL                     = 0x21
+	IP_NODEFRAG                          = 0x16
+	IP_OFFMASK                           = 0x1fff
+	IP_OPTIONS                           = 0x4
+	IP_ORIGDSTADDR                       = 0x14
+	IP_PASSSEC                           = 0x12
+	IP_PKTINFO                           = 0x8
+	IP_PKTOPTIONS                        = 0x9
+	IP_PMTUDISC                          = 0xa
+	IP_PMTUDISC_DO                       = 0x2
+	IP_PMTUDISC_DONT                     = 0x0
+	IP_PMTUDISC_INTERFACE                = 0x4
+	IP_PMTUDISC_OMIT                     = 0x5
+	IP_PMTUDISC_PROBE                    = 0x3
+	IP_PMTUDISC_WANT                     = 0x1
+	IP_RECVERR                           = 0xb
+	IP_RECVFRAGSIZE                      = 0x19
+	IP_RECVOPTS                          = 0x6
+	IP_RECVORIGDSTADDR                   = 0x14
+	IP_RECVRETOPTS                       = 0x7
+	IP_RECVTOS                           = 0xd
+	IP_RECVTTL                           = 0xc
+	IP_RETOPTS                           = 0x7
+	IP_RF                                = 0x8000
+	IP_ROUTER_ALERT                      = 0x5
+	IP_TOS                               = 0x1
+	IP_TRANSPARENT                       = 0x13
+	IP_TTL                               = 0x2
+	IP_UNBLOCK_SOURCE                    = 0x25
+	IP_UNICAST_IF                        = 0x32
+	IP_XFRM_POLICY                       = 0x11
+	ISIG                                 = 0x1
+	ISOFS_SUPER_MAGIC                    = 0x9660
+	ISTRIP                               = 0x20
+	IUCLC                                = 0x200
+	IUTF8                                = 0x4000
+	IXANY                                = 0x800
+	IXOFF                                = 0x1000
+	IXON                                 = 0x400
+	JFFS2_SUPER_MAGIC                    = 0x72b6
+	KEXEC_ARCH_386                       = 0x30000
+	KEXEC_ARCH_68K                       = 0x40000
+	KEXEC_ARCH_AARCH64                   = 0xb70000
+	KEXEC_ARCH_ARM                       = 0x280000
+	KEXEC_ARCH_DEFAULT                   = 0x0
+	KEXEC_ARCH_IA_64                     = 0x320000
+	KEXEC_ARCH_MASK                      = 0xffff0000
+	KEXEC_ARCH_MIPS                      = 0x80000
+	KEXEC_ARCH_MIPS_LE                   = 0xa0000
+	KEXEC_ARCH_PPC                       = 0x140000
+	KEXEC_ARCH_PPC64                     = 0x150000
+	KEXEC_ARCH_S390                      = 0x160000
+	KEXEC_ARCH_SH                        = 0x2a0000
+	KEXEC_ARCH_X86_64                    = 0x3e0000
+	KEXEC_FILE_NO_INITRAMFS              = 0x4
+	KEXEC_FILE_ON_CRASH                  = 0x2
+	KEXEC_FILE_UNLOAD                    = 0x1
+	KEXEC_ON_CRASH                       = 0x1
+	KEXEC_PRESERVE_CONTEXT               = 0x2
+	KEXEC_SEGMENT_MAX                    = 0x10
+	KEYCTL_ASSUME_AUTHORITY              = 0x10
+	KEYCTL_CHOWN                         = 0x4
+	KEYCTL_CLEAR                         = 0x7
+	KEYCTL_DESCRIBE                      = 0x6
+	KEYCTL_DH_COMPUTE                    = 0x17
+	KEYCTL_GET_KEYRING_ID                = 0x0
+	KEYCTL_GET_PERSISTENT                = 0x16
+	KEYCTL_GET_SECURITY                  = 0x11
+	KEYCTL_INSTANTIATE                   = 0xc
+	KEYCTL_INSTANTIATE_IOV               = 0x14
+	KEYCTL_INVALIDATE                    = 0x15
+	KEYCTL_JOIN_SESSION_KEYRING          = 0x1
+	KEYCTL_LINK                          = 0x8
+	KEYCTL_NEGATE                        = 0xd
+	KEYCTL_PKEY_DECRYPT                  = 0x1a
+	KEYCTL_PKEY_ENCRYPT                  = 0x19
+	KEYCTL_PKEY_QUERY                    = 0x18
+	KEYCTL_PKEY_SIGN                     = 0x1b
+	KEYCTL_PKEY_VERIFY                   = 0x1c
+	KEYCTL_READ                          = 0xb
+	KEYCTL_REJECT                        = 0x13
+	KEYCTL_RESTRICT_KEYRING              = 0x1d
+	KEYCTL_REVOKE                        = 0x3
+	KEYCTL_SEARCH                        = 0xa
+	KEYCTL_SESSION_TO_PARENT             = 0x12
+	KEYCTL_SETPERM                       = 0x5
+	KEYCTL_SET_REQKEY_KEYRING            = 0xe
+	KEYCTL_SET_TIMEOUT                   = 0xf
+	KEYCTL_SUPPORTS_DECRYPT              = 0x2
+	KEYCTL_SUPPORTS_ENCRYPT              = 0x1
+	KEYCTL_SUPPORTS_SIGN                 = 0x4
+	KEYCTL_SUPPORTS_VERIFY               = 0x8
+	KEYCTL_UNLINK                        = 0x9
+	KEYCTL_UPDATE                        = 0x2
+	KEY_REQKEY_DEFL_DEFAULT              = 0x0
+	KEY_REQKEY_DEFL_GROUP_KEYRING        = 0x6
+	KEY_REQKEY_DEFL_NO_CHANGE            = -0x1
+	KEY_REQKEY_DEFL_PROCESS_KEYRING      = 0x2
+	KEY_REQKEY_DEFL_REQUESTOR_KEYRING    = 0x7
+	KEY_REQKEY_DEFL_SESSION_KEYRING      = 0x3
+	KEY_REQKEY_DEFL_THREAD_KEYRING       = 0x1
+	KEY_REQKEY_DEFL_USER_KEYRING         = 0x4
+	KEY_REQKEY_DEFL_USER_SESSION_KEYRING = 0x5
+	KEY_SPEC_GROUP_KEYRING               = -0x6
+	KEY_SPEC_PROCESS_KEYRING             = -0x2
+	KEY_SPEC_REQKEY_AUTH_KEY             = -0x7
+	KEY_SPEC_REQUESTOR_KEYRING           = -0x8
+	KEY_SPEC_SESSION_KEYRING             = -0x3
+	KEY_SPEC_THREAD_KEYRING              = -0x1
+	KEY_SPEC_USER_KEYRING                = -0x4
+	KEY_SPEC_USER_SESSION_KEYRING        = -0x5
+	LINUX_REBOOT_CMD_CAD_OFF             = 0x0
+	LINUX_REBOOT_CMD_CAD_ON              = 0x89abcdef
+	LINUX_REBOOT_CMD_HALT                = 0xcdef0123
+	LINUX_REBOOT_CMD_KEXEC               = 0x45584543
+	LINUX_REBOOT_CMD_POWER_OFF           = 0x4321fedc
+	LINUX_REBOOT_CMD_RESTART             = 0x1234567
+	LINUX_REBOOT_CMD_RESTART2            = 0xa1b2c3d4
+	LINUX_REBOOT_CMD_SW_SUSPEND          = 0xd000fce2
+	LINUX_REBOOT_MAGIC1                  = 0xfee1dead
+	LINUX_REBOOT_MAGIC2                  = 0x28121969
+	LOCK_EX                              = 0x2
+	LOCK_NB                              = 0x4
+	LOCK_SH                              = 0x1
+	LOCK_UN                              = 0x8
+	MADV_DODUMP                          = 0x11
+	MADV_DOFORK                          = 0xb
+	MADV_DONTDUMP                        = 0x10
+	MADV_DONTFORK                        = 0xa
+	MADV_DONTNEED                        = 0x4
+	MADV_FREE                            = 0x8
+	MADV_HUGEPAGE                        = 0xe
+	MADV_HWPOISON                        = 0x64
+	MADV_KEEPONFORK                      = 0x13
+	MADV_MERGEABLE                       = 0xc
+	MADV_NOHUGEPAGE                      = 0xf
+	MADV_NORMAL                          = 0x0
+	MADV_RANDOM                          = 0x1
+	MADV_REMOVE                          = 0x9
+	MADV_SEQUENTIAL                      = 0x2
+	MADV_UNMERGEABLE                     = 0xd
+	MADV_WILLNEED                        = 0x3
+	MADV_WIPEONFORK                      = 0x12
+	MAP_ANON                             = 0x20
+	MAP_ANONYMOUS                        = 0x20
+	MAP_DENYWRITE                        = 0x800
+	MAP_EXECUTABLE                       = 0x1000
+	MAP_FILE                             = 0x0
+	MAP_FIXED                            = 0x10
+	MAP_FIXED_NOREPLACE                  = 0x100000
+	MAP_GROWSDOWN                        = 0x100
+	MAP_HUGETLB                          = 0x40000
+	MAP_HUGE_MASK                        = 0x3f
+	MAP_HUGE_SHIFT                       = 0x1a
+	MAP_LOCKED                           = 0x2000
+	MAP_NONBLOCK                         = 0x10000
+	MAP_NORESERVE                        = 0x4000
+	MAP_POPULATE                         = 0x8000
+	MAP_PRIVATE                          = 0x2
+	MAP_SHARED                           = 0x1
+	MAP_SHARED_VALIDATE                  = 0x3
+	MAP_STACK                            = 0x20000
+	MAP_SYNC                             = 0x80000
+	MAP_TYPE                             = 0xf
+	MCL_CURRENT                          = 0x1
+	MCL_FUTURE                           = 0x2
+	MCL_ONFAULT                          = 0x4
+	MFD_ALLOW_SEALING                    = 0x2
+	MFD_CLOEXEC                          = 0x1
+	MFD_HUGETLB                          = 0x4
+	MFD_HUGE_16GB                        = -0x78000000
+	MFD_HUGE_16MB                        = 0x60000000
+	MFD_HUGE_1GB                         = 0x78000000
+	MFD_HUGE_1MB                         = 0x50000000
+	MFD_HUGE_256MB                       = 0x70000000
+	MFD_HUGE_2GB                         = 0x7c000000
+	MFD_HUGE_2MB                         = 0x54000000
+	MFD_HUGE_32MB                        = 0x64000000
+	MFD_HUGE_512KB                       = 0x4c000000
+	MFD_HUGE_512MB                       = 0x74000000
+	MFD_HUGE_64KB                        = 0x40000000
+	MFD_HUGE_8MB                         = 0x5c000000
+	MFD_HUGE_MASK                        = 0x3f
+	MFD_HUGE_SHIFT                       = 0x1a
+	MINIX2_SUPER_MAGIC                   = 0x2468
+	MINIX2_SUPER_MAGIC2                  = 0x2478
+	MINIX3_SUPER_MAGIC                   = 0x4d5a
+	MINIX_SUPER_MAGIC                    = 0x137f
+	MINIX_SUPER_MAGIC2                   = 0x138f
+	MNT_DETACH                           = 0x2
+	MNT_EXPIRE                           = 0x4
+	MNT_FORCE                            = 0x1
+	MODULE_INIT_IGNORE_MODVERSIONS       = 0x1
+	MODULE_INIT_IGNORE_VERMAGIC          = 0x2
+	MSDOS_SUPER_MAGIC                    = 0x4d44
+	MSG_BATCH                            = 0x40000
+	MSG_CMSG_CLOEXEC                     = 0x40000000
+	MSG_CONFIRM                          = 0x800
+	MSG_CTRUNC                           = 0x8
+	MSG_DONTROUTE                        = 0x4
+	MSG_DONTWAIT                         = 0x40
+	MSG_EOR                              = 0x80
+	MSG_ERRQUEUE                         = 0x2000
+	MSG_FASTOPEN                         = 0x20000000
+	MSG_FIN                              = 0x200
+	MSG_MORE                             = 0x8000
+	MSG_NOSIGNAL                         = 0x4000
+	MSG_OOB                              = 0x1
+	MSG_PEEK                             = 0x2
+	MSG_PROXY                            = 0x10
+	MSG_RST                              = 0x1000
+	MSG_SYN                              = 0x400
+	MSG_TRUNC                            = 0x20
+	MSG_TRYHARD                          = 0x4
+	MSG_WAITALL                          = 0x100
+	MSG_WAITFORONE                       = 0x10000
+	MSG_ZEROCOPY                         = 0x4000000
+	MS_ACTIVE                            = 0x40000000
+	MS_ASYNC                             = 0x1
+	MS_BIND                              = 0x1000
+	MS_BORN                              = 0x20000000
+	MS_DIRSYNC                           = 0x80
+	MS_INVALIDATE                        = 0x2
+	MS_I_VERSION                         = 0x800000
+	MS_KERNMOUNT                         = 0x400000
+	MS_LAZYTIME                          = 0x2000000
+	MS_MANDLOCK                          = 0x40
+	MS_MGC_MSK                           = 0xffff0000
+	MS_MGC_VAL                           = 0xc0ed0000
+	MS_MOVE                              = 0x2000
+	MS_NOATIME                           = 0x400
+	MS_NODEV                             = 0x4
+	MS_NODIRATIME                        = 0x800
+	MS_NOEXEC                            = 0x8
+	MS_NOREMOTELOCK                      = 0x8000000
+	MS_NOSEC                             = 0x10000000
+	MS_NOSUID                            = 0x2
+	MS_NOUSER                            = -0x80000000
+	MS_POSIXACL                          = 0x10000
+	MS_PRIVATE                           = 0x40000
+	MS_RDONLY                            = 0x1
+	MS_REC                               = 0x4000
+	MS_RELATIME                          = 0x200000
+	MS_REMOUNT                           = 0x20
+	MS_RMT_MASK                          = 0x2800051
+	MS_SHARED                            = 0x100000
+	MS_SILENT                            = 0x8000
+	MS_SLAVE                             = 0x80000
+	MS_STRICTATIME                       = 0x1000000
+	MS_SUBMOUNT                          = 0x4000000
+	MS_SYNC                              = 0x4
+	MS_SYNCHRONOUS                       = 0x10
+	MS_UNBINDABLE                        = 0x20000
+	MS_VERBOSE                           = 0x8000
+	MTD_INODE_FS_MAGIC                   = 0x11307854
+	NAME_MAX                             = 0xff
+	NCP_SUPER_MAGIC                      = 0x564c
+	NETLINK_ADD_MEMBERSHIP               = 0x1
+	NETLINK_AUDIT                        = 0x9
+	NETLINK_BROADCAST_ERROR              = 0x4
+	NETLINK_CAP_ACK                      = 0xa
+	NETLINK_CONNECTOR                    = 0xb
+	NETLINK_CRYPTO                       = 0x15
+	NETLINK_DNRTMSG                      = 0xe
+	NETLINK_DROP_MEMBERSHIP              = 0x2
+	NETLINK_ECRYPTFS                     = 0x13
+	NETLINK_EXT_ACK                      = 0xb
+	NETLINK_FIB_LOOKUP                   = 0xa
+	NETLINK_FIREWALL                     = 0x3
+	NETLINK_GENERIC                      = 0x10
+	NETLINK_GET_STRICT_CHK               = 0xc
+	NETLINK_INET_DIAG                    = 0x4
+	NETLINK_IP6_FW                       = 0xd
+	NETLINK_ISCSI                        = 0x8
+	NETLINK_KOBJECT_UEVENT               = 0xf
+	NETLINK_LISTEN_ALL_NSID              = 0x8
+	NETLINK_LIST_MEMBERSHIPS             = 0x9
+	NETLINK_NETFILTER                    = 0xc
+	NETLINK_NFLOG                        = 0x5
+	NETLINK_NO_ENOBUFS                   = 0x5
+	NETLINK_PKTINFO                      = 0x3
+	NETLINK_RDMA                         = 0x14
+	NETLINK_ROUTE                        = 0x0
+	NETLINK_RX_RING                      = 0x6
+	NETLINK_SCSITRANSPORT                = 0x12
+	NETLINK_SELINUX                      = 0x7
+	NETLINK_SMC                          = 0x16
+	NETLINK_SOCK_DIAG                    = 0x4
+	NETLINK_TX_RING                      = 0x7
+	NETLINK_UNUSED                       = 0x1
+	NETLINK_USERSOCK                     = 0x2
+	NETLINK_XFRM                         = 0x6
+	NETNSA_MAX                           = 0x5
+	NETNSA_NSID_NOT_ASSIGNED             = -0x1
+	NFNETLINK_V0                         = 0x0
+	NFNLGRP_ACCT_QUOTA                   = 0x8
+	NFNLGRP_CONNTRACK_DESTROY            = 0x3
+	NFNLGRP_CONNTRACK_EXP_DESTROY        = 0x6
+	NFNLGRP_CONNTRACK_EXP_NEW            = 0x4
+	NFNLGRP_CONNTRACK_EXP_UPDATE         = 0x5
+	NFNLGRP_CONNTRACK_NEW                = 0x1
+	NFNLGRP_CONNTRACK_UPDATE             = 0x2
+	NFNLGRP_MAX                          = 0x9
+	NFNLGRP_NFTABLES                     = 0x7
+	NFNLGRP_NFTRACE                      = 0x9
+	NFNLGRP_NONE                         = 0x0
+	NFNL_BATCH_MAX                       = 0x1
+	NFNL_MSG_BATCH_BEGIN                 = 0x10
+	NFNL_MSG_BATCH_END                   = 0x11
+	NFNL_NFA_NEST                        = 0x8000
+	NFNL_SUBSYS_ACCT                     = 0x7
+	NFNL_SUBSYS_COUNT                    = 0xc
+	NFNL_SUBSYS_CTHELPER                 = 0x9
+	NFNL_SUBSYS_CTNETLINK                = 0x1
+	NFNL_SUBSYS_CTNETLINK_EXP            = 0x2
+	NFNL_SUBSYS_CTNETLINK_TIMEOUT        = 0x8
+	NFNL_SUBSYS_IPSET                    = 0x6
+	NFNL_SUBSYS_NFTABLES                 = 0xa
+	NFNL_SUBSYS_NFT_COMPAT               = 0xb
+	NFNL_SUBSYS_NONE                     = 0x0
+	NFNL_SUBSYS_OSF                      = 0x5
+	NFNL_SUBSYS_QUEUE                    = 0x3
+	NFNL_SUBSYS_ULOG                     = 0x4
+	NFS_SUPER_MAGIC                      = 0x6969
+	NILFS_SUPER_MAGIC                    = 0x3434
+	NL0                                  = 0x0
+	NL1                                  = 0x100
+	NLA_ALIGNTO                          = 0x4
+	NLA_F_NESTED                         = 0x8000
+	NLA_F_NET_BYTEORDER                  = 0x4000
+	NLA_HDRLEN                           = 0x4
+	NLDLY                                = 0x100
+	NLMSG_ALIGNTO                        = 0x4
+	NLMSG_DONE                           = 0x3
+	NLMSG_ERROR                          = 0x2
+	NLMSG_HDRLEN                         = 0x10
+	NLMSG_MIN_TYPE                       = 0x10
+	NLMSG_NOOP                           = 0x1
+	NLMSG_OVERRUN                        = 0x4
+	NLM_F_ACK                            = 0x4
+	NLM_F_ACK_TLVS                       = 0x200
+	NLM_F_APPEND                         = 0x800
+	NLM_F_ATOMIC                         = 0x400
+	NLM_F_CAPPED                         = 0x100
+	NLM_F_CREATE                         = 0x400
+	NLM_F_DUMP                           = 0x300
+	NLM_F_DUMP_FILTERED                  = 0x20
+	NLM_F_DUMP_INTR                      = 0x10
+	NLM_F_ECHO                           = 0x8
+	NLM_F_EXCL                           = 0x200
+	NLM_F_MATCH                          = 0x200
+	NLM_F_MULTI                          = 0x2
+	NLM_F_NONREC                         = 0x100
+	NLM_F_REPLACE                        = 0x100
+	NLM_F_REQUEST                        = 0x1
+	NLM_F_ROOT                           = 0x100
+	NOFLSH                               = 0x80
+	NSFS_MAGIC                           = 0x6e736673
+	OCFS2_SUPER_MAGIC                    = 0x7461636f
+	OCRNL                                = 0x8
+	OFDEL                                = 0x80
+	OFILL                                = 0x40
+	OLCUC                                = 0x2
+	ONLCR                                = 0x4
+	ONLRET                               = 0x20
+	ONOCR                                = 0x10
+	OPENPROM_SUPER_MAGIC                 = 0x9fa1
+	OPOST                                = 0x1
+	OVERLAYFS_SUPER_MAGIC                = 0x794c7630
+	O_ACCMODE                            = 0x3
+	O_APPEND                             = 0x400
+	O_ASYNC                              = 0x2000
+	O_CLOEXEC                            = 0x80000
+	O_CREAT                              = 0x40
+	O_DIRECT                             = 0x10000
+	O_DIRECTORY                          = 0x4000
+	O_DSYNC                              = 0x1000
+	O_EXCL                               = 0x80
+	O_FSYNC                              = 0x101000
+	O_LARGEFILE                          = 0x20000
+	O_NDELAY                             = 0x800
+	O_NOATIME                            = 0x40000
+	O_NOCTTY                             = 0x100
+	O_NOFOLLOW                           = 0x8000
+	O_NONBLOCK                           = 0x800
+	O_PATH                               = 0x200000
+	O_RDONLY                             = 0x0
+	O_RDWR                               = 0x2
+	O_RSYNC                              = 0x101000
+	O_SYNC                               = 0x101000
+	O_TMPFILE                            = 0x404000
+	O_TRUNC                              = 0x200
+	O_WRONLY                             = 0x1
+	PACKET_ADD_MEMBERSHIP                = 0x1
+	PACKET_AUXDATA                       = 0x8
+	PACKET_BROADCAST                     = 0x1
+	PACKET_COPY_THRESH                   = 0x7
+	PACKET_DROP_MEMBERSHIP               = 0x2
+	PACKET_FANOUT                        = 0x12
+	PACKET_FANOUT_CBPF                   = 0x6
+	PACKET_FANOUT_CPU                    = 0x2
+	PACKET_FANOUT_DATA                   = 0x16
+	PACKET_FANOUT_EBPF                   = 0x7
+	PACKET_FANOUT_FLAG_DEFRAG            = 0x8000
+	PACKET_FANOUT_FLAG_ROLLOVER          = 0x1000
+	PACKET_FANOUT_FLAG_UNIQUEID          = 0x2000
+	PACKET_FANOUT_HASH                   = 0x0
+	PACKET_FANOUT_LB                     = 0x1
+	PACKET_FANOUT_QM                     = 0x5
+	PACKET_FANOUT_RND                    = 0x4
+	PACKET_FANOUT_ROLLOVER               = 0x3
+	PACKET_FASTROUTE                     = 0x6
+	PACKET_HDRLEN                        = 0xb
+	PACKET_HOST                          = 0x0
+	PACKET_IGNORE_OUTGOING               = 0x17
+	PACKET_KERNEL                        = 0x7
+	PACKET_LOOPBACK                      = 0x5
+	PACKET_LOSS                          = 0xe
+	PACKET_MR_ALLMULTI                   = 0x2
+	PACKET_MR_MULTICAST                  = 0x0
+	PACKET_MR_PROMISC                    = 0x1
+	PACKET_MR_UNICAST                    = 0x3
+	PACKET_MULTICAST                     = 0x2
+	PACKET_ORIGDEV                       = 0x9
+	PACKET_OTHERHOST                     = 0x3
+	PACKET_OUTGOING                      = 0x4
+	PACKET_QDISC_BYPASS                  = 0x14
+	PACKET_RECV_OUTPUT                   = 0x3
+	PACKET_RESERVE                       = 0xc
+	PACKET_ROLLOVER_STATS                = 0x15
+	PACKET_RX_RING                       = 0x5
+	PACKET_STATISTICS                    = 0x6
+	PACKET_TIMESTAMP                     = 0x11
+	PACKET_TX_HAS_OFF                    = 0x13
+	PACKET_TX_RING                       = 0xd
+	PACKET_TX_TIMESTAMP                  = 0x10
+	PACKET_USER                          = 0x6
+	PACKET_VERSION                       = 0xa
+	PACKET_VNET_HDR                      = 0xf
+	PARENB                               = 0x100
+	PARITY_CRC16_PR0                     = 0x2
+	PARITY_CRC16_PR0_CCITT               = 0x4
+	PARITY_CRC16_PR1                     = 0x3
+	PARITY_CRC16_PR1_CCITT               = 0x5
+	PARITY_CRC32_PR0_CCITT               = 0x6
+	PARITY_CRC32_PR1_CCITT               = 0x7
+	PARITY_DEFAULT                       = 0x0
+	PARITY_NONE                          = 0x1
+	PARMRK                               = 0x8
+	PARODD                               = 0x200
+	PENDIN                               = 0x4000
+	PERF_EVENT_IOC_DISABLE               = 0x2401
+	PERF_EVENT_IOC_ENABLE                = 0x2400
+	PERF_EVENT_IOC_ID                    = 0x80042407
+	PERF_EVENT_IOC_MODIFY_ATTRIBUTES     = 0x4004240b
+	PERF_EVENT_IOC_PAUSE_OUTPUT          = 0x40042409
+	PERF_EVENT_IOC_PERIOD                = 0x40082404
+	PERF_EVENT_IOC_QUERY_BPF             = 0xc004240a
+	PERF_EVENT_IOC_REFRESH               = 0x2402
+	PERF_EVENT_IOC_RESET                 = 0x2403
+	PERF_EVENT_IOC_SET_BPF               = 0x40042408
+	PERF_EVENT_IOC_SET_FILTER            = 0x40042406
+	PERF_EVENT_IOC_SET_OUTPUT            = 0x2405
+	PIPEFS_MAGIC                         = 0x50495045
+	PPPIOCATTACH                         = 0x4004743d
+	PPPIOCATTCHAN                        = 0x40047438
+	PPPIOCCONNECT                        = 0x4004743a
+	PPPIOCDETACH                         = 0x4004743c
+	PPPIOCDISCONN                        = 0x7439
+	PPPIOCGASYNCMAP                      = 0x80047458
+	PPPIOCGCHAN                          = 0x80047437
+	PPPIOCGDEBUG                         = 0x80047441
+	PPPIOCGFLAGS                         = 0x8004745a
+	PPPIOCGIDLE                          = 0x8008743f
+	PPPIOCGL2TPSTATS                     = 0x80487436
+	PPPIOCGMRU                           = 0x80047453
+	PPPIOCGNPMODE                        = 0xc008744c
+	PPPIOCGRASYNCMAP                     = 0x80047455
+	PPPIOCGUNIT                          = 0x80047456
+	PPPIOCGXASYNCMAP                     = 0x80207450
+	PPPIOCNEWUNIT                        = 0xc004743e
+	PPPIOCSACTIVE                        = 0x40087446
+	PPPIOCSASYNCMAP                      = 0x40047457
+	PPPIOCSCOMPRESS                      = 0x400c744d
+	PPPIOCSDEBUG                         = 0x40047440
+	PPPIOCSFLAGS                         = 0x40047459
+	PPPIOCSMAXCID                        = 0x40047451
+	PPPIOCSMRRU                          = 0x4004743b
+	PPPIOCSMRU                           = 0x40047452
+	PPPIOCSNPMODE                        = 0x4008744b
+	PPPIOCSPASS                          = 0x40087447
+	PPPIOCSRASYNCMAP                     = 0x40047454
+	PPPIOCSXASYNCMAP                     = 0x4020744f
+	PPPIOCXFERUNIT                       = 0x744e
+	PRIO_PGRP                            = 0x1
+	PRIO_PROCESS                         = 0x0
+	PRIO_USER                            = 0x2
+	PROC_SUPER_MAGIC                     = 0x9fa0
+	PROT_EXEC                            = 0x4
+	PROT_GROWSDOWN                       = 0x1000000
+	PROT_GROWSUP                         = 0x2000000
+	PROT_NONE                            = 0x0
+	PROT_READ                            = 0x1
+	PROT_WRITE                           = 0x2
+	PR_CAPBSET_DROP                      = 0x18
+	PR_CAPBSET_READ                      = 0x17
+	PR_CAP_AMBIENT                       = 0x2f
+	PR_CAP_AMBIENT_CLEAR_ALL             = 0x4
+	PR_CAP_AMBIENT_IS_SET                = 0x1
+	PR_CAP_AMBIENT_LOWER                 = 0x3
+	PR_CAP_AMBIENT_RAISE                 = 0x2
+	PR_ENDIAN_BIG                        = 0x0
+	PR_ENDIAN_LITTLE                     = 0x1
+	PR_ENDIAN_PPC_LITTLE                 = 0x2
+	PR_FPEMU_NOPRINT                     = 0x1
+	PR_FPEMU_SIGFPE                      = 0x2
+	PR_FP_EXC_ASYNC                      = 0x2
+	PR_FP_EXC_DISABLED                   = 0x0
+	PR_FP_EXC_DIV                        = 0x10000
+	PR_FP_EXC_INV                        = 0x100000
+	PR_FP_EXC_NONRECOV                   = 0x1
+	PR_FP_EXC_OVF                        = 0x20000
+	PR_FP_EXC_PRECISE                    = 0x3
+	PR_FP_EXC_RES                        = 0x80000
+	PR_FP_EXC_SW_ENABLE                  = 0x80
+	PR_FP_EXC_UND                        = 0x40000
+	PR_FP_MODE_FR                        = 0x1
+	PR_FP_MODE_FRE                       = 0x2
+	PR_GET_CHILD_SUBREAPER               = 0x25
+	PR_GET_DUMPABLE                      = 0x3
+	PR_GET_ENDIAN                        = 0x13
+	PR_GET_FPEMU                         = 0x9
+	PR_GET_FPEXC                         = 0xb
+	PR_GET_FP_MODE                       = 0x2e
+	PR_GET_KEEPCAPS                      = 0x7
+	PR_GET_NAME                          = 0x10
+	PR_GET_NO_NEW_PRIVS                  = 0x27
+	PR_GET_PDEATHSIG                     = 0x2
+	PR_GET_SECCOMP                       = 0x15
+	PR_GET_SECUREBITS                    = 0x1b
+	PR_GET_SPECULATION_CTRL              = 0x34
+	PR_GET_THP_DISABLE                   = 0x2a
+	PR_GET_TID_ADDRESS                   = 0x28
+	PR_GET_TIMERSLACK                    = 0x1e
+	PR_GET_TIMING                        = 0xd
+	PR_GET_TSC                           = 0x19
+	PR_GET_UNALIGN                       = 0x5
+	PR_MCE_KILL                          = 0x21
+	PR_MCE_KILL_CLEAR                    = 0x0
+	PR_MCE_KILL_DEFAULT                  = 0x2
+	PR_MCE_KILL_EARLY                    = 0x1
+	PR_MCE_KILL_GET                      = 0x22
+	PR_MCE_KILL_LATE                     = 0x0
+	PR_MCE_KILL_SET                      = 0x1
+	PR_MPX_DISABLE_MANAGEMENT            = 0x2c
+	PR_MPX_ENABLE_MANAGEMENT             = 0x2b
+	PR_PAC_APDAKEY                       = 0x4
+	PR_PAC_APDBKEY                       = 0x8
+	PR_PAC_APGAKEY                       = 0x10
+	PR_PAC_APIAKEY                       = 0x1
+	PR_PAC_APIBKEY                       = 0x2
+	PR_PAC_RESET_KEYS                    = 0x36
+	PR_SET_CHILD_SUBREAPER               = 0x24
+	PR_SET_DUMPABLE                      = 0x4
+	PR_SET_ENDIAN                        = 0x14
+	PR_SET_FPEMU                         = 0xa
+	PR_SET_FPEXC                         = 0xc
+	PR_SET_FP_MODE                       = 0x2d
+	PR_SET_KEEPCAPS                      = 0x8
+	PR_SET_MM                            = 0x23
+	PR_SET_MM_ARG_END                    = 0x9
+	PR_SET_MM_ARG_START                  = 0x8
+	PR_SET_MM_AUXV                       = 0xc
+	PR_SET_MM_BRK                        = 0x7
+	PR_SET_MM_END_CODE                   = 0x2
+	PR_SET_MM_END_DATA                   = 0x4
+	PR_SET_MM_ENV_END                    = 0xb
+	PR_SET_MM_ENV_START                  = 0xa
+	PR_SET_MM_EXE_FILE                   = 0xd
+	PR_SET_MM_MAP                        = 0xe
+	PR_SET_MM_MAP_SIZE                   = 0xf
+	PR_SET_MM_START_BRK                  = 0x6
+	PR_SET_MM_START_CODE                 = 0x1
+	PR_SET_MM_START_DATA                 = 0x3
+	PR_SET_MM_START_STACK                = 0x5
+	PR_SET_NAME                          = 0xf
+	PR_SET_NO_NEW_PRIVS                  = 0x26
+	PR_SET_PDEATHSIG                     = 0x1
+	PR_SET_PTRACER                       = 0x59616d61
+	PR_SET_PTRACER_ANY                   = 0xffffffff
+	PR_SET_SECCOMP                       = 0x16
+	PR_SET_SECUREBITS                    = 0x1c
+	PR_SET_SPECULATION_CTRL              = 0x35
+	PR_SET_THP_DISABLE                   = 0x29
+	PR_SET_TIMERSLACK                    = 0x1d
+	PR_SET_TIMING                        = 0xe
+	PR_SET_TSC                           = 0x1a
+	PR_SET_UNALIGN                       = 0x6
+	PR_SPEC_DISABLE                      = 0x4
+	PR_SPEC_ENABLE                       = 0x2
+	PR_SPEC_FORCE_DISABLE                = 0x8
+	PR_SPEC_INDIRECT_BRANCH              = 0x1
+	PR_SPEC_NOT_AFFECTED                 = 0x0
+	PR_SPEC_PRCTL                        = 0x1
+	PR_SPEC_STORE_BYPASS                 = 0x0
+	PR_SVE_GET_VL                        = 0x33
+	PR_SVE_SET_VL                        = 0x32
+	PR_SVE_SET_VL_ONEXEC                 = 0x40000
+	PR_SVE_VL_INHERIT                    = 0x20000
+	PR_SVE_VL_LEN_MASK                   = 0xffff
+	PR_TASK_PERF_EVENTS_DISABLE          = 0x1f
+	PR_TASK_PERF_EVENTS_ENABLE           = 0x20
+	PR_TIMING_STATISTICAL                = 0x0
+	PR_TIMING_TIMESTAMP                  = 0x1
+	PR_TSC_ENABLE                        = 0x1
+	PR_TSC_SIGSEGV                       = 0x2
+	PR_UNALIGN_NOPRINT                   = 0x1
+	PR_UNALIGN_SIGBUS                    = 0x2
+	PSTOREFS_MAGIC                       = 0x6165676c
+	PTRACE_ATTACH                        = 0x10
+	PTRACE_CONT                          = 0x7
+	PTRACE_DETACH                        = 0x11
+	PTRACE_EVENT_CLONE                   = 0x3
+	PTRACE_EVENT_EXEC                    = 0x4
+	PTRACE_EVENT_EXIT                    = 0x6
+	PTRACE_EVENT_FORK                    = 0x1
+	PTRACE_EVENT_SECCOMP                 = 0x7
+	PTRACE_EVENT_STOP                    = 0x80
+	PTRACE_EVENT_VFORK                   = 0x2
+	PTRACE_EVENT_VFORK_DONE              = 0x5
+	PTRACE_GETCRUNCHREGS                 = 0x19
+	PTRACE_GETEVENTMSG                   = 0x4201
+	PTRACE_GETFDPIC                      = 0x1f
+	PTRACE_GETFDPIC_EXEC                 = 0x0
+	PTRACE_GETFDPIC_INTERP               = 0x1
+	PTRACE_GETFPREGS                     = 0xe
+	PTRACE_GETHBPREGS                    = 0x1d
+	PTRACE_GETREGS                       = 0xc
+	PTRACE_GETREGSET                     = 0x4204
+	PTRACE_GETSIGINFO                    = 0x4202
+	PTRACE_GETSIGMASK                    = 0x420a
+	PTRACE_GETVFPREGS                    = 0x1b
+	PTRACE_GETWMMXREGS                   = 0x12
+	PTRACE_GET_THREAD_AREA               = 0x16
+	PTRACE_INTERRUPT                     = 0x4207
+	PTRACE_KILL                          = 0x8
+	PTRACE_LISTEN                        = 0x4208
+	PTRACE_OLDSETOPTIONS                 = 0x15
+	PTRACE_O_EXITKILL                    = 0x100000
+	PTRACE_O_MASK                        = 0x3000ff
+	PTRACE_O_SUSPEND_SECCOMP             = 0x200000
+	PTRACE_O_TRACECLONE                  = 0x8
+	PTRACE_O_TRACEEXEC                   = 0x10
+	PTRACE_O_TRACEEXIT                   = 0x40
+	PTRACE_O_TRACEFORK                   = 0x2
+	PTRACE_O_TRACESECCOMP                = 0x80
+	PTRACE_O_TRACESYSGOOD                = 0x1
+	PTRACE_O_TRACEVFORK                  = 0x4
+	PTRACE_O_TRACEVFORKDONE              = 0x20
+	PTRACE_PEEKDATA                      = 0x2
+	PTRACE_PEEKSIGINFO                   = 0x4209
+	PTRACE_PEEKSIGINFO_SHARED            = 0x1
+	PTRACE_PEEKTEXT                      = 0x1
+	PTRACE_PEEKUSR                       = 0x3
+	PTRACE_POKEDATA                      = 0x5
+	PTRACE_POKETEXT                      = 0x4
+	PTRACE_POKEUSR                       = 0x6
+	PTRACE_SECCOMP_GET_FILTER            = 0x420c
+	PTRACE_SECCOMP_GET_METADATA          = 0x420d
+	PTRACE_SEIZE                         = 0x4206
+	PTRACE_SETCRUNCHREGS                 = 0x1a
+	PTRACE_SETFPREGS                     = 0xf
+	PTRACE_SETHBPREGS                    = 0x1e
+	PTRACE_SETOPTIONS                    = 0x4200
+	PTRACE_SETREGS                       = 0xd
+	PTRACE_SETREGSET                     = 0x4205
+	PTRACE_SETSIGINFO                    = 0x4203
+	PTRACE_SETSIGMASK                    = 0x420b
+	PTRACE_SETVFPREGS                    = 0x1c
+	PTRACE_SETWMMXREGS                   = 0x13
+	PTRACE_SET_SYSCALL                   = 0x17
+	PTRACE_SINGLESTEP                    = 0x9
+	PTRACE_SYSCALL                       = 0x18
+	PTRACE_TRACEME                       = 0x0
+	PT_DATA_ADDR                         = 0x10004
+	PT_TEXT_ADDR                         = 0x10000
+	PT_TEXT_END_ADDR                     = 0x10008
+	QNX4_SUPER_MAGIC                     = 0x2f
+	QNX6_SUPER_MAGIC                     = 0x68191122
+	RAMFS_MAGIC                          = 0x858458f6
+	RDTGROUP_SUPER_MAGIC                 = 0x7655821
+	REISERFS_SUPER_MAGIC                 = 0x52654973
+	RENAME_EXCHANGE                      = 0x2
+	RENAME_NOREPLACE                     = 0x1
+	RENAME_WHITEOUT                      = 0x4
+	RLIMIT_AS                            = 0x9
+	RLIMIT_CORE                          = 0x4
+	RLIMIT_CPU                           = 0x0
+	RLIMIT_DATA                          = 0x2
+	RLIMIT_FSIZE                         = 0x1
+	RLIMIT_LOCKS                         = 0xa
+	RLIMIT_MEMLOCK                       = 0x8
+	RLIMIT_MSGQUEUE                      = 0xc
+	RLIMIT_NICE                          = 0xd
+	RLIMIT_NOFILE                        = 0x7
+	RLIMIT_NPROC                         = 0x6
+	RLIMIT_RSS                           = 0x5
+	RLIMIT_RTPRIO                        = 0xe
+	RLIMIT_RTTIME                        = 0xf
+	RLIMIT_SIGPENDING                    = 0xb
+	RLIMIT_STACK                         = 0x3
+	RLIM_INFINITY                        = 0xffffffffffffffff
+	RNDADDENTROPY                        = 0x40085203
+	RNDADDTOENTCNT                       = 0x40045201
+	RNDCLEARPOOL                         = 0x5206
+	RNDGETENTCNT                         = 0x80045200
+	RNDGETPOOL                           = 0x80085202
+	RNDRESEEDCRNG                        = 0x5207
+	RNDZAPENTCNT                         = 0x5204
+	RTAX_ADVMSS                          = 0x8
+	RTAX_CC_ALGO                         = 0x10
+	RTAX_CWND                            = 0x7
+	RTAX_FASTOPEN_NO_COOKIE              = 0x11
+	RTAX_FEATURES                        = 0xc
+	RTAX_FEATURE_ALLFRAG                 = 0x8
+	RTAX_FEATURE_ECN                     = 0x1
+	RTAX_FEATURE_MASK                    = 0xf
+	RTAX_FEATURE_SACK                    = 0x2
+	RTAX_FEATURE_TIMESTAMP               = 0x4
+	RTAX_HOPLIMIT                        = 0xa
+	RTAX_INITCWND                        = 0xb
+	RTAX_INITRWND                        = 0xe
+	RTAX_LOCK                            = 0x1
+	RTAX_MAX                             = 0x11
+	RTAX_MTU                             = 0x2
+	RTAX_QUICKACK                        = 0xf
+	RTAX_REORDERING                      = 0x9
+	RTAX_RTO_MIN                         = 0xd
+	RTAX_RTT                             = 0x4
+	RTAX_RTTVAR                          = 0x5
+	RTAX_SSTHRESH                        = 0x6
+	RTAX_UNSPEC                          = 0x0
+	RTAX_WINDOW                          = 0x3
+	RTA_ALIGNTO                          = 0x4
+	RTA_MAX                              = 0x1d
+	RTCF_DIRECTSRC                       = 0x4000000
+	RTCF_DOREDIRECT                      = 0x1000000
+	RTCF_LOG                             = 0x2000000
+	RTCF_MASQ                            = 0x400000
+	RTCF_NAT                             = 0x800000
+	RTCF_VALVE                           = 0x200000
+	RTC_AF                               = 0x20
+	RTC_AIE_OFF                          = 0x7002
+	RTC_AIE_ON                           = 0x7001
+	RTC_ALM_READ                         = 0x80247008
+	RTC_ALM_SET                          = 0x40247007
+	RTC_EPOCH_READ                       = 0x8004700d
+	RTC_EPOCH_SET                        = 0x4004700e
+	RTC_IRQF                             = 0x80
+	RTC_IRQP_READ                        = 0x8004700b
+	RTC_IRQP_SET                         = 0x4004700c
+	RTC_MAX_FREQ                         = 0x2000
+	RTC_PF                               = 0x40
+	RTC_PIE_OFF                          = 0x7006
+	RTC_PIE_ON                           = 0x7005
+	RTC_PLL_GET                          = 0x801c7011
+	RTC_PLL_SET                          = 0x401c7012
+	RTC_RD_TIME                          = 0x80247009
+	RTC_SET_TIME                         = 0x4024700a
+	RTC_UF                               = 0x10
+	RTC_UIE_OFF                          = 0x7004
+	RTC_UIE_ON                           = 0x7003
+	RTC_VL_CLR                           = 0x7014
+	RTC_VL_READ                          = 0x80047013
+	RTC_WIE_OFF                          = 0x7010
+	RTC_WIE_ON                           = 0x700f
+	RTC_WKALM_RD                         = 0x80287010
+	RTC_WKALM_SET                        = 0x4028700f
+	RTF_ADDRCLASSMASK                    = 0xf8000000
+	RTF_ADDRCONF                         = 0x40000
+	RTF_ALLONLINK                        = 0x20000
+	RTF_BROADCAST                        = 0x10000000
+	RTF_CACHE                            = 0x1000000
+	RTF_DEFAULT                          = 0x10000
+	RTF_DYNAMIC                          = 0x10
+	RTF_FLOW                             = 0x2000000
+	RTF_GATEWAY                          = 0x2
+	RTF_HOST                             = 0x4
+	RTF_INTERFACE                        = 0x40000000
+	RTF_IRTT                             = 0x100
+	RTF_LINKRT                           = 0x100000
+	RTF_LOCAL                            = 0x80000000
+	RTF_MODIFIED                         = 0x20
+	RTF_MSS                              = 0x40
+	RTF_MTU                              = 0x40
+	RTF_MULTICAST                        = 0x20000000
+	RTF_NAT                              = 0x8000000
+	RTF_NOFORWARD                        = 0x1000
+	RTF_NONEXTHOP                        = 0x200000
+	RTF_NOPMTUDISC                       = 0x4000
+	RTF_POLICY                           = 0x4000000
+	RTF_REINSTATE                        = 0x8
+	RTF_REJECT                           = 0x200
+	RTF_STATIC                           = 0x400
+	RTF_THROW                            = 0x2000
+	RTF_UP                               = 0x1
+	RTF_WINDOW                           = 0x80
+	RTF_XRESOLVE                         = 0x800
+	RTM_BASE                             = 0x10
+	RTM_DELACTION                        = 0x31
+	RTM_DELADDR                          = 0x15
+	RTM_DELADDRLABEL                     = 0x49
+	RTM_DELCHAIN                         = 0x65
+	RTM_DELLINK                          = 0x11
+	RTM_DELMDB                           = 0x55
+	RTM_DELNEIGH                         = 0x1d
+	RTM_DELNETCONF                       = 0x51
+	RTM_DELNSID                          = 0x59
+	RTM_DELQDISC                         = 0x25
+	RTM_DELROUTE                         = 0x19
+	RTM_DELRULE                          = 0x21
+	RTM_DELTCLASS                        = 0x29
+	RTM_DELTFILTER                       = 0x2d
+	RTM_F_CLONED                         = 0x200
+	RTM_F_EQUALIZE                       = 0x400
+	RTM_F_FIB_MATCH                      = 0x2000
+	RTM_F_LOOKUP_TABLE                   = 0x1000
+	RTM_F_NOTIFY                         = 0x100
+	RTM_F_PREFIX                         = 0x800
+	RTM_GETACTION                        = 0x32
+	RTM_GETADDR                          = 0x16
+	RTM_GETADDRLABEL                     = 0x4a
+	RTM_GETANYCAST                       = 0x3e
+	RTM_GETCHAIN                         = 0x66
+	RTM_GETDCB                           = 0x4e
+	RTM_GETLINK                          = 0x12
+	RTM_GETMDB                           = 0x56
+	RTM_GETMULTICAST                     = 0x3a
+	RTM_GETNEIGH                         = 0x1e
+	RTM_GETNEIGHTBL                      = 0x42
+	RTM_GETNETCONF                       = 0x52
+	RTM_GETNSID                          = 0x5a
+	RTM_GETQDISC                         = 0x26
+	RTM_GETROUTE                         = 0x1a
+	RTM_GETRULE                          = 0x22
+	RTM_GETSTATS                         = 0x5e
+	RTM_GETTCLASS                        = 0x2a
+	RTM_GETTFILTER                       = 0x2e
+	RTM_MAX                              = 0x67
+	RTM_NEWACTION                        = 0x30
+	RTM_NEWADDR                          = 0x14
+	RTM_NEWADDRLABEL                     = 0x48
+	RTM_NEWCACHEREPORT                   = 0x60
+	RTM_NEWCHAIN                         = 0x64
+	RTM_NEWLINK                          = 0x10
+	RTM_NEWMDB                           = 0x54
+	RTM_NEWNDUSEROPT                     = 0x44
+	RTM_NEWNEIGH                         = 0x1c
+	RTM_NEWNEIGHTBL                      = 0x40
+	RTM_NEWNETCONF                       = 0x50
+	RTM_NEWNSID                          = 0x58
+	RTM_NEWPREFIX                        = 0x34
+	RTM_NEWQDISC                         = 0x24
+	RTM_NEWROUTE                         = 0x18
+	RTM_NEWRULE                          = 0x20
+	RTM_NEWSTATS                         = 0x5c
+	RTM_NEWTCLASS                        = 0x28
+	RTM_NEWTFILTER                       = 0x2c
+	RTM_NR_FAMILIES                      = 0x16
+	RTM_NR_MSGTYPES                      = 0x58
+	RTM_SETDCB                           = 0x4f
+	RTM_SETLINK                          = 0x13
+	RTM_SETNEIGHTBL                      = 0x43
+	RTNH_ALIGNTO                         = 0x4
+	RTNH_COMPARE_MASK                    = 0x19
+	RTNH_F_DEAD                          = 0x1
+	RTNH_F_LINKDOWN                      = 0x10
+	RTNH_F_OFFLOAD                       = 0x8
+	RTNH_F_ONLINK                        = 0x4
+	RTNH_F_PERVASIVE                     = 0x2
+	RTNH_F_UNRESOLVED                    = 0x20
+	RTN_MAX                              = 0xb
+	RTPROT_BABEL                         = 0x2a
+	RTPROT_BGP                           = 0xba
+	RTPROT_BIRD                          = 0xc
+	RTPROT_BOOT                          = 0x3
+	RTPROT_DHCP                          = 0x10
+	RTPROT_DNROUTED                      = 0xd
+	RTPROT_EIGRP                         = 0xc0
+	RTPROT_GATED                         = 0x8
+	RTPROT_ISIS                          = 0xbb
+	RTPROT_KERNEL                        = 0x2
+	RTPROT_MROUTED                       = 0x11
+	RTPROT_MRT                           = 0xa
+	RTPROT_NTK                           = 0xf
+	RTPROT_OSPF                          = 0xbc
+	RTPROT_RA                            = 0x9
+	RTPROT_REDIRECT                      = 0x1
+	RTPROT_RIP                           = 0xbd
+	RTPROT_STATIC                        = 0x4
+	RTPROT_UNSPEC                        = 0x0
+	RTPROT_XORP                          = 0xe
+	RTPROT_ZEBRA                         = 0xb
+	RT_CLASS_DEFAULT                     = 0xfd
+	RT_CLASS_LOCAL                       = 0xff
+	RT_CLASS_MAIN                        = 0xfe
+	RT_CLASS_MAX                         = 0xff
+	RT_CLASS_UNSPEC                      = 0x0
+	RUSAGE_CHILDREN                      = -0x1
+	RUSAGE_SELF                          = 0x0
+	RUSAGE_THREAD                        = 0x1
+	SCM_CREDENTIALS                      = 0x2
+	SCM_RIGHTS                           = 0x1
+	SCM_TIMESTAMP                        = 0x1d
+	SCM_TIMESTAMPING                     = 0x25
+	SCM_TIMESTAMPING_OPT_STATS           = 0x36
+	SCM_TIMESTAMPING_PKTINFO             = 0x3a
+	SCM_TIMESTAMPNS                      = 0x23
+	SCM_TXTIME                           = 0x3d
+	SCM_WIFI_STATUS                      = 0x29
+	SC_LOG_FLUSH                         = 0x100000
+	SECCOMP_MODE_DISABLED                = 0x0
+	SECCOMP_MODE_FILTER                  = 0x2
+	SECCOMP_MODE_STRICT                  = 0x1
+	SECURITYFS_MAGIC                     = 0x73636673
+	SELINUX_MAGIC                        = 0xf97cff8c
+	SFD_CLOEXEC                          = 0x80000
+	SFD_NONBLOCK                         = 0x800
+	SHUT_RD                              = 0x0
+	SHUT_RDWR                            = 0x2
+	SHUT_WR                              = 0x1
+	SIOCADDDLCI                          = 0x8980
+	SIOCADDMULTI                         = 0x8931
+	SIOCADDRT                            = 0x890b
+	SIOCATMARK                           = 0x8905
+	SIOCBONDCHANGEACTIVE                 = 0x8995
+	SIOCBONDENSLAVE                      = 0x8990
+	SIOCBONDINFOQUERY                    = 0x8994
+	SIOCBONDRELEASE                      = 0x8991
+	SIOCBONDSETHWADDR                    = 0x8992
+	SIOCBONDSLAVEINFOQUERY               = 0x8993
+	SIOCBRADDBR                          = 0x89a0
+	SIOCBRADDIF                          = 0x89a2
+	SIOCBRDELBR                          = 0x89a1
+	SIOCBRDELIF                          = 0x89a3
+	SIOCDARP                             = 0x8953
+	SIOCDELDLCI                          = 0x8981
+	SIOCDELMULTI                         = 0x8932
+	SIOCDELRT                            = 0x890c
+	SIOCDEVPRIVATE                       = 0x89f0
+	SIOCDIFADDR                          = 0x8936
+	SIOCDRARP                            = 0x8960
+	SIOCETHTOOL                          = 0x8946
+	SIOCGARP                             = 0x8954
+	SIOCGHWTSTAMP                        = 0x89b1
+	SIOCGIFADDR                          = 0x8915
+	SIOCGIFBR                            = 0x8940
+	SIOCGIFBRDADDR                       = 0x8919
+	SIOCGIFCONF                          = 0x8912
+	SIOCGIFCOUNT                         = 0x8938
+	SIOCGIFDSTADDR                       = 0x8917
+	SIOCGIFENCAP                         = 0x8925
+	SIOCGIFFLAGS                         = 0x8913
+	SIOCGIFHWADDR                        = 0x8927
+	SIOCGIFINDEX                         = 0x8933
+	SIOCGIFMAP                           = 0x8970
+	SIOCGIFMEM                           = 0x891f
+	SIOCGIFMETRIC                        = 0x891d
+	SIOCGIFMTU                           = 0x8921
+	SIOCGIFNAME                          = 0x8910
+	SIOCGIFNETMASK                       = 0x891b
+	SIOCGIFPFLAGS                        = 0x8935
+	SIOCGIFSLAVE                         = 0x8929
+	SIOCGIFTXQLEN                        = 0x8942
+	SIOCGIFVLAN                          = 0x8982
+	SIOCGMIIPHY                          = 0x8947
+	SIOCGMIIREG                          = 0x8948
+	SIOCGPGRP                            = 0x8904
+	SIOCGPPPCSTATS                       = 0x89f2
+	SIOCGPPPSTATS                        = 0x89f0
+	SIOCGPPPVER                          = 0x89f1
+	SIOCGRARP                            = 0x8961
+	SIOCGSKNS                            = 0x894c
+	SIOCGSTAMP                           = 0x8906
+	SIOCGSTAMPNS                         = 0x8907
+	SIOCINQ                              = 0x541b
+	SIOCOUTQ                             = 0x5411
+	SIOCOUTQNSD                          = 0x894b
+	SIOCPROTOPRIVATE                     = 0x89e0
+	SIOCRTMSG                            = 0x890d
+	SIOCSARP                             = 0x8955
+	SIOCSHWTSTAMP                        = 0x89b0
+	SIOCSIFADDR                          = 0x8916
+	SIOCSIFBR                            = 0x8941
+	SIOCSIFBRDADDR                       = 0x891a
+	SIOCSIFDSTADDR                       = 0x8918
+	SIOCSIFENCAP                         = 0x8926
+	SIOCSIFFLAGS                         = 0x8914
+	SIOCSIFHWADDR                        = 0x8924
+	SIOCSIFHWBROADCAST                   = 0x8937
+	SIOCSIFLINK                          = 0x8911
+	SIOCSIFMAP                           = 0x8971
+	SIOCSIFMEM                           = 0x8920
+	SIOCSIFMETRIC                        = 0x891e
+	SIOCSIFMTU                           = 0x8922
+	SIOCSIFNAME                          = 0x8923
+	SIOCSIFNETMASK                       = 0x891c
+	SIOCSIFPFLAGS                        = 0x8934
+	SIOCSIFSLAVE                         = 0x8930
+	SIOCSIFTXQLEN                        = 0x8943
+	SIOCSIFVLAN                          = 0x8983
+	SIOCSMIIREG                          = 0x8949
+	SIOCSPGRP                            = 0x8902
+	SIOCSRARP                            = 0x8962
+	SIOCWANDEV                           = 0x894a
+	SMACK_MAGIC                          = 0x43415d53
+	SMART_AUTOSAVE                       = 0xd2
+	SMART_AUTO_OFFLINE                   = 0xdb
+	SMART_DISABLE                        = 0xd9
+	SMART_ENABLE                         = 0xd8
+	SMART_HCYL_PASS                      = 0xc2
+	SMART_IMMEDIATE_OFFLINE              = 0xd4
+	SMART_LCYL_PASS                      = 0x4f
+	SMART_READ_LOG_SECTOR                = 0xd5
+	SMART_READ_THRESHOLDS                = 0xd1
+	SMART_READ_VALUES                    = 0xd0
+	SMART_SAVE                           = 0xd3
+	SMART_STATUS                         = 0xda
+	SMART_WRITE_LOG_SECTOR               = 0xd6
+	SMART_WRITE_THRESHOLDS               = 0xd7
+	SMB_SUPER_MAGIC                      = 0x517b
+	SOCKFS_MAGIC                         = 0x534f434b
+	SOCK_CLOEXEC                         = 0x80000
+	SOCK_DCCP                            = 0x6
+	SOCK_DGRAM                           = 0x2
+	SOCK_IOC_TYPE                        = 0x89
+	SOCK_NONBLOCK                        = 0x800
+	SOCK_PACKET                          = 0xa
+	SOCK_RAW                             = 0x3
+	SOCK_RDM                             = 0x4
+	SOCK_SEQPACKET                       = 0x5
+	SOCK_STREAM                          = 0x1
+	SOL_AAL                              = 0x109
+	SOL_ALG                              = 0x117
+	SOL_ATM                              = 0x108
+	SOL_CAIF                             = 0x116
+	SOL_CAN_BASE                         = 0x64
+	SOL_DCCP                             = 0x10d
+	SOL_DECNET                           = 0x105
+	SOL_ICMPV6                           = 0x3a
+	SOL_IP                               = 0x0
+	SOL_IPV6                             = 0x29
+	SOL_IRDA                             = 0x10a
+	SOL_IUCV                             = 0x115
+	SOL_KCM                              = 0x119
+	SOL_LLC                              = 0x10c
+	SOL_NETBEUI                          = 0x10b
+	SOL_NETLINK                          = 0x10e
+	SOL_NFC                              = 0x118
+	SOL_PACKET                           = 0x107
+	SOL_PNPIPE                           = 0x113
+	SOL_PPPOL2TP                         = 0x111
+	SOL_RAW                              = 0xff
+	SOL_RDS                              = 0x114
+	SOL_RXRPC                            = 0x110
+	SOL_SOCKET                           = 0x1
+	SOL_TCP                              = 0x6
+	SOL_TIPC                             = 0x10f
+	SOL_TLS                              = 0x11a
+	SOL_X25                              = 0x106
+	SOL_XDP                              = 0x11b
+	SOMAXCONN                            = 0x80
+	SO_ACCEPTCONN                        = 0x1e
+	SO_ATTACH_BPF                        = 0x32
+	SO_ATTACH_FILTER                     = 0x1a
+	SO_ATTACH_REUSEPORT_CBPF             = 0x33
+	SO_ATTACH_REUSEPORT_EBPF             = 0x34
+	SO_BINDTODEVICE                      = 0x19
+	SO_BPF_EXTENSIONS                    = 0x30
+	SO_BROADCAST                         = 0x6
+	SO_BSDCOMPAT                         = 0xe
+	SO_BUSY_POLL                         = 0x2e
+	SO_CNX_ADVICE                        = 0x35
+	SO_COOKIE                            = 0x39
+	SO_DEBUG                             = 0x1
+	SO_DETACH_BPF                        = 0x1b
+	SO_DETACH_FILTER                     = 0x1b
+	SO_DOMAIN                            = 0x27
+	SO_DONTROUTE                         = 0x5
+	SO_EE_CODE_TXTIME_INVALID_PARAM      = 0x1
+	SO_EE_CODE_TXTIME_MISSED             = 0x2
+	SO_EE_CODE_ZEROCOPY_COPIED           = 0x1
+	SO_EE_ORIGIN_ICMP                    = 0x2
+	SO_EE_ORIGIN_ICMP6                   = 0x3
+	SO_EE_ORIGIN_LOCAL                   = 0x1
+	SO_EE_ORIGIN_NONE                    = 0x0
+	SO_EE_ORIGIN_TIMESTAMPING            = 0x4
+	SO_EE_ORIGIN_TXSTATUS                = 0x4
+	SO_EE_ORIGIN_TXTIME                  = 0x6
+	SO_EE_ORIGIN_ZEROCOPY                = 0x5
+	SO_ERROR                             = 0x4
+	SO_GET_FILTER                        = 0x1a
+	SO_INCOMING_CPU                      = 0x31
+	SO_INCOMING_NAPI_ID                  = 0x38
+	SO_KEEPALIVE                         = 0x9
+	SO_LINGER                            = 0xd
+	SO_LOCK_FILTER                       = 0x2c
+	SO_MARK                              = 0x24
+	SO_MAX_PACING_RATE                   = 0x2f
+	SO_MEMINFO                           = 0x37
+	SO_NOFCS                             = 0x2b
+	SO_NO_CHECK                          = 0xb
+	SO_OOBINLINE                         = 0xa
+	SO_PASSCRED                          = 0x10
+	SO_PASSSEC                           = 0x22
+	SO_PEEK_OFF                          = 0x2a
+	SO_PEERCRED                          = 0x11
+	SO_PEERGROUPS                        = 0x3b
+	SO_PEERNAME                          = 0x1c
+	SO_PEERSEC                           = 0x1f
+	SO_PRIORITY                          = 0xc
+	SO_PROTOCOL                          = 0x26
+	SO_RCVBUF                            = 0x8
+	SO_RCVBUFFORCE                       = 0x21
+	SO_RCVLOWAT                          = 0x12
+	SO_RCVTIMEO                          = 0x14
+	SO_REUSEADDR                         = 0x2
+	SO_REUSEPORT                         = 0xf
+	SO_RXQ_OVFL                          = 0x28
+	SO_SECURITY_AUTHENTICATION           = 0x16
+	SO_SECURITY_ENCRYPTION_NETWORK       = 0x18
+	SO_SECURITY_ENCRYPTION_TRANSPORT     = 0x17
+	SO_SELECT_ERR_QUEUE                  = 0x2d
+	SO_SNDBUF                            = 0x7
+	SO_SNDBUFFORCE                       = 0x20
+	SO_SNDLOWAT                          = 0x13
+	SO_SNDTIMEO                          = 0x15
+	SO_TIMESTAMP                         = 0x1d
+	SO_TIMESTAMPING                      = 0x25
+	SO_TIMESTAMPNS                       = 0x23
+	SO_TXTIME                            = 0x3d
+	SO_TYPE                              = 0x3
+	SO_VM_SOCKETS_BUFFER_MAX_SIZE        = 0x2
+	SO_VM_SOCKETS_BUFFER_MIN_SIZE        = 0x1
+	SO_VM_SOCKETS_BUFFER_SIZE            = 0x0
+	SO_VM_SOCKETS_CONNECT_TIMEOUT        = 0x6
+	SO_VM_SOCKETS_NONBLOCK_TXRX          = 0x7
+	SO_VM_SOCKETS_PEER_HOST_VM_ID        = 0x3
+	SO_VM_SOCKETS_TRUSTED                = 0x5
+	SO_WIFI_STATUS                       = 0x29
+	SO_ZEROCOPY                          = 0x3c
+	SPLICE_F_GIFT                        = 0x8
+	SPLICE_F_MORE                        = 0x4
+	SPLICE_F_MOVE                        = 0x1
+	SPLICE_F_NONBLOCK                    = 0x2
+	SQUASHFS_MAGIC                       = 0x73717368
+	STACK_END_MAGIC                      = 0x57ac6e9d
+	STATX_ALL                            = 0xfff
+	STATX_ATIME                          = 0x20
+	STATX_ATTR_APPEND                    = 0x20
+	STATX_ATTR_AUTOMOUNT                 = 0x1000
+	STATX_ATTR_COMPRESSED                = 0x4
+	STATX_ATTR_ENCRYPTED                 = 0x800
+	STATX_ATTR_IMMUTABLE                 = 0x10
+	STATX_ATTR_NODUMP                    = 0x40
+	STATX_BASIC_STATS                    = 0x7ff
+	STATX_BLOCKS                         = 0x400
+	STATX_BTIME                          = 0x800
+	STATX_CTIME                          = 0x80
+	STATX_GID                            = 0x10
+	STATX_INO                            = 0x100
+	STATX_MODE                           = 0x2
+	STATX_MTIME                          = 0x40
+	STATX_NLINK                          = 0x4
+	STATX_SIZE                           = 0x200
+	STATX_TYPE                           = 0x1
+	STATX_UID                            = 0x8
+	STATX__RESERVED                      = 0x80000000
+	SYNC_FILE_RANGE_WAIT_AFTER           = 0x4
+	SYNC_FILE_RANGE_WAIT_BEFORE          = 0x1
+	SYNC_FILE_RANGE_WRITE                = 0x2
+	SYSFS_MAGIC                          = 0x62656572
+	S_BLKSIZE                            = 0x200
+	S_IEXEC                              = 0x40
+	S_IFBLK                              = 0x6000
+	S_IFCHR                              = 0x2000
+	S_IFDIR                              = 0x4000
+	S_IFIFO                              = 0x1000
+	S_IFLNK                              = 0xa000
+	S_IFMT                               = 0xf000
+	S_IFREG                              = 0x8000
+	S_IFSOCK                             = 0xc000
+	S_IREAD                              = 0x100
+	S_IRGRP                              = 0x20
+	S_IROTH                              = 0x4
+	S_IRUSR                              = 0x100
+	S_IRWXG                              = 0x38
+	S_IRWXO                              = 0x7
+	S_IRWXU                              = 0x1c0
+	S_ISGID                              = 0x400
+	S_ISUID                              = 0x800
+	S_ISVTX                              = 0x200
+	S_IWGRP                              = 0x10
+	S_IWOTH                              = 0x2
+	S_IWRITE                             = 0x80
+	S_IWUSR                              = 0x80
+	S_IXGRP                              = 0x8
+	S_IXOTH                              = 0x1
+	S_IXUSR                              = 0x40
+	TAB0                                 = 0x0
+	TAB1                                 = 0x800
+	TAB2                                 = 0x1000
+	TAB3                                 = 0x1800
+	TABDLY                               = 0x1800
+	TASKSTATS_CMD_ATTR_MAX               = 0x4
+	TASKSTATS_CMD_MAX                    = 0x2
+	TASKSTATS_GENL_NAME                  = "TASKSTATS"
+	TASKSTATS_GENL_VERSION               = 0x1
+	TASKSTATS_TYPE_MAX                   = 0x6
+	TASKSTATS_VERSION                    = 0x9
+	TCFLSH                               = 0x540b
+	TCGETA                               = 0x5405
+	TCGETS                               = 0x5401
+	TCGETS2                              = 0x802c542a
+	TCGETX                               = 0x5432
+	TCIFLUSH                             = 0x0
+	TCIOFF                               = 0x2
+	TCIOFLUSH                            = 0x2
+	TCION                                = 0x3
+	TCOFLUSH                             = 0x1
+	TCOOFF                               = 0x0
+	TCOON                                = 0x1
+	TCP_CC_INFO                          = 0x1a
+	TCP_CM_INQ                           = 0x24
+	TCP_CONGESTION                       = 0xd
+	TCP_COOKIE_IN_ALWAYS                 = 0x1
+	TCP_COOKIE_MAX                       = 0x10
+	TCP_COOKIE_MIN                       = 0x8
+	TCP_COOKIE_OUT_NEVER                 = 0x2
+	TCP_COOKIE_PAIR_SIZE                 = 0x20
+	TCP_COOKIE_TRANSACTIONS              = 0xf
+	TCP_CORK                             = 0x3
+	TCP_DEFER_ACCEPT                     = 0x9
+	TCP_FASTOPEN                         = 0x17
+	TCP_FASTOPEN_CONNECT                 = 0x1e
+	TCP_FASTOPEN_KEY                     = 0x21
+	TCP_FASTOPEN_NO_COOKIE               = 0x22
+	TCP_INFO                             = 0xb
+	TCP_INQ                              = 0x24
+	TCP_KEEPCNT                          = 0x6
+	TCP_KEEPIDLE                         = 0x4
+	TCP_KEEPINTVL                        = 0x5
+	TCP_LINGER2                          = 0x8
+	TCP_MAXSEG                           = 0x2
+	TCP_MAXWIN                           = 0xffff
+	TCP_MAX_WINSHIFT                     = 0xe
+	TCP_MD5SIG                           = 0xe
+	TCP_MD5SIG_EXT                       = 0x20
+	TCP_MD5SIG_FLAG_PREFIX               = 0x1
+	TCP_MD5SIG_MAXKEYLEN                 = 0x50
+	TCP_MSS                              = 0x200
+	TCP_MSS_DEFAULT                      = 0x218
+	TCP_MSS_DESIRED                      = 0x4c4
+	TCP_NODELAY                          = 0x1
+	TCP_NOTSENT_LOWAT                    = 0x19
+	TCP_QUEUE_SEQ                        = 0x15
+	TCP_QUICKACK                         = 0xc
+	TCP_REPAIR                           = 0x13
+	TCP_REPAIR_OFF                       = 0x0
+	TCP_REPAIR_OFF_NO_WP                 = -0x1
+	TCP_REPAIR_ON                        = 0x1
+	TCP_REPAIR_OPTIONS                   = 0x16
+	TCP_REPAIR_QUEUE                     = 0x14
+	TCP_REPAIR_WINDOW                    = 0x1d
+	TCP_SAVED_SYN                        = 0x1c
+	TCP_SAVE_SYN                         = 0x1b
+	TCP_SYNCNT                           = 0x7
+	TCP_S_DATA_IN                        = 0x4
+	TCP_S_DATA_OUT                       = 0x8
+	TCP_THIN_DUPACK                      = 0x11
+	TCP_THIN_LINEAR_TIMEOUTS             = 0x10
+	TCP_TIMESTAMP                        = 0x18
+	TCP_ULP                              = 0x1f
+	TCP_USER_TIMEOUT                     = 0x12
+	TCP_WINDOW_CLAMP                     = 0xa
+	TCP_ZEROCOPY_RECEIVE                 = 0x23
+	TCSAFLUSH                            = 0x2
+	TCSBRK                               = 0x5409
+	TCSBRKP                              = 0x5425
+	TCSETA                               = 0x5406
+	TCSETAF                              = 0x5408
+	TCSETAW                              = 0x5407
+	TCSETS                               = 0x5402
+	TCSETS2                              = 0x402c542b
+	TCSETSF                              = 0x5404
+	TCSETSF2                             = 0x402c542d
+	TCSETSW                              = 0x5403
+	TCSETSW2                             = 0x402c542c
+	TCSETX                               = 0x5433
+	TCSETXF                              = 0x5434
+	TCSETXW                              = 0x5435
+	TCXONC                               = 0x540a
+	TIMER_ABSTIME                        = 0x1
+	TIOCCBRK                             = 0x5428
+	TIOCCONS                             = 0x541d
+	TIOCEXCL                             = 0x540c
+	TIOCGDEV                             = 0x80045432
+	TIOCGETD                             = 0x5424
+	TIOCGEXCL                            = 0x80045440
+	TIOCGICOUNT                          = 0x545d
+	TIOCGISO7816                         = 0x80285442
+	TIOCGLCKTRMIOS                       = 0x5456
+	TIOCGPGRP                            = 0x540f
+	TIOCGPKT                             = 0x80045438
+	TIOCGPTLCK                           = 0x80045439
+	TIOCGPTN                             = 0x80045430
+	TIOCGPTPEER                          = 0x5441
+	TIOCGRS485                           = 0x542e
+	TIOCGSERIAL                          = 0x541e
+	TIOCGSID                             = 0x5429
+	TIOCGSOFTCAR                         = 0x5419
+	TIOCGWINSZ                           = 0x5413
+	TIOCINQ                              = 0x541b
+	TIOCLINUX                            = 0x541c
+	TIOCMBIC                             = 0x5417
+	TIOCMBIS                             = 0x5416
+	TIOCMGET                             = 0x5415
+	TIOCMIWAIT                           = 0x545c
+	TIOCMSET                             = 0x5418
+	TIOCM_CAR                            = 0x40
+	TIOCM_CD                             = 0x40
+	TIOCM_CTS                            = 0x20
+	TIOCM_DSR                            = 0x100
+	TIOCM_DTR                            = 0x2
+	TIOCM_LE                             = 0x1
+	TIOCM_RI                             = 0x80
+	TIOCM_RNG                            = 0x80
+	TIOCM_RTS                            = 0x4
+	TIOCM_SR                             = 0x10
+	TIOCM_ST                             = 0x8
+	TIOCNOTTY                            = 0x5422
+	TIOCNXCL                             = 0x540d
+	TIOCOUTQ                             = 0x5411
+	TIOCPKT                              = 0x5420
+	TIOCPKT_DATA                         = 0x0
+	TIOCPKT_DOSTOP                       = 0x20
+	TIOCPKT_FLUSHREAD                    = 0x1
+	TIOCPKT_FLUSHWRITE                   = 0x2
+	TIOCPKT_IOCTL                        = 0x40
+	TIOCPKT_NOSTOP                       = 0x10
+	TIOCPKT_START                        = 0x8
+	TIOCPKT_STOP                         = 0x4
+	TIOCSBRK                             = 0x5427
+	TIOCSCTTY                            = 0x540e
+	TIOCSERCONFIG                        = 0x5453
+	TIOCSERGETLSR                        = 0x5459
+	TIOCSERGETMULTI                      = 0x545a
+	TIOCSERGSTRUCT                       = 0x5458
+	TIOCSERGWILD                         = 0x5454
+	TIOCSERSETMULTI                      = 0x545b
+	TIOCSERSWILD                         = 0x5455
+	TIOCSER_TEMT                         = 0x1
+	TIOCSETD                             = 0x5423
+	TIOCSIG                              = 0x40045436
+	TIOCSISO7816                         = 0xc0285443
+	TIOCSLCKTRMIOS                       = 0x5457
+	TIOCSPGRP                            = 0x5410
+	TIOCSPTLCK                           = 0x40045431
+	TIOCSRS485                           = 0x542f
+	TIOCSSERIAL                          = 0x541f
+	TIOCSSOFTCAR                         = 0x541a
+	TIOCSTI                              = 0x5412
+	TIOCSWINSZ                           = 0x5414
+	TIOCVHANGUP                          = 0x5437
+	TMPFS_MAGIC                          = 0x1021994
+	TOSTOP                               = 0x100
+	TPACKET_ALIGNMENT                    = 0x10
+	TPACKET_HDRLEN                       = 0x34
+	TP_STATUS_AVAILABLE                  = 0x0
+	TP_STATUS_BLK_TMO                    = 0x20
+	TP_STATUS_COPY                       = 0x2
+	TP_STATUS_CSUMNOTREADY               = 0x8
+	TP_STATUS_CSUM_VALID                 = 0x80
+	TP_STATUS_KERNEL                     = 0x0
+	TP_STATUS_LOSING                     = 0x4
+	TP_STATUS_SENDING                    = 0x2
+	TP_STATUS_SEND_REQUEST               = 0x1
+	TP_STATUS_TS_RAW_HARDWARE            = -0x80000000
+	TP_STATUS_TS_SOFTWARE                = 0x20000000
+	TP_STATUS_TS_SYS_HARDWARE            = 0x40000000
+	TP_STATUS_USER                       = 0x1
+	TP_STATUS_VLAN_TPID_VALID            = 0x40
+	TP_STATUS_VLAN_VALID                 = 0x10
+	TP_STATUS_WRONG_FORMAT               = 0x4
+	TRACEFS_MAGIC                        = 0x74726163
+	TS_COMM_LEN                          = 0x20
+	TUNATTACHFILTER                      = 0x400854d5
+	TUNDETACHFILTER                      = 0x400854d6
+	TUNGETFEATURES                       = 0x800454cf
+	TUNGETFILTER                         = 0x800854db
+	TUNGETIFF                            = 0x800454d2
+	TUNGETSNDBUF                         = 0x800454d3
+	TUNGETVNETBE                         = 0x800454df
+	TUNGETVNETHDRSZ                      = 0x800454d7
+	TUNGETVNETLE                         = 0x800454dd
+	TUNSETCARRIER                        = 0x400454e2
+	TUNSETDEBUG                          = 0x400454c9
+	TUNSETFILTEREBPF                     = 0x800454e1
+	TUNSETGROUP                          = 0x400454ce
+	TUNSETIFF                            = 0x400454ca
+	TUNSETIFINDEX                        = 0x400454da
+	TUNSETLINK                           = 0x400454cd
+	TUNSETNOCSUM                         = 0x400454c8
+	TUNSETOFFLOAD                        = 0x400454d0
+	TUNSETOWNER                          = 0x400454cc
+	TUNSETPERSIST                        = 0x400454cb
+	TUNSETQUEUE                          = 0x400454d9
+	TUNSETSNDBUF                         = 0x400454d4
+	TUNSETSTEERINGEBPF                   = 0x800454e0
+	TUNSETTXFILTER                       = 0x400454d1
+	TUNSETVNETBE                         = 0x400454de
+	TUNSETVNETHDRSZ                      = 0x400454d8
+	TUNSETVNETLE                         = 0x400454dc
+	UBI_IOCATT                           = 0x40186f40
+	UBI_IOCDET                           = 0x40046f41
+	UBI_IOCEBCH                          = 0x40044f02
+	UBI_IOCEBER                          = 0x40044f01
+	UBI_IOCEBISMAP                       = 0x80044f05
+	UBI_IOCEBMAP                         = 0x40084f03
+	UBI_IOCEBUNMAP                       = 0x40044f04
+	UBI_IOCMKVOL                         = 0x40986f00
+	UBI_IOCRMVOL                         = 0x40046f01
+	UBI_IOCRNVOL                         = 0x51106f03
+	UBI_IOCRSVOL                         = 0x400c6f02
+	UBI_IOCSETVOLPROP                    = 0x40104f06
+	UBI_IOCVOLCRBLK                      = 0x40804f07
+	UBI_IOCVOLRMBLK                      = 0x4f08
+	UBI_IOCVOLUP                         = 0x40084f00
+	UDF_SUPER_MAGIC                      = 0x15013346
+	UMOUNT_NOFOLLOW                      = 0x8
+	USBDEVICE_SUPER_MAGIC                = 0x9fa2
+	UTIME_NOW                            = 0x3fffffff
+	UTIME_OMIT                           = 0x3ffffffe
+	V9FS_MAGIC                           = 0x1021997
+	VDISCARD                             = 0xd
+	VEOF                                 = 0x4
+	VEOL                                 = 0xb
+	VEOL2                                = 0x10
+	VERASE                               = 0x2
+	VINTR                                = 0x0
+	VKILL                                = 0x3
+	VLNEXT                               = 0xf
+	VMADDR_CID_ANY                       = 0xffffffff
+	VMADDR_CID_HOST                      = 0x2
+	VMADDR_CID_HYPERVISOR                = 0x0
+	VMADDR_CID_RESERVED                  = 0x1
+	VMADDR_PORT_ANY                      = 0xffffffff
+	VMIN                                 = 0x6
+	VM_SOCKETS_INVALID_VERSION           = 0xffffffff
+	VQUIT                                = 0x1
+	VREPRINT                             = 0xc
+	VSTART                               = 0x8
+	VSTOP                                = 0x9
+	VSUSP                                = 0xa
+	VSWTC                                = 0x7
+	VT0                                  = 0x0
+	VT1                                  = 0x4000
+	VTDLY                                = 0x4000
+	VTIME                                = 0x5
+	VWERASE                              = 0xe
+	WALL                                 = 0x40000000
+	WCLONE                               = 0x80000000
+	WCONTINUED                           = 0x8
+	WDIOC_GETBOOTSTATUS                  = 0x80045702
+	WDIOC_GETPRETIMEOUT                  = 0x80045709
+	WDIOC_GETSTATUS                      = 0x80045701
+	WDIOC_GETSUPPORT                     = 0x80285700
+	WDIOC_GETTEMP                        = 0x80045703
+	WDIOC_GETTIMELEFT                    = 0x8004570a
+	WDIOC_GETTIMEOUT                     = 0x80045707
+	WDIOC_KEEPALIVE                      = 0x80045705
+	WDIOC_SETOPTIONS                     = 0x80045704
+	WDIOC_SETPRETIMEOUT                  = 0xc0045708
+	WDIOC_SETTIMEOUT                     = 0xc0045706
+	WEXITED                              = 0x4
+	WIN_ACKMEDIACHANGE                   = 0xdb
+	WIN_CHECKPOWERMODE1                  = 0xe5
+	WIN_CHECKPOWERMODE2                  = 0x98
+	WIN_DEVICE_RESET                     = 0x8
+	WIN_DIAGNOSE                         = 0x90
+	WIN_DOORLOCK                         = 0xde
+	WIN_DOORUNLOCK                       = 0xdf
+	WIN_DOWNLOAD_MICROCODE               = 0x92
+	WIN_FLUSH_CACHE                      = 0xe7
+	WIN_FLUSH_CACHE_EXT                  = 0xea
+	WIN_FORMAT                           = 0x50
+	WIN_GETMEDIASTATUS                   = 0xda
+	WIN_IDENTIFY                         = 0xec
+	WIN_IDENTIFY_DMA                     = 0xee
+	WIN_IDLEIMMEDIATE                    = 0xe1
+	WIN_INIT                             = 0x60
+	WIN_MEDIAEJECT                       = 0xed
+	WIN_MULTREAD                         = 0xc4
+	WIN_MULTREAD_EXT                     = 0x29
+	WIN_MULTWRITE                        = 0xc5
+	WIN_MULTWRITE_EXT                    = 0x39
+	WIN_NOP                              = 0x0
+	WIN_PACKETCMD                        = 0xa0
+	WIN_PIDENTIFY                        = 0xa1
+	WIN_POSTBOOT                         = 0xdc
+	WIN_PREBOOT                          = 0xdd
+	WIN_QUEUED_SERVICE                   = 0xa2
+	WIN_READ                             = 0x20
+	WIN_READDMA                          = 0xc8
+	WIN_READDMA_EXT                      = 0x25
+	WIN_READDMA_ONCE                     = 0xc9
+	WIN_READDMA_QUEUED                   = 0xc7
+	WIN_READDMA_QUEUED_EXT               = 0x26
+	WIN_READ_BUFFER                      = 0xe4
+	WIN_READ_EXT                         = 0x24
+	WIN_READ_LONG                        = 0x22
+	WIN_READ_LONG_ONCE                   = 0x23
+	WIN_READ_NATIVE_MAX                  = 0xf8
+	WIN_READ_NATIVE_MAX_EXT              = 0x27
+	WIN_READ_ONCE                        = 0x21
+	WIN_RECAL                            = 0x10
+	WIN_RESTORE                          = 0x10
+	WIN_SECURITY_DISABLE                 = 0xf6
+	WIN_SECURITY_ERASE_PREPARE           = 0xf3
+	WIN_SECURITY_ERASE_UNIT              = 0xf4
+	WIN_SECURITY_FREEZE_LOCK             = 0xf5
+	WIN_SECURITY_SET_PASS                = 0xf1
+	WIN_SECURITY_UNLOCK                  = 0xf2
+	WIN_SEEK                             = 0x70
+	WIN_SETFEATURES                      = 0xef
+	WIN_SETIDLE1                         = 0xe3
+	WIN_SETIDLE2                         = 0x97
+	WIN_SETMULT                          = 0xc6
+	WIN_SET_MAX                          = 0xf9
+	WIN_SET_MAX_EXT                      = 0x37
+	WIN_SLEEPNOW1                        = 0xe6
+	WIN_SLEEPNOW2                        = 0x99
+	WIN_SMART                            = 0xb0
+	WIN_SPECIFY                          = 0x91
+	WIN_SRST                             = 0x8
+	WIN_STANDBY                          = 0xe2
+	WIN_STANDBY2                         = 0x96
+	WIN_STANDBYNOW1                      = 0xe0
+	WIN_STANDBYNOW2                      = 0x94
+	WIN_VERIFY                           = 0x40
+	WIN_VERIFY_EXT                       = 0x42
+	WIN_VERIFY_ONCE                      = 0x41
+	WIN_WRITE                            = 0x30
+	WIN_WRITEDMA                         = 0xca
+	WIN_WRITEDMA_EXT                     = 0x35
+	WIN_WRITEDMA_ONCE                    = 0xcb
+	WIN_WRITEDMA_QUEUED                  = 0xcc
+	WIN_WRITEDMA_QUEUED_EXT              = 0x36
+	WIN_WRITE_BUFFER                     = 0xe8
+	WIN_WRITE_EXT                        = 0x34
+	WIN_WRITE_LONG                       = 0x32
+	WIN_WRITE_LONG_ONCE                  = 0x33
+	WIN_WRITE_ONCE                       = 0x31
+	WIN_WRITE_SAME                       = 0xe9
+	WIN_WRITE_VERIFY                     = 0x3c
+	WNOHANG                              = 0x1
+	WNOTHREAD                            = 0x20000000
+	WNOWAIT                              = 0x1000000
+	WORDSIZE                             = 0x20
+	WSTOPPED                             = 0x2
+	WUNTRACED                            = 0x2
+	XATTR_CREATE                         = 0x1
+	XATTR_REPLACE                        = 0x2
+	XCASE                                = 0x4
+	XDP_COPY                             = 0x2
+	XDP_FLAGS_DRV_MODE                   = 0x4
+	XDP_FLAGS_HW_MODE                    = 0x8
+	XDP_FLAGS_MASK                       = 0xf
+	XDP_FLAGS_MODES                      = 0xe
+	XDP_FLAGS_SKB_MODE                   = 0x2
+	XDP_FLAGS_UPDATE_IF_NOEXIST          = 0x1
+	XDP_MMAP_OFFSETS                     = 0x1
+	XDP_PGOFF_RX_RING                    = 0x0
+	XDP_PGOFF_TX_RING                    = 0x80000000
+	XDP_RX_RING                          = 0x2
+	XDP_SHARED_UMEM                      = 0x1
+	XDP_STATISTICS                       = 0x7
+	XDP_TX_RING                          = 0x3
+	XDP_UMEM_COMPLETION_RING             = 0x6
+	XDP_UMEM_FILL_RING                   = 0x5
+	XDP_UMEM_PGOFF_COMPLETION_RING       = 0x180000000
+	XDP_UMEM_PGOFF_FILL_RING             = 0x100000000
+	XDP_UMEM_REG                         = 0x4
+	XDP_ZEROCOPY                         = 0x4
+	XENFS_SUPER_MAGIC                    = 0xabba1974
+	XFS_SUPER_MAGIC                      = 0x58465342
+	XTABS                                = 0x1800
+	ZSMALLOC_MAGIC                       = 0x58295829
+)
+
+// Errors
+const (
+	E2BIG           = syscall.Errno(0x7)
+	EACCES          = syscall.Errno(0xd)
+	EADDRINUSE      = syscall.Errno(0x62)
+	EADDRNOTAVAIL   = syscall.Errno(0x63)
+	EADV            = syscall.Errno(0x44)
+	EAFNOSUPPORT    = syscall.Errno(0x61)
+	EAGAIN          = syscall.Errno(0xb)
+	EALREADY        = syscall.Errno(0x72)
+	EBADE           = syscall.Errno(0x34)
+	EBADF           = syscall.Errno(0x9)
+	EBADFD          = syscall.Errno(0x4d)
+	EBADMSG         = syscall.Errno(0x4a)
+	EBADR           = syscall.Errno(0x35)
+	EBADRQC         = syscall.Errno(0x38)
+	EBADSLT         = syscall.Errno(0x39)
+	EBFONT          = syscall.Errno(0x3b)
+	EBUSY           = syscall.Errno(0x10)
+	ECANCELED       = syscall.Errno(0x7d)
+	ECHILD          = syscall.Errno(0xa)
+	ECHRNG          = syscall.Errno(0x2c)
+	ECOMM           = syscall.Errno(0x46)
+	ECONNABORTED    = syscall.Errno(0x67)
+	ECONNREFUSED    = syscall.Errno(0x6f)
+	ECONNRESET      = syscall.Errno(0x68)
+	EDEADLK         = syscall.Errno(0x23)
+	EDEADLOCK       = syscall.Errno(0x23)
+	EDESTADDRREQ    = syscall.Errno(0x59)
+	EDOM            = syscall.Errno(0x21)
+	EDOTDOT         = syscall.Errno(0x49)
+	EDQUOT          = syscall.Errno(0x7a)
+	EEXIST          = syscall.Errno(0x11)
+	EFAULT          = syscall.Errno(0xe)
+	EFBIG           = syscall.Errno(0x1b)
+	EHOSTDOWN       = syscall.Errno(0x70)
+	EHOSTUNREACH    = syscall.Errno(0x71)
+	EHWPOISON       = syscall.Errno(0x85)
+	EIDRM           = syscall.Errno(0x2b)
+	EILSEQ          = syscall.Errno(0x54)
+	EINPROGRESS     = syscall.Errno(0x73)
+	EINTR           = syscall.Errno(0x4)
+	EINVAL          = syscall.Errno(0x16)
+	EIO             = syscall.Errno(0x5)
+	EISCONN         = syscall.Errno(0x6a)
+	EISDIR          = syscall.Errno(0x15)
+	EISNAM          = syscall.Errno(0x78)
+	EKEYEXPIRED     = syscall.Errno(0x7f)
+	EKEYREJECTED    = syscall.Errno(0x81)
+	EKEYREVOKED     = syscall.Errno(0x80)
+	EL2HLT          = syscall.Errno(0x33)
+	EL2NSYNC        = syscall.Errno(0x2d)
+	EL3HLT          = syscall.Errno(0x2e)
+	EL3RST          = syscall.Errno(0x2f)
+	ELIBACC         = syscall.Errno(0x4f)
+	ELIBBAD         = syscall.Errno(0x50)
+	ELIBEXEC        = syscall.Errno(0x53)
+	ELIBMAX         = syscall.Errno(0x52)
+	ELIBSCN         = syscall.Errno(0x51)
+	ELNRNG          = syscall.Errno(0x30)
+	ELOOP           = syscall.Errno(0x28)
+	EMEDIUMTYPE     = syscall.Errno(0x7c)
+	EMFILE          = syscall.Errno(0x18)
+	EMLINK          = syscall.Errno(0x1f)
+	EMSGSIZE        = syscall.Errno(0x5a)
+	EMULTIHOP       = syscall.Errno(0x48)
+	ENAMETOOLONG    = syscall.Errno(0x24)
+	ENAVAIL         = syscall.Errno(0x77)
+	ENETDOWN        = syscall.Errno(0x64)
+	ENETRESET       = syscall.Errno(0x66)
+	ENETUNREACH     = syscall.Errno(0x65)
+	ENFILE          = syscall.Errno(0x17)
+	ENOANO          = syscall.Errno(0x37)
+	ENOBUFS         = syscall.Errno(0x69)
+	ENOCSI          = syscall.Errno(0x32)
+	ENODATA         = syscall.Errno(0x3d)
+	ENODEV          = syscall.Errno(0x13)
+	ENOENT          = syscall.Errno(0x2)
+	ENOEXEC         = syscall.Errno(0x8)
+	ENOKEY          = syscall.Errno(0x7e)
+	ENOLCK          = syscall.Errno(0x25)
+	ENOLINK         = syscall.Errno(0x43)
+	ENOMEDIUM       = syscall.Errno(0x7b)
+	ENOMEM          = syscall.Errno(0xc)
+	ENOMSG          = syscall.Errno(0x2a)
+	ENONET          = syscall.Errno(0x40)
+	ENOPKG          = syscall.Errno(0x41)
+	ENOPROTOOPT     = syscall.Errno(0x5c)
+	ENOSPC          = syscall.Errno(0x1c)
+	ENOSR           = syscall.Errno(0x3f)
+	ENOSTR          = syscall.Errno(0x3c)
+	ENOSYS          = syscall.Errno(0x26)
+	ENOTBLK         = syscall.Errno(0xf)
+	ENOTCONN        = syscall.Errno(0x6b)
+	ENOTDIR         = syscall.Errno(0x14)
+	ENOTEMPTY       = syscall.Errno(0x27)
+	ENOTNAM         = syscall.Errno(0x76)
+	ENOTRECOVERABLE = syscall.Errno(0x83)
+	ENOTSOCK        = syscall.Errno(0x58)
+	ENOTSUP         = syscall.Errno(0x5f)
+	ENOTTY          = syscall.Errno(0x19)
+	ENOTUNIQ        = syscall.Errno(0x4c)
+	ENXIO           = syscall.Errno(0x6)
+	EOPNOTSUPP      = syscall.Errno(0x5f)
+	EOVERFLOW       = syscall.Errno(0x4b)
+	EOWNERDEAD      = syscall.Errno(0x82)
+	EPERM           = syscall.Errno(0x1)
+	EPFNOSUPPORT    = syscall.Errno(0x60)
+	EPIPE           = syscall.Errno(0x20)
+	EPROTO          = syscall.Errno(0x47)
+	EPROTONOSUPPORT = syscall.Errno(0x5d)
+	EPROTOTYPE      = syscall.Errno(0x5b)
+	ERANGE          = syscall.Errno(0x22)
+	EREMCHG         = syscall.Errno(0x4e)
+	EREMOTE         = syscall.Errno(0x42)
+	EREMOTEIO       = syscall.Errno(0x79)
+	ERESTART        = syscall.Errno(0x55)
+	ERFKILL         = syscall.Errno(0x84)
+	EROFS           = syscall.Errno(0x1e)
+	ESHUTDOWN       = syscall.Errno(0x6c)
+	ESOCKTNOSUPPORT = syscall.Errno(0x5e)
+	ESPIPE          = syscall.Errno(0x1d)
+	ESRCH           = syscall.Errno(0x3)
+	ESRMNT          = syscall.Errno(0x45)
+	ESTALE          = syscall.Errno(0x74)
+	ESTRPIPE        = syscall.Errno(0x56)
+	ETIME           = syscall.Errno(0x3e)
+	ETIMEDOUT       = syscall.Errno(0x6e)
+	ETOOMANYREFS    = syscall.Errno(0x6d)
+	ETXTBSY         = syscall.Errno(0x1a)
+	EUCLEAN         = syscall.Errno(0x75)
+	EUNATCH         = syscall.Errno(0x31)
+	EUSERS          = syscall.Errno(0x57)
+	EWOULDBLOCK     = syscall.Errno(0xb)
+	EXDEV           = syscall.Errno(0x12)
+	EXFULL          = syscall.Errno(0x36)
+)
+
+// Signals
+const (
+	SIGABRT   = syscall.Signal(0x6)
+	SIGALRM   = syscall.Signal(0xe)
+	SIGBUS    = syscall.Signal(0x7)
+	SIGCHLD   = syscall.Signal(0x11)
+	SIGCLD    = syscall.Signal(0x11)
+	SIGCONT   = syscall.Signal(0x12)
+	SIGFPE    = syscall.Signal(0x8)
+	SIGHUP    = syscall.Signal(0x1)
+	SIGILL    = syscall.Signal(0x4)
+	SIGINT    = syscall.Signal(0x2)
+	SIGIO     = syscall.Signal(0x1d)
+	SIGIOT    = syscall.Signal(0x6)
+	SIGKILL   = syscall.Signal(0x9)
+	SIGPIPE   = syscall.Signal(0xd)
+	SIGPOLL   = syscall.Signal(0x1d)
+	SIGPROF   = syscall.Signal(0x1b)
+	SIGPWR    = syscall.Signal(0x1e)
+	SIGQUIT   = syscall.Signal(0x3)
+	SIGSEGV   = syscall.Signal(0xb)
+	SIGSTKFLT = syscall.Signal(0x10)
+	SIGSTOP   = syscall.Signal(0x13)
+	SIGSYS    = syscall.Signal(0x1f)
+	SIGTERM   = syscall.Signal(0xf)
+	SIGTRAP   = syscall.Signal(0x5)
+	SIGTSTP   = syscall.Signal(0x14)
+	SIGTTIN   = syscall.Signal(0x15)
+	SIGTTOU   = syscall.Signal(0x16)
+	SIGURG    = syscall.Signal(0x17)
+	SIGUSR1   = syscall.Signal(0xa)
+	SIGUSR2   = syscall.Signal(0xc)
+	SIGVTALRM = syscall.Signal(0x1a)
+	SIGWINCH  = syscall.Signal(0x1c)
+	SIGXCPU   = syscall.Signal(0x18)
+	SIGXFSZ   = syscall.Signal(0x19)
+)
+
+// Error table
+var errorList = [...]struct {
+	num  syscall.Errno
+	name string
+	desc string
+}{
+	{1, "EPERM", "operation not permitted"},
+	{2, "ENOENT", "no such file or directory"},
+	{3, "ESRCH", "no such process"},
+	{4, "EINTR", "interrupted system call"},
+	{5, "EIO", "input/output error"},
+	{6, "ENXIO", "no such device or address"},
+	{7, "E2BIG", "argument list too long"},
+	{8, "ENOEXEC", "exec format error"},
+	{9, "EBADF", "bad file descriptor"},
+	{10, "ECHILD", "no child processes"},
+	{11, "EAGAIN", "resource temporarily unavailable"},
+	{12, "ENOMEM", "cannot allocate memory"},
+	{13, "EACCES", "permission denied"},
+	{14, "EFAULT", "bad address"},
+	{15, "ENOTBLK", "block device required"},
+	{16, "EBUSY", "device or resource busy"},
+	{17, "EEXIST", "file exists"},
+	{18, "EXDEV", "invalid cross-device link"},
+	{19, "ENODEV", "no such device"},
+	{20, "ENOTDIR", "not a directory"},
+	{21, "EISDIR", "is a directory"},
+	{22, "EINVAL", "invalid argument"},
+	{23, "ENFILE", "too many open files in system"},
+	{24, "EMFILE", "too many open files"},
+	{25, "ENOTTY", "inappropriate ioctl for device"},
+	{26, "ETXTBSY", "text file busy"},
+	{27, "EFBIG", "file too large"},
+	{28, "ENOSPC", "no space left on device"},
+	{29, "ESPIPE", "illegal seek"},
+	{30, "EROFS", "read-only file system"},
+	{31, "EMLINK", "too many links"},
+	{32, "EPIPE", "broken pipe"},
+	{33, "EDOM", "numerical argument out of domain"},
+	{34, "ERANGE", "numerical result out of range"},
+	{35, "EDEADLK", "resource deadlock avoided"},
+	{36, "ENAMETOOLONG", "file name too long"},
+	{37, "ENOLCK", "no locks available"},
+	{38, "ENOSYS", "function not implemented"},
+	{39, "ENOTEMPTY", "directory not empty"},
+	{40, "ELOOP", "too many levels of symbolic links"},
+	{42, "ENOMSG", "no message of desired type"},
+	{43, "EIDRM", "identifier removed"},
+	{44, "ECHRNG", "channel number out of range"},
+	{45, "EL2NSYNC", "level 2 not synchronized"},
+	{46, "EL3HLT", "level 3 halted"},
+	{47, "EL3RST", "level 3 reset"},
+	{48, "ELNRNG", "link number out of range"},
+	{49, "EUNATCH", "protocol driver not attached"},
+	{50, "ENOCSI", "no CSI structure available"},
+	{51, "EL2HLT", "level 2 halted"},
+	{52, "EBADE", "invalid exchange"},
+	{53, "EBADR", "invalid request descriptor"},
+	{54, "EXFULL", "exchange full"},
+	{55, "ENOANO", "no anode"},
+	{56, "EBADRQC", "invalid request code"},
+	{57, "EBADSLT", "invalid slot"},
+	{59, "EBFONT", "bad font file format"},
+	{60, "ENOSTR", "device not a stream"},
+	{61, "ENODATA", "no data available"},
+	{62, "ETIME", "timer expired"},
+	{63, "ENOSR", "out of streams resources"},
+	{64, "ENONET", "machine is not on the network"},
+	{65, "ENOPKG", "package not installed"},
+	{66, "EREMOTE", "object is remote"},
+	{67, "ENOLINK", "link has been severed"},
+	{68, "EADV", "advertise error"},
+	{69, "ESRMNT", "srmount error"},
+	{70, "ECOMM", "communication error on send"},
+	{71, "EPROTO", "protocol error"},
+	{72, "EMULTIHOP", "multihop attempted"},
+	{73, "EDOTDOT", "RFS specific error"},
+	{74, "EBADMSG", "bad message"},
+	{75, "EOVERFLOW", "value too large for defined data type"},
+	{76, "ENOTUNIQ", "name not unique on network"},
+	{77, "EBADFD", "file descriptor in bad state"},
+	{78, "EREMCHG", "remote address changed"},
+	{79, "ELIBACC", "can not access a needed shared library"},
+	{80, "ELIBBAD", "accessing a corrupted shared library"},
+	{81, "ELIBSCN", ".lib section in a.out corrupted"},
+	{82, "ELIBMAX", "attempting to link in too many shared libraries"},
+	{83, "ELIBEXEC", "cannot exec a shared library directly"},
+	{84, "EILSEQ", "invalid or incomplete multibyte or wide character"},
+	{85, "ERESTART", "interrupted system call should be restarted"},
+	{86, "ESTRPIPE", "streams pipe error"},
+	{87, "EUSERS", "too many users"},
+	{88, "ENOTSOCK", "socket operation on non-socket"},
+	{89, "EDESTADDRREQ", "destination address required"},
+	{90, "EMSGSIZE", "message too long"},
+	{91, "EPROTOTYPE", "protocol wrong type for socket"},
+	{92, "ENOPROTOOPT", "protocol not available"},
+	{93, "EPROTONOSUPPORT", "protocol not supported"},
+	{94, "ESOCKTNOSUPPORT", "socket type not supported"},
+	{95, "ENOTSUP", "operation not supported"},
+	{96, "EPFNOSUPPORT", "protocol family not supported"},
+	{97, "EAFNOSUPPORT", "address family not supported by protocol"},
+	{98, "EADDRINUSE", "address already in use"},
+	{99, "EADDRNOTAVAIL", "cannot assign requested address"},
+	{100, "ENETDOWN", "network is down"},
+	{101, "ENETUNREACH", "network is unreachable"},
+	{102, "ENETRESET", "network dropped connection on reset"},
+	{103, "ECONNABORTED", "software caused connection abort"},
+	{104, "ECONNRESET", "connection reset by peer"},
+	{105, "ENOBUFS", "no buffer space available"},
+	{106, "EISCONN", "transport endpoint is already connected"},
+	{107, "ENOTCONN", "transport endpoint is not connected"},
+	{108, "ESHUTDOWN", "cannot send after transport endpoint shutdown"},
+	{109, "ETOOMANYREFS", "too many references: cannot splice"},
+	{110, "ETIMEDOUT", "connection timed out"},
+	{111, "ECONNREFUSED", "connection refused"},
+	{112, "EHOSTDOWN", "host is down"},
+	{113, "EHOSTUNREACH", "no route to host"},
+	{114, "EALREADY", "operation already in progress"},
+	{115, "EINPROGRESS", "operation now in progress"},
+	{116, "ESTALE", "stale file handle"},
+	{117, "EUCLEAN", "structure needs cleaning"},
+	{118, "ENOTNAM", "not a XENIX named type file"},
+	{119, "ENAVAIL", "no XENIX semaphores available"},
+	{120, "EISNAM", "is a named type file"},
+	{121, "EREMOTEIO", "remote I/O error"},
+	{122, "EDQUOT", "disk quota exceeded"},
+	{123, "ENOMEDIUM", "no medium found"},
+	{124, "EMEDIUMTYPE", "wrong medium type"},
+	{125, "ECANCELED", "operation canceled"},
+	{126, "ENOKEY", "required key not available"},
+	{127, "EKEYEXPIRED", "key has expired"},
+	{128, "EKEYREVOKED", "key has been revoked"},
+	{129, "EKEYREJECTED", "key was rejected by service"},
+	{130, "EOWNERDEAD", "owner died"},
+	{131, "ENOTRECOVERABLE", "state not recoverable"},
+	{132, "ERFKILL", "operation not possible due to RF-kill"},
+	{133, "EHWPOISON", "memory page has hardware error"},
+}
+
+// Signal table
+var signalList = [...]struct {
+	num  syscall.Signal
+	name string
+	desc string
+}{
+	{1, "SIGHUP", "hangup"},
+	{2, "SIGINT", "interrupt"},
+	{3, "SIGQUIT", "quit"},
+	{4, "SIGILL", "illegal instruction"},
+	{5, "SIGTRAP", "trace/breakpoint trap"},
+	{6, "SIGABRT", "aborted"},
+	{7, "SIGBUS", "bus error"},
+	{8, "SIGFPE", "floating point exception"},
+	{9, "SIGKILL", "killed"},
+	{10, "SIGUSR1", "user defined signal 1"},
+	{11, "SIGSEGV", "segmentation fault"},
+	{12, "SIGUSR2", "user defined signal 2"},
+	{13, "SIGPIPE", "broken pipe"},
+	{14, "SIGALRM", "alarm clock"},
+	{15, "SIGTERM", "terminated"},
+	{16, "SIGSTKFLT", "stack fault"},
+	{17, "SIGCHLD", "child exited"},
+	{18, "SIGCONT", "continued"},
+	{19, "SIGSTOP", "stopped (signal)"},
+	{20, "SIGTSTP", "stopped"},
+	{21, "SIGTTIN", "stopped (tty input)"},
+	{22, "SIGTTOU", "stopped (tty output)"},
+	{23, "SIGURG", "urgent I/O condition"},
+	{24, "SIGXCPU", "CPU time limit exceeded"},
+	{25, "SIGXFSZ", "file size limit exceeded"},
+	{26, "SIGVTALRM", "virtual timer expired"},
+	{27, "SIGPROF", "profiling timer expired"},
+	{28, "SIGWINCH", "window changed"},
+	{29, "SIGIO", "I/O possible"},
+	{30, "SIGPWR", "power failure"},
+	{31, "SIGSYS", "bad system call"},
+}
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/zsyscall_linux_thumb.go b/src/cmd/vendor/golang.org/x/sys/unix/zsyscall_linux_thumb.go
new file mode 100644
index 0000000000..2c92e4d5c8
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/zsyscall_linux_thumb.go
@@ -0,0 +1,2368 @@
+// go run mksyscall.go -l32 -arm -tags linux,arm syscall_linux.go syscall_linux_arm.go
+// Code generated by the command above; see README.md. DO NOT EDIT.
+
+// +build linux,thumb
+
+package unix
+
+import (
+	"syscall"
+	"unsafe"
+)
+
+var _ syscall.Errno
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func FanotifyInit(flags uint, event_f_flags uint) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_FANOTIFY_INIT, uintptr(flags), uintptr(event_f_flags), 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fanotifyMark(fd int, flags uint, mask uint64, dirFd int, pathname *byte) (err error) {
+	_, _, e1 := Syscall6(SYS_FANOTIFY_MARK, uintptr(fd), uintptr(flags), uintptr(mask), uintptr(mask>>32), uintptr(dirFd), uintptr(unsafe.Pointer(pathname)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fchmodat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FCHMODAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func ioctl(fd int, req uint, arg uintptr) (err error) {
+	_, _, e1 := Syscall(SYS_IOCTL, uintptr(fd), uintptr(req), uintptr(arg))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Linkat(olddirfd int, oldpath string, newdirfd int, newpath string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_LINKAT, uintptr(olddirfd), uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func openat(dirfd int, path string, flags int, mode uint32) (fd int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall6(SYS_OPENAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(flags), uintptr(mode), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func ppoll(fds *PollFd, nfds int, timeout *Timespec, sigmask *Sigset_t) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS_PPOLL, uintptr(unsafe.Pointer(fds)), uintptr(nfds), uintptr(unsafe.Pointer(timeout)), uintptr(unsafe.Pointer(sigmask)), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Readlinkat(dirfd int, path string, buf []byte) (n int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(buf) > 0 {
+		_p1 = unsafe.Pointer(&buf[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_READLINKAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(buf)), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Symlinkat(oldpath string, newdirfd int, newpath string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_SYMLINKAT, uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Unlinkat(dirfd int, path string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UNLINKAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(flags))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func utimensat(dirfd int, path string, times *[2]Timespec, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_UTIMENSAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)), uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getcwd(buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_GETCWD, uintptr(_p0), uintptr(len(buf)), 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func wait4(pid int, wstatus *_C_int, options int, rusage *Rusage) (wpid int, err error) {
+	r0, _, e1 := Syscall6(SYS_WAIT4, uintptr(pid), uintptr(unsafe.Pointer(wstatus)), uintptr(options), uintptr(unsafe.Pointer(rusage)), 0, 0)
+	wpid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func KeyctlInt(cmd int, arg2 int, arg3 int, arg4 int, arg5 int) (ret int, err error) {
+	r0, _, e1 := Syscall6(SYS_KEYCTL, uintptr(cmd), uintptr(arg2), uintptr(arg3), uintptr(arg4), uintptr(arg5), 0)
+	ret = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func KeyctlBuffer(cmd int, arg2 int, buf []byte, arg5 int) (ret int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_KEYCTL, uintptr(cmd), uintptr(arg2), uintptr(_p0), uintptr(len(buf)), uintptr(arg5), 0)
+	ret = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func keyctlJoin(cmd int, arg2 string) (ret int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(arg2)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall(SYS_KEYCTL, uintptr(cmd), uintptr(unsafe.Pointer(_p0)), 0)
+	ret = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func keyctlSearch(cmd int, arg2 int, arg3 string, arg4 string, arg5 int) (ret int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(arg3)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(arg4)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall6(SYS_KEYCTL, uintptr(cmd), uintptr(arg2), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(arg5), 0)
+	ret = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func keyctlIOV(cmd int, arg2 int, payload []Iovec, arg5 int) (err error) {
+	var _p0 unsafe.Pointer
+	if len(payload) > 0 {
+		_p0 = unsafe.Pointer(&payload[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_KEYCTL, uintptr(cmd), uintptr(arg2), uintptr(_p0), uintptr(len(payload)), uintptr(arg5), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func keyctlDH(cmd int, arg2 *KeyctlDHParams, buf []byte) (ret int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_KEYCTL, uintptr(cmd), uintptr(unsafe.Pointer(arg2)), uintptr(_p0), uintptr(len(buf)), 0, 0)
+	ret = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func ptrace(request int, pid int, addr uintptr, data uintptr) (err error) {
+	_, _, e1 := Syscall6(SYS_PTRACE, uintptr(request), uintptr(pid), uintptr(addr), uintptr(data), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func reboot(magic1 uint, magic2 uint, cmd int, arg string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(arg)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_REBOOT, uintptr(magic1), uintptr(magic2), uintptr(cmd), uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func mount(source string, target string, fstype string, flags uintptr, data *byte) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(source)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(target)
+	if err != nil {
+		return
+	}
+	var _p2 *byte
+	_p2, err = BytePtrFromString(fstype)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_MOUNT, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(unsafe.Pointer(_p2)), uintptr(flags), uintptr(unsafe.Pointer(data)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Acct(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_ACCT, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func AddKey(keyType string, description string, payload []byte, ringid int) (id int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(keyType)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(description)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(payload) > 0 {
+		_p2 = unsafe.Pointer(&payload[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_ADD_KEY, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(payload)), uintptr(ringid), 0)
+	id = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Adjtimex(buf *Timex) (state int, err error) {
+	r0, _, e1 := Syscall(SYS_ADJTIMEX, uintptr(unsafe.Pointer(buf)), 0, 0)
+	state = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Chdir(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_CHDIR, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Chroot(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_CHROOT, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func ClockGetres(clockid int32, res *Timespec) (err error) {
+	_, _, e1 := Syscall(SYS_CLOCK_GETRES, uintptr(clockid), uintptr(unsafe.Pointer(res)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func ClockGettime(clockid int32, time *Timespec) (err error) {
+	_, _, e1 := Syscall(SYS_CLOCK_GETTIME, uintptr(clockid), uintptr(unsafe.Pointer(time)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func ClockNanosleep(clockid int32, flags int, request *Timespec, remain *Timespec) (err error) {
+	_, _, e1 := Syscall6(SYS_CLOCK_NANOSLEEP, uintptr(clockid), uintptr(flags), uintptr(unsafe.Pointer(request)), uintptr(unsafe.Pointer(remain)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Close(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_CLOSE, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func CopyFileRange(rfd int, roff *int64, wfd int, woff *int64, len int, flags int) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS_COPY_FILE_RANGE, uintptr(rfd), uintptr(unsafe.Pointer(roff)), uintptr(wfd), uintptr(unsafe.Pointer(woff)), uintptr(len), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func DeleteModule(name string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(name)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_DELETE_MODULE, uintptr(unsafe.Pointer(_p0)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup(oldfd int) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_DUP, uintptr(oldfd), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup3(oldfd int, newfd int, flags int) (err error) {
+	_, _, e1 := Syscall(SYS_DUP3, uintptr(oldfd), uintptr(newfd), uintptr(flags))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCreate1(flag int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_EPOLL_CREATE1, uintptr(flag), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCtl(epfd int, op int, fd int, event *EpollEvent) (err error) {
+	_, _, e1 := RawSyscall6(SYS_EPOLL_CTL, uintptr(epfd), uintptr(op), uintptr(fd), uintptr(unsafe.Pointer(event)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Eventfd(initval uint, flags int) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_EVENTFD2, uintptr(initval), uintptr(flags), 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Exit(code int) {
+	SyscallNoError(SYS_EXIT_GROUP, uintptr(code), 0, 0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fallocate(fd int, mode uint32, off int64, len int64) (err error) {
+	_, _, e1 := Syscall6(SYS_FALLOCATE, uintptr(fd), uintptr(mode), uintptr(off), uintptr(off>>32), uintptr(len), uintptr(len>>32))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchdir(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FCHDIR, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchmod(fd int, mode uint32) (err error) {
+	_, _, e1 := Syscall(SYS_FCHMOD, uintptr(fd), uintptr(mode), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchownat(dirfd int, path string, uid int, gid int, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_FCHOWNAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(uid), uintptr(gid), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fcntl(fd int, cmd int, arg int) (val int, err error) {
+	r0, _, e1 := Syscall(SYS_FCNTL, uintptr(fd), uintptr(cmd), uintptr(arg))
+	val = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fdatasync(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FDATASYNC, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fgetxattr(fd int, attr string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(dest) > 0 {
+		_p1 = unsafe.Pointer(&dest[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_FGETXATTR, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(dest)), 0, 0)
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func FinitModule(fd int, params string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(params)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FINIT_MODULE, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(flags))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Flistxattr(fd int, dest []byte) (sz int, err error) {
+	var _p0 unsafe.Pointer
+	if len(dest) > 0 {
+		_p0 = unsafe.Pointer(&dest[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_FLISTXATTR, uintptr(fd), uintptr(_p0), uintptr(len(dest)))
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Flock(fd int, how int) (err error) {
+	_, _, e1 := Syscall(SYS_FLOCK, uintptr(fd), uintptr(how), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fremovexattr(fd int, attr string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FREMOVEXATTR, uintptr(fd), uintptr(unsafe.Pointer(_p0)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fsetxattr(fd int, attr string, dest []byte, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(dest) > 0 {
+		_p1 = unsafe.Pointer(&dest[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_FSETXATTR, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(dest)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fsync(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FSYNC, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getdents(fd int, buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_GETDENTS64, uintptr(fd), uintptr(_p0), uintptr(len(buf)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpgid(pid int) (pgid int, err error) {
+	r0, _, e1 := RawSyscall(SYS_GETPGID, uintptr(pid), 0, 0)
+	pgid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpid() (pid int) {
+	r0, _ := RawSyscallNoError(SYS_GETPID, 0, 0, 0)
+	pid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getppid() (ppid int) {
+	r0, _ := RawSyscallNoError(SYS_GETPPID, 0, 0, 0)
+	ppid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpriority(which int, who int) (prio int, err error) {
+	r0, _, e1 := Syscall(SYS_GETPRIORITY, uintptr(which), uintptr(who), 0)
+	prio = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getrandom(buf []byte, flags int) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_GETRANDOM, uintptr(_p0), uintptr(len(buf)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getrusage(who int, rusage *Rusage) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETRUSAGE, uintptr(who), uintptr(unsafe.Pointer(rusage)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getsid(pid int) (sid int, err error) {
+	r0, _, e1 := RawSyscall(SYS_GETSID, uintptr(pid), 0, 0)
+	sid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Gettid() (tid int) {
+	r0, _ := RawSyscallNoError(SYS_GETTID, 0, 0, 0)
+	tid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getxattr(path string, attr string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(dest) > 0 {
+		_p2 = unsafe.Pointer(&dest[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_GETXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(dest)), 0, 0)
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InitModule(moduleImage []byte, params string) (err error) {
+	var _p0 unsafe.Pointer
+	if len(moduleImage) > 0 {
+		_p0 = unsafe.Pointer(&moduleImage[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(params)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_INIT_MODULE, uintptr(_p0), uintptr(len(moduleImage)), uintptr(unsafe.Pointer(_p1)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyAddWatch(fd int, pathname string, mask uint32) (watchdesc int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(pathname)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall(SYS_INOTIFY_ADD_WATCH, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(mask))
+	watchdesc = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyInit1(flags int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_INIT1, uintptr(flags), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyRmWatch(fd int, watchdesc uint32) (success int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_RM_WATCH, uintptr(fd), uintptr(watchdesc), 0)
+	success = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Kill(pid int, sig syscall.Signal) (err error) {
+	_, _, e1 := RawSyscall(SYS_KILL, uintptr(pid), uintptr(sig), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Klogctl(typ int, buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_SYSLOG, uintptr(typ), uintptr(_p0), uintptr(len(buf)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Lgetxattr(path string, attr string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(dest) > 0 {
+		_p2 = unsafe.Pointer(&dest[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_LGETXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(dest)), 0, 0)
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Listxattr(path string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(dest) > 0 {
+		_p1 = unsafe.Pointer(&dest[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_LISTXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(dest)))
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Llistxattr(path string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(dest) > 0 {
+		_p1 = unsafe.Pointer(&dest[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_LLISTXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(dest)))
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Lremovexattr(path string, attr string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_LREMOVEXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Lsetxattr(path string, attr string, data []byte, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(data) > 0 {
+		_p2 = unsafe.Pointer(&data[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_LSETXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(data)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func MemfdCreate(name string, flags int) (fd int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(name)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall(SYS_MEMFD_CREATE, uintptr(unsafe.Pointer(_p0)), uintptr(flags), 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mkdirat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_MKDIRAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mknodat(dirfd int, path string, mode uint32, dev int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_MKNODAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode), uintptr(dev), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Nanosleep(time *Timespec, leftover *Timespec) (err error) {
+	_, _, e1 := Syscall(SYS_NANOSLEEP, uintptr(unsafe.Pointer(time)), uintptr(unsafe.Pointer(leftover)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func PerfEventOpen(attr *PerfEventAttr, pid int, cpu int, groupFd int, flags int) (fd int, err error) {
+	r0, _, e1 := Syscall6(SYS_PERF_EVENT_OPEN, uintptr(unsafe.Pointer(attr)), uintptr(pid), uintptr(cpu), uintptr(groupFd), uintptr(flags), 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func PivotRoot(newroot string, putold string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(newroot)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(putold)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_PIVOT_ROOT, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func prlimit(pid int, resource int, newlimit *Rlimit, old *Rlimit) (err error) {
+	_, _, e1 := RawSyscall6(SYS_PRLIMIT64, uintptr(pid), uintptr(resource), uintptr(unsafe.Pointer(newlimit)), uintptr(unsafe.Pointer(old)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Prctl(option int, arg2 uintptr, arg3 uintptr, arg4 uintptr, arg5 uintptr) (err error) {
+	_, _, e1 := Syscall6(SYS_PRCTL, uintptr(option), uintptr(arg2), uintptr(arg3), uintptr(arg4), uintptr(arg5), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pselect(nfd int, r *FdSet, w *FdSet, e *FdSet, timeout *Timespec, sigmask *Sigset_t) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS_PSELECT6, uintptr(nfd), uintptr(unsafe.Pointer(r)), uintptr(unsafe.Pointer(w)), uintptr(unsafe.Pointer(e)), uintptr(unsafe.Pointer(timeout)), uintptr(unsafe.Pointer(sigmask)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func read(fd int, p []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_READ, uintptr(fd), uintptr(_p0), uintptr(len(p)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Removexattr(path string, attr string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_REMOVEXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Renameat2(olddirfd int, oldpath string, newdirfd int, newpath string, flags uint) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_RENAMEAT2, uintptr(olddirfd), uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func RequestKey(keyType string, description string, callback string, destRingid int) (id int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(keyType)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(description)
+	if err != nil {
+		return
+	}
+	var _p2 *byte
+	_p2, err = BytePtrFromString(callback)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall6(SYS_REQUEST_KEY, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(unsafe.Pointer(_p2)), uintptr(destRingid), 0, 0)
+	id = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setdomainname(p []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_SETDOMAINNAME, uintptr(_p0), uintptr(len(p)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sethostname(p []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_SETHOSTNAME, uintptr(_p0), uintptr(len(p)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setpgid(pid int, pgid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETPGID, uintptr(pid), uintptr(pgid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setsid() (pid int, err error) {
+	r0, _, e1 := RawSyscall(SYS_SETSID, 0, 0, 0)
+	pid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Settimeofday(tv *Timeval) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETTIMEOFDAY, uintptr(unsafe.Pointer(tv)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setns(fd int, nstype int) (err error) {
+	_, _, e1 := Syscall(SYS_SETNS, uintptr(fd), uintptr(nstype), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setpriority(which int, who int, prio int) (err error) {
+	_, _, e1 := Syscall(SYS_SETPRIORITY, uintptr(which), uintptr(who), uintptr(prio))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setxattr(path string, attr string, data []byte, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(data) > 0 {
+		_p2 = unsafe.Pointer(&data[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_SETXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(data)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Signalfd(fd int, mask *Sigset_t, flags int) {
+	SyscallNoError(SYS_SIGNALFD4, uintptr(fd), uintptr(unsafe.Pointer(mask)), uintptr(flags))
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Statx(dirfd int, path string, flags int, mask int, stat *Statx_t) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_STATX, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(flags), uintptr(mask), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sync() {
+	SyscallNoError(SYS_SYNC, 0, 0, 0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Syncfs(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_SYNCFS, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sysinfo(info *Sysinfo_t) (err error) {
+	_, _, e1 := RawSyscall(SYS_SYSINFO, uintptr(unsafe.Pointer(info)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Tee(rfd int, wfd int, len int, flags int) (n int64, err error) {
+	r0, r1, e1 := Syscall6(SYS_TEE, uintptr(rfd), uintptr(wfd), uintptr(len), uintptr(flags), 0, 0)
+	n = int64(int64(r1)<<32 | int64(r0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Tgkill(tgid int, tid int, sig syscall.Signal) (err error) {
+	_, _, e1 := RawSyscall(SYS_TGKILL, uintptr(tgid), uintptr(tid), uintptr(sig))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Times(tms *Tms) (ticks uintptr, err error) {
+	r0, _, e1 := RawSyscall(SYS_TIMES, uintptr(unsafe.Pointer(tms)), 0, 0)
+	ticks = uintptr(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Umask(mask int) (oldmask int) {
+	r0, _ := RawSyscallNoError(SYS_UMASK, uintptr(mask), 0, 0)
+	oldmask = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Uname(buf *Utsname) (err error) {
+	_, _, e1 := RawSyscall(SYS_UNAME, uintptr(unsafe.Pointer(buf)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Unmount(target string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(target)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UMOUNT2, uintptr(unsafe.Pointer(_p0)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Unshare(flags int) (err error) {
+	_, _, e1 := Syscall(SYS_UNSHARE, uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func write(fd int, p []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_WRITE, uintptr(fd), uintptr(_p0), uintptr(len(p)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func exitThread(code int) (err error) {
+	_, _, e1 := Syscall(SYS_EXIT, uintptr(code), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func readlen(fd int, p *byte, np int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_READ, uintptr(fd), uintptr(unsafe.Pointer(p)), uintptr(np))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func writelen(fd int, p *byte, np int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_WRITE, uintptr(fd), uintptr(unsafe.Pointer(p)), uintptr(np))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func munmap(addr uintptr, length uintptr) (err error) {
+	_, _, e1 := Syscall(SYS_MUNMAP, uintptr(addr), uintptr(length), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Madvise(b []byte, advice int) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MADVISE, uintptr(_p0), uintptr(len(b)), uintptr(advice))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mprotect(b []byte, prot int) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MPROTECT, uintptr(_p0), uintptr(len(b)), uintptr(prot))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mlock(b []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MLOCK, uintptr(_p0), uintptr(len(b)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mlockall(flags int) (err error) {
+	_, _, e1 := Syscall(SYS_MLOCKALL, uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Msync(b []byte, flags int) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MSYNC, uintptr(_p0), uintptr(len(b)), uintptr(flags))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Munlock(b []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MUNLOCK, uintptr(_p0), uintptr(len(b)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Munlockall() (err error) {
+	_, _, e1 := Syscall(SYS_MUNLOCKALL, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func faccessat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FACCESSAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func nameToHandleAt(dirFD int, pathname string, fh *fileHandle, mountID *_C_int, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(pathname)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_NAME_TO_HANDLE_AT, uintptr(dirFD), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(fh)), uintptr(unsafe.Pointer(mountID)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func openByHandleAt(mountFD int, fh *fileHandle, flags int) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_OPEN_BY_HANDLE_AT, uintptr(mountFD), uintptr(unsafe.Pointer(fh)), uintptr(flags))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func pipe(p *[2]_C_int) (err error) {
+	_, _, e1 := RawSyscall(SYS_PIPE, uintptr(unsafe.Pointer(p)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func pipe2(p *[2]_C_int, flags int) (err error) {
+	_, _, e1 := RawSyscall(SYS_PIPE2, uintptr(unsafe.Pointer(p)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func accept(s int, rsa *RawSockaddrAny, addrlen *_Socklen) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_ACCEPT, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func accept4(s int, rsa *RawSockaddrAny, addrlen *_Socklen, flags int) (fd int, err error) {
+	r0, _, e1 := Syscall6(SYS_ACCEPT4, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), uintptr(flags), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func bind(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := Syscall(SYS_BIND, uintptr(s), uintptr(addr), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func connect(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := Syscall(SYS_CONNECT, uintptr(s), uintptr(addr), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getgroups(n int, list *_Gid_t) (nn int, err error) {
+	r0, _, e1 := RawSyscall(SYS_GETGROUPS32, uintptr(n), uintptr(unsafe.Pointer(list)), 0)
+	nn = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setgroups(n int, list *_Gid_t) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETGROUPS32, uintptr(n), uintptr(unsafe.Pointer(list)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getsockopt(s int, level int, name int, val unsafe.Pointer, vallen *_Socklen) (err error) {
+	_, _, e1 := Syscall6(SYS_GETSOCKOPT, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(unsafe.Pointer(vallen)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setsockopt(s int, level int, name int, val unsafe.Pointer, vallen uintptr) (err error) {
+	_, _, e1 := Syscall6(SYS_SETSOCKOPT, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(vallen), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func socket(domain int, typ int, proto int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_SOCKET, uintptr(domain), uintptr(typ), uintptr(proto))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getpeername(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETPEERNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getsockname(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETSOCKNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func recvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_RECVFROM, uintptr(fd), uintptr(_p0), uintptr(len(p)), uintptr(flags), uintptr(unsafe.Pointer(from)), uintptr(unsafe.Pointer(fromlen)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_SENDTO, uintptr(s), uintptr(_p0), uintptr(len(buf)), uintptr(flags), uintptr(to), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func socketpair(domain int, typ int, flags int, fd *[2]int32) (err error) {
+	_, _, e1 := RawSyscall6(SYS_SOCKETPAIR, uintptr(domain), uintptr(typ), uintptr(flags), uintptr(unsafe.Pointer(fd)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func recvmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_RECVMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_SENDMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup2(oldfd int, newfd int) (err error) {
+	_, _, e1 := Syscall(SYS_DUP2, uintptr(oldfd), uintptr(newfd), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCreate(size int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_EPOLL_CREATE, uintptr(size), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(events) > 0 {
+		_p0 = unsafe.Pointer(&events[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_EPOLL_WAIT, uintptr(epfd), uintptr(_p0), uintptr(len(events)), uintptr(msec), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchown(fd int, uid int, gid int) (err error) {
+	_, _, e1 := Syscall(SYS_FCHOWN32, uintptr(fd), uintptr(uid), uintptr(gid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fstat(fd int, stat *Stat_t) (err error) {
+	_, _, e1 := Syscall(SYS_FSTAT64, uintptr(fd), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fstatat(dirfd int, path string, stat *Stat_t, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_FSTATAT64, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getegid() (egid int) {
+	r0, _ := RawSyscallNoError(SYS_GETEGID32, 0, 0, 0)
+	egid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Geteuid() (euid int) {
+	r0, _ := RawSyscallNoError(SYS_GETEUID32, 0, 0, 0)
+	euid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getgid() (gid int) {
+	r0, _ := RawSyscallNoError(SYS_GETGID32, 0, 0, 0)
+	gid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getuid() (uid int) {
+	r0, _ := RawSyscallNoError(SYS_GETUID32, 0, 0, 0)
+	uid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyInit() (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_INIT, 0, 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Lchown(path string, uid int, gid int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_LCHOWN32, uintptr(unsafe.Pointer(_p0)), uintptr(uid), uintptr(gid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Listen(s int, n int) (err error) {
+	_, _, e1 := Syscall(SYS_LISTEN, uintptr(s), uintptr(n), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Lstat(path string, stat *Stat_t) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_LSTAT64, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pause() (err error) {
+	_, _, e1 := Syscall(SYS_PAUSE, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Renameat(olddirfd int, oldpath string, newdirfd int, newpath string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_RENAMEAT, uintptr(olddirfd), uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendfile(outfd int, infd int, offset *int64, count int) (written int, err error) {
+	r0, _, e1 := Syscall6(SYS_SENDFILE64, uintptr(outfd), uintptr(infd), uintptr(unsafe.Pointer(offset)), uintptr(count), 0, 0)
+	written = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Select(nfd int, r *FdSet, w *FdSet, e *FdSet, timeout *Timeval) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS__NEWSELECT, uintptr(nfd), uintptr(unsafe.Pointer(r)), uintptr(unsafe.Pointer(w)), uintptr(unsafe.Pointer(e)), uintptr(unsafe.Pointer(timeout)), 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setfsgid(gid int) (err error) {
+	_, _, e1 := Syscall(SYS_SETFSGID32, uintptr(gid), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setfsuid(uid int) (err error) {
+	_, _, e1 := Syscall(SYS_SETFSUID32, uintptr(uid), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setregid(rgid int, egid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETREGID32, uintptr(rgid), uintptr(egid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setresgid(rgid int, egid int, sgid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRESGID32, uintptr(rgid), uintptr(egid), uintptr(sgid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setresuid(ruid int, euid int, suid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRESUID32, uintptr(ruid), uintptr(euid), uintptr(suid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setreuid(ruid int, euid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETREUID32, uintptr(ruid), uintptr(euid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Shutdown(fd int, how int) (err error) {
+	_, _, e1 := Syscall(SYS_SHUTDOWN, uintptr(fd), uintptr(how), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Splice(rfd int, roff *int64, wfd int, woff *int64, len int, flags int) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS_SPLICE, uintptr(rfd), uintptr(unsafe.Pointer(roff)), uintptr(wfd), uintptr(unsafe.Pointer(woff)), uintptr(len), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Stat(path string, stat *Stat_t) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_STAT64, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Ustat(dev int, ubuf *Ustat_t) (err error) {
+	_, _, e1 := Syscall(SYS_USTAT, uintptr(dev), uintptr(unsafe.Pointer(ubuf)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func futimesat(dirfd int, path string, times *[2]Timeval) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FUTIMESAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Gettimeofday(tv *Timeval) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETTIMEOFDAY, uintptr(unsafe.Pointer(tv)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func utimes(path string, times *[2]Timeval) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UTIMES, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pread(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_PREAD64, uintptr(fd), uintptr(_p0), uintptr(len(p)), 0, uintptr(offset), uintptr(offset>>32))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pwrite(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_PWRITE64, uintptr(fd), uintptr(_p0), uintptr(len(p)), 0, uintptr(offset), uintptr(offset>>32))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Truncate(path string, length int64) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_TRUNCATE64, uintptr(unsafe.Pointer(_p0)), 0, uintptr(length), uintptr(length>>32), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Ftruncate(fd int, length int64) (err error) {
+	_, _, e1 := Syscall6(SYS_FTRUNCATE64, uintptr(fd), 0, uintptr(length), uintptr(length>>32), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func mmap2(addr uintptr, length uintptr, prot int, flags int, fd int, pageOffset uintptr) (xaddr uintptr, err error) {
+	r0, _, e1 := Syscall6(SYS_MMAP2, uintptr(addr), uintptr(length), uintptr(prot), uintptr(flags), uintptr(fd), uintptr(pageOffset))
+	xaddr = uintptr(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getrlimit(resource int, rlim *rlimit32) (err error) {
+	_, _, e1 := RawSyscall(SYS_UGETRLIMIT, uintptr(resource), uintptr(unsafe.Pointer(rlim)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setrlimit(resource int, rlim *rlimit32) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRLIMIT, uintptr(resource), uintptr(unsafe.Pointer(rlim)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func poll(fds *PollFd, nfds int, timeout int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_POLL, uintptr(unsafe.Pointer(fds)), uintptr(nfds), uintptr(timeout))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func armSyncFileRange(fd int, flags int, off int64, n int64) (err error) {
+	_, _, e1 := Syscall6(SYS_ARM_SYNC_FILE_RANGE, uintptr(fd), uintptr(flags), uintptr(off), uintptr(off>>32), uintptr(n), uintptr(n>>32))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/zsysnum_linux_thumb.go b/src/cmd/vendor/golang.org/x/sys/unix/zsysnum_linux_thumb.go
new file mode 100644
index 0000000000..9461e4d722
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/zsysnum_linux_thumb.go
@@ -0,0 +1,364 @@
+// go run linux/mksysnum.go -Wall -Werror -static -I/tmp/include /tmp/include/asm/unistd.h
+// Code generated by the command above; see README.md. DO NOT EDIT.
+
+// +build thumb,linux
+
+package unix
+
+const (
+	SYS_RESTART_SYSCALL        = 0
+	SYS_EXIT                   = 1
+	SYS_FORK                   = 2
+	SYS_READ                   = 3
+	SYS_WRITE                  = 4
+	SYS_OPEN                   = 5
+	SYS_CLOSE                  = 6
+	SYS_CREAT                  = 8
+	SYS_LINK                   = 9
+	SYS_UNLINK                 = 10
+	SYS_EXECVE                 = 11
+	SYS_CHDIR                  = 12
+	SYS_MKNOD                  = 14
+	SYS_CHMOD                  = 15
+	SYS_LCHOWN                 = 16
+	SYS_LSEEK                  = 19
+	SYS_GETPID                 = 20
+	SYS_MOUNT                  = 21
+	SYS_SETUID                 = 23
+	SYS_GETUID                 = 24
+	SYS_PTRACE                 = 26
+	SYS_PAUSE                  = 29
+	SYS_ACCESS                 = 33
+	SYS_NICE                   = 34
+	SYS_SYNC                   = 36
+	SYS_KILL                   = 37
+	SYS_RENAME                 = 38
+	SYS_MKDIR                  = 39
+	SYS_RMDIR                  = 40
+	SYS_DUP                    = 41
+	SYS_PIPE                   = 42
+	SYS_TIMES                  = 43
+	SYS_BRK                    = 45
+	SYS_SETGID                 = 46
+	SYS_GETGID                 = 47
+	SYS_GETEUID                = 49
+	SYS_GETEGID                = 50
+	SYS_ACCT                   = 51
+	SYS_UMOUNT2                = 52
+	SYS_IOCTL                  = 54
+	SYS_FCNTL                  = 55
+	SYS_SETPGID                = 57
+	SYS_UMASK                  = 60
+	SYS_CHROOT                 = 61
+	SYS_USTAT                  = 62
+	SYS_DUP2                   = 63
+	SYS_GETPPID                = 64
+	SYS_GETPGRP                = 65
+	SYS_SETSID                 = 66
+	SYS_SIGACTION              = 67
+	SYS_SETREUID               = 70
+	SYS_SETREGID               = 71
+	SYS_SIGSUSPEND             = 72
+	SYS_SIGPENDING             = 73
+	SYS_SETHOSTNAME            = 74
+	SYS_SETRLIMIT              = 75
+	SYS_GETRUSAGE              = 77
+	SYS_GETTIMEOFDAY           = 78
+	SYS_SETTIMEOFDAY           = 79
+	SYS_GETGROUPS              = 80
+	SYS_SETGROUPS              = 81
+	SYS_SYMLINK                = 83
+	SYS_READLINK               = 85
+	SYS_USELIB                 = 86
+	SYS_SWAPON                 = 87
+	SYS_REBOOT                 = 88
+	SYS_MUNMAP                 = 91
+	SYS_TRUNCATE               = 92
+	SYS_FTRUNCATE              = 93
+	SYS_FCHMOD                 = 94
+	SYS_FCHOWN                 = 95
+	SYS_GETPRIORITY            = 96
+	SYS_SETPRIORITY            = 97
+	SYS_STATFS                 = 99
+	SYS_FSTATFS                = 100
+	SYS_SYSLOG                 = 103
+	SYS_SETITIMER              = 104
+	SYS_GETITIMER              = 105
+	SYS_STAT                   = 106
+	SYS_LSTAT                  = 107
+	SYS_FSTAT                  = 108
+	SYS_VHANGUP                = 111
+	SYS_WAIT4                  = 114
+	SYS_SWAPOFF                = 115
+	SYS_SYSINFO                = 116
+	SYS_FSYNC                  = 118
+	SYS_SIGRETURN              = 119
+	SYS_CLONE                  = 120
+	SYS_SETDOMAINNAME          = 121
+	SYS_UNAME                  = 122
+	SYS_ADJTIMEX               = 124
+	SYS_MPROTECT               = 125
+	SYS_SIGPROCMASK            = 126
+	SYS_INIT_MODULE            = 128
+	SYS_DELETE_MODULE          = 129
+	SYS_QUOTACTL               = 131
+	SYS_GETPGID                = 132
+	SYS_FCHDIR                 = 133
+	SYS_BDFLUSH                = 134
+	SYS_SYSFS                  = 135
+	SYS_PERSONALITY            = 136
+	SYS_SETFSUID               = 138
+	SYS_SETFSGID               = 139
+	SYS__LLSEEK                = 140
+	SYS_GETDENTS               = 141
+	SYS__NEWSELECT             = 142
+	SYS_FLOCK                  = 143
+	SYS_MSYNC                  = 144
+	SYS_READV                  = 145
+	SYS_WRITEV                 = 146
+	SYS_GETSID                 = 147
+	SYS_FDATASYNC              = 148
+	SYS__SYSCTL                = 149
+	SYS_MLOCK                  = 150
+	SYS_MUNLOCK                = 151
+	SYS_MLOCKALL               = 152
+	SYS_MUNLOCKALL             = 153
+	SYS_SCHED_SETPARAM         = 154
+	SYS_SCHED_GETPARAM         = 155
+	SYS_SCHED_SETSCHEDULER     = 156
+	SYS_SCHED_GETSCHEDULER     = 157
+	SYS_SCHED_YIELD            = 158
+	SYS_SCHED_GET_PRIORITY_MAX = 159
+	SYS_SCHED_GET_PRIORITY_MIN = 160
+	SYS_SCHED_RR_GET_INTERVAL  = 161
+	SYS_NANOSLEEP              = 162
+	SYS_MREMAP                 = 163
+	SYS_SETRESUID              = 164
+	SYS_GETRESUID              = 165
+	SYS_POLL                   = 168
+	SYS_NFSSERVCTL             = 169
+	SYS_SETRESGID              = 170
+	SYS_GETRESGID              = 171
+	SYS_PRCTL                  = 172
+	SYS_RT_SIGRETURN           = 173
+	SYS_RT_SIGACTION           = 174
+	SYS_RT_SIGPROCMASK         = 175
+	SYS_RT_SIGPENDING          = 176
+	SYS_RT_SIGTIMEDWAIT        = 177
+	SYS_RT_SIGQUEUEINFO        = 178
+	SYS_RT_SIGSUSPEND          = 179
+	SYS_PREAD64                = 180
+	SYS_PWRITE64               = 181
+	SYS_CHOWN                  = 182
+	SYS_GETCWD                 = 183
+	SYS_CAPGET                 = 184
+	SYS_CAPSET                 = 185
+	SYS_SIGALTSTACK            = 186
+	SYS_SENDFILE               = 187
+	SYS_VFORK                  = 190
+	SYS_UGETRLIMIT             = 191
+	SYS_MMAP2                  = 192
+	SYS_TRUNCATE64             = 193
+	SYS_FTRUNCATE64            = 194
+	SYS_STAT64                 = 195
+	SYS_LSTAT64                = 196
+	SYS_FSTAT64                = 197
+	SYS_LCHOWN32               = 198
+	SYS_GETUID32               = 199
+	SYS_GETGID32               = 200
+	SYS_GETEUID32              = 201
+	SYS_GETEGID32              = 202
+	SYS_SETREUID32             = 203
+	SYS_SETREGID32             = 204
+	SYS_GETGROUPS32            = 205
+	SYS_SETGROUPS32            = 206
+	SYS_FCHOWN32               = 207
+	SYS_SETRESUID32            = 208
+	SYS_GETRESUID32            = 209
+	SYS_SETRESGID32            = 210
+	SYS_GETRESGID32            = 211
+	SYS_CHOWN32                = 212
+	SYS_SETUID32               = 213
+	SYS_SETGID32               = 214
+	SYS_SETFSUID32             = 215
+	SYS_SETFSGID32             = 216
+	SYS_GETDENTS64             = 217
+	SYS_PIVOT_ROOT             = 218
+	SYS_MINCORE                = 219
+	SYS_MADVISE                = 220
+	SYS_FCNTL64                = 221
+	SYS_GETTID                 = 224
+	SYS_READAHEAD              = 225
+	SYS_SETXATTR               = 226
+	SYS_LSETXATTR              = 227
+	SYS_FSETXATTR              = 228
+	SYS_GETXATTR               = 229
+	SYS_LGETXATTR              = 230
+	SYS_FGETXATTR              = 231
+	SYS_LISTXATTR              = 232
+	SYS_LLISTXATTR             = 233
+	SYS_FLISTXATTR             = 234
+	SYS_REMOVEXATTR            = 235
+	SYS_LREMOVEXATTR           = 236
+	SYS_FREMOVEXATTR           = 237
+	SYS_TKILL                  = 238
+	SYS_SENDFILE64             = 239
+	SYS_FUTEX                  = 240
+	SYS_SCHED_SETAFFINITY      = 241
+	SYS_SCHED_GETAFFINITY      = 242
+	SYS_IO_SETUP               = 243
+	SYS_IO_DESTROY             = 244
+	SYS_IO_GETEVENTS           = 245
+	SYS_IO_SUBMIT              = 246
+	SYS_IO_CANCEL              = 247
+	SYS_EXIT_GROUP             = 248
+	SYS_LOOKUP_DCOOKIE         = 249
+	SYS_EPOLL_CREATE           = 250
+	SYS_EPOLL_CTL              = 251
+	SYS_EPOLL_WAIT             = 252
+	SYS_REMAP_FILE_PAGES       = 253
+	SYS_SET_TID_ADDRESS        = 256
+	SYS_TIMER_CREATE           = 257
+	SYS_TIMER_SETTIME          = 258
+	SYS_TIMER_GETTIME          = 259
+	SYS_TIMER_GETOVERRUN       = 260
+	SYS_TIMER_DELETE           = 261
+	SYS_CLOCK_SETTIME          = 262
+	SYS_CLOCK_GETTIME          = 263
+	SYS_CLOCK_GETRES           = 264
+	SYS_CLOCK_NANOSLEEP        = 265
+	SYS_STATFS64               = 266
+	SYS_FSTATFS64              = 267
+	SYS_TGKILL                 = 268
+	SYS_UTIMES                 = 269
+	SYS_ARM_FADVISE64_64       = 270
+	SYS_PCICONFIG_IOBASE       = 271
+	SYS_PCICONFIG_READ         = 272
+	SYS_PCICONFIG_WRITE        = 273
+	SYS_MQ_OPEN                = 274
+	SYS_MQ_UNLINK              = 275
+	SYS_MQ_TIMEDSEND           = 276
+	SYS_MQ_TIMEDRECEIVE        = 277
+	SYS_MQ_NOTIFY              = 278
+	SYS_MQ_GETSETATTR          = 279
+	SYS_WAITID                 = 280
+	SYS_SOCKET                 = 281
+	SYS_BIND                   = 282
+	SYS_CONNECT                = 283
+	SYS_LISTEN                 = 284
+	SYS_ACCEPT                 = 285
+	SYS_GETSOCKNAME            = 286
+	SYS_GETPEERNAME            = 287
+	SYS_SOCKETPAIR             = 288
+	SYS_SEND                   = 289
+	SYS_SENDTO                 = 290
+	SYS_RECV                   = 291
+	SYS_RECVFROM               = 292
+	SYS_SHUTDOWN               = 293
+	SYS_SETSOCKOPT             = 294
+	SYS_GETSOCKOPT             = 295
+	SYS_SENDMSG                = 296
+	SYS_RECVMSG                = 297
+	SYS_SEMOP                  = 298
+	SYS_SEMGET                 = 299
+	SYS_SEMCTL                 = 300
+	SYS_MSGSND                 = 301
+	SYS_MSGRCV                 = 302
+	SYS_MSGGET                 = 303
+	SYS_MSGCTL                 = 304
+	SYS_SHMAT                  = 305
+	SYS_SHMDT                  = 306
+	SYS_SHMGET                 = 307
+	SYS_SHMCTL                 = 308
+	SYS_ADD_KEY                = 309
+	SYS_REQUEST_KEY            = 310
+	SYS_KEYCTL                 = 311
+	SYS_SEMTIMEDOP             = 312
+	SYS_VSERVER                = 313
+	SYS_IOPRIO_SET             = 314
+	SYS_IOPRIO_GET             = 315
+	SYS_INOTIFY_INIT           = 316
+	SYS_INOTIFY_ADD_WATCH      = 317
+	SYS_INOTIFY_RM_WATCH       = 318
+	SYS_MBIND                  = 319
+	SYS_GET_MEMPOLICY          = 320
+	SYS_SET_MEMPOLICY          = 321
+	SYS_OPENAT                 = 322
+	SYS_MKDIRAT                = 323
+	SYS_MKNODAT                = 324
+	SYS_FCHOWNAT               = 325
+	SYS_FUTIMESAT              = 326
+	SYS_FSTATAT64              = 327
+	SYS_UNLINKAT               = 328
+	SYS_RENAMEAT               = 329
+	SYS_LINKAT                 = 330
+	SYS_SYMLINKAT              = 331
+	SYS_READLINKAT             = 332
+	SYS_FCHMODAT               = 333
+	SYS_FACCESSAT              = 334
+	SYS_PSELECT6               = 335
+	SYS_PPOLL                  = 336
+	SYS_UNSHARE                = 337
+	SYS_SET_ROBUST_LIST        = 338
+	SYS_GET_ROBUST_LIST        = 339
+	SYS_SPLICE                 = 340
+	SYS_ARM_SYNC_FILE_RANGE    = 341
+	SYS_TEE                    = 342
+	SYS_VMSPLICE               = 343
+	SYS_MOVE_PAGES             = 344
+	SYS_GETCPU                 = 345
+	SYS_EPOLL_PWAIT            = 346
+	SYS_KEXEC_LOAD             = 347
+	SYS_UTIMENSAT              = 348
+	SYS_SIGNALFD               = 349
+	SYS_TIMERFD_CREATE         = 350
+	SYS_EVENTFD                = 351
+	SYS_FALLOCATE              = 352
+	SYS_TIMERFD_SETTIME        = 353
+	SYS_TIMERFD_GETTIME        = 354
+	SYS_SIGNALFD4              = 355
+	SYS_EVENTFD2               = 356
+	SYS_EPOLL_CREATE1          = 357
+	SYS_DUP3                   = 358
+	SYS_PIPE2                  = 359
+	SYS_INOTIFY_INIT1          = 360
+	SYS_PREADV                 = 361
+	SYS_PWRITEV                = 362
+	SYS_RT_TGSIGQUEUEINFO      = 363
+	SYS_PERF_EVENT_OPEN        = 364
+	SYS_RECVMMSG               = 365
+	SYS_ACCEPT4                = 366
+	SYS_FANOTIFY_INIT          = 367
+	SYS_FANOTIFY_MARK          = 368
+	SYS_PRLIMIT64              = 369
+	SYS_NAME_TO_HANDLE_AT      = 370
+	SYS_OPEN_BY_HANDLE_AT      = 371
+	SYS_CLOCK_ADJTIME          = 372
+	SYS_SYNCFS                 = 373
+	SYS_SENDMMSG               = 374
+	SYS_SETNS                  = 375
+	SYS_PROCESS_VM_READV       = 376
+	SYS_PROCESS_VM_WRITEV      = 377
+	SYS_KCMP                   = 378
+	SYS_FINIT_MODULE           = 379
+	SYS_SCHED_SETATTR          = 380
+	SYS_SCHED_GETATTR          = 381
+	SYS_RENAMEAT2              = 382
+	SYS_SECCOMP                = 383
+	SYS_GETRANDOM              = 384
+	SYS_MEMFD_CREATE           = 385
+	SYS_BPF                    = 386
+	SYS_EXECVEAT               = 387
+	SYS_USERFAULTFD            = 388
+	SYS_MEMBARRIER             = 389
+	SYS_MLOCK2                 = 390
+	SYS_COPY_FILE_RANGE        = 391
+	SYS_PREADV2                = 392
+	SYS_PWRITEV2               = 393
+	SYS_PKEY_MPROTECT          = 394
+	SYS_PKEY_ALLOC             = 395
+	SYS_PKEY_FREE              = 396
+	SYS_STATX                  = 397
+	SYS_RSEQ                   = 398
+	SYS_IO_PGETEVENTS          = 399
+)
diff --git a/src/cmd/vendor/golang.org/x/sys/unix/ztypes_linux_thumb.go b/src/cmd/vendor/golang.org/x/sys/unix/ztypes_linux_thumb.go
new file mode 100644
index 0000000000..b923a4b94f
--- /dev/null
+++ b/src/cmd/vendor/golang.org/x/sys/unix/ztypes_linux_thumb.go
@@ -0,0 +1,2306 @@
+// cgo -godefs -- -Wall -Werror -static -I/tmp/include linux/types.go | go run mkpost.go
+// Code generated by the command above; see README.md. DO NOT EDIT.
+
+// +build thumb,linux
+
+package unix
+
+const (
+	SizeofPtr      = 0x4
+	SizeofShort    = 0x2
+	SizeofInt      = 0x4
+	SizeofLong     = 0x4
+	SizeofLongLong = 0x8
+	PathMax        = 0x1000
+)
+
+type (
+	_C_short     int16
+	_C_int       int32
+	_C_long      int32
+	_C_long_long int64
+)
+
+type Timespec struct {
+	Sec  int32
+	Nsec int32
+}
+
+type Timeval struct {
+	Sec  int32
+	Usec int32
+}
+
+type Timex struct {
+	Modes     uint32
+	Offset    int32
+	Freq      int32
+	Maxerror  int32
+	Esterror  int32
+	Status    int32
+	Constant  int32
+	Precision int32
+	Tolerance int32
+	Time      Timeval
+	Tick      int32
+	Ppsfreq   int32
+	Jitter    int32
+	Shift     int32
+	Stabil    int32
+	Jitcnt    int32
+	Calcnt    int32
+	Errcnt    int32
+	Stbcnt    int32
+	Tai       int32
+	_         [44]byte
+}
+
+type Time_t int32
+
+type Tms struct {
+	Utime  int32
+	Stime  int32
+	Cutime int32
+	Cstime int32
+}
+
+type Utimbuf struct {
+	Actime  int32
+	Modtime int32
+}
+
+type Rusage struct {
+	Utime    Timeval
+	Stime    Timeval
+	Maxrss   int32
+	Ixrss    int32
+	Idrss    int32
+	Isrss    int32
+	Minflt   int32
+	Majflt   int32
+	Nswap    int32
+	Inblock  int32
+	Oublock  int32
+	Msgsnd   int32
+	Msgrcv   int32
+	Nsignals int32
+	Nvcsw    int32
+	Nivcsw   int32
+}
+
+type Rlimit struct {
+	Cur uint64
+	Max uint64
+}
+
+type _Gid_t uint32
+
+type Stat_t struct {
+	Dev     uint64
+	_       uint16
+	_       uint32
+	Mode    uint32
+	Nlink   uint32
+	Uid     uint32
+	Gid     uint32
+	Rdev    uint64
+	_       uint16
+	_       [4]byte
+	Size    int64
+	Blksize int32
+	_       [4]byte
+	Blocks  int64
+	Atim    Timespec
+	Mtim    Timespec
+	Ctim    Timespec
+	Ino     uint64
+}
+
+type StatxTimestamp struct {
+	Sec  int64
+	Nsec uint32
+	_    int32
+}
+
+type Statx_t struct {
+	Mask            uint32
+	Blksize         uint32
+	Attributes      uint64
+	Nlink           uint32
+	Uid             uint32
+	Gid             uint32
+	Mode            uint16
+	_               [1]uint16
+	Ino             uint64
+	Size            uint64
+	Blocks          uint64
+	Attributes_mask uint64
+	Atime           StatxTimestamp
+	Btime           StatxTimestamp
+	Ctime           StatxTimestamp
+	Mtime           StatxTimestamp
+	Rdev_major      uint32
+	Rdev_minor      uint32
+	Dev_major       uint32
+	Dev_minor       uint32
+	_               [14]uint64
+}
+
+type Dirent struct {
+	Ino    uint64
+	Off    int64
+	Reclen uint16
+	Type   uint8
+	Name   [256]uint8
+	_      [5]byte
+}
+
+type Fsid struct {
+	Val [2]int32
+}
+
+type Flock_t struct {
+	Type   int16
+	Whence int16
+	_      [4]byte
+	Start  int64
+	Len    int64
+	Pid    int32
+	_      [4]byte
+}
+
+type FscryptPolicy struct {
+	Version                   uint8
+	Contents_encryption_mode  uint8
+	Filenames_encryption_mode uint8
+	Flags                     uint8
+	Master_key_descriptor     [8]uint8
+}
+
+type FscryptKey struct {
+	Mode uint32
+	Raw  [64]uint8
+	Size uint32
+}
+
+type KeyctlDHParams struct {
+	Private int32
+	Prime   int32
+	Base    int32
+}
+
+const (
+	FADV_NORMAL     = 0x0
+	FADV_RANDOM     = 0x1
+	FADV_SEQUENTIAL = 0x2
+	FADV_WILLNEED   = 0x3
+	FADV_DONTNEED   = 0x4
+	FADV_NOREUSE    = 0x5
+)
+
+type RawSockaddrInet4 struct {
+	Family uint16
+	Port   uint16
+	Addr   [4]byte /* in_addr */
+	Zero   [8]uint8
+}
+
+type RawSockaddrInet6 struct {
+	Family   uint16
+	Port     uint16
+	Flowinfo uint32
+	Addr     [16]byte /* in6_addr */
+	Scope_id uint32
+}
+
+type RawSockaddrUnix struct {
+	Family uint16
+	Path   [108]int8
+}
+
+type RawSockaddrLinklayer struct {
+	Family   uint16
+	Protocol uint16
+	Ifindex  int32
+	Hatype   uint16
+	Pkttype  uint8
+	Halen    uint8
+	Addr     [8]uint8
+}
+
+type RawSockaddrNetlink struct {
+	Family uint16
+	Pad    uint16
+	Pid    uint32
+	Groups uint32
+}
+
+type RawSockaddrHCI struct {
+	Family  uint16
+	Dev     uint16
+	Channel uint16
+}
+
+type RawSockaddrL2 struct {
+	Family      uint16
+	Psm         uint16
+	Bdaddr      [6]uint8
+	Cid         uint16
+	Bdaddr_type uint8
+	_           [1]byte
+}
+
+type RawSockaddrRFCOMM struct {
+	Family  uint16
+	Bdaddr  [6]uint8
+	Channel uint8
+	_       [1]byte
+}
+
+type RawSockaddrCAN struct {
+	Family  uint16
+	Ifindex int32
+	Addr    [8]byte
+}
+
+type RawSockaddrALG struct {
+	Family uint16
+	Type   [14]uint8
+	Feat   uint32
+	Mask   uint32
+	Name   [64]uint8
+}
+
+type RawSockaddrVM struct {
+	Family    uint16
+	Reserved1 uint16
+	Port      uint32
+	Cid       uint32
+	Zero      [4]uint8
+}
+
+type RawSockaddrXDP struct {
+	Family         uint16
+	Flags          uint16
+	Ifindex        uint32
+	Queue_id       uint32
+	Shared_umem_fd uint32
+}
+
+type RawSockaddrPPPoX [0x1e]byte
+
+type RawSockaddr struct {
+	Family uint16
+	Data   [14]uint8
+}
+
+type RawSockaddrAny struct {
+	Addr RawSockaddr
+	Pad  [96]uint8
+}
+
+type _Socklen uint32
+
+type Linger struct {
+	Onoff  int32
+	Linger int32
+}
+
+type Iovec struct {
+	Base *byte
+	Len  uint32
+}
+
+type IPMreq struct {
+	Multiaddr [4]byte /* in_addr */
+	Interface [4]byte /* in_addr */
+}
+
+type IPMreqn struct {
+	Multiaddr [4]byte /* in_addr */
+	Address   [4]byte /* in_addr */
+	Ifindex   int32
+}
+
+type IPv6Mreq struct {
+	Multiaddr [16]byte /* in6_addr */
+	Interface uint32
+}
+
+type PacketMreq struct {
+	Ifindex int32
+	Type    uint16
+	Alen    uint16
+	Address [8]uint8
+}
+
+type Msghdr struct {
+	Name       *byte
+	Namelen    uint32
+	Iov        *Iovec
+	Iovlen     uint32
+	Control    *byte
+	Controllen uint32
+	Flags      int32
+}
+
+type Cmsghdr struct {
+	Len   uint32
+	Level int32
+	Type  int32
+}
+
+type Inet4Pktinfo struct {
+	Ifindex  int32
+	Spec_dst [4]byte /* in_addr */
+	Addr     [4]byte /* in_addr */
+}
+
+type Inet6Pktinfo struct {
+	Addr    [16]byte /* in6_addr */
+	Ifindex uint32
+}
+
+type IPv6MTUInfo struct {
+	Addr RawSockaddrInet6
+	Mtu  uint32
+}
+
+type ICMPv6Filter struct {
+	Data [8]uint32
+}
+
+type Ucred struct {
+	Pid int32
+	Uid uint32
+	Gid uint32
+}
+
+type TCPInfo struct {
+	State          uint8
+	Ca_state       uint8
+	Retransmits    uint8
+	Probes         uint8
+	Backoff        uint8
+	Options        uint8
+	Rto            uint32
+	Ato            uint32
+	Snd_mss        uint32
+	Rcv_mss        uint32
+	Unacked        uint32
+	Sacked         uint32
+	Lost           uint32
+	Retrans        uint32
+	Fackets        uint32
+	Last_data_sent uint32
+	Last_ack_sent  uint32
+	Last_data_recv uint32
+	Last_ack_recv  uint32
+	Pmtu           uint32
+	Rcv_ssthresh   uint32
+	Rtt            uint32
+	Rttvar         uint32
+	Snd_ssthresh   uint32
+	Snd_cwnd       uint32
+	Advmss         uint32
+	Reordering     uint32
+	Rcv_rtt        uint32
+	Rcv_space      uint32
+	Total_retrans  uint32
+}
+
+type CanFilter struct {
+	Id   uint32
+	Mask uint32
+}
+
+const (
+	SizeofSockaddrInet4     = 0x10
+	SizeofSockaddrInet6     = 0x1c
+	SizeofSockaddrAny       = 0x70
+	SizeofSockaddrUnix      = 0x6e
+	SizeofSockaddrLinklayer = 0x14
+	SizeofSockaddrNetlink   = 0xc
+	SizeofSockaddrHCI       = 0x6
+	SizeofSockaddrL2        = 0xe
+	SizeofSockaddrRFCOMM    = 0xa
+	SizeofSockaddrCAN       = 0x10
+	SizeofSockaddrALG       = 0x58
+	SizeofSockaddrVM        = 0x10
+	SizeofSockaddrXDP       = 0x10
+	SizeofSockaddrPPPoX     = 0x1e
+	SizeofLinger            = 0x8
+	SizeofIovec             = 0x8
+	SizeofIPMreq            = 0x8
+	SizeofIPMreqn           = 0xc
+	SizeofIPv6Mreq          = 0x14
+	SizeofPacketMreq        = 0x10
+	SizeofMsghdr            = 0x1c
+	SizeofCmsghdr           = 0xc
+	SizeofInet4Pktinfo      = 0xc
+	SizeofInet6Pktinfo      = 0x14
+	SizeofIPv6MTUInfo       = 0x20
+	SizeofICMPv6Filter      = 0x20
+	SizeofUcred             = 0xc
+	SizeofTCPInfo           = 0x68
+	SizeofCanFilter         = 0x8
+)
+
+const (
+	NDA_UNSPEC              = 0x0
+	NDA_DST                 = 0x1
+	NDA_LLADDR              = 0x2
+	NDA_CACHEINFO           = 0x3
+	NDA_PROBES              = 0x4
+	NDA_VLAN                = 0x5
+	NDA_PORT                = 0x6
+	NDA_VNI                 = 0x7
+	NDA_IFINDEX             = 0x8
+	NDA_MASTER              = 0x9
+	NDA_LINK_NETNSID        = 0xa
+	NDA_SRC_VNI             = 0xb
+	NTF_USE                 = 0x1
+	NTF_SELF                = 0x2
+	NTF_MASTER              = 0x4
+	NTF_PROXY               = 0x8
+	NTF_EXT_LEARNED         = 0x10
+	NTF_OFFLOADED           = 0x20
+	NTF_ROUTER              = 0x80
+	NUD_INCOMPLETE          = 0x1
+	NUD_REACHABLE           = 0x2
+	NUD_STALE               = 0x4
+	NUD_DELAY               = 0x8
+	NUD_PROBE               = 0x10
+	NUD_FAILED              = 0x20
+	NUD_NOARP               = 0x40
+	NUD_PERMANENT           = 0x80
+	NUD_NONE                = 0x0
+	IFA_UNSPEC              = 0x0
+	IFA_ADDRESS             = 0x1
+	IFA_LOCAL               = 0x2
+	IFA_LABEL               = 0x3
+	IFA_BROADCAST           = 0x4
+	IFA_ANYCAST             = 0x5
+	IFA_CACHEINFO           = 0x6
+	IFA_MULTICAST           = 0x7
+	IFA_FLAGS               = 0x8
+	IFA_RT_PRIORITY         = 0x9
+	IFA_TARGET_NETNSID      = 0xa
+	IFLA_UNSPEC             = 0x0
+	IFLA_ADDRESS            = 0x1
+	IFLA_BROADCAST          = 0x2
+	IFLA_IFNAME             = 0x3
+	IFLA_MTU                = 0x4
+	IFLA_LINK               = 0x5
+	IFLA_QDISC              = 0x6
+	IFLA_STATS              = 0x7
+	IFLA_COST               = 0x8
+	IFLA_PRIORITY           = 0x9
+	IFLA_MASTER             = 0xa
+	IFLA_WIRELESS           = 0xb
+	IFLA_PROTINFO           = 0xc
+	IFLA_TXQLEN             = 0xd
+	IFLA_MAP                = 0xe
+	IFLA_WEIGHT             = 0xf
+	IFLA_OPERSTATE          = 0x10
+	IFLA_LINKMODE           = 0x11
+	IFLA_LINKINFO           = 0x12
+	IFLA_NET_NS_PID         = 0x13
+	IFLA_IFALIAS            = 0x14
+	IFLA_NUM_VF             = 0x15
+	IFLA_VFINFO_LIST        = 0x16
+	IFLA_STATS64            = 0x17
+	IFLA_VF_PORTS           = 0x18
+	IFLA_PORT_SELF          = 0x19
+	IFLA_AF_SPEC            = 0x1a
+	IFLA_GROUP              = 0x1b
+	IFLA_NET_NS_FD          = 0x1c
+	IFLA_EXT_MASK           = 0x1d
+	IFLA_PROMISCUITY        = 0x1e
+	IFLA_NUM_TX_QUEUES      = 0x1f
+	IFLA_NUM_RX_QUEUES      = 0x20
+	IFLA_CARRIER            = 0x21
+	IFLA_PHYS_PORT_ID       = 0x22
+	IFLA_CARRIER_CHANGES    = 0x23
+	IFLA_PHYS_SWITCH_ID     = 0x24
+	IFLA_LINK_NETNSID       = 0x25
+	IFLA_PHYS_PORT_NAME     = 0x26
+	IFLA_PROTO_DOWN         = 0x27
+	IFLA_GSO_MAX_SEGS       = 0x28
+	IFLA_GSO_MAX_SIZE       = 0x29
+	IFLA_PAD                = 0x2a
+	IFLA_XDP                = 0x2b
+	IFLA_EVENT              = 0x2c
+	IFLA_NEW_NETNSID        = 0x2d
+	IFLA_IF_NETNSID         = 0x2e
+	IFLA_TARGET_NETNSID     = 0x2e
+	IFLA_CARRIER_UP_COUNT   = 0x2f
+	IFLA_CARRIER_DOWN_COUNT = 0x30
+	IFLA_NEW_IFINDEX        = 0x31
+	IFLA_MIN_MTU            = 0x32
+	IFLA_MAX_MTU            = 0x33
+	IFLA_MAX                = 0x33
+	IFLA_INFO_KIND          = 0x1
+	IFLA_INFO_DATA          = 0x2
+	IFLA_INFO_XSTATS        = 0x3
+	IFLA_INFO_SLAVE_KIND    = 0x4
+	IFLA_INFO_SLAVE_DATA    = 0x5
+	RT_SCOPE_UNIVERSE       = 0x0
+	RT_SCOPE_SITE           = 0xc8
+	RT_SCOPE_LINK           = 0xfd
+	RT_SCOPE_HOST           = 0xfe
+	RT_SCOPE_NOWHERE        = 0xff
+	RT_TABLE_UNSPEC         = 0x0
+	RT_TABLE_COMPAT         = 0xfc
+	RT_TABLE_DEFAULT        = 0xfd
+	RT_TABLE_MAIN           = 0xfe
+	RT_TABLE_LOCAL          = 0xff
+	RT_TABLE_MAX            = 0xffffffff
+	RTA_UNSPEC              = 0x0
+	RTA_DST                 = 0x1
+	RTA_SRC                 = 0x2
+	RTA_IIF                 = 0x3
+	RTA_OIF                 = 0x4
+	RTA_GATEWAY             = 0x5
+	RTA_PRIORITY            = 0x6
+	RTA_PREFSRC             = 0x7
+	RTA_METRICS             = 0x8
+	RTA_MULTIPATH           = 0x9
+	RTA_FLOW                = 0xb
+	RTA_CACHEINFO           = 0xc
+	RTA_TABLE               = 0xf
+	RTA_MARK                = 0x10
+	RTA_MFC_STATS           = 0x11
+	RTA_VIA                 = 0x12
+	RTA_NEWDST              = 0x13
+	RTA_PREF                = 0x14
+	RTA_ENCAP_TYPE          = 0x15
+	RTA_ENCAP               = 0x16
+	RTA_EXPIRES             = 0x17
+	RTA_PAD                 = 0x18
+	RTA_UID                 = 0x19
+	RTA_TTL_PROPAGATE       = 0x1a
+	RTA_IP_PROTO            = 0x1b
+	RTA_SPORT               = 0x1c
+	RTA_DPORT               = 0x1d
+	RTN_UNSPEC              = 0x0
+	RTN_UNICAST             = 0x1
+	RTN_LOCAL               = 0x2
+	RTN_BROADCAST           = 0x3
+	RTN_ANYCAST             = 0x4
+	RTN_MULTICAST           = 0x5
+	RTN_BLACKHOLE           = 0x6
+	RTN_UNREACHABLE         = 0x7
+	RTN_PROHIBIT            = 0x8
+	RTN_THROW               = 0x9
+	RTN_NAT                 = 0xa
+	RTN_XRESOLVE            = 0xb
+	RTNLGRP_NONE            = 0x0
+	RTNLGRP_LINK            = 0x1
+	RTNLGRP_NOTIFY          = 0x2
+	RTNLGRP_NEIGH           = 0x3
+	RTNLGRP_TC              = 0x4
+	RTNLGRP_IPV4_IFADDR     = 0x5
+	RTNLGRP_IPV4_MROUTE     = 0x6
+	RTNLGRP_IPV4_ROUTE      = 0x7
+	RTNLGRP_IPV4_RULE       = 0x8
+	RTNLGRP_IPV6_IFADDR     = 0x9
+	RTNLGRP_IPV6_MROUTE     = 0xa
+	RTNLGRP_IPV6_ROUTE      = 0xb
+	RTNLGRP_IPV6_IFINFO     = 0xc
+	RTNLGRP_IPV6_PREFIX     = 0x12
+	RTNLGRP_IPV6_RULE       = 0x13
+	RTNLGRP_ND_USEROPT      = 0x14
+	SizeofNlMsghdr          = 0x10
+	SizeofNlMsgerr          = 0x14
+	SizeofRtGenmsg          = 0x1
+	SizeofNlAttr            = 0x4
+	SizeofRtAttr            = 0x4
+	SizeofIfInfomsg         = 0x10
+	SizeofIfAddrmsg         = 0x8
+	SizeofRtMsg             = 0xc
+	SizeofRtNexthop         = 0x8
+	SizeofNdUseroptmsg      = 0x10
+	SizeofNdMsg             = 0xc
+)
+
+type NlMsghdr struct {
+	Len   uint32
+	Type  uint16
+	Flags uint16
+	Seq   uint32
+	Pid   uint32
+}
+
+type NlMsgerr struct {
+	Error int32
+	Msg   NlMsghdr
+}
+
+type RtGenmsg struct {
+	Family uint8
+}
+
+type NlAttr struct {
+	Len  uint16
+	Type uint16
+}
+
+type RtAttr struct {
+	Len  uint16
+	Type uint16
+}
+
+type IfInfomsg struct {
+	Family uint8
+	_      uint8
+	Type   uint16
+	Index  int32
+	Flags  uint32
+	Change uint32
+}
+
+type IfAddrmsg struct {
+	Family    uint8
+	Prefixlen uint8
+	Flags     uint8
+	Scope     uint8
+	Index     uint32
+}
+
+type RtMsg struct {
+	Family   uint8
+	Dst_len  uint8
+	Src_len  uint8
+	Tos      uint8
+	Table    uint8
+	Protocol uint8
+	Scope    uint8
+	Type     uint8
+	Flags    uint32
+}
+
+type RtNexthop struct {
+	Len     uint16
+	Flags   uint8
+	Hops    uint8
+	Ifindex int32
+}
+
+type NdUseroptmsg struct {
+	Family    uint8
+	Pad1      uint8
+	Opts_len  uint16
+	Ifindex   int32
+	Icmp_type uint8
+	Icmp_code uint8
+	Pad2      uint16
+	Pad3      uint32
+}
+
+type NdMsg struct {
+	Family  uint8
+	Pad1    uint8
+	Pad2    uint16
+	Ifindex int32
+	State   uint16
+	Flags   uint8
+	Type    uint8
+}
+
+const (
+	SizeofSockFilter = 0x8
+	SizeofSockFprog  = 0x8
+)
+
+type SockFilter struct {
+	Code uint16
+	Jt   uint8
+	Jf   uint8
+	K    uint32
+}
+
+type SockFprog struct {
+	Len    uint16
+	Filter *SockFilter
+}
+
+type InotifyEvent struct {
+	Wd     int32
+	Mask   uint32
+	Cookie uint32
+	Len    uint32
+}
+
+const SizeofInotifyEvent = 0x10
+
+type PtraceRegs struct {
+	Uregs [18]uint32
+}
+
+type FdSet struct {
+	Bits [32]int32
+}
+
+type Sysinfo_t struct {
+	Uptime    int32
+	Loads     [3]uint32
+	Totalram  uint32
+	Freeram   uint32
+	Sharedram uint32
+	Bufferram uint32
+	Totalswap uint32
+	Freeswap  uint32
+	Procs     uint16
+	Pad       uint16
+	Totalhigh uint32
+	Freehigh  uint32
+	Unit      uint32
+	_         [8]uint8
+}
+
+type Utsname struct {
+	Sysname    [65]byte
+	Nodename   [65]byte
+	Release    [65]byte
+	Version    [65]byte
+	Machine    [65]byte
+	Domainname [65]byte
+}
+
+type Ustat_t struct {
+	Tfree  int32
+	Tinode uint32
+	Fname  [6]uint8
+	Fpack  [6]uint8
+}
+
+type EpollEvent struct {
+	Events uint32
+	PadFd  int32
+	Fd     int32
+	Pad    int32
+}
+
+const (
+	AT_EMPTY_PATH   = 0x1000
+	AT_FDCWD        = -0x64
+	AT_NO_AUTOMOUNT = 0x800
+	AT_REMOVEDIR    = 0x200
+
+	AT_STATX_SYNC_AS_STAT = 0x0
+	AT_STATX_FORCE_SYNC   = 0x2000
+	AT_STATX_DONT_SYNC    = 0x4000
+
+	AT_SYMLINK_FOLLOW   = 0x400
+	AT_SYMLINK_NOFOLLOW = 0x100
+
+	AT_EACCESS = 0x200
+)
+
+type PollFd struct {
+	Fd      int32
+	Events  int16
+	Revents int16
+}
+
+const (
+	POLLIN    = 0x1
+	POLLPRI   = 0x2
+	POLLOUT   = 0x4
+	POLLRDHUP = 0x2000
+	POLLERR   = 0x8
+	POLLHUP   = 0x10
+	POLLNVAL  = 0x20
+)
+
+type Sigset_t struct {
+	Val [32]uint32
+}
+
+type SignalfdSiginfo struct {
+	Signo     uint32
+	Errno     int32
+	Code      int32
+	Pid       uint32
+	Uid       uint32
+	Fd        int32
+	Tid       uint32
+	Band      uint32
+	Overrun   uint32
+	Trapno    uint32
+	Status    int32
+	Int       int32
+	Ptr       uint64
+	Utime     uint64
+	Stime     uint64
+	Addr      uint64
+	Addr_lsb  uint16
+	_         uint16
+	Syscall   int32
+	Call_addr uint64
+	Arch      uint32
+	_         [28]uint8
+}
+
+const PERF_IOC_FLAG_GROUP = 0x1
+
+type Termios struct {
+	Iflag  uint32
+	Oflag  uint32
+	Cflag  uint32
+	Lflag  uint32
+	Line   uint8
+	Cc     [19]uint8
+	Ispeed uint32
+	Ospeed uint32
+}
+
+type Winsize struct {
+	Row    uint16
+	Col    uint16
+	Xpixel uint16
+	Ypixel uint16
+}
+
+type Taskstats struct {
+	Version                   uint16
+	Ac_exitcode               uint32
+	Ac_flag                   uint8
+	Ac_nice                   uint8
+	_                         [4]byte
+	Cpu_count                 uint64
+	Cpu_delay_total           uint64
+	Blkio_count               uint64
+	Blkio_delay_total         uint64
+	Swapin_count              uint64
+	Swapin_delay_total        uint64
+	Cpu_run_real_total        uint64
+	Cpu_run_virtual_total     uint64
+	Ac_comm                   [32]uint8
+	Ac_sched                  uint8
+	Ac_pad                    [3]uint8
+	_                         [4]byte
+	Ac_uid                    uint32
+	Ac_gid                    uint32
+	Ac_pid                    uint32
+	Ac_ppid                   uint32
+	Ac_btime                  uint32
+	_                         [4]byte
+	Ac_etime                  uint64
+	Ac_utime                  uint64
+	Ac_stime                  uint64
+	Ac_minflt                 uint64
+	Ac_majflt                 uint64
+	Coremem                   uint64
+	Virtmem                   uint64
+	Hiwater_rss               uint64
+	Hiwater_vm                uint64
+	Read_char                 uint64
+	Write_char                uint64
+	Read_syscalls             uint64
+	Write_syscalls            uint64
+	Read_bytes                uint64
+	Write_bytes               uint64
+	Cancelled_write_bytes     uint64
+	Nvcsw                     uint64
+	Nivcsw                    uint64
+	Ac_utimescaled            uint64
+	Ac_stimescaled            uint64
+	Cpu_scaled_run_real_total uint64
+	Freepages_count           uint64
+	Freepages_delay_total     uint64
+	Thrashing_count           uint64
+	Thrashing_delay_total     uint64
+}
+
+const (
+	TASKSTATS_CMD_UNSPEC                  = 0x0
+	TASKSTATS_CMD_GET                     = 0x1
+	TASKSTATS_CMD_NEW                     = 0x2
+	TASKSTATS_TYPE_UNSPEC                 = 0x0
+	TASKSTATS_TYPE_PID                    = 0x1
+	TASKSTATS_TYPE_TGID                   = 0x2
+	TASKSTATS_TYPE_STATS                  = 0x3
+	TASKSTATS_TYPE_AGGR_PID               = 0x4
+	TASKSTATS_TYPE_AGGR_TGID              = 0x5
+	TASKSTATS_TYPE_NULL                   = 0x6
+	TASKSTATS_CMD_ATTR_UNSPEC             = 0x0
+	TASKSTATS_CMD_ATTR_PID                = 0x1
+	TASKSTATS_CMD_ATTR_TGID               = 0x2
+	TASKSTATS_CMD_ATTR_REGISTER_CPUMASK   = 0x3
+	TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK = 0x4
+)
+
+type CGroupStats struct {
+	Sleeping        uint64
+	Running         uint64
+	Stopped         uint64
+	Uninterruptible uint64
+	Io_wait         uint64
+}
+
+const (
+	CGROUPSTATS_CMD_UNSPEC        = 0x3
+	CGROUPSTATS_CMD_GET           = 0x4
+	CGROUPSTATS_CMD_NEW           = 0x5
+	CGROUPSTATS_TYPE_UNSPEC       = 0x0
+	CGROUPSTATS_TYPE_CGROUP_STATS = 0x1
+	CGROUPSTATS_CMD_ATTR_UNSPEC   = 0x0
+	CGROUPSTATS_CMD_ATTR_FD       = 0x1
+)
+
+type Genlmsghdr struct {
+	Cmd      uint8
+	Version  uint8
+	Reserved uint16
+}
+
+const (
+	CTRL_CMD_UNSPEC            = 0x0
+	CTRL_CMD_NEWFAMILY         = 0x1
+	CTRL_CMD_DELFAMILY         = 0x2
+	CTRL_CMD_GETFAMILY         = 0x3
+	CTRL_CMD_NEWOPS            = 0x4
+	CTRL_CMD_DELOPS            = 0x5
+	CTRL_CMD_GETOPS            = 0x6
+	CTRL_CMD_NEWMCAST_GRP      = 0x7
+	CTRL_CMD_DELMCAST_GRP      = 0x8
+	CTRL_CMD_GETMCAST_GRP      = 0x9
+	CTRL_ATTR_UNSPEC           = 0x0
+	CTRL_ATTR_FAMILY_ID        = 0x1
+	CTRL_ATTR_FAMILY_NAME      = 0x2
+	CTRL_ATTR_VERSION          = 0x3
+	CTRL_ATTR_HDRSIZE          = 0x4
+	CTRL_ATTR_MAXATTR          = 0x5
+	CTRL_ATTR_OPS              = 0x6
+	CTRL_ATTR_MCAST_GROUPS     = 0x7
+	CTRL_ATTR_OP_UNSPEC        = 0x0
+	CTRL_ATTR_OP_ID            = 0x1
+	CTRL_ATTR_OP_FLAGS         = 0x2
+	CTRL_ATTR_MCAST_GRP_UNSPEC = 0x0
+	CTRL_ATTR_MCAST_GRP_NAME   = 0x1
+	CTRL_ATTR_MCAST_GRP_ID     = 0x2
+)
+
+type cpuMask uint32
+
+const (
+	_CPU_SETSIZE = 0x400
+	_NCPUBITS    = 0x20
+)
+
+const (
+	BDADDR_BREDR     = 0x0
+	BDADDR_LE_PUBLIC = 0x1
+	BDADDR_LE_RANDOM = 0x2
+)
+
+type PerfEventAttr struct {
+	Type               uint32
+	Size               uint32
+	Config             uint64
+	Sample             uint64
+	Sample_type        uint64
+	Read_format        uint64
+	Bits               uint64
+	Wakeup             uint32
+	Bp_type            uint32
+	Ext1               uint64
+	Ext2               uint64
+	Branch_sample_type uint64
+	Sample_regs_user   uint64
+	Sample_stack_user  uint32
+	Clockid            int32
+	Sample_regs_intr   uint64
+	Aux_watermark      uint32
+	Sample_max_stack   uint16
+	_                  uint16
+}
+
+type PerfEventMmapPage struct {
+	Version        uint32
+	Compat_version uint32
+	Lock           uint32
+	Index          uint32
+	Offset         int64
+	Time_enabled   uint64
+	Time_running   uint64
+	Capabilities   uint64
+	Pmc_width      uint16
+	Time_shift     uint16
+	Time_mult      uint32
+	Time_offset    uint64
+	Time_zero      uint64
+	Size           uint32
+	_              [948]uint8
+	Data_head      uint64
+	Data_tail      uint64
+	Data_offset    uint64
+	Data_size      uint64
+	Aux_head       uint64
+	Aux_tail       uint64
+	Aux_offset     uint64
+	Aux_size       uint64
+}
+
+const (
+	PerfBitDisabled               uint64 = CBitFieldMaskBit0
+	PerfBitInherit                       = CBitFieldMaskBit1
+	PerfBitPinned                        = CBitFieldMaskBit2
+	PerfBitExclusive                     = CBitFieldMaskBit3
+	PerfBitExcludeUser                   = CBitFieldMaskBit4
+	PerfBitExcludeKernel                 = CBitFieldMaskBit5
+	PerfBitExcludeHv                     = CBitFieldMaskBit6
+	PerfBitExcludeIdle                   = CBitFieldMaskBit7
+	PerfBitMmap                          = CBitFieldMaskBit8
+	PerfBitComm                          = CBitFieldMaskBit9
+	PerfBitFreq                          = CBitFieldMaskBit10
+	PerfBitInheritStat                   = CBitFieldMaskBit11
+	PerfBitEnableOnExec                  = CBitFieldMaskBit12
+	PerfBitTask                          = CBitFieldMaskBit13
+	PerfBitWatermark                     = CBitFieldMaskBit14
+	PerfBitPreciseIPBit1                 = CBitFieldMaskBit15
+	PerfBitPreciseIPBit2                 = CBitFieldMaskBit16
+	PerfBitMmapData                      = CBitFieldMaskBit17
+	PerfBitSampleIDAll                   = CBitFieldMaskBit18
+	PerfBitExcludeHost                   = CBitFieldMaskBit19
+	PerfBitExcludeGuest                  = CBitFieldMaskBit20
+	PerfBitExcludeCallchainKernel        = CBitFieldMaskBit21
+	PerfBitExcludeCallchainUser          = CBitFieldMaskBit22
+	PerfBitMmap2                         = CBitFieldMaskBit23
+	PerfBitCommExec                      = CBitFieldMaskBit24
+	PerfBitUseClockID                    = CBitFieldMaskBit25
+	PerfBitContextSwitch                 = CBitFieldMaskBit26
+)
+
+const (
+	PERF_TYPE_HARDWARE   = 0x0
+	PERF_TYPE_SOFTWARE   = 0x1
+	PERF_TYPE_TRACEPOINT = 0x2
+	PERF_TYPE_HW_CACHE   = 0x3
+	PERF_TYPE_RAW        = 0x4
+	PERF_TYPE_BREAKPOINT = 0x5
+
+	PERF_COUNT_HW_CPU_CYCLES              = 0x0
+	PERF_COUNT_HW_INSTRUCTIONS            = 0x1
+	PERF_COUNT_HW_CACHE_REFERENCES        = 0x2
+	PERF_COUNT_HW_CACHE_MISSES            = 0x3
+	PERF_COUNT_HW_BRANCH_INSTRUCTIONS     = 0x4
+	PERF_COUNT_HW_BRANCH_MISSES           = 0x5
+	PERF_COUNT_HW_BUS_CYCLES              = 0x6
+	PERF_COUNT_HW_STALLED_CYCLES_FRONTEND = 0x7
+	PERF_COUNT_HW_STALLED_CYCLES_BACKEND  = 0x8
+	PERF_COUNT_HW_REF_CPU_CYCLES          = 0x9
+
+	PERF_COUNT_HW_CACHE_L1D  = 0x0
+	PERF_COUNT_HW_CACHE_L1I  = 0x1
+	PERF_COUNT_HW_CACHE_LL   = 0x2
+	PERF_COUNT_HW_CACHE_DTLB = 0x3
+	PERF_COUNT_HW_CACHE_ITLB = 0x4
+	PERF_COUNT_HW_CACHE_BPU  = 0x5
+	PERF_COUNT_HW_CACHE_NODE = 0x6
+
+	PERF_COUNT_HW_CACHE_OP_READ     = 0x0
+	PERF_COUNT_HW_CACHE_OP_WRITE    = 0x1
+	PERF_COUNT_HW_CACHE_OP_PREFETCH = 0x2
+
+	PERF_COUNT_HW_CACHE_RESULT_ACCESS = 0x0
+	PERF_COUNT_HW_CACHE_RESULT_MISS   = 0x1
+
+	PERF_COUNT_SW_CPU_CLOCK        = 0x0
+	PERF_COUNT_SW_TASK_CLOCK       = 0x1
+	PERF_COUNT_SW_PAGE_FAULTS      = 0x2
+	PERF_COUNT_SW_CONTEXT_SWITCHES = 0x3
+	PERF_COUNT_SW_CPU_MIGRATIONS   = 0x4
+	PERF_COUNT_SW_PAGE_FAULTS_MIN  = 0x5
+	PERF_COUNT_SW_PAGE_FAULTS_MAJ  = 0x6
+	PERF_COUNT_SW_ALIGNMENT_FAULTS = 0x7
+	PERF_COUNT_SW_EMULATION_FAULTS = 0x8
+	PERF_COUNT_SW_DUMMY            = 0x9
+	PERF_COUNT_SW_BPF_OUTPUT       = 0xa
+
+	PERF_SAMPLE_IP           = 0x1
+	PERF_SAMPLE_TID          = 0x2
+	PERF_SAMPLE_TIME         = 0x4
+	PERF_SAMPLE_ADDR         = 0x8
+	PERF_SAMPLE_READ         = 0x10
+	PERF_SAMPLE_CALLCHAIN    = 0x20
+	PERF_SAMPLE_ID           = 0x40
+	PERF_SAMPLE_CPU          = 0x80
+	PERF_SAMPLE_PERIOD       = 0x100
+	PERF_SAMPLE_STREAM_ID    = 0x200
+	PERF_SAMPLE_RAW          = 0x400
+	PERF_SAMPLE_BRANCH_STACK = 0x800
+
+	PERF_SAMPLE_BRANCH_USER       = 0x1
+	PERF_SAMPLE_BRANCH_KERNEL     = 0x2
+	PERF_SAMPLE_BRANCH_HV         = 0x4
+	PERF_SAMPLE_BRANCH_ANY        = 0x8
+	PERF_SAMPLE_BRANCH_ANY_CALL   = 0x10
+	PERF_SAMPLE_BRANCH_ANY_RETURN = 0x20
+	PERF_SAMPLE_BRANCH_IND_CALL   = 0x40
+	PERF_SAMPLE_BRANCH_ABORT_TX   = 0x80
+	PERF_SAMPLE_BRANCH_IN_TX      = 0x100
+	PERF_SAMPLE_BRANCH_NO_TX      = 0x200
+	PERF_SAMPLE_BRANCH_COND       = 0x400
+	PERF_SAMPLE_BRANCH_CALL_STACK = 0x800
+	PERF_SAMPLE_BRANCH_IND_JUMP   = 0x1000
+	PERF_SAMPLE_BRANCH_CALL       = 0x2000
+	PERF_SAMPLE_BRANCH_NO_FLAGS   = 0x4000
+	PERF_SAMPLE_BRANCH_NO_CYCLES  = 0x8000
+	PERF_SAMPLE_BRANCH_TYPE_SAVE  = 0x10000
+
+	PERF_FORMAT_TOTAL_TIME_ENABLED = 0x1
+	PERF_FORMAT_TOTAL_TIME_RUNNING = 0x2
+	PERF_FORMAT_ID                 = 0x4
+	PERF_FORMAT_GROUP              = 0x8
+
+	PERF_RECORD_MMAP            = 0x1
+	PERF_RECORD_LOST            = 0x2
+	PERF_RECORD_COMM            = 0x3
+	PERF_RECORD_EXIT            = 0x4
+	PERF_RECORD_THROTTLE        = 0x5
+	PERF_RECORD_UNTHROTTLE      = 0x6
+	PERF_RECORD_FORK            = 0x7
+	PERF_RECORD_READ            = 0x8
+	PERF_RECORD_SAMPLE          = 0x9
+	PERF_RECORD_MMAP2           = 0xa
+	PERF_RECORD_AUX             = 0xb
+	PERF_RECORD_ITRACE_START    = 0xc
+	PERF_RECORD_LOST_SAMPLES    = 0xd
+	PERF_RECORD_SWITCH          = 0xe
+	PERF_RECORD_SWITCH_CPU_WIDE = 0xf
+	PERF_RECORD_NAMESPACES      = 0x10
+
+	PERF_CONTEXT_HV     = -0x20
+	PERF_CONTEXT_KERNEL = -0x80
+	PERF_CONTEXT_USER   = -0x200
+
+	PERF_CONTEXT_GUEST        = -0x800
+	PERF_CONTEXT_GUEST_KERNEL = -0x880
+	PERF_CONTEXT_GUEST_USER   = -0xa00
+
+	PERF_FLAG_FD_NO_GROUP = 0x1
+	PERF_FLAG_FD_OUTPUT   = 0x2
+	PERF_FLAG_PID_CGROUP  = 0x4
+	PERF_FLAG_FD_CLOEXEC  = 0x8
+)
+
+const (
+	CBitFieldMaskBit0  = 0x1
+	CBitFieldMaskBit1  = 0x2
+	CBitFieldMaskBit2  = 0x4
+	CBitFieldMaskBit3  = 0x8
+	CBitFieldMaskBit4  = 0x10
+	CBitFieldMaskBit5  = 0x20
+	CBitFieldMaskBit6  = 0x40
+	CBitFieldMaskBit7  = 0x80
+	CBitFieldMaskBit8  = 0x100
+	CBitFieldMaskBit9  = 0x200
+	CBitFieldMaskBit10 = 0x400
+	CBitFieldMaskBit11 = 0x800
+	CBitFieldMaskBit12 = 0x1000
+	CBitFieldMaskBit13 = 0x2000
+	CBitFieldMaskBit14 = 0x4000
+	CBitFieldMaskBit15 = 0x8000
+	CBitFieldMaskBit16 = 0x10000
+	CBitFieldMaskBit17 = 0x20000
+	CBitFieldMaskBit18 = 0x40000
+	CBitFieldMaskBit19 = 0x80000
+	CBitFieldMaskBit20 = 0x100000
+	CBitFieldMaskBit21 = 0x200000
+	CBitFieldMaskBit22 = 0x400000
+	CBitFieldMaskBit23 = 0x800000
+	CBitFieldMaskBit24 = 0x1000000
+	CBitFieldMaskBit25 = 0x2000000
+	CBitFieldMaskBit26 = 0x4000000
+	CBitFieldMaskBit27 = 0x8000000
+	CBitFieldMaskBit28 = 0x10000000
+	CBitFieldMaskBit29 = 0x20000000
+	CBitFieldMaskBit30 = 0x40000000
+	CBitFieldMaskBit31 = 0x80000000
+	CBitFieldMaskBit32 = 0x100000000
+	CBitFieldMaskBit33 = 0x200000000
+	CBitFieldMaskBit34 = 0x400000000
+	CBitFieldMaskBit35 = 0x800000000
+	CBitFieldMaskBit36 = 0x1000000000
+	CBitFieldMaskBit37 = 0x2000000000
+	CBitFieldMaskBit38 = 0x4000000000
+	CBitFieldMaskBit39 = 0x8000000000
+	CBitFieldMaskBit40 = 0x10000000000
+	CBitFieldMaskBit41 = 0x20000000000
+	CBitFieldMaskBit42 = 0x40000000000
+	CBitFieldMaskBit43 = 0x80000000000
+	CBitFieldMaskBit44 = 0x100000000000
+	CBitFieldMaskBit45 = 0x200000000000
+	CBitFieldMaskBit46 = 0x400000000000
+	CBitFieldMaskBit47 = 0x800000000000
+	CBitFieldMaskBit48 = 0x1000000000000
+	CBitFieldMaskBit49 = 0x2000000000000
+	CBitFieldMaskBit50 = 0x4000000000000
+	CBitFieldMaskBit51 = 0x8000000000000
+	CBitFieldMaskBit52 = 0x10000000000000
+	CBitFieldMaskBit53 = 0x20000000000000
+	CBitFieldMaskBit54 = 0x40000000000000
+	CBitFieldMaskBit55 = 0x80000000000000
+	CBitFieldMaskBit56 = 0x100000000000000
+	CBitFieldMaskBit57 = 0x200000000000000
+	CBitFieldMaskBit58 = 0x400000000000000
+	CBitFieldMaskBit59 = 0x800000000000000
+	CBitFieldMaskBit60 = 0x1000000000000000
+	CBitFieldMaskBit61 = 0x2000000000000000
+	CBitFieldMaskBit62 = 0x4000000000000000
+	CBitFieldMaskBit63 = 0x8000000000000000
+)
+
+type SockaddrStorage struct {
+	Family uint16
+	_      [122]uint8
+	_      uint32
+}
+
+type TCPMD5Sig struct {
+	Addr      SockaddrStorage
+	Flags     uint8
+	Prefixlen uint8
+	Keylen    uint16
+	_         uint32
+	Key       [80]uint8
+}
+
+type HDDriveCmdHdr struct {
+	Command uint8
+	Number  uint8
+	Feature uint8
+	Count   uint8
+}
+
+type HDGeometry struct {
+	Heads     uint8
+	Sectors   uint8
+	Cylinders uint16
+	Start     uint32
+}
+
+type HDDriveID struct {
+	Config         uint16
+	Cyls           uint16
+	Reserved2      uint16
+	Heads          uint16
+	Track_bytes    uint16
+	Sector_bytes   uint16
+	Sectors        uint16
+	Vendor0        uint16
+	Vendor1        uint16
+	Vendor2        uint16
+	Serial_no      [20]uint8
+	Buf_type       uint16
+	Buf_size       uint16
+	Ecc_bytes      uint16
+	Fw_rev         [8]uint8
+	Model          [40]uint8
+	Max_multsect   uint8
+	Vendor3        uint8
+	Dword_io       uint16
+	Vendor4        uint8
+	Capability     uint8
+	Reserved50     uint16
+	Vendor5        uint8
+	TPIO           uint8
+	Vendor6        uint8
+	TDMA           uint8
+	Field_valid    uint16
+	Cur_cyls       uint16
+	Cur_heads      uint16
+	Cur_sectors    uint16
+	Cur_capacity0  uint16
+	Cur_capacity1  uint16
+	Multsect       uint8
+	Multsect_valid uint8
+	Lba_capacity   uint32
+	Dma_1word      uint16
+	Dma_mword      uint16
+	Eide_pio_modes uint16
+	Eide_dma_min   uint16
+	Eide_dma_time  uint16
+	Eide_pio       uint16
+	Eide_pio_iordy uint16
+	Words69_70     [2]uint16
+	Words71_74     [4]uint16
+	Queue_depth    uint16
+	Words76_79     [4]uint16
+	Major_rev_num  uint16
+	Minor_rev_num  uint16
+	Command_set_1  uint16
+	Command_set_2  uint16
+	Cfsse          uint16
+	Cfs_enable_1   uint16
+	Cfs_enable_2   uint16
+	Csf_default    uint16
+	Dma_ultra      uint16
+	Trseuc         uint16
+	TrsEuc         uint16
+	CurAPMvalues   uint16
+	Mprc           uint16
+	Hw_config      uint16
+	Acoustic       uint16
+	Msrqs          uint16
+	Sxfert         uint16
+	Sal            uint16
+	Spg            uint32
+	Lba_capacity_2 uint64
+	Words104_125   [22]uint16
+	Last_lun       uint16
+	Word127        uint16
+	Dlf            uint16
+	Csfo           uint16
+	Words130_155   [26]uint16
+	Word156        uint16
+	Words157_159   [3]uint16
+	Cfa_power      uint16
+	Words161_175   [15]uint16
+	Words176_205   [30]uint16
+	Words206_254   [49]uint16
+	Integrity_word uint16
+}
+
+type Statfs_t struct {
+	Type    int32
+	Bsize   int32
+	Blocks  uint64
+	Bfree   uint64
+	Bavail  uint64
+	Files   uint64
+	Ffree   uint64
+	Fsid    Fsid
+	Namelen int32
+	Frsize  int32
+	Flags   int32
+	Spare   [4]int32
+	_       [4]byte
+}
+
+const (
+	ST_MANDLOCK    = 0x40
+	ST_NOATIME     = 0x400
+	ST_NODEV       = 0x4
+	ST_NODIRATIME  = 0x800
+	ST_NOEXEC      = 0x8
+	ST_NOSUID      = 0x2
+	ST_RDONLY      = 0x1
+	ST_RELATIME    = 0x1000
+	ST_SYNCHRONOUS = 0x10
+)
+
+type TpacketHdr struct {
+	Status  uint32
+	Len     uint32
+	Snaplen uint32
+	Mac     uint16
+	Net     uint16
+	Sec     uint32
+	Usec    uint32
+}
+
+type Tpacket2Hdr struct {
+	Status    uint32
+	Len       uint32
+	Snaplen   uint32
+	Mac       uint16
+	Net       uint16
+	Sec       uint32
+	Nsec      uint32
+	Vlan_tci  uint16
+	Vlan_tpid uint16
+	_         [4]uint8
+}
+
+type Tpacket3Hdr struct {
+	Next_offset uint32
+	Sec         uint32
+	Nsec        uint32
+	Snaplen     uint32
+	Len         uint32
+	Status      uint32
+	Mac         uint16
+	Net         uint16
+	Hv1         TpacketHdrVariant1
+	_           [8]uint8
+}
+
+type TpacketHdrVariant1 struct {
+	Rxhash    uint32
+	Vlan_tci  uint32
+	Vlan_tpid uint16
+	_         uint16
+}
+
+type TpacketBlockDesc struct {
+	Version uint32
+	To_priv uint32
+	Hdr     [40]byte
+}
+
+type TpacketBDTS struct {
+	Sec  uint32
+	Usec uint32
+}
+
+type TpacketHdrV1 struct {
+	Block_status        uint32
+	Num_pkts            uint32
+	Offset_to_first_pkt uint32
+	Blk_len             uint32
+	Seq_num             uint64
+	Ts_first_pkt        TpacketBDTS
+	Ts_last_pkt         TpacketBDTS
+}
+
+type TpacketReq struct {
+	Block_size uint32
+	Block_nr   uint32
+	Frame_size uint32
+	Frame_nr   uint32
+}
+
+type TpacketReq3 struct {
+	Block_size       uint32
+	Block_nr         uint32
+	Frame_size       uint32
+	Frame_nr         uint32
+	Retire_blk_tov   uint32
+	Sizeof_priv      uint32
+	Feature_req_word uint32
+}
+
+type TpacketStats struct {
+	Packets uint32
+	Drops   uint32
+}
+
+type TpacketStatsV3 struct {
+	Packets      uint32
+	Drops        uint32
+	Freeze_q_cnt uint32
+}
+
+type TpacketAuxdata struct {
+	Status    uint32
+	Len       uint32
+	Snaplen   uint32
+	Mac       uint16
+	Net       uint16
+	Vlan_tci  uint16
+	Vlan_tpid uint16
+}
+
+const (
+	TPACKET_V1 = 0x0
+	TPACKET_V2 = 0x1
+	TPACKET_V3 = 0x2
+)
+
+const (
+	SizeofTpacketHdr  = 0x18
+	SizeofTpacket2Hdr = 0x20
+	SizeofTpacket3Hdr = 0x30
+
+	SizeofTpacketStats   = 0x8
+	SizeofTpacketStatsV3 = 0xc
+)
+
+const (
+	NF_INET_PRE_ROUTING  = 0x0
+	NF_INET_LOCAL_IN     = 0x1
+	NF_INET_FORWARD      = 0x2
+	NF_INET_LOCAL_OUT    = 0x3
+	NF_INET_POST_ROUTING = 0x4
+	NF_INET_NUMHOOKS     = 0x5
+)
+
+const (
+	NF_NETDEV_INGRESS  = 0x0
+	NF_NETDEV_NUMHOOKS = 0x1
+)
+
+const (
+	NFPROTO_UNSPEC   = 0x0
+	NFPROTO_INET     = 0x1
+	NFPROTO_IPV4     = 0x2
+	NFPROTO_ARP      = 0x3
+	NFPROTO_NETDEV   = 0x5
+	NFPROTO_BRIDGE   = 0x7
+	NFPROTO_IPV6     = 0xa
+	NFPROTO_DECNET   = 0xc
+	NFPROTO_NUMPROTO = 0xd
+)
+
+type Nfgenmsg struct {
+	Nfgen_family uint8
+	Version      uint8
+	Res_id       uint16
+}
+
+const (
+	NFNL_BATCH_UNSPEC = 0x0
+	NFNL_BATCH_GENID  = 0x1
+)
+
+const (
+	NFT_REG_VERDICT                   = 0x0
+	NFT_REG_1                         = 0x1
+	NFT_REG_2                         = 0x2
+	NFT_REG_3                         = 0x3
+	NFT_REG_4                         = 0x4
+	NFT_REG32_00                      = 0x8
+	NFT_REG32_01                      = 0x9
+	NFT_REG32_02                      = 0xa
+	NFT_REG32_03                      = 0xb
+	NFT_REG32_04                      = 0xc
+	NFT_REG32_05                      = 0xd
+	NFT_REG32_06                      = 0xe
+	NFT_REG32_07                      = 0xf
+	NFT_REG32_08                      = 0x10
+	NFT_REG32_09                      = 0x11
+	NFT_REG32_10                      = 0x12
+	NFT_REG32_11                      = 0x13
+	NFT_REG32_12                      = 0x14
+	NFT_REG32_13                      = 0x15
+	NFT_REG32_14                      = 0x16
+	NFT_REG32_15                      = 0x17
+	NFT_CONTINUE                      = -0x1
+	NFT_BREAK                         = -0x2
+	NFT_JUMP                          = -0x3
+	NFT_GOTO                          = -0x4
+	NFT_RETURN                        = -0x5
+	NFT_MSG_NEWTABLE                  = 0x0
+	NFT_MSG_GETTABLE                  = 0x1
+	NFT_MSG_DELTABLE                  = 0x2
+	NFT_MSG_NEWCHAIN                  = 0x3
+	NFT_MSG_GETCHAIN                  = 0x4
+	NFT_MSG_DELCHAIN                  = 0x5
+	NFT_MSG_NEWRULE                   = 0x6
+	NFT_MSG_GETRULE                   = 0x7
+	NFT_MSG_DELRULE                   = 0x8
+	NFT_MSG_NEWSET                    = 0x9
+	NFT_MSG_GETSET                    = 0xa
+	NFT_MSG_DELSET                    = 0xb
+	NFT_MSG_NEWSETELEM                = 0xc
+	NFT_MSG_GETSETELEM                = 0xd
+	NFT_MSG_DELSETELEM                = 0xe
+	NFT_MSG_NEWGEN                    = 0xf
+	NFT_MSG_GETGEN                    = 0x10
+	NFT_MSG_TRACE                     = 0x11
+	NFT_MSG_NEWOBJ                    = 0x12
+	NFT_MSG_GETOBJ                    = 0x13
+	NFT_MSG_DELOBJ                    = 0x14
+	NFT_MSG_GETOBJ_RESET              = 0x15
+	NFT_MSG_MAX                       = 0x19
+	NFTA_LIST_UNPEC                   = 0x0
+	NFTA_LIST_ELEM                    = 0x1
+	NFTA_HOOK_UNSPEC                  = 0x0
+	NFTA_HOOK_HOOKNUM                 = 0x1
+	NFTA_HOOK_PRIORITY                = 0x2
+	NFTA_HOOK_DEV                     = 0x3
+	NFT_TABLE_F_DORMANT               = 0x1
+	NFTA_TABLE_UNSPEC                 = 0x0
+	NFTA_TABLE_NAME                   = 0x1
+	NFTA_TABLE_FLAGS                  = 0x2
+	NFTA_TABLE_USE                    = 0x3
+	NFTA_CHAIN_UNSPEC                 = 0x0
+	NFTA_CHAIN_TABLE                  = 0x1
+	NFTA_CHAIN_HANDLE                 = 0x2
+	NFTA_CHAIN_NAME                   = 0x3
+	NFTA_CHAIN_HOOK                   = 0x4
+	NFTA_CHAIN_POLICY                 = 0x5
+	NFTA_CHAIN_USE                    = 0x6
+	NFTA_CHAIN_TYPE                   = 0x7
+	NFTA_CHAIN_COUNTERS               = 0x8
+	NFTA_CHAIN_PAD                    = 0x9
+	NFTA_RULE_UNSPEC                  = 0x0
+	NFTA_RULE_TABLE                   = 0x1
+	NFTA_RULE_CHAIN                   = 0x2
+	NFTA_RULE_HANDLE                  = 0x3
+	NFTA_RULE_EXPRESSIONS             = 0x4
+	NFTA_RULE_COMPAT                  = 0x5
+	NFTA_RULE_POSITION                = 0x6
+	NFTA_RULE_USERDATA                = 0x7
+	NFTA_RULE_PAD                     = 0x8
+	NFTA_RULE_ID                      = 0x9
+	NFT_RULE_COMPAT_F_INV             = 0x2
+	NFT_RULE_COMPAT_F_MASK            = 0x2
+	NFTA_RULE_COMPAT_UNSPEC           = 0x0
+	NFTA_RULE_COMPAT_PROTO            = 0x1
+	NFTA_RULE_COMPAT_FLAGS            = 0x2
+	NFT_SET_ANONYMOUS                 = 0x1
+	NFT_SET_CONSTANT                  = 0x2
+	NFT_SET_INTERVAL                  = 0x4
+	NFT_SET_MAP                       = 0x8
+	NFT_SET_TIMEOUT                   = 0x10
+	NFT_SET_EVAL                      = 0x20
+	NFT_SET_OBJECT                    = 0x40
+	NFT_SET_POL_PERFORMANCE           = 0x0
+	NFT_SET_POL_MEMORY                = 0x1
+	NFTA_SET_DESC_UNSPEC              = 0x0
+	NFTA_SET_DESC_SIZE                = 0x1
+	NFTA_SET_UNSPEC                   = 0x0
+	NFTA_SET_TABLE                    = 0x1
+	NFTA_SET_NAME                     = 0x2
+	NFTA_SET_FLAGS                    = 0x3
+	NFTA_SET_KEY_TYPE                 = 0x4
+	NFTA_SET_KEY_LEN                  = 0x5
+	NFTA_SET_DATA_TYPE                = 0x6
+	NFTA_SET_DATA_LEN                 = 0x7
+	NFTA_SET_POLICY                   = 0x8
+	NFTA_SET_DESC                     = 0x9
+	NFTA_SET_ID                       = 0xa
+	NFTA_SET_TIMEOUT                  = 0xb
+	NFTA_SET_GC_INTERVAL              = 0xc
+	NFTA_SET_USERDATA                 = 0xd
+	NFTA_SET_PAD                      = 0xe
+	NFTA_SET_OBJ_TYPE                 = 0xf
+	NFT_SET_ELEM_INTERVAL_END         = 0x1
+	NFTA_SET_ELEM_UNSPEC              = 0x0
+	NFTA_SET_ELEM_KEY                 = 0x1
+	NFTA_SET_ELEM_DATA                = 0x2
+	NFTA_SET_ELEM_FLAGS               = 0x3
+	NFTA_SET_ELEM_TIMEOUT             = 0x4
+	NFTA_SET_ELEM_EXPIRATION          = 0x5
+	NFTA_SET_ELEM_USERDATA            = 0x6
+	NFTA_SET_ELEM_EXPR                = 0x7
+	NFTA_SET_ELEM_PAD                 = 0x8
+	NFTA_SET_ELEM_OBJREF              = 0x9
+	NFTA_SET_ELEM_LIST_UNSPEC         = 0x0
+	NFTA_SET_ELEM_LIST_TABLE          = 0x1
+	NFTA_SET_ELEM_LIST_SET            = 0x2
+	NFTA_SET_ELEM_LIST_ELEMENTS       = 0x3
+	NFTA_SET_ELEM_LIST_SET_ID         = 0x4
+	NFT_DATA_VALUE                    = 0x0
+	NFT_DATA_VERDICT                  = 0xffffff00
+	NFTA_DATA_UNSPEC                  = 0x0
+	NFTA_DATA_VALUE                   = 0x1
+	NFTA_DATA_VERDICT                 = 0x2
+	NFTA_VERDICT_UNSPEC               = 0x0
+	NFTA_VERDICT_CODE                 = 0x1
+	NFTA_VERDICT_CHAIN                = 0x2
+	NFTA_EXPR_UNSPEC                  = 0x0
+	NFTA_EXPR_NAME                    = 0x1
+	NFTA_EXPR_DATA                    = 0x2
+	NFTA_IMMEDIATE_UNSPEC             = 0x0
+	NFTA_IMMEDIATE_DREG               = 0x1
+	NFTA_IMMEDIATE_DATA               = 0x2
+	NFTA_BITWISE_UNSPEC               = 0x0
+	NFTA_BITWISE_SREG                 = 0x1
+	NFTA_BITWISE_DREG                 = 0x2
+	NFTA_BITWISE_LEN                  = 0x3
+	NFTA_BITWISE_MASK                 = 0x4
+	NFTA_BITWISE_XOR                  = 0x5
+	NFT_BYTEORDER_NTOH                = 0x0
+	NFT_BYTEORDER_HTON                = 0x1
+	NFTA_BYTEORDER_UNSPEC             = 0x0
+	NFTA_BYTEORDER_SREG               = 0x1
+	NFTA_BYTEORDER_DREG               = 0x2
+	NFTA_BYTEORDER_OP                 = 0x3
+	NFTA_BYTEORDER_LEN                = 0x4
+	NFTA_BYTEORDER_SIZE               = 0x5
+	NFT_CMP_EQ                        = 0x0
+	NFT_CMP_NEQ                       = 0x1
+	NFT_CMP_LT                        = 0x2
+	NFT_CMP_LTE                       = 0x3
+	NFT_CMP_GT                        = 0x4
+	NFT_CMP_GTE                       = 0x5
+	NFTA_CMP_UNSPEC                   = 0x0
+	NFTA_CMP_SREG                     = 0x1
+	NFTA_CMP_OP                       = 0x2
+	NFTA_CMP_DATA                     = 0x3
+	NFT_RANGE_EQ                      = 0x0
+	NFT_RANGE_NEQ                     = 0x1
+	NFTA_RANGE_UNSPEC                 = 0x0
+	NFTA_RANGE_SREG                   = 0x1
+	NFTA_RANGE_OP                     = 0x2
+	NFTA_RANGE_FROM_DATA              = 0x3
+	NFTA_RANGE_TO_DATA                = 0x4
+	NFT_LOOKUP_F_INV                  = 0x1
+	NFTA_LOOKUP_UNSPEC                = 0x0
+	NFTA_LOOKUP_SET                   = 0x1
+	NFTA_LOOKUP_SREG                  = 0x2
+	NFTA_LOOKUP_DREG                  = 0x3
+	NFTA_LOOKUP_SET_ID                = 0x4
+	NFTA_LOOKUP_FLAGS                 = 0x5
+	NFT_DYNSET_OP_ADD                 = 0x0
+	NFT_DYNSET_OP_UPDATE              = 0x1
+	NFT_DYNSET_F_INV                  = 0x1
+	NFTA_DYNSET_UNSPEC                = 0x0
+	NFTA_DYNSET_SET_NAME              = 0x1
+	NFTA_DYNSET_SET_ID                = 0x2
+	NFTA_DYNSET_OP                    = 0x3
+	NFTA_DYNSET_SREG_KEY              = 0x4
+	NFTA_DYNSET_SREG_DATA             = 0x5
+	NFTA_DYNSET_TIMEOUT               = 0x6
+	NFTA_DYNSET_EXPR                  = 0x7
+	NFTA_DYNSET_PAD                   = 0x8
+	NFTA_DYNSET_FLAGS                 = 0x9
+	NFT_PAYLOAD_LL_HEADER             = 0x0
+	NFT_PAYLOAD_NETWORK_HEADER        = 0x1
+	NFT_PAYLOAD_TRANSPORT_HEADER      = 0x2
+	NFT_PAYLOAD_CSUM_NONE             = 0x0
+	NFT_PAYLOAD_CSUM_INET             = 0x1
+	NFT_PAYLOAD_L4CSUM_PSEUDOHDR      = 0x1
+	NFTA_PAYLOAD_UNSPEC               = 0x0
+	NFTA_PAYLOAD_DREG                 = 0x1
+	NFTA_PAYLOAD_BASE                 = 0x2
+	NFTA_PAYLOAD_OFFSET               = 0x3
+	NFTA_PAYLOAD_LEN                  = 0x4
+	NFTA_PAYLOAD_SREG                 = 0x5
+	NFTA_PAYLOAD_CSUM_TYPE            = 0x6
+	NFTA_PAYLOAD_CSUM_OFFSET          = 0x7
+	NFTA_PAYLOAD_CSUM_FLAGS           = 0x8
+	NFT_EXTHDR_F_PRESENT              = 0x1
+	NFT_EXTHDR_OP_IPV6                = 0x0
+	NFT_EXTHDR_OP_TCPOPT              = 0x1
+	NFTA_EXTHDR_UNSPEC                = 0x0
+	NFTA_EXTHDR_DREG                  = 0x1
+	NFTA_EXTHDR_TYPE                  = 0x2
+	NFTA_EXTHDR_OFFSET                = 0x3
+	NFTA_EXTHDR_LEN                   = 0x4
+	NFTA_EXTHDR_FLAGS                 = 0x5
+	NFTA_EXTHDR_OP                    = 0x6
+	NFTA_EXTHDR_SREG                  = 0x7
+	NFT_META_LEN                      = 0x0
+	NFT_META_PROTOCOL                 = 0x1
+	NFT_META_PRIORITY                 = 0x2
+	NFT_META_MARK                     = 0x3
+	NFT_META_IIF                      = 0x4
+	NFT_META_OIF                      = 0x5
+	NFT_META_IIFNAME                  = 0x6
+	NFT_META_OIFNAME                  = 0x7
+	NFT_META_IIFTYPE                  = 0x8
+	NFT_META_OIFTYPE                  = 0x9
+	NFT_META_SKUID                    = 0xa
+	NFT_META_SKGID                    = 0xb
+	NFT_META_NFTRACE                  = 0xc
+	NFT_META_RTCLASSID                = 0xd
+	NFT_META_SECMARK                  = 0xe
+	NFT_META_NFPROTO                  = 0xf
+	NFT_META_L4PROTO                  = 0x10
+	NFT_META_BRI_IIFNAME              = 0x11
+	NFT_META_BRI_OIFNAME              = 0x12
+	NFT_META_PKTTYPE                  = 0x13
+	NFT_META_CPU                      = 0x14
+	NFT_META_IIFGROUP                 = 0x15
+	NFT_META_OIFGROUP                 = 0x16
+	NFT_META_CGROUP                   = 0x17
+	NFT_META_PRANDOM                  = 0x18
+	NFT_RT_CLASSID                    = 0x0
+	NFT_RT_NEXTHOP4                   = 0x1
+	NFT_RT_NEXTHOP6                   = 0x2
+	NFT_RT_TCPMSS                     = 0x3
+	NFT_HASH_JENKINS                  = 0x0
+	NFT_HASH_SYM                      = 0x1
+	NFTA_HASH_UNSPEC                  = 0x0
+	NFTA_HASH_SREG                    = 0x1
+	NFTA_HASH_DREG                    = 0x2
+	NFTA_HASH_LEN                     = 0x3
+	NFTA_HASH_MODULUS                 = 0x4
+	NFTA_HASH_SEED                    = 0x5
+	NFTA_HASH_OFFSET                  = 0x6
+	NFTA_HASH_TYPE                    = 0x7
+	NFTA_META_UNSPEC                  = 0x0
+	NFTA_META_DREG                    = 0x1
+	NFTA_META_KEY                     = 0x2
+	NFTA_META_SREG                    = 0x3
+	NFTA_RT_UNSPEC                    = 0x0
+	NFTA_RT_DREG                      = 0x1
+	NFTA_RT_KEY                       = 0x2
+	NFT_CT_STATE                      = 0x0
+	NFT_CT_DIRECTION                  = 0x1
+	NFT_CT_STATUS                     = 0x2
+	NFT_CT_MARK                       = 0x3
+	NFT_CT_SECMARK                    = 0x4
+	NFT_CT_EXPIRATION                 = 0x5
+	NFT_CT_HELPER                     = 0x6
+	NFT_CT_L3PROTOCOL                 = 0x7
+	NFT_CT_SRC                        = 0x8
+	NFT_CT_DST                        = 0x9
+	NFT_CT_PROTOCOL                   = 0xa
+	NFT_CT_PROTO_SRC                  = 0xb
+	NFT_CT_PROTO_DST                  = 0xc
+	NFT_CT_LABELS                     = 0xd
+	NFT_CT_PKTS                       = 0xe
+	NFT_CT_BYTES                      = 0xf
+	NFT_CT_AVGPKT                     = 0x10
+	NFT_CT_ZONE                       = 0x11
+	NFT_CT_EVENTMASK                  = 0x12
+	NFTA_CT_UNSPEC                    = 0x0
+	NFTA_CT_DREG                      = 0x1
+	NFTA_CT_KEY                       = 0x2
+	NFTA_CT_DIRECTION                 = 0x3
+	NFTA_CT_SREG                      = 0x4
+	NFT_LIMIT_PKTS                    = 0x0
+	NFT_LIMIT_PKT_BYTES               = 0x1
+	NFT_LIMIT_F_INV                   = 0x1
+	NFTA_LIMIT_UNSPEC                 = 0x0
+	NFTA_LIMIT_RATE                   = 0x1
+	NFTA_LIMIT_UNIT                   = 0x2
+	NFTA_LIMIT_BURST                  = 0x3
+	NFTA_LIMIT_TYPE                   = 0x4
+	NFTA_LIMIT_FLAGS                  = 0x5
+	NFTA_LIMIT_PAD                    = 0x6
+	NFTA_COUNTER_UNSPEC               = 0x0
+	NFTA_COUNTER_BYTES                = 0x1
+	NFTA_COUNTER_PACKETS              = 0x2
+	NFTA_COUNTER_PAD                  = 0x3
+	NFTA_LOG_UNSPEC                   = 0x0
+	NFTA_LOG_GROUP                    = 0x1
+	NFTA_LOG_PREFIX                   = 0x2
+	NFTA_LOG_SNAPLEN                  = 0x3
+	NFTA_LOG_QTHRESHOLD               = 0x4
+	NFTA_LOG_LEVEL                    = 0x5
+	NFTA_LOG_FLAGS                    = 0x6
+	NFTA_QUEUE_UNSPEC                 = 0x0
+	NFTA_QUEUE_NUM                    = 0x1
+	NFTA_QUEUE_TOTAL                  = 0x2
+	NFTA_QUEUE_FLAGS                  = 0x3
+	NFTA_QUEUE_SREG_QNUM              = 0x4
+	NFT_QUOTA_F_INV                   = 0x1
+	NFT_QUOTA_F_DEPLETED              = 0x2
+	NFTA_QUOTA_UNSPEC                 = 0x0
+	NFTA_QUOTA_BYTES                  = 0x1
+	NFTA_QUOTA_FLAGS                  = 0x2
+	NFTA_QUOTA_PAD                    = 0x3
+	NFTA_QUOTA_CONSUMED               = 0x4
+	NFT_REJECT_ICMP_UNREACH           = 0x0
+	NFT_REJECT_TCP_RST                = 0x1
+	NFT_REJECT_ICMPX_UNREACH          = 0x2
+	NFT_REJECT_ICMPX_NO_ROUTE         = 0x0
+	NFT_REJECT_ICMPX_PORT_UNREACH     = 0x1
+	NFT_REJECT_ICMPX_HOST_UNREACH     = 0x2
+	NFT_REJECT_ICMPX_ADMIN_PROHIBITED = 0x3
+	NFTA_REJECT_UNSPEC                = 0x0
+	NFTA_REJECT_TYPE                  = 0x1
+	NFTA_REJECT_ICMP_CODE             = 0x2
+	NFT_NAT_SNAT                      = 0x0
+	NFT_NAT_DNAT                      = 0x1
+	NFTA_NAT_UNSPEC                   = 0x0
+	NFTA_NAT_TYPE                     = 0x1
+	NFTA_NAT_FAMILY                   = 0x2
+	NFTA_NAT_REG_ADDR_MIN             = 0x3
+	NFTA_NAT_REG_ADDR_MAX             = 0x4
+	NFTA_NAT_REG_PROTO_MIN            = 0x5
+	NFTA_NAT_REG_PROTO_MAX            = 0x6
+	NFTA_NAT_FLAGS                    = 0x7
+	NFTA_MASQ_UNSPEC                  = 0x0
+	NFTA_MASQ_FLAGS                   = 0x1
+	NFTA_MASQ_REG_PROTO_MIN           = 0x2
+	NFTA_MASQ_REG_PROTO_MAX           = 0x3
+	NFTA_REDIR_UNSPEC                 = 0x0
+	NFTA_REDIR_REG_PROTO_MIN          = 0x1
+	NFTA_REDIR_REG_PROTO_MAX          = 0x2
+	NFTA_REDIR_FLAGS                  = 0x3
+	NFTA_DUP_UNSPEC                   = 0x0
+	NFTA_DUP_SREG_ADDR                = 0x1
+	NFTA_DUP_SREG_DEV                 = 0x2
+	NFTA_FWD_UNSPEC                   = 0x0
+	NFTA_FWD_SREG_DEV                 = 0x1
+	NFTA_OBJREF_UNSPEC                = 0x0
+	NFTA_OBJREF_IMM_TYPE              = 0x1
+	NFTA_OBJREF_IMM_NAME              = 0x2
+	NFTA_OBJREF_SET_SREG              = 0x3
+	NFTA_OBJREF_SET_NAME              = 0x4
+	NFTA_OBJREF_SET_ID                = 0x5
+	NFTA_GEN_UNSPEC                   = 0x0
+	NFTA_GEN_ID                       = 0x1
+	NFTA_GEN_PROC_PID                 = 0x2
+	NFTA_GEN_PROC_NAME                = 0x3
+	NFTA_FIB_UNSPEC                   = 0x0
+	NFTA_FIB_DREG                     = 0x1
+	NFTA_FIB_RESULT                   = 0x2
+	NFTA_FIB_FLAGS                    = 0x3
+	NFT_FIB_RESULT_UNSPEC             = 0x0
+	NFT_FIB_RESULT_OIF                = 0x1
+	NFT_FIB_RESULT_OIFNAME            = 0x2
+	NFT_FIB_RESULT_ADDRTYPE           = 0x3
+	NFTA_FIB_F_SADDR                  = 0x1
+	NFTA_FIB_F_DADDR                  = 0x2
+	NFTA_FIB_F_MARK                   = 0x4
+	NFTA_FIB_F_IIF                    = 0x8
+	NFTA_FIB_F_OIF                    = 0x10
+	NFTA_FIB_F_PRESENT                = 0x20
+	NFTA_CT_HELPER_UNSPEC             = 0x0
+	NFTA_CT_HELPER_NAME               = 0x1
+	NFTA_CT_HELPER_L3PROTO            = 0x2
+	NFTA_CT_HELPER_L4PROTO            = 0x3
+	NFTA_OBJ_UNSPEC                   = 0x0
+	NFTA_OBJ_TABLE                    = 0x1
+	NFTA_OBJ_NAME                     = 0x2
+	NFTA_OBJ_TYPE                     = 0x3
+	NFTA_OBJ_DATA                     = 0x4
+	NFTA_OBJ_USE                      = 0x5
+	NFTA_TRACE_UNSPEC                 = 0x0
+	NFTA_TRACE_TABLE                  = 0x1
+	NFTA_TRACE_CHAIN                  = 0x2
+	NFTA_TRACE_RULE_HANDLE            = 0x3
+	NFTA_TRACE_TYPE                   = 0x4
+	NFTA_TRACE_VERDICT                = 0x5
+	NFTA_TRACE_ID                     = 0x6
+	NFTA_TRACE_LL_HEADER              = 0x7
+	NFTA_TRACE_NETWORK_HEADER         = 0x8
+	NFTA_TRACE_TRANSPORT_HEADER       = 0x9
+	NFTA_TRACE_IIF                    = 0xa
+	NFTA_TRACE_IIFTYPE                = 0xb
+	NFTA_TRACE_OIF                    = 0xc
+	NFTA_TRACE_OIFTYPE                = 0xd
+	NFTA_TRACE_MARK                   = 0xe
+	NFTA_TRACE_NFPROTO                = 0xf
+	NFTA_TRACE_POLICY                 = 0x10
+	NFTA_TRACE_PAD                    = 0x11
+	NFT_TRACETYPE_UNSPEC              = 0x0
+	NFT_TRACETYPE_POLICY              = 0x1
+	NFT_TRACETYPE_RETURN              = 0x2
+	NFT_TRACETYPE_RULE                = 0x3
+	NFTA_NG_UNSPEC                    = 0x0
+	NFTA_NG_DREG                      = 0x1
+	NFTA_NG_MODULUS                   = 0x2
+	NFTA_NG_TYPE                      = 0x3
+	NFTA_NG_OFFSET                    = 0x4
+	NFT_NG_INCREMENTAL                = 0x0
+	NFT_NG_RANDOM                     = 0x1
+)
+
+type RTCTime struct {
+	Sec   int32
+	Min   int32
+	Hour  int32
+	Mday  int32
+	Mon   int32
+	Year  int32
+	Wday  int32
+	Yday  int32
+	Isdst int32
+}
+
+type RTCWkAlrm struct {
+	Enabled uint8
+	Pending uint8
+	Time    RTCTime
+}
+
+type RTCPLLInfo struct {
+	Ctrl    int32
+	Value   int32
+	Max     int32
+	Min     int32
+	Posmult int32
+	Negmult int32
+	Clock   int32
+}
+
+type BlkpgIoctlArg struct {
+	Op      int32
+	Flags   int32
+	Datalen int32
+	Data    *byte
+}
+
+type BlkpgPartition struct {
+	Start   int64
+	Length  int64
+	Pno     int32
+	Devname [64]uint8
+	Volname [64]uint8
+	_       [4]byte
+}
+
+const (
+	BLKPG                  = 0x1269
+	BLKPG_ADD_PARTITION    = 0x1
+	BLKPG_DEL_PARTITION    = 0x2
+	BLKPG_RESIZE_PARTITION = 0x3
+)
+
+const (
+	NETNSA_NONE = 0x0
+	NETNSA_NSID = 0x1
+	NETNSA_PID  = 0x2
+	NETNSA_FD   = 0x3
+)
+
+type XDPRingOffset struct {
+	Producer uint64
+	Consumer uint64
+	Desc     uint64
+}
+
+type XDPMmapOffsets struct {
+	Rx XDPRingOffset
+	Tx XDPRingOffset
+	Fr XDPRingOffset
+	Cr XDPRingOffset
+}
+
+type XDPUmemReg struct {
+	Addr     uint64
+	Len      uint64
+	Size     uint32
+	Headroom uint32
+}
+
+type XDPStatistics struct {
+	Rx_dropped       uint64
+	Rx_invalid_descs uint64
+	Tx_invalid_descs uint64
+}
+
+type XDPDesc struct {
+	Addr    uint64
+	Len     uint32
+	Options uint32
+}
+
+const (
+	NCSI_CMD_UNSPEC                 = 0x0
+	NCSI_CMD_PKG_INFO               = 0x1
+	NCSI_CMD_SET_INTERFACE          = 0x2
+	NCSI_CMD_CLEAR_INTERFACE        = 0x3
+	NCSI_ATTR_UNSPEC                = 0x0
+	NCSI_ATTR_IFINDEX               = 0x1
+	NCSI_ATTR_PACKAGE_LIST          = 0x2
+	NCSI_ATTR_PACKAGE_ID            = 0x3
+	NCSI_ATTR_CHANNEL_ID            = 0x4
+	NCSI_PKG_ATTR_UNSPEC            = 0x0
+	NCSI_PKG_ATTR                   = 0x1
+	NCSI_PKG_ATTR_ID                = 0x2
+	NCSI_PKG_ATTR_FORCED            = 0x3
+	NCSI_PKG_ATTR_CHANNEL_LIST      = 0x4
+	NCSI_CHANNEL_ATTR_UNSPEC        = 0x0
+	NCSI_CHANNEL_ATTR               = 0x1
+	NCSI_CHANNEL_ATTR_ID            = 0x2
+	NCSI_CHANNEL_ATTR_VERSION_MAJOR = 0x3
+	NCSI_CHANNEL_ATTR_VERSION_MINOR = 0x4
+	NCSI_CHANNEL_ATTR_VERSION_STR   = 0x5
+	NCSI_CHANNEL_ATTR_LINK_STATE    = 0x6
+	NCSI_CHANNEL_ATTR_ACTIVE        = 0x7
+	NCSI_CHANNEL_ATTR_FORCED        = 0x8
+	NCSI_CHANNEL_ATTR_VLAN_LIST     = 0x9
+	NCSI_CHANNEL_ATTR_VLAN_ID       = 0xa
+)
+
+type ScmTimestamping struct {
+	Ts [3]Timespec
+}
+
+const (
+	SOF_TIMESTAMPING_TX_HARDWARE  = 0x1
+	SOF_TIMESTAMPING_TX_SOFTWARE  = 0x2
+	SOF_TIMESTAMPING_RX_HARDWARE  = 0x4
+	SOF_TIMESTAMPING_RX_SOFTWARE  = 0x8
+	SOF_TIMESTAMPING_SOFTWARE     = 0x10
+	SOF_TIMESTAMPING_SYS_HARDWARE = 0x20
+	SOF_TIMESTAMPING_RAW_HARDWARE = 0x40
+	SOF_TIMESTAMPING_OPT_ID       = 0x80
+	SOF_TIMESTAMPING_TX_SCHED     = 0x100
+	SOF_TIMESTAMPING_TX_ACK       = 0x200
+	SOF_TIMESTAMPING_OPT_CMSG     = 0x400
+	SOF_TIMESTAMPING_OPT_TSONLY   = 0x800
+	SOF_TIMESTAMPING_OPT_STATS    = 0x1000
+	SOF_TIMESTAMPING_OPT_PKTINFO  = 0x2000
+	SOF_TIMESTAMPING_OPT_TX_SWHW  = 0x4000
+
+	SOF_TIMESTAMPING_LAST = 0x4000
+	SOF_TIMESTAMPING_MASK = 0x7fff
+
+	SCM_TSTAMP_SND   = 0x0
+	SCM_TSTAMP_SCHED = 0x1
+	SCM_TSTAMP_ACK   = 0x2
+)
+
+type SockExtendedErr struct {
+	Errno  uint32
+	Origin uint8
+	Type   uint8
+	Code   uint8
+	Pad    uint8
+	Info   uint32
+	Data   uint32
+}
+
+type FanotifyEventMetadata struct {
+	Event_len    uint32
+	Vers         uint8
+	Reserved     uint8
+	Metadata_len uint16
+	Mask         uint64
+	Fd           int32
+	Pid          int32
+}
+
+type FanotifyResponse struct {
+	Fd       int32
+	Response uint32
+}
+
+const (
+	CRYPTO_MSG_BASE      = 0x10
+	CRYPTO_MSG_NEWALG    = 0x10
+	CRYPTO_MSG_DELALG    = 0x11
+	CRYPTO_MSG_UPDATEALG = 0x12
+	CRYPTO_MSG_GETALG    = 0x13
+	CRYPTO_MSG_DELRNG    = 0x14
+	CRYPTO_MSG_GETSTAT   = 0x15
+)
+
+const (
+	CRYPTOCFGA_UNSPEC           = 0x0
+	CRYPTOCFGA_PRIORITY_VAL     = 0x1
+	CRYPTOCFGA_REPORT_LARVAL    = 0x2
+	CRYPTOCFGA_REPORT_HASH      = 0x3
+	CRYPTOCFGA_REPORT_BLKCIPHER = 0x4
+	CRYPTOCFGA_REPORT_AEAD      = 0x5
+	CRYPTOCFGA_REPORT_COMPRESS  = 0x6
+	CRYPTOCFGA_REPORT_RNG       = 0x7
+	CRYPTOCFGA_REPORT_CIPHER    = 0x8
+	CRYPTOCFGA_REPORT_AKCIPHER  = 0x9
+	CRYPTOCFGA_REPORT_KPP       = 0xa
+	CRYPTOCFGA_REPORT_ACOMP     = 0xb
+	CRYPTOCFGA_STAT_LARVAL      = 0xc
+	CRYPTOCFGA_STAT_HASH        = 0xd
+	CRYPTOCFGA_STAT_BLKCIPHER   = 0xe
+	CRYPTOCFGA_STAT_AEAD        = 0xf
+	CRYPTOCFGA_STAT_COMPRESS    = 0x10
+	CRYPTOCFGA_STAT_RNG         = 0x11
+	CRYPTOCFGA_STAT_CIPHER      = 0x12
+	CRYPTOCFGA_STAT_AKCIPHER    = 0x13
+	CRYPTOCFGA_STAT_KPP         = 0x14
+	CRYPTOCFGA_STAT_ACOMP       = 0x15
+)
+
+type CryptoUserAlg struct {
+	Name        [64]uint8
+	Driver_name [64]uint8
+	Module_name [64]uint8
+	Type        uint32
+	Mask        uint32
+	Refcnt      uint32
+	Flags       uint32
+}
+
+type CryptoStatAEAD struct {
+	Type         [64]uint8
+	Encrypt_cnt  uint64
+	Encrypt_tlen uint64
+	Decrypt_cnt  uint64
+	Decrypt_tlen uint64
+	Err_cnt      uint64
+}
+
+type CryptoStatAKCipher struct {
+	Type         [64]uint8
+	Encrypt_cnt  uint64
+	Encrypt_tlen uint64
+	Decrypt_cnt  uint64
+	Decrypt_tlen uint64
+	Verify_cnt   uint64
+	Sign_cnt     uint64
+	Err_cnt      uint64
+}
+
+type CryptoStatCipher struct {
+	Type         [64]uint8
+	Encrypt_cnt  uint64
+	Encrypt_tlen uint64
+	Decrypt_cnt  uint64
+	Decrypt_tlen uint64
+	Err_cnt      uint64
+}
+
+type CryptoStatCompress struct {
+	Type            [64]uint8
+	Compress_cnt    uint64
+	Compress_tlen   uint64
+	Decompress_cnt  uint64
+	Decompress_tlen uint64
+	Err_cnt         uint64
+}
+
+type CryptoStatHash struct {
+	Type      [64]uint8
+	Hash_cnt  uint64
+	Hash_tlen uint64
+	Err_cnt   uint64
+}
+
+type CryptoStatKPP struct {
+	Type                      [64]uint8
+	Setsecret_cnt             uint64
+	Generate_public_key_cnt   uint64
+	Compute_shared_secret_cnt uint64
+	Err_cnt                   uint64
+}
+
+type CryptoStatRNG struct {
+	Type          [64]uint8
+	Generate_cnt  uint64
+	Generate_tlen uint64
+	Seed_cnt      uint64
+	Err_cnt       uint64
+}
+
+type CryptoStatLarval struct {
+	Type [64]uint8
+}
+
+type CryptoReportLarval struct {
+	Type [64]uint8
+}
+
+type CryptoReportHash struct {
+	Type       [64]uint8
+	Blocksize  uint32
+	Digestsize uint32
+}
+
+type CryptoReportCipher struct {
+	Type        [64]uint8
+	Blocksize   uint32
+	Min_keysize uint32
+	Max_keysize uint32
+}
+
+type CryptoReportBlkCipher struct {
+	Type        [64]uint8
+	Geniv       [64]uint8
+	Blocksize   uint32
+	Min_keysize uint32
+	Max_keysize uint32
+	Ivsize      uint32
+}
+
+type CryptoReportAEAD struct {
+	Type        [64]uint8
+	Geniv       [64]uint8
+	Blocksize   uint32
+	Maxauthsize uint32
+	Ivsize      uint32
+}
+
+type CryptoReportComp struct {
+	Type [64]uint8
+}
+
+type CryptoReportRNG struct {
+	Type     [64]uint8
+	Seedsize uint32
+}
+
+type CryptoReportAKCipher struct {
+	Type [64]uint8
+}
+
+type CryptoReportKPP struct {
+	Type [64]uint8
+}
+
+type CryptoReportAcomp struct {
+	Type [64]uint8
+}
diff --git a/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/asmdecl/asmdecl.go b/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/asmdecl/asmdecl.go
index d41c4e97e3..9e22b7da45 100644
--- a/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/asmdecl/asmdecl.go
+++ b/src/cmd/vendor/golang.org/x/tools/go/analysis/passes/asmdecl/asmdecl.go
@@ -87,6 +87,7 @@ var (
 	asmArchPpc64    = asmArch{name: "ppc64", bigEndian: true, stack: "R1", lr: true}
 	asmArchPpc64LE  = asmArch{name: "ppc64le", bigEndian: false, stack: "R1", lr: true}
 	asmArchS390X    = asmArch{name: "s390x", bigEndian: true, stack: "R15", lr: true}
+	asmArchThumb    = asmArch{name: "thumb", bigEndian: false, stack: "R13", lr: true}
 	asmArchWasm     = asmArch{name: "wasm", bigEndian: false, stack: "SP", lr: false}
 
 	arches = []*asmArch{
@@ -102,6 +103,7 @@ var (
 		&asmArchPpc64,
 		&asmArchPpc64LE,
 		&asmArchS390X,
+		&asmArchThumb,
 		&asmArchWasm,
 	}
 )
@@ -312,7 +314,7 @@ Files:
 				continue
 			}
 
-			if strings.Contains(line, ", "+archDef.stack) || strings.Contains(line, ",\t"+archDef.stack) || strings.Contains(line, "NOP "+archDef.stack) || strings.Contains(line, "NOP\t"+archDef.stack) {
+			if strings.Contains(line, ", "+archDef.stack) || strings.Contains(line, ",\t"+archDef.stack) || (strings.Contains(line, "NOP") && strings.Contains(line, archDef.stack)) {
 				wroteSP = true
 				continue
 			}
diff --git a/src/crypto/md5/md5block_decl.go b/src/crypto/md5/md5block_decl.go
index 1ac82cf08c..b21da40f83 100644
--- a/src/crypto/md5/md5block_decl.go
+++ b/src/crypto/md5/md5block_decl.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build amd64 amd64p32 386 arm ppc64le ppc64 s390x arm64
+// +build amd64 amd64p32 386 arm thumb ppc64le ppc64 s390x arm64
 
 package md5
 
diff --git a/src/crypto/md5/md5block_generic.go b/src/crypto/md5/md5block_generic.go
index 86e3b64e9f..858338baf5 100644
--- a/src/crypto/md5/md5block_generic.go
+++ b/src/crypto/md5/md5block_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !amd64,!amd64p32,!386,!arm,!ppc64le,!ppc64,!s390x,!arm64
+// +build !amd64,!amd64p32,!386,!arm,!thumb,!ppc64le,!ppc64,!s390x,!arm64
 
 package md5
 
diff --git a/src/crypto/md5/md5block_thumb.s b/src/crypto/md5/md5block_thumb.s
new file mode 100644
index 0000000000..9d9a625283
--- /dev/null
+++ b/src/crypto/md5/md5block_thumb.s
@@ -0,0 +1,300 @@
+// Copyright 2013 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+//
+// Thumb version of md5block.go
+
+#include "textflag.h"
+
+// Register definitions
+#define Rtable	R0	// Pointer to MD5 constants table
+#define Rdata	R1	// Pointer to data to hash
+#define Ra	R2	// MD5 accumulator
+#define Rb	R3	// MD5 accumulator
+#define Rc	R4	// MD5 accumulator
+#define Rd	R5	// MD5 accumulator
+#define Rc0	R6	// MD5 constant
+// r7 is OK provided you check the assembler that no synthetic instructions use it
+#define Rc1	R7	// MD5 constant
+#define Rc2	R8	// MD5 constant
+// r9, r10 are forbidden
+#define Rc3	R11	// MD5 constant
+#define Rt0	R12	// temporary
+#define Rt1	R14	// temporary
+
+// func block(dig *digest, p []byte)
+// 0(FP) is *digest
+// 4(FP) is p.array (struct Slice)
+// 8(FP) is p.len
+//12(FP) is p.cap
+//
+// Stack frame
+#define p_end	end-4(SP)	// pointer to the end of data
+#define p_data	data-8(SP)	// current data pointer
+#define buf	buffer-(8+4*16)(SP)	//16 words temporary buffer
+		// 3 words at 4..12(R13) for called routine parameters
+
+TEXT	block(SB), NOSPLIT, $84-16
+	MOVW	p+4(FP), Rdata	// pointer to the data
+	MOVW	p_len+8(FP), Rt0	// number of bytes
+	ADD	Rdata, Rt0
+	MOVW	Rt0, p_end	// pointer to end of data
+
+loop:
+	MOVW	Rdata, p_data	// Save Rdata
+	AND.S	$3, Rdata, Rt0	// TST $3, Rdata not working see issue 5921
+	BEQ	aligned			// aligned detected - skip copy
+
+	// Copy the unaligned source data into the aligned temporary buffer
+	// memmove(to=4(R13), from=8(R13), n=12(R13)) - Corrupts all registers
+	MOVW	$buf, Rtable	// to
+	MOVW	$64, Rc0		// n
+	ADD     $4, R13, Ra
+	MOVM.IA	[Rtable,Rdata,Rc0], (Ra)
+	BL	runtimememmove(SB)
+
+	// Point to the local aligned copy of the data
+	MOVW	$buf, Rdata
+
+aligned:
+	// Point to the table of constants
+	// A PC relative add would be cheaper than this
+	MOVW	$table(SB), Rtable
+
+	// Load up initial MD5 accumulator
+	MOVW	dig+0(FP), Rc0
+	MOVM.IA (Rc0), [Ra,Rb,Rc,Rd]
+
+// a += (((c^d)&b)^d) + X[index] + const
+// a = a<<shift | a>>(32-shift) + b
+#define ROUND1(Ra, Rb, Rc, Rd, index, shift, Rconst) \
+	EOR	Rc, Rd, Rt0		; \
+	AND	Rb, Rt0			; \
+	EOR	Rd, Rt0			; \
+	MOVW	(index<<2)(Rdata), Rt1	; \
+	ADD	Rt1, Rt0			; \
+	ADD	Rconst, Rt0			; \
+	ADD	Rt0, Ra			; \
+	ADD	Ra@>(32-shift), Rb, Ra	;
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND1(Ra, Rb, Rc, Rd,  0,	7, Rc0)
+	ROUND1(Rd, Ra, Rb, Rc,  1, 12, Rc1)
+	ROUND1(Rc, Rd, Ra, Rb,  2, 17, Rc2)
+	ROUND1(Rb, Rc, Rd, Ra,  3, 22, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND1(Ra, Rb, Rc, Rd,  4,	7, Rc0)
+	ROUND1(Rd, Ra, Rb, Rc,  5, 12, Rc1)
+	ROUND1(Rc, Rd, Ra, Rb,  6, 17, Rc2)
+	ROUND1(Rb, Rc, Rd, Ra,  7, 22, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND1(Ra, Rb, Rc, Rd,  8,	7, Rc0)
+	ROUND1(Rd, Ra, Rb, Rc,  9, 12, Rc1)
+	ROUND1(Rc, Rd, Ra, Rb, 10, 17, Rc2)
+	ROUND1(Rb, Rc, Rd, Ra, 11, 22, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND1(Ra, Rb, Rc, Rd, 12,	7, Rc0)
+	ROUND1(Rd, Ra, Rb, Rc, 13, 12, Rc1)
+	ROUND1(Rc, Rd, Ra, Rb, 14, 17, Rc2)
+	ROUND1(Rb, Rc, Rd, Ra, 15, 22, Rc3)
+
+// a += (((b^c)&d)^c) + X[index] + const
+// a = a<<shift | a>>(32-shift) + b
+#define ROUND2(Ra, Rb, Rc, Rd, index, shift, Rconst) \
+	EOR	Rb, Rc, Rt0		; \
+	AND	Rd, Rt0			; \
+	EOR	Rc, Rt0			; \
+	MOVW	(index<<2)(Rdata), Rt1	; \
+	ADD	Rt1, Rt0			; \
+	ADD	Rconst, Rt0			; \
+	ADD	Rt0, Ra			; \
+	ADD	Ra@>(32-shift), Rb, Ra	;
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND2(Ra, Rb, Rc, Rd,  1,	5, Rc0)
+	ROUND2(Rd, Ra, Rb, Rc,  6,	9, Rc1)
+	ROUND2(Rc, Rd, Ra, Rb, 11, 14, Rc2)
+	ROUND2(Rb, Rc, Rd, Ra,  0, 20, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND2(Ra, Rb, Rc, Rd,  5,	5, Rc0)
+	ROUND2(Rd, Ra, Rb, Rc, 10,	9, Rc1)
+	ROUND2(Rc, Rd, Ra, Rb, 15, 14, Rc2)
+	ROUND2(Rb, Rc, Rd, Ra,  4, 20, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND2(Ra, Rb, Rc, Rd,  9,	5, Rc0)
+	ROUND2(Rd, Ra, Rb, Rc, 14,	9, Rc1)
+	ROUND2(Rc, Rd, Ra, Rb,  3, 14, Rc2)
+	ROUND2(Rb, Rc, Rd, Ra,  8, 20, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND2(Ra, Rb, Rc, Rd, 13,	5, Rc0)
+	ROUND2(Rd, Ra, Rb, Rc,  2,	9, Rc1)
+	ROUND2(Rc, Rd, Ra, Rb,  7, 14, Rc2)
+	ROUND2(Rb, Rc, Rd, Ra, 12, 20, Rc3)
+
+// a += (b^c^d) + X[index] + const
+// a = a<<shift | a>>(32-shift) + b
+#define ROUND3(Ra, Rb, Rc, Rd, index, shift, Rconst) \
+	EOR	Rb, Rc, Rt0		; \
+	EOR	Rd, Rt0			; \
+	MOVW	(index<<2)(Rdata), Rt1	; \
+	ADD	Rt1, Rt0			; \
+	ADD	Rconst, Rt0			; \
+	ADD	Rt0, Ra			; \
+	ADD	Ra@>(32-shift), Rb, Ra	;
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND3(Ra, Rb, Rc, Rd,  5,	4, Rc0)
+	ROUND3(Rd, Ra, Rb, Rc,  8, 11, Rc1)
+	ROUND3(Rc, Rd, Ra, Rb, 11, 16, Rc2)
+	ROUND3(Rb, Rc, Rd, Ra, 14, 23, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND3(Ra, Rb, Rc, Rd,  1,	4, Rc0)
+	ROUND3(Rd, Ra, Rb, Rc,  4, 11, Rc1)
+	ROUND3(Rc, Rd, Ra, Rb,  7, 16, Rc2)
+	ROUND3(Rb, Rc, Rd, Ra, 10, 23, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND3(Ra, Rb, Rc, Rd, 13,	4, Rc0)
+	ROUND3(Rd, Ra, Rb, Rc,  0, 11, Rc1)
+	ROUND3(Rc, Rd, Ra, Rb,  3, 16, Rc2)
+	ROUND3(Rb, Rc, Rd, Ra,  6, 23, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND3(Ra, Rb, Rc, Rd,  9,	4, Rc0)
+	ROUND3(Rd, Ra, Rb, Rc, 12, 11, Rc1)
+	ROUND3(Rc, Rd, Ra, Rb, 15, 16, Rc2)
+	ROUND3(Rb, Rc, Rd, Ra,  2, 23, Rc3)
+
+// a += (c^(b|^d)) + X[index] + const
+// a = a<<shift | a>>(32-shift) + b
+#define ROUND4(Ra, Rb, Rc, Rd, index, shift, Rconst) \
+	MVN	Rd, Rt0			; \
+	ORR	Rb, Rt0			; \
+	EOR	Rc, Rt0			; \
+	MOVW	(index<<2)(Rdata), Rt1	; \
+	ADD	Rt1, Rt0			; \
+	ADD	Rconst, Rt0			; \
+	ADD	Rt0, Ra			; \
+	ADD	Ra@>(32-shift), Rb, Ra	;
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND4(Ra, Rb, Rc, Rd,  0,	6, Rc0)
+	ROUND4(Rd, Ra, Rb, Rc,  7, 10, Rc1)
+	ROUND4(Rc, Rd, Ra, Rb, 14, 15, Rc2)
+	ROUND4(Rb, Rc, Rd, Ra,  5, 21, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND4(Ra, Rb, Rc, Rd, 12,	6, Rc0)
+	ROUND4(Rd, Ra, Rb, Rc,  3, 10, Rc1)
+	ROUND4(Rc, Rd, Ra, Rb, 10, 15, Rc2)
+	ROUND4(Rb, Rc, Rd, Ra,  1, 21, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND4(Ra, Rb, Rc, Rd,  8,	6, Rc0)
+	ROUND4(Rd, Ra, Rb, Rc, 15, 10, Rc1)
+	ROUND4(Rc, Rd, Ra, Rb,  6, 15, Rc2)
+	ROUND4(Rb, Rc, Rd, Ra, 13, 21, Rc3)
+
+	MOVM.IA.W (Rtable), [Rc0,Rc1,Rc2,Rc3]
+	ROUND4(Ra, Rb, Rc, Rd,  4,	6, Rc0)
+	ROUND4(Rd, Ra, Rb, Rc, 11, 10, Rc1)
+	ROUND4(Rc, Rd, Ra, Rb,  2, 15, Rc2)
+	ROUND4(Rb, Rc, Rd, Ra,  9, 21, Rc3)
+
+	MOVW	dig+0(FP), Rt0
+	MOVM.IA (Rt0), [Rc0,Rc1,Rc2,Rc3]
+
+	ADD	Rc0, Ra
+	ADD	Rc1, Rb
+	ADD	Rc2, Rc
+	ADD	Rc3, Rd
+
+	MOVM.IA [Ra,Rb,Rc,Rd], (Rt0)
+
+	MOVW	p_data, Rdata
+	MOVW	p_end, Rt0
+	ADD	$64, Rdata
+	CMP	Rt0, Rdata
+	BLO	loop
+
+	RET
+
+// MD5 constants table
+
+	// Round 1
+	DATA	table+0x00(SB)/4, $0xd76aa478
+	DATA	table+0x04(SB)/4, $0xe8c7b756
+	DATA	table+0x08(SB)/4, $0x242070db
+	DATA	table+0x0c(SB)/4, $0xc1bdceee
+	DATA	table+0x10(SB)/4, $0xf57c0faf
+	DATA	table+0x14(SB)/4, $0x4787c62a
+	DATA	table+0x18(SB)/4, $0xa8304613
+	DATA	table+0x1c(SB)/4, $0xfd469501
+	DATA	table+0x20(SB)/4, $0x698098d8
+	DATA	table+0x24(SB)/4, $0x8b44f7af
+	DATA	table+0x28(SB)/4, $0xffff5bb1
+	DATA	table+0x2c(SB)/4, $0x895cd7be
+	DATA	table+0x30(SB)/4, $0x6b901122
+	DATA	table+0x34(SB)/4, $0xfd987193
+	DATA	table+0x38(SB)/4, $0xa679438e
+	DATA	table+0x3c(SB)/4, $0x49b40821
+	// Round 2
+	DATA	table+0x40(SB)/4, $0xf61e2562
+	DATA	table+0x44(SB)/4, $0xc040b340
+	DATA	table+0x48(SB)/4, $0x265e5a51
+	DATA	table+0x4c(SB)/4, $0xe9b6c7aa
+	DATA	table+0x50(SB)/4, $0xd62f105d
+	DATA	table+0x54(SB)/4, $0x02441453
+	DATA	table+0x58(SB)/4, $0xd8a1e681
+	DATA	table+0x5c(SB)/4, $0xe7d3fbc8
+	DATA	table+0x60(SB)/4, $0x21e1cde6
+	DATA	table+0x64(SB)/4, $0xc33707d6
+	DATA	table+0x68(SB)/4, $0xf4d50d87
+	DATA	table+0x6c(SB)/4, $0x455a14ed
+	DATA	table+0x70(SB)/4, $0xa9e3e905
+	DATA	table+0x74(SB)/4, $0xfcefa3f8
+	DATA	table+0x78(SB)/4, $0x676f02d9
+	DATA	table+0x7c(SB)/4, $0x8d2a4c8a
+	// Round 3
+	DATA	table+0x80(SB)/4, $0xfffa3942
+	DATA	table+0x84(SB)/4, $0x8771f681
+	DATA	table+0x88(SB)/4, $0x6d9d6122
+	DATA	table+0x8c(SB)/4, $0xfde5380c
+	DATA	table+0x90(SB)/4, $0xa4beea44
+	DATA	table+0x94(SB)/4, $0x4bdecfa9
+	DATA	table+0x98(SB)/4, $0xf6bb4b60
+	DATA	table+0x9c(SB)/4, $0xbebfbc70
+	DATA	table+0xa0(SB)/4, $0x289b7ec6
+	DATA	table+0xa4(SB)/4, $0xeaa127fa
+	DATA	table+0xa8(SB)/4, $0xd4ef3085
+	DATA	table+0xac(SB)/4, $0x04881d05
+	DATA	table+0xb0(SB)/4, $0xd9d4d039
+	DATA	table+0xb4(SB)/4, $0xe6db99e5
+	DATA	table+0xb8(SB)/4, $0x1fa27cf8
+	DATA	table+0xbc(SB)/4, $0xc4ac5665
+	// Round 4
+	DATA	table+0xc0(SB)/4, $0xf4292244
+	DATA	table+0xc4(SB)/4, $0x432aff97
+	DATA	table+0xc8(SB)/4, $0xab9423a7
+	DATA	table+0xcc(SB)/4, $0xfc93a039
+	DATA	table+0xd0(SB)/4, $0x655b59c3
+	DATA	table+0xd4(SB)/4, $0x8f0ccc92
+	DATA	table+0xd8(SB)/4, $0xffeff47d
+	DATA	table+0xdc(SB)/4, $0x85845dd1
+	DATA	table+0xe0(SB)/4, $0x6fa87e4f
+	DATA	table+0xe4(SB)/4, $0xfe2ce6e0
+	DATA	table+0xe8(SB)/4, $0xa3014314
+	DATA	table+0xec(SB)/4, $0x4e0811a1
+	DATA	table+0xf0(SB)/4, $0xf7537e82
+	DATA	table+0xf4(SB)/4, $0xbd3af235
+	DATA	table+0xf8(SB)/4, $0x2ad7d2bb
+	DATA	table+0xfc(SB)/4, $0xeb86d391
+	// Global definition
+	GLOBL	table(SB),8,$256
diff --git a/src/crypto/sha1/sha1block_decl.go b/src/crypto/sha1/sha1block_decl.go
index 6d2d073d13..925393929c 100644
--- a/src/crypto/sha1/sha1block_decl.go
+++ b/src/crypto/sha1/sha1block_decl.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build amd64p32 arm 386 s390x
+// +build amd64p32 arm thumb 386 s390x
 
 package sha1
 
diff --git a/src/crypto/sha1/sha1block_generic.go b/src/crypto/sha1/sha1block_generic.go
index 5823e08941..bddcdf2d9c 100644
--- a/src/crypto/sha1/sha1block_generic.go
+++ b/src/crypto/sha1/sha1block_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !amd64,!amd64p32,!386,!arm,!s390x,!arm64
+// +build !amd64,!amd64p32,!386,!arm,!thumb,!s390x,!arm64
 
 package sha1
 
diff --git a/src/crypto/sha1/sha1block_thumb.s b/src/crypto/sha1/sha1block_thumb.s
new file mode 100644
index 0000000000..cf2cf67b41
--- /dev/null
+++ b/src/crypto/sha1/sha1block_thumb.s
@@ -0,0 +1,220 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+//
+// Thumb version of md5block.go
+
+#include "/home/michal/P/go/goroot/src/runtime/textflag.h"
+
+// SHA-1 block routine. See sha1block.go for Go equivalent.
+//
+// There are 80 rounds of 4 types:
+//   - rounds 0-15 are type 1 and load data (ROUND1 macro).
+//   - rounds 16-19 are type 1 and do not load data (ROUND1x macro).
+//   - rounds 20-39 are type 2 and do not load data (ROUND2 macro).
+//   - rounds 40-59 are type 3 and do not load data (ROUND3 macro).
+//   - rounds 60-79 are type 4 and do not load data (ROUND4 macro).
+//
+// Each round loads or shuffles the data, then computes a per-round
+// function of b, c, d, and then mixes the result into and rotates the
+// five registers a, b, c, d, e holding the intermediate results.
+//
+// The register rotation is implemented by rotating the arguments to
+// the round macros instead of by explicit move instructions.
+
+// Register definitions
+#define Rdata	R0	// Pointer to incoming data
+#define Rconst	R1	// Current constant for SHA round
+#define Ra	R2		// SHA-1 accumulator
+#define Rb	R3		// SHA-1 accumulator
+#define Rc	R4		// SHA-1 accumulator
+#define Rd	R5		// SHA-1 accumulator
+#define Re	R6		// SHA-1 accumulator
+// R7 is OK provided you check the assembler that no synthetic instructions use it
+#define Rt0	R7		// Temporary
+#define Rt1	R8		// Temporary
+// r9, r10 are forbidden
+#define Rt2	R11		// Temporary
+#define Rctr	R12	// loop counter
+#define Rw	R14		// point to w buffer
+
+// func block(dig *digest, p []byte)
+// 0(FP) is *digest
+// 4(FP) is p.array (struct Slice)
+// 8(FP) is p.len
+//12(FP) is p.cap
+//
+// Stack frame
+#define p_end	end-4(SP)		// pointer to the end of data
+#define p_data	data-8(SP)	// current data pointer (unused?)
+#define w_buf	buf-(8+4*80)(SP)	//80 words temporary buffer w uint32[80]
+#define saved	abcde-(8+4*80+4*5)(SP)	// saved sha1 registers a,b,c,d,e - these must be last (unused?)
+// Total size +4 for saved LR is 352
+
+	// w[i] = p[j]<<24 | p[j+1]<<16 | p[j+2]<<8 | p[j+3]
+	// e += w[i]
+#define LOAD(Re) \
+	MOVBU	2(Rdata), Rt0 ; \
+	MOVBU	3(Rdata), Rt1 ; \
+	MOVBU	1(Rdata), Rt2 ; \
+	ORR	Rt0<<8, Rt1, Rt0	    ; \
+	MOVBU.P	4(Rdata), Rt1 ; \
+	ORR	Rt2<<16, Rt0, Rt0	    ; \
+	ORR	Rt1<<24, Rt0, Rt0	    ; \
+	MOVW.P	Rt0, 4(Rw)		    ; \
+	ADD	Rt0, Re, Re
+
+	// tmp := w[(i-3)&0xf] ^ w[(i-8)&0xf] ^ w[(i-14)&0xf] ^ w[(i)&0xf]
+	// w[i&0xf] = tmp<<1 | tmp>>(32-1)
+	// e += w[i&0xf]
+#define SHUFFLE(Re) \
+	MOVW	(-16*4)(Rw), Rt0 ; \
+	MOVW	(-14*4)(Rw), Rt1 ; \
+	MOVW	(-8*4)(Rw), Rt2  ; \
+	EOR	Rt0, Rt1, Rt0  ; \
+	MOVW	(-3*4)(Rw), Rt1  ; \
+	EOR	Rt2, Rt0, Rt0  ; \
+	EOR	Rt0, Rt1, Rt0  ; \
+	MOVW	Rt0@>(32-1), Rt0  ; \
+	MOVW.P	Rt0, 4(Rw)	  ; \
+	ADD	Rt0, Re, Re
+
+	// t1 = (b & c) | ((~b) & d)
+#define FUNC1(Ra, Rb, Rc, Rd, Re) \
+	MVN	Rb, Rt1	   ; \
+	AND	Rb, Rc, Rt0  ; \
+	AND	Rd, Rt1, Rt1 ; \
+	ORR	Rt0, Rt1, Rt1
+
+	// t1 = b ^ c ^ d
+#define FUNC2(Ra, Rb, Rc, Rd, Re) \
+	EOR	Rb, Rc, Rt1 ; \
+	EOR	Rd, Rt1, Rt1
+
+	// t1 = (b & c) | (b & d) | (c & d) =
+	// t1 = (b & c) | ((b | c) & d)
+#define FUNC3(Ra, Rb, Rc, Rd, Re) \
+	ORR	Rb, Rc, Rt0  ; \
+	AND	Rb, Rc, Rt1  ; \
+	AND	Rd, Rt0, Rt0 ; \
+	ORR	Rt0, Rt1, Rt1
+
+#define FUNC4 FUNC2
+
+	// a5 := a<<5 | a>>(32-5)
+	// b = b<<30 | b>>(32-30)
+	// e = a5 + t1 + e + const
+#define MIX(Ra, Rb, Rc, Rd, Re) \
+	ADD	Rt1, Re, Re	 ; \
+	MOVW	Rb@>(32-30), Rb	 ; \
+	ADD	Ra@>(32-5), Re, Re ; \
+	ADD	Rconst, Re, Re
+
+#define ROUND1(Ra, Rb, Rc, Rd, Re) \
+	LOAD(Re)		; \
+	FUNC1(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+#define ROUND1x(Ra, Rb, Rc, Rd, Re) \
+	SHUFFLE(Re)	; \
+	FUNC1(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+#define ROUND2(Ra, Rb, Rc, Rd, Re) \
+	SHUFFLE(Re)	; \
+	FUNC2(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+#define ROUND3(Ra, Rb, Rc, Rd, Re) \
+	SHUFFLE(Re)	; \
+	FUNC3(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+#define ROUND4(Ra, Rb, Rc, Rd, Re) \
+	SHUFFLE(Re)	; \
+	FUNC4(Ra, Rb, Rc, Rd, Re)	; \
+	MIX(Ra, Rb, Rc, Rd, Re)
+
+
+// func block(dig *digest, p []byte)
+TEXT	block(SB), 0, $352-16
+	MOVW	p+4(FP), Rdata	// pointer to the data
+	MOVW	p_len+8(FP), Rt0	// number of bytes
+	ADD	Rdata, Rt0
+	MOVW	Rt0, p_end	// pointer to end of data
+
+	// Load up initial SHA-1 accumulator
+	MOVW	dig+0(FP), Rt0
+	MOVM.IA (Rt0), [Ra,Rb,Rc,Rd,Re]
+
+loop:
+	// Save registers at SP+4 onwards
+	
+	ADD     $4, R13, Rt0
+	MOVM.IA [Ra,Rb,Rc,Rd,Re], (Rt0)
+
+	MOVW	$w_buf, Rw
+	MOVW	$0x5A827999, Rconst
+	MOVW	$3, Rctr
+loop1:	ROUND1(Ra, Rb, Rc, Rd, Re)
+	ROUND1(Re, Ra, Rb, Rc, Rd)
+	ROUND1(Rd, Re, Ra, Rb, Rc)
+	ROUND1(Rc, Rd, Re, Ra, Rb)
+	ROUND1(Rb, Rc, Rd, Re, Ra)
+	SUB.S	$1, Rctr
+	BNE	loop1
+
+	ROUND1(Ra, Rb, Rc, Rd, Re)
+	ROUND1x(Re, Ra, Rb, Rc, Rd)
+	ROUND1x(Rd, Re, Ra, Rb, Rc)
+	ROUND1x(Rc, Rd, Re, Ra, Rb)
+	ROUND1x(Rb, Rc, Rd, Re, Ra)
+
+	MOVW	$0x6ED9EBA1, Rconst
+	MOVW	$4, Rctr
+loop2:	ROUND2(Ra, Rb, Rc, Rd, Re)
+	ROUND2(Re, Ra, Rb, Rc, Rd)
+	ROUND2(Rd, Re, Ra, Rb, Rc)
+	ROUND2(Rc, Rd, Re, Ra, Rb)
+	ROUND2(Rb, Rc, Rd, Re, Ra)
+	SUB.S	$1, Rctr
+	BNE	loop2
+
+	MOVW	$0x8F1BBCDC, Rconst
+	MOVW	$4, Rctr
+loop3:	ROUND3(Ra, Rb, Rc, Rd, Re)
+	ROUND3(Re, Ra, Rb, Rc, Rd)
+	ROUND3(Rd, Re, Ra, Rb, Rc)
+	ROUND3(Rc, Rd, Re, Ra, Rb)
+	ROUND3(Rb, Rc, Rd, Re, Ra)
+	SUB.S	$1, Rctr
+	BNE	loop3
+
+	MOVW	$0xCA62C1D6, Rconst
+	MOVW	$4, Rctr
+loop4:	ROUND4(Ra, Rb, Rc, Rd, Re)
+	ROUND4(Re, Ra, Rb, Rc, Rd)
+	ROUND4(Rd, Re, Ra, Rb, Rc)
+	ROUND4(Rc, Rd, Re, Ra, Rb)
+	ROUND4(Rb, Rc, Rd, Re, Ra)
+	SUB.S	$1, Rctr
+	BNE	loop4
+
+	// Accumulate - restoring registers from SP+4
+	ADD     $4, R13, Rt0
+	MOVM.IA (Rt0), [Rt0,Rt1,Rt2,Rctr,Rw]
+	ADD	Rt0, Ra
+	ADD	Rt1, Rb
+	ADD	Rt2, Rc
+	ADD	Rctr, Rd
+	ADD	Rw, Re
+
+	MOVW	p_end, Rt0
+	CMP	Rt0, Rdata
+	BLO	loop
+
+	// Save final SHA-1 accumulator
+	MOVW	dig+0(FP), Rt0
+	MOVM.IA [Ra,Rb,Rc,Rd,Re], (Rt0)
+
+	RET
diff --git a/src/embedded/arch/cortexm/systim/asm_thumb.s b/src/embedded/arch/cortexm/systim/asm_thumb.s
new file mode 100644
index 0000000000..045ca88ace
--- /dev/null
+++ b/src/embedded/arch/cortexm/systim/asm_thumb.s
@@ -0,0 +1,26 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+#define ICSR_ADDR 0xE000ED04
+#define ICSR_PENDSVSET (1<<28)
+
+TEXT SysTick_Handler(SB),NOSPLIT|NOFRAME,$0-0
+	// set PendSV bit first to avoid DSB but ensure exception tail-chaining
+	MOVW  $ICSR_ADDR, R0
+	MOVW  $ICSR_PENDSVSET, R1
+	MOVW  R1, (R0)
+
+	// increment systim.reloadcnt (64-bit counter)
+	MOVW   $systim(SB), R0
+	MOVW   (R0), R1
+	ADD.S  $1, R1
+	MOVW   R1, (R0)
+	RET.CC
+	MOVW  4(R0), R1
+	ADD   $1, R1
+	MOVW  R1, 4(R0)
+
+	RET
diff --git a/src/embedded/arch/cortexm/systim/systim_thumb.go b/src/embedded/arch/cortexm/systim/systim_thumb.go
new file mode 100644
index 0000000000..363716df9c
--- /dev/null
+++ b/src/embedded/arch/cortexm/systim/systim_thumb.go
@@ -0,0 +1,75 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package systim iplements ticking system timer using the ARMv7-M SysTick
+// peripheral. The SysTick limitations do not allow to implement precise
+// tickless system timer.
+//
+// This implementation uses non-atomic loads of 64-bit counter and for this
+// reason it can be used only in uniprocessor system and only if the Nanotime
+// function is called with priority lower or same as the SysTick exception.
+package systim
+
+import (
+	"embedded/rtos"
+	"internal/cpu/cortexm/scb"
+	"internal/cpu/cortexm/systick"
+	_ "unsafe"
+)
+
+var systim struct {
+	reloadcnt int64 // must be the first field
+	periodns  int64
+}
+
+// Nanotime returns
+//go:nosplit
+func Nanotime() int64 {
+	st := systick.SYSTICK()
+	var downtick uint32
+	reloadcnt := systim.reloadcnt // non-atomic!
+	for {
+		downtick = uint32(st.CURRENT().Load())
+		reloadcnt1 := systim.reloadcnt // non-atomic!
+		if reloadcnt1 == reloadcnt {
+			break
+		}
+		reloadcnt = reloadcnt1
+	}
+	// the SysTick asserts an exception when the CURRENT changes from 1 to 0
+	periodtick := uint32(st.RELOAD().Load() + 1)
+	tick := uint32(0)
+	if downtick != 0 {
+		tick = periodtick - downtick
+	}
+	return systim.periodns*reloadcnt +
+		systim.periodns*int64(tick)/int64(periodtick)
+}
+
+// Setup setups Cortex-M SysTick timer to work as sytem timer.
+//  periodns - number of nanoseconds between SysTick interrupts,
+//  clkhz    - frequency of SysTick clock source,
+//  external - clock source (true: external clock, false: CPU clock).
+// Setup must be run in privileged mode.
+func Setup(periodns, clkhz int64, external bool) {
+	st := systick.SYSTICK()
+	if periodns <= 0 || clkhz <= 0 {
+		st.CSR.ClearBits(systick.ENABLE | systick.TICKINT)
+		systim.periodns = 0
+		return
+	}
+	// Set SysTick exception priority accortding to rtos package.
+	scb.SCB().PRI_SysTick().Store(255 - rtos.IntPrioSysTimer)
+	systim.periodns = periodns
+	systim.reloadcnt = 1 // ensure that nanotime never return zero
+	periodtick := uint32((periodns*clkhz + 5e8) / 1e9)
+	st.RELOAD().Store(systick.RVR(periodtick - 1))
+	st.CURRENT().Store(0)
+
+	cfg := systick.ENABLE | systick.TICKINT
+	if !external {
+		cfg |= systick.CLKSOURCE
+	}
+	st.CSR.Store(systick.ENABLE | systick.TICKINT)
+}
diff --git a/src/embedded/doc.go b/src/embedded/doc.go
new file mode 100644
index 0000000000..cb634ecc6a
--- /dev/null
+++ b/src/embedded/doc.go
@@ -0,0 +1,3 @@
+// Embedded programming with Go
+
+package embedded
diff --git a/src/embedded/mmio/asm_386.s b/src/embedded/mmio/asm_386.s
new file mode 100644
index 0000000000..2af0c0590d
--- /dev/null
+++ b/src/embedded/mmio/asm_386.s
@@ -0,0 +1,41 @@
+#include "textflag.h"
+
+TEXT load32(SB),NOSPLIT,$0-12
+	MOVL  addr+0(FP), AX
+	MOVL  (AX), BX
+	MOVL  BX, ret+4(FP)
+	RET
+
+TEXT load16(SB),NOSPLIT,$0-10
+	MOVL  addr+0(FP), AX
+	MOVW  (AX), BX
+	MOVW  BX, ret+4(FP)
+	RET
+
+TEXT load8(SB),NOSPLIT,$0-9
+	MOVL  addr+0(FP), AX
+	MOVB  (AX), BX
+	MOVB  BX, ret+4(FP)
+	RET
+
+TEXT store32(SB),NOSPLIT,$0-12
+	MOVL  addr+0(FP), AX
+	MOVL  v+4(FP), BX
+	MOVL  BX, (AX)
+	RET
+
+TEXT store16(SB),NOSPLIT,$0-10
+	MOVL  addr+0(FP), AX
+	MOVW  v+4(FP), BX
+	MOVW  BX, (AX)
+	RET
+
+TEXT store8(SB),NOSPLIT,$0-9
+	MOVL  addr+0(FP), AX
+	MOVB  v+4(FP), BX
+	MOVB  BX, (AX)
+	RET
+
+TEXT MB(SB),NOSPLIT,$0
+	// BUG: memory barrier instruction
+	RET
diff --git a/src/embedded/mmio/asm_amd64.s b/src/embedded/mmio/asm_amd64.s
new file mode 100644
index 0000000000..56b1702be1
--- /dev/null
+++ b/src/embedded/mmio/asm_amd64.s
@@ -0,0 +1,41 @@
+#include "textflag.h"
+
+TEXT load32(SB),NOSPLIT,$0-12
+	MOVQ  addr+0(FP), AX
+	MOVL  (AX), BX
+	MOVL  BX, ret+4(FP)
+	RET
+
+TEXT load16(SB),NOSPLIT,$0-10
+	MOVQ  addr+0(FP), AX
+	MOVW  (AX), BX
+	MOVW  BX, ret+4(FP)
+	RET
+
+TEXT load8(SB),NOSPLIT,$0-9
+	MOVQ  addr+0(FP), AX
+	MOVB  (AX), BX
+	MOVB  BX, ret+4(FP)
+	RET
+
+TEXT store32(SB),NOSPLIT,$0-12
+	MOVQ  addr+0(FP), AX
+	MOVL  v+4(FP), BX
+	MOVL  BX, (AX)
+	RET
+
+TEXT store16(SB),NOSPLIT,$0-10
+	MOVQ  addr+0(FP), AX
+	MOVW  v+4(FP), BX
+	MOVW  BX, (AX)
+	RET
+
+TEXT store8(SB),NOSPLIT,$0-9
+	MOVQ  addr+0(FP), AX
+	MOVB  v+4(FP), BX
+	MOVB  BX, (AX)
+	RET
+
+TEXT MB(SB),NOSPLIT,$0
+	// BUG: memory barrier instruction
+	RET
diff --git a/src/embedded/mmio/asm_arm.s b/src/embedded/mmio/asm_arm.s
new file mode 100644
index 0000000000..01f6d6c06e
--- /dev/null
+++ b/src/embedded/mmio/asm_arm.s
@@ -0,0 +1,43 @@
+#include "textflag.h"
+
+TEXT load32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  (R0), R1
+	MOVW  R1, ret+4(FP)
+	RET
+
+TEXT load16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  (R0), R1
+	MOVH   R1, ret+4(FP)
+	RET
+
+TEXT load8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  (R0), R1
+	MOVB   R1, ret+4(FP)
+	RET
+
+TEXT store32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  v+4(FP), R1
+	MOVW  R1, (R0)
+	RET
+
+TEXT store16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  v+4(FP), R1
+	MOVH   R1, (R0)
+	RET
+
+TEXT store8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  v+4(FP), R1
+	MOVB   R1, (R0)
+	RET
+
+TEXT MB(SB),NOSPLIT,$0
+	// use DSB instead of DMB because an IO access can affect CPU directly (eg:
+	// generate interrupt, change CPU behavior via memory-mapped control reg.)
+	DMB // BUG: DSB not supported
+	RET
diff --git a/src/embedded/mmio/asm_arm64.s b/src/embedded/mmio/asm_arm64.s
new file mode 100644
index 0000000000..deda63517c
--- /dev/null
+++ b/src/embedded/mmio/asm_arm64.s
@@ -0,0 +1,43 @@
+#include "textflag.h"
+
+TEXT load32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  (R0), R1
+	MOVW  R1, ret+4(FP)
+	RET
+
+TEXT load16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  (R0), R1
+	MOVH   R1, ret+4(FP)
+	RET
+
+TEXT load8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  (R0), R1
+	MOVB   R1, ret+4(FP)
+	RET
+
+TEXT store32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  v+4(FP), R1
+	MOVW  R1, (R0)
+	RET
+
+TEXT store16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  v+4(FP), R1
+	MOVH   R1, (R0)
+	RET
+
+TEXT store8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  v+4(FP), R1
+	MOVB   R1, (R0)
+	RET
+
+TEXT MB(SB),NOSPLIT,$0
+	// use DSB instead of DMB because an IO access can affect CPU directly (eg:
+	// generate interrupt, change CPU behavior via memory-mapped control reg.)
+	DSB $0xF
+	RET
diff --git a/src/embedded/mmio/asm_thumb.s b/src/embedded/mmio/asm_thumb.s
new file mode 100644
index 0000000000..e81563f556
--- /dev/null
+++ b/src/embedded/mmio/asm_thumb.s
@@ -0,0 +1,43 @@
+#include "textflag.h"
+
+TEXT load32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  (R0), R1
+	MOVW  R1, ret+4(FP)
+	RET
+
+TEXT load16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  (R0), R1
+	MOVH   R1, ret+4(FP)
+	RET
+
+TEXT load8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  (R0), R1
+	MOVB   R1, ret+4(FP)
+	RET
+
+TEXT store32(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  v+4(FP), R1
+	MOVW  R1, (R0)
+	RET
+
+TEXT store16(SB),NOSPLIT,$0-6
+	MOVW   addr+0(FP), R0
+	MOVHU  v+4(FP), R1
+	MOVH   R1, (R0)
+	RET
+
+TEXT store8(SB),NOSPLIT,$0-5
+	MOVW   addr+0(FP), R0
+	MOVBU  v+4(FP), R1
+	MOVB   R1, (R0)
+	RET
+
+TEXT MB(SB),NOSPLIT,$0
+	 // use DSB instead of DMB because an IO access can affect CPU directly (eg:
+	 // generate interrupt, change CPU behavior via memory-mapped control reg.)
+	DSB 
+	RET
diff --git a/src/embedded/mmio/doc.go b/src/embedded/mmio/doc.go
new file mode 100644
index 0000000000..7d5839ff8c
--- /dev/null
+++ b/src/embedded/mmio/doc.go
@@ -0,0 +1,12 @@
+// Package mmio provides data types and methods that can be used to define and
+// access memory mapped registers of peripherals in embedded systems.
+//
+// You can not use ordinary load/store operations to access memory mapped
+// peripheral registers because these operations can be reordered or optimized
+// out by the compiler. This is unacceptable because in case of MMIO the order
+// of accessing registers matters and even reading from register can have side
+// effects.
+//
+// This package has some support from compiler to ensure order and speed of
+// I/O operations.
+package mmio
\ No newline at end of file
diff --git a/src/embedded/mmio/gen.sh b/src/embedded/mmio/gen.sh
new file mode 100755
index 0000000000..3a73f9e393
--- /dev/null
+++ b/src/embedded/mmio/gen.sh
@@ -0,0 +1,4 @@
+#!/bin/sh
+
+sed 's/32/16/g' mmio32.go >mmio16.go
+sed 's/32/8/g' mmio32.go >mmio8.go
\ No newline at end of file
diff --git a/src/embedded/mmio/mb.go b/src/embedded/mmio/mb.go
new file mode 100644
index 0000000000..d9cb0cdcb9
--- /dev/null
+++ b/src/embedded/mmio/mb.go
@@ -0,0 +1,16 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+// MB is a full memory barrier. It ensures that any memory access (normal or
+// I/O) after it, in program order, do not execute until all explicit memory
+// accesses before it complete.
+//
+// Many embedded systems ensure the access order within their I/O address space
+// but the order between load/store in normal RAM and I/O memory is not
+// guaranteed mainly because of different buses for I/O and RAM. Some systems
+// only ensure the access order to registers of the same peripheral. Use MB to
+// properly order operations on different peripherals and RAM.
+func MB()
diff --git a/src/embedded/mmio/mmio16.go b/src/embedded/mmio/mmio16.go
new file mode 100644
index 0000000000..1a0f1b2685
--- /dev/null
+++ b/src/embedded/mmio/mmio16.go
@@ -0,0 +1,106 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+import "unsafe"
+
+//go:noescape
+func load16(addr *uint16) uint16
+
+//go:noescape
+func store16(addr *uint16, v uint16)
+
+// An U16 represents 16-bit memory mapped register.
+//go:notinheap
+type U16 struct {
+	r uint16
+}
+
+// Addr returns the address of r as uintptr.
+func (r *U16) Addr() uintptr {
+	return uintptr(unsafe.Pointer(r))
+}
+
+// SetBit sets n-th bit in r. This is not an atomic operation.
+func (r *U16) SetBit(n int) {
+	store16(&r.r, load16(&r.r)|uint16(1)<<uint(n))
+}
+
+// ClearBit clears n-th bit in r. This is not an atomic operation.
+func (r *U16) ClearBit(n int) {
+	store16(&r.r, load16(&r.r)&^uint16(1)<<uint(n))
+}
+
+// Bit returns the value of n-th bit in r (0 or 1).
+func (r *U16) LoadBit(n int) int {
+	return int(load16(&r.r)>>uint(n)) & 1
+}
+
+// StoreBit sets the value of n-th bit in r to least significant bit of v. This
+// is not an atomic operation.
+func (r *U16) StoreBit(n, v int) {
+	mask := uint16(1) << uint(n)
+	store16(&r.r, load16(&r.r)&^mask|uint16(v<<uint(n))&mask)
+}
+
+// Bits returns the value od r logicaly anded with mask. It is a convenient
+// replacement for r.Load()&mask.
+func (r *U16) LoadBits(mask uint16) uint16 {
+	return load16(&r.r) & mask
+}
+
+// StoreBits stores bits in r selected by mask. It is convenient replacement for
+// r.Store(r.Load()&^mask | bits&mask). This is not an atomic operation.
+func (r *U16) StoreBits(mask, bits uint16) {
+	store16(&r.r, load16(&r.r)&^mask|bits&mask)
+}
+
+// SetBits sets bits in r selected by mask. This is not an atomic operation.
+func (r *U16) SetBits(mask uint16) {
+	store16(&r.r, load16(&r.r)|mask)
+}
+
+// ClearBits clears bits in r selected by mask. This is not an atomic operation.
+func (r *U16) ClearBits(mask uint16) {
+	store16(&r.r, load16(&r.r)&^mask)
+}
+
+// Load returns the value of r.
+func (r *U16) Load() uint16 {
+	return load16(&r.r)
+}
+
+// Store stores v in r.
+func (r *U16) Store(v uint16) {
+	store16(&r.r, v)
+}
+
+//func (r *U16) Field(mask uint16) int {
+//	return bits.Field16(r.r, mask)
+//}
+//func (r *U16) SetField(mask uint16, v int) {
+//	r.StoreBits(mask, bits.MakeField16(v, mask))
+//}
+
+// An UM16 represents a set of bits in R selected by Mask.
+type UM16 struct {
+	R    *U16
+	Mask uint16
+}
+
+// Set sets all bits in b. This is not an atomic operation.
+func (b UM16) Set() { b.R.SetBits(b.Mask) }
+
+// Clear clears all bits in b. This is not an atomic operation.
+func (b UM16) Clear() { b.R.ClearBits(b.Mask) }
+
+// Load returns the value of b.
+func (b UM16) Load() uint16 { return b.R.LoadBits(b.Mask) }
+
+// Store stores bits in b. This is not an atomic operation.
+func (b UM16) Store(bits uint16) { b.R.StoreBits(b.Mask, bits) }
+
+//func (b UM16) LoadVal() int   { return b.R.Field(uint16(b.Mask)) }
+//func (b UM16) StoreVal(v int) { b.R.SetField(b.Mask, v) }
diff --git a/src/embedded/mmio/mmio32.go b/src/embedded/mmio/mmio32.go
new file mode 100644
index 0000000000..6a821a7cc4
--- /dev/null
+++ b/src/embedded/mmio/mmio32.go
@@ -0,0 +1,106 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+import "unsafe"
+
+//go:noescape
+func load32(addr *uint32) uint32
+
+//go:noescape
+func store32(addr *uint32, v uint32)
+
+// An U32 represents 32-bit memory mapped register.
+//go:notinheap
+type U32 struct {
+	r uint32
+}
+
+// Addr returns the address of r as uintptr.
+func (r *U32) Addr() uintptr {
+	return uintptr(unsafe.Pointer(r))
+}
+
+// SetBit sets n-th bit in r. This is not an atomic operation.
+func (r *U32) SetBit(n int) {
+	store32(&r.r, load32(&r.r)|uint32(1)<<uint(n))
+}
+
+// ClearBit clears n-th bit in r. This is not an atomic operation.
+func (r *U32) ClearBit(n int) {
+	store32(&r.r, load32(&r.r)&^uint32(1)<<uint(n))
+}
+
+// Bit returns the value of n-th bit in r (0 or 1).
+func (r *U32) LoadBit(n int) int {
+	return int(load32(&r.r)>>uint(n)) & 1
+}
+
+// StoreBit sets the value of n-th bit in r to least significant bit of v. This
+// is not an atomic operation.
+func (r *U32) StoreBit(n, v int) {
+	mask := uint32(1) << uint(n)
+	store32(&r.r, load32(&r.r)&^mask|uint32(v<<uint(n))&mask)
+}
+
+// Bits returns the value od r logicaly anded with mask. It is a convenient
+// replacement for r.Load()&mask.
+func (r *U32) LoadBits(mask uint32) uint32 {
+	return load32(&r.r) & mask
+}
+
+// StoreBits stores bits in r selected by mask. It is convenient replacement for
+// r.Store(r.Load()&^mask | bits&mask). This is not an atomic operation.
+func (r *U32) StoreBits(mask, bits uint32) {
+	store32(&r.r, load32(&r.r)&^mask|bits&mask)
+}
+
+// SetBits sets bits in r selected by mask. This is not an atomic operation.
+func (r *U32) SetBits(mask uint32) {
+	store32(&r.r, load32(&r.r)|mask)
+}
+
+// ClearBits clears bits in r selected by mask. This is not an atomic operation.
+func (r *U32) ClearBits(mask uint32) {
+	store32(&r.r, load32(&r.r)&^mask)
+}
+
+// Load returns the value of r.
+func (r *U32) Load() uint32 {
+	return load32(&r.r)
+}
+
+// Store stores v in r.
+func (r *U32) Store(v uint32) {
+	store32(&r.r, v)
+}
+
+//func (r *U32) Field(mask uint32) int {
+//	return bits.Field32(r.r, mask)
+//}
+//func (r *U32) SetField(mask uint32, v int) {
+//	r.StoreBits(mask, bits.MakeField32(v, mask))
+//}
+
+// An UM32 represents a set of bits in R selected by Mask.
+type UM32 struct {
+	R    *U32
+	Mask uint32
+}
+
+// Set sets all bits in b. This is not an atomic operation.
+func (b UM32) Set() { b.R.SetBits(b.Mask) }
+
+// Clear clears all bits in b. This is not an atomic operation.
+func (b UM32) Clear() { b.R.ClearBits(b.Mask) }
+
+// Load returns the value of b.
+func (b UM32) Load() uint32 { return b.R.LoadBits(b.Mask) }
+
+// Store stores bits in b. This is not an atomic operation.
+func (b UM32) Store(bits uint32) { b.R.StoreBits(b.Mask, bits) }
+
+//func (b UM32) LoadVal() int   { return b.R.Field(uint32(b.Mask)) }
+//func (b UM32) StoreVal(v int) { b.R.SetField(b.Mask, v) }
diff --git a/src/embedded/mmio/mmio8.go b/src/embedded/mmio/mmio8.go
new file mode 100644
index 0000000000..29bba9b979
--- /dev/null
+++ b/src/embedded/mmio/mmio8.go
@@ -0,0 +1,106 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package mmio
+
+import "unsafe"
+
+//go:noescape
+func load8(addr *uint8) uint8
+
+//go:noescape
+func store8(addr *uint8, v uint8)
+
+// An U8 represents 8-bit memory mapped register.
+//go:notinheap
+type U8 struct {
+	r uint8
+}
+
+// Addr returns the address of r as uintptr.
+func (r *U8) Addr() uintptr {
+	return uintptr(unsafe.Pointer(r))
+}
+
+// SetBit sets n-th bit in r. This is not an atomic operation.
+func (r *U8) SetBit(n int) {
+	store8(&r.r, load8(&r.r)|uint8(1)<<uint(n))
+}
+
+// ClearBit clears n-th bit in r. This is not an atomic operation.
+func (r *U8) ClearBit(n int) {
+	store8(&r.r, load8(&r.r)&^uint8(1)<<uint(n))
+}
+
+// Bit returns the value of n-th bit in r (0 or 1).
+func (r *U8) LoadBit(n int) int {
+	return int(load8(&r.r)>>uint(n)) & 1
+}
+
+// StoreBit sets the value of n-th bit in r to least significant bit of v. This
+// is not an atomic operation.
+func (r *U8) StoreBit(n, v int) {
+	mask := uint8(1) << uint(n)
+	store8(&r.r, load8(&r.r)&^mask|uint8(v<<uint(n))&mask)
+}
+
+// Bits returns the value od r logicaly anded with mask. It is a convenient
+// replacement for r.Load()&mask.
+func (r *U8) LoadBits(mask uint8) uint8 {
+	return load8(&r.r) & mask
+}
+
+// StoreBits stores bits in r selected by mask. It is convenient replacement for
+// r.Store(r.Load()&^mask | bits&mask). This is not an atomic operation.
+func (r *U8) StoreBits(mask, bits uint8) {
+	store8(&r.r, load8(&r.r)&^mask|bits&mask)
+}
+
+// SetBits sets bits in r selected by mask. This is not an atomic operation.
+func (r *U8) SetBits(mask uint8) {
+	store8(&r.r, load8(&r.r)|mask)
+}
+
+// ClearBits clears bits in r selected by mask. This is not an atomic operation.
+func (r *U8) ClearBits(mask uint8) {
+	store8(&r.r, load8(&r.r)&^mask)
+}
+
+// Load returns the value of r.
+func (r *U8) Load() uint8 {
+	return load8(&r.r)
+}
+
+// Store stores v in r.
+func (r *U8) Store(v uint8) {
+	store8(&r.r, v)
+}
+
+//func (r *U8) Field(mask uint8) int {
+//	return bits.Field8(r.r, mask)
+//}
+//func (r *U8) SetField(mask uint8, v int) {
+//	r.StoreBits(mask, bits.MakeField8(v, mask))
+//}
+
+// An UM8 represents a set of bits in R selected by Mask.
+type UM8 struct {
+	R    *U8
+	Mask uint8
+}
+
+// Set sets all bits in b. This is not an atomic operation.
+func (b UM8) Set() { b.R.SetBits(b.Mask) }
+
+// Clear clears all bits in b. This is not an atomic operation.
+func (b UM8) Clear() { b.R.ClearBits(b.Mask) }
+
+// Load returns the value of b.
+func (b UM8) Load() uint8 { return b.R.LoadBits(b.Mask) }
+
+// Store stores bits in b. This is not an atomic operation.
+func (b UM8) Store(bits uint8) { b.R.StoreBits(b.Mask, bits) }
+
+//func (b UM8) LoadVal() int   { return b.R.Field(uint8(b.Mask)) }
+//func (b UM8) StoreVal(v int) { b.R.SetField(b.Mask, v) }
diff --git a/src/embedded/rtos/asm_thumb.s b/src/embedded/rtos/asm_thumb.s
new file mode 100644
index 0000000000..4433a7fdab
--- /dev/null
+++ b/src/embedded/rtos/asm_thumb.s
@@ -0,0 +1,6 @@
+#include "textflag.h"
+
+// func publicationBarrier()
+TEXT publicationBarrier(SB),NOSPLIT|NOFRAME,$0-0
+	DMB  MB_ST // must be system wide
+	RET
diff --git a/src/embedded/rtos/errors.go b/src/embedded/rtos/errors.go
new file mode 100644
index 0000000000..a5681609f1
--- /dev/null
+++ b/src/embedded/rtos/errors.go
@@ -0,0 +1,18 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+type Error struct{ s string }
+
+func (e *Error) Error() string { return e.s }
+
+var (
+	ErrUknown         = &Error{"rtos: unknown error"}
+	ErrNotSuppoted    = &Error{"rtos: operation not supported"}
+	ErrBadPrivLevel   = &Error{"rtos: bad privilege level"}
+	ErrInsufPrivLevel = &Error{"rtos: insufficient privilege level"}
+	ErrBadIntNumber   = &Error{"rtos: bad interrupt number"}
+	ErrBadIntPrio     = &Error{"rtos: bad interrupt priority"}
+)
diff --git a/src/embedded/rtos/errors_noos.go b/src/embedded/rtos/errors_noos.go
new file mode 100644
index 0000000000..211958dc58
--- /dev/null
+++ b/src/embedded/rtos/errors_noos.go
@@ -0,0 +1,24 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+var errorsByNumber = [...]*Error{
+	0: ErrUknown,
+	1: ErrNotSuppoted,
+	2: ErrBadPrivLevel,
+	3: ErrInsufPrivLevel,
+	4: ErrBadIntNumber,
+	5: ErrBadIntPrio,
+}
+
+func errnoError(errno int) error {
+	if errno == 0 {
+		return nil
+	}
+	if uint(errno) > uint(len(errorsByNumber)) {
+		errno = 0
+	}
+	return errorsByNumber[errno]
+}
diff --git a/src/embedded/rtos/irq.go b/src/embedded/rtos/irq.go
new file mode 100644
index 0000000000..3a22eb0491
--- /dev/null
+++ b/src/embedded/rtos/irq.go
@@ -0,0 +1,55 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+// Package rtos defines a seven well known interrupt priority levels ordered by
+// increasing urgency as follows: IntPrioLowest, IntPrioLow, IntPrioSysCall,
+// IntPrioMid, IntPrioSysTimer, IntPrioHigh, IntPrioHighest. There can be
+// additional priority levels between the defined ones and some can have the
+// same level. You can use ordinary equality operators to compare priority
+// levels.
+//
+// The architecture specific code should provide (if possible) at least four
+// different real priority levels so the defined levels can satisfy the
+// following inequality:
+//
+// Lowest <= Low < SysCall < IntPrioMid <= SysTimer < High <= Highest
+//
+// Architecture that supports interrupt nesting must ensure that the incoming
+// interrupt request can not preempt the interrupt handler that runs with the
+// same or higher priority.
+const (
+	IntPrioHighest  = intPrioHighest
+	IntPrioHigh     = intPrioHigh
+	IntPrioSysTimer = intPrioSysTimer
+	IntPrioMid      = intPrioMid
+	IntPrioSysCall  = intPrioSysCall
+	IntPrioLow      = intPrioLow
+	IntPrioLowest   = intPrioLowest
+
+	IntPrioCurrent = intPrioCurrent
+)
+
+// IRQ represents an user accessible interrupt. It provides interface to basic
+// operations such as enabling/disabling handling of interrupt requests and
+// setting interrupt priorities. There can be other (system) interrupts not
+// exposed by this interface.
+type IRQ int
+
+// Enable sets the priority of interrupt and enables handling of interrupt
+// requests.
+func (irq IRQ) Enable(prio int) error {
+	return irqEnable(irq, prio)
+}
+
+// Disable disables handling of interrupt requests.
+func (irq IRQ) Disable() error {
+	return irqDisable(irq)
+}
+
+// Status reports whether the irq is enabled and returns its priority.
+func (irq IRQ) Status() (enabled bool, prio int, err error) {
+	return irqStatus(irq)
+}
diff --git a/src/embedded/rtos/irq_noos.go b/src/embedded/rtos/irq_noos.go
new file mode 100644
index 0000000000..0178dcd81a
--- /dev/null
+++ b/src/embedded/rtos/irq_noos.go
@@ -0,0 +1,30 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import _ "unsafe"
+
+const intPrioCurrent = -1
+
+func irqEnable(irq IRQ, prio int) error {
+	if uint(prio+1) > intPrioHighest+1 {
+		return ErrBadIntPrio
+	}
+	_, _, errno := runtime_irqctl(int(irq), prio)
+	return errnoError(errno)
+}
+
+func irqDisable(irq IRQ) error {
+	_, _, errno := runtime_irqctl(int(irq), -2)
+	return errnoError(errno)
+}
+
+func irqStatus(irq IRQ) (enabled bool, prio int, err error) {
+	en, prio, errno := runtime_irqctl(int(irq), -3)
+	return en != 0, prio, errnoError(errno)
+}
+
+//go:linkname runtime_irqctl runtime.irqctl
+func runtime_irqctl(irq, ctl int) (enabled, prio, errno int)
diff --git a/src/embedded/rtos/irq_noos_thumb.go b/src/embedded/rtos/irq_noos_thumb.go
new file mode 100644
index 0000000000..1a9d84a8a7
--- /dev/null
+++ b/src/embedded/rtos/irq_noos_thumb.go
@@ -0,0 +1,16 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+const (
+	intPrioHighest  = 255 - 0<<5 // do not use with nRF52 SoftDevice
+	intPrioHigh     = 255 - 1<<5
+	intPrioSysTimer = 255 - 2<<5
+	intPrioMid      = 255 - 3<<5
+	intPrioSysCall  = 255 - 4<<5 // compatible with nRF52 SoftDevice
+	intPrioLow      = 255 - 5<<5
+	intPrioLowest   = 255 - 6<<5
+	intPrioPendSV   = 255 - 255 // unusable because can not wake the scheduler
+)
diff --git a/src/embedded/rtos/irq_x.go b/src/embedded/rtos/irq_x.go
new file mode 100644
index 0000000000..dd3d98b380
--- /dev/null
+++ b/src/embedded/rtos/irq_x.go
@@ -0,0 +1,30 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+const (
+	intPrioHighest  = 0
+	intPrioHigh     = 0
+	intPrioSysTimer = 0
+	intPrioMid      = 0
+	intPrioSysCall  = 0
+	intPrioLow      = 0
+	intPrioLowest   = 0
+	intPrioCurrent  = -1
+)
+
+func irqEnable(irq IRQ, prio int) error {
+	return ErrNotSuppoted
+}
+
+func irqDisable(irq IRQ) error {
+	return ErrNotSuppoted
+}
+
+func irqStatus(irq IRQ) (enabled bool, prio int, err error) {
+	return false, 0, ErrNotSuppoted
+}
diff --git a/src/embedded/rtos/nanotime.go b/src/embedded/rtos/nanotime.go
new file mode 100644
index 0000000000..aa6fce0d7d
--- /dev/null
+++ b/src/embedded/rtos/nanotime.go
@@ -0,0 +1,19 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"time"
+	_ "unsafe"
+)
+
+// Nanotime returns a time duration from some event in the past. Typically it's
+// roughly corresponds to the system runtime.
+func Nanotime() time.Duration {
+	return time.Duration(runtime_nanotime())
+}
+
+//go:linkname runtime_nanotime runtime.nanotime
+func runtime_nanotime() int64
diff --git a/src/embedded/rtos/note.go b/src/embedded/rtos/note.go
new file mode 100644
index 0000000000..1e2ba09611
--- /dev/null
+++ b/src/embedded/rtos/note.go
@@ -0,0 +1,48 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"time"
+	_ "unsafe"
+)
+
+// Note allows to communicate the occurrence of an event.
+//
+// Before any calls to Sleep or Wakeup, must call Clear to initialize the Note.
+//
+// Exactly one gorutine can call Sleep but there is allowed to multiple
+// gorutines or interrupt handlers to call Wakeup. Future Sleep will return
+// immediately.
+//
+// Subsequent Clear must be called only after previous Sleep has returned, e.g.
+// it is disallowed to call Clear straight after Wakeup.
+type Note struct {
+	// must be in sync with runtime.notel
+	key  uintptr
+	link uintptr
+}
+
+// Sleep sleeps on the cleared note until other goroutine or interrupt handler
+// call Wakeup or until the timeout.
+func (n *Note) Sleep(timeout time.Duration) bool {
+	return runtime_notetsleepg(n, int64(timeout))
+}
+
+// Wakeup wakeups the goroutine that sleeps on the note. The Wakeup remains in
+// effect until subequent Clear so future Sleep will return immediately.
+func (n *Note) Wakeup() { notewakeup(n) }
+
+// Clear clears the note. Any Wakeup that happened before Clear is forgotten.
+// Clear works as a publication barrier, that is, the Clear itself and any
+// memory writes preceding it in the program order happens before any memory
+// writes that follows it.
+func (n *Note) Clear() {
+	n.key = 0
+	publicationBarrier()
+}
+
+//go:linkname runtime_notetsleepg runtime.notetsleepg
+func runtime_notetsleepg(n *Note, ns int64) bool
diff --git a/src/embedded/rtos/note_noos.go b/src/embedded/rtos/note_noos.go
new file mode 100644
index 0000000000..d5eb336d7b
--- /dev/null
+++ b/src/embedded/rtos/note_noos.go
@@ -0,0 +1,12 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import _ "unsafe"
+
+//go:linkname notewakeup runtime.rtos_notewakeup
+func notewakeup(n *Note)
+
+func publicationBarrier()
\ No newline at end of file
diff --git a/src/embedded/rtos/note_os.go b/src/embedded/rtos/note_os.go
new file mode 100644
index 0000000000..51a42abc2c
--- /dev/null
+++ b/src/embedded/rtos/note_os.go
@@ -0,0 +1,15 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+import _ "unsafe"
+
+//go:linkname notewakeup runtime.notewakeup
+func notewakeup(n *Note)
+
+//go:linkname publicationBarrier runtime.publicationBarrier
+func publicationBarrier()
\ No newline at end of file
diff --git a/src/embedded/rtos/privlevel.go b/src/embedded/rtos/privlevel.go
new file mode 100644
index 0000000000..c442decfe5
--- /dev/null
+++ b/src/embedded/rtos/privlevel.go
@@ -0,0 +1,15 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+// SetPrivLevel sets privilege level for current thread to newlevel. Level 0 is
+// the most privileged and allows access to all system resources. Any resource
+// available to level n is also available to level 0..n. If n < 0 the privilege
+// level is not changed. SetPrivLevel returns previous level number and error.
+// SetPrivLevel is usually preceded by runtime.LockOSThread to ensure effect for
+// current gorutine.
+func SetPrivLevel(newlevel int) (oldlevel int, err error) {
+	return setPrivLevel(newlevel)
+}
diff --git a/src/embedded/rtos/privlevel_noos.go b/src/embedded/rtos/privlevel_noos.go
new file mode 100644
index 0000000000..08dd721f98
--- /dev/null
+++ b/src/embedded/rtos/privlevel_noos.go
@@ -0,0 +1,15 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import _ "unsafe"
+
+func setPrivLevel(newlevel int) (oldlevel int, err error) {
+	oldlevel, errno := runtime_setprivlevel(newlevel)
+	return oldlevel, errnoError(errno)
+}
+
+//go:linkname runtime_setprivlevel runtime.setprivlevel
+func runtime_setprivlevel(newlevel int) (oldlevel, errno int)
diff --git a/src/embedded/rtos/privlevel_x.go b/src/embedded/rtos/privlevel_x.go
new file mode 100644
index 0000000000..2b03a6b291
--- /dev/null
+++ b/src/embedded/rtos/privlevel_x.go
@@ -0,0 +1,11 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+func setPrivLevel(newlevel int) (oldlevel int, err error) {
+	return -1, ErrNotSuppoted
+}
diff --git a/src/embedded/rtos/systim.go b/src/embedded/rtos/systim.go
new file mode 100644
index 0000000000..8eb8935b50
--- /dev/null
+++ b/src/embedded/rtos/systim.go
@@ -0,0 +1,35 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+// SetSysTimer registers two functions that the thread scheduler uses to
+// communicate with the system timer.
+//
+// Nanotime should return the monotonic time in nanoseconds.
+//
+// Setalarm is called by the scheduler with the ns >= 0 to ask the system timer
+// to wake the current CPU at the specified time. Sheduler can also pass ns < 0
+// if it do not want to be woken up by the system timer. A spurious or
+// inaccurate wakeups are acceptable.
+//
+// There are two main types of system timer implementations.
+//
+// The ticking implementation can simply wake up the CPU with a constant period.
+// In this case setalarm can be nil.
+//
+// The tickless implementation schould try to wake up the CPU only once, at ns
+// or just after ns.
+//
+// The System Timer interrupt handler, if used, must run with IntPrioSysTimer
+// priority.
+//
+// SetSystemTimer is intended to be called only once, at the very beginning of
+// the system initialization. If you want to change the system timer
+// implementation at a later stage (which is discouraged) you must ensure a
+// continuity of time and that all gorutines that sleeeps using old setalarm
+// function will be wakeup.
+func SetSystemTimer(nanotime func() int64, setalarm func(ns int64) bool) error {
+	return setSystemTimer(nanotime, setalarm)
+}
diff --git a/src/embedded/rtos/systim_noos.go b/src/embedded/rtos/systim_noos.go
new file mode 100644
index 0000000000..6976db220a
--- /dev/null
+++ b/src/embedded/rtos/systim_noos.go
@@ -0,0 +1,23 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import (
+	"sync"
+	_ "unsafe"
+)
+
+var setsystimmx sync.Mutex
+
+func setSystemTimer(nanotime func() int64, setalarm func(ns int64) bool) error {
+	setsystimmx.Lock()
+	runtime_setsystim(nanotime, setalarm)
+	setsystimmx.Unlock()
+	return nil
+}
+
+//go:linkname runtime_setsystim runtime.setsystim
+func runtime_setsystim(nanotime func() int64, setalarm func(ns int64) bool)
+
diff --git a/src/embedded/rtos/systim_x.go b/src/embedded/rtos/systim_x.go
new file mode 100644
index 0000000000..1e772fefed
--- /dev/null
+++ b/src/embedded/rtos/systim_x.go
@@ -0,0 +1,11 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package rtos
+
+func setSystemTimer(nanotime func() int64, setalarm func(ns int64) bool) error {
+	return ErrNotSuppoted
+}
diff --git a/src/embedded/rtos/utils.go b/src/embedded/rtos/utils.go
new file mode 100644
index 0000000000..e110659f27
--- /dev/null
+++ b/src/embedded/rtos/utils.go
@@ -0,0 +1,10 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package rtos
+
+import "unsafe"
+
+// copied from /home/michal/P/go/goroot/src/runtime/stubs.go
+func bool2int(x bool) int { return int(uint8(*(*uint8)(unsafe.Pointer(&x)))) }
diff --git a/src/go/build/deps_test.go b/src/go/build/deps_test.go
index 179b0bfef5..40322d305b 100644
--- a/src/go/build/deps_test.go
+++ b/src/go/build/deps_test.go
@@ -36,9 +36,9 @@ var pkgDeps = map[string][]string{
 	// L0 is the lowest level, core, nearly unavoidable packages.
 	"errors":                  {"runtime", "internal/reflectlite"},
 	"io":                      {"errors", "sync", "sync/atomic"},
-	"runtime":                 {"unsafe", "runtime/internal/atomic", "runtime/internal/sys", "runtime/internal/math", "internal/cpu", "internal/bytealg"},
+	"runtime":                 {"unsafe", "runtime/internal/atomic", "runtime/internal/sys", "runtime/internal/math", "internal/cpu", "internal/cpu/cortexm", "internal/cpu/cortexm/nvic", "internal/cpu/cortexm/scb", "internal/cpu/cortexm/scid", "internal/cpu/cortexm/debug/itm", "internal/cpu/cortexm/mpu", "internal/bytealg", "embedded/mmio"},
 	"runtime/internal/sys":    {},
-	"runtime/internal/atomic": {"unsafe", "internal/cpu"},
+	"runtime/internal/atomic": {"unsafe", "internal/cpu", "runtime/internal/sys"},
 	"runtime/internal/math":   {"runtime/internal/sys"},
 	"internal/race":           {"runtime", "unsafe"},
 	"sync":                    {"internal/race", "runtime", "sync/atomic", "unsafe"},
@@ -451,6 +451,22 @@ var pkgDeps = map[string][]string{
 	"net/http/pprof":    {"L4", "OS", "html/template", "net/http", "runtime/pprof", "runtime/trace"},
 	"net/rpc":           {"L4", "NET", "encoding/gob", "html/template", "net/http", "go/token"},
 	"net/rpc/jsonrpc":   {"L4", "NET", "encoding/json", "net/rpc"},
+
+	// Embedded programming
+	"internal/cpu/cortexm/acc":       {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/bitband":   {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/cmt":       {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/fpu":       {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/mpu":       {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/nvic":      {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/pft":       {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/scb":       {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/scid":      {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/systick":   {"unsafe", "embedded/mmio"},
+	"internal/cpu/cortexm/debug/itm": {"unsafe", "embedded/mmio"},
+	"embedded/mmio":                  {"unsafe"},
+	"embedded/arch/cortexm/systim":   {"unsafe", "embedded/mmio", "embedded/rtos", "internal/cpu/cortexm", "internal/cpu/cortexm/scb", "internal/cpu/cortexm/systick"},
+	"embedded/rtos":                  {"unsafe", "errors", "sync", "time"},
 }
 
 // isMacro reports whether p is a package dependency macro
diff --git a/src/go/build/syslist.go b/src/go/build/syslist.go
index 197c646d5c..a120f4992c 100644
--- a/src/go/build/syslist.go
+++ b/src/go/build/syslist.go
@@ -4,5 +4,5 @@
 
 package build
 
-const goosList = "aix android darwin dragonfly freebsd hurd illumos js linux nacl netbsd openbsd plan9 solaris windows zos "
-const goarchList = "386 amd64 amd64p32 arm armbe arm64 arm64be ppc64 ppc64le mips mipsle mips64 mips64le mips64p32 mips64p32le ppc riscv riscv64 s390 s390x sparc sparc64 wasm "
+const goosList = "aix android darwin dragonfly freebsd hurd illumos js linux nacl netbsd noos openbsd plan9 solaris windows zos "
+const goarchList = "386 amd64 amd64p32 arm armbe arm64 arm64be ppc64 ppc64le mips mipsle mips64 mips64le mips64p32 mips64p32le ppc riscv riscv64 s390 s390x sparc sparc64 thumb wasm "
diff --git a/src/go/types/example_test.go b/src/go/types/example_test.go
index 492127bbab..c45bc697fc 100644
--- a/src/go/types/example_test.go
+++ b/src/go/types/example_test.go
@@ -5,7 +5,7 @@
 // Only run where builders (build.golang.org) have
 // access to compiled packages for import.
 //
-// +build !arm,!arm64,!nacl
+// +build !arm,!thumb,!arm64,!nacl
 
 package types_test
 
diff --git a/src/go/types/sizes.go b/src/go/types/sizes.go
index 6ab6157b82..f78c430352 100644
--- a/src/go/types/sizes.go
+++ b/src/go/types/sizes.go
@@ -169,6 +169,7 @@ var gcArchSizes = map[string]*StdSizes{
 	"ppc64le":  {8, 8},
 	"riscv64":  {8, 8},
 	"s390x":    {8, 8},
+	"thumb":    {4, 4},
 	"sparc64":  {8, 8},
 	"wasm":     {8, 8},
 	// When adding more architectures here,
@@ -180,7 +181,7 @@ var gcArchSizes = map[string]*StdSizes{
 //
 // Supported architectures for compiler "gc":
 // "386", "arm", "arm64", "amd64", "amd64p32", "mips", "mipsle",
-// "mips64", "mips64le", "ppc64", "ppc64le", "riscv64", "s390x", "sparc64", "wasm".
+// "mips64", "mips64le", "ppc64", "ppc64le", "riscv64", "s390x", "sparc64", "thumb", "wasm".
 func SizesFor(compiler, arch string) Sizes {
 	var m map[string]*StdSizes
 	switch compiler {
diff --git a/src/internal/bytealg/compare_generic.go b/src/internal/bytealg/compare_generic.go
index 2ac60f3df9..927ae67298 100644
--- a/src/internal/bytealg/compare_generic.go
+++ b/src/internal/bytealg/compare_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !386,!amd64,!amd64p32,!s390x,!arm,!arm64,!ppc64,!ppc64le,!mips,!mipsle,!wasm
+// +build !386,!amd64,!amd64p32,!s390x,!arm,!thumb,!arm64,!ppc64,!ppc64le,!mips,!mipsle,!wasm
 
 package bytealg
 
diff --git a/src/internal/bytealg/compare_native.go b/src/internal/bytealg/compare_native.go
index b14aa8c72c..e5eeb11aa7 100644
--- a/src/internal/bytealg/compare_native.go
+++ b/src/internal/bytealg/compare_native.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 amd64 amd64p32 s390x arm arm64 ppc64 ppc64le mips mipsle wasm
+// +build 386 amd64 amd64p32 s390x arm thumb arm64 ppc64 ppc64le mips mipsle wasm
 
 package bytealg
 
diff --git a/src/internal/bytealg/compare_thumb.s b/src/internal/bytealg/compare_thumb.s
new file mode 100644
index 0000000000..d9cfff1d73
--- /dev/null
+++ b/src/internal/bytealg/compare_thumb.s
@@ -0,0 +1,83 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+TEXT Compare(SB),NOSPLIT|NOFRAME,$0-28
+	MOVW  a_base+0(FP), R2
+	MOVW  a_len+4(FP), R0
+	MOVW  b_base+12(FP), R3
+	MOVW  b_len+16(FP), R1
+	ADD   $28, R13, R7
+	B     cmpbody<>(SB)
+
+TEXT runtimecmpstring(SB),NOSPLIT|NOFRAME,$0-20
+	MOVW  a_base+0(FP), R2
+	MOVW  a_len+4(FP), R0
+	MOVW  b_base+8(FP), R3
+	MOVW  b_len+12(FP), R1
+	ADD   $20, R13, R7
+	B     cmpbody<>(SB)
+
+// On entry:
+// R0 is the length of a
+// R1 is the length of b
+// R2 points to the start of a
+// R3 points to the start of b
+// R7 points to return value (-1/0/1 will be written here); be carefull: R7 is REGTMP on thumb
+//
+// On exit:
+// R4, R5, R6 and R8 are clobbered
+TEXT cmpbody<>(SB),NOSPLIT|NOFRAME,$0-0
+	CMP      R2, R3
+	BEQ      samebytes
+	MOVW     R0, R6
+	CMP      R0, R1
+	MOVW.LT  R1, R6  // R6 is min(R0, R1)
+
+	CBZ   R6, samebytes
+	CMP   $4, R6
+	ADD   R2, R6     // R2 is current byte in a, R6 is the end of the range to compare
+	BLT   byte_loop  // length < 4
+	AND   $3, R2, R4
+	CBNZ  R4, byte_loop  // unaligned a, use byte-wise compare (TODO: try to align a)
+aligned_a:
+	AND   $3, R3, R4
+	CBNZ  R4, byte_loop  // unaligned b, use byte-wise compare
+	AND   $0xfffffffc, R6, R8
+	// length >= 4
+chunk4_loop:
+	MOVW.P  4(R2), R4
+	MOVW.P  4(R3), R5
+	CMP     R4, R5
+	BNE     cmp
+	CMP     R2, R8
+	BNE     chunk4_loop
+	CMP     R2, R6
+	BEQ     samebytes  // all compared bytes were the same; compare lengths
+byte_loop:
+	MOVBU.P  1(R2), R4
+	MOVBU.P  1(R3), R5
+	CMP      R4, R5
+	BNE      ret
+	CMP      R2, R6
+	BNE      byte_loop
+samebytes:
+	CMP      R0, R1
+	MOVW.LT  $1, R0
+	MOVW.GT  $-1, R0
+	MOVW.EQ  $0, R0
+	MOVW     R0, (R7)
+	RET
+ret:
+	// bytes differed
+	MOVW.LT  $1, R0
+	MOVW.GT  $-1, R0
+	MOVW     R0, (R7)
+	RET
+cmp:
+	SUB  $4, R2, R2
+	SUB  $4, R3, R3
+	B    byte_loop
diff --git a/src/internal/bytealg/count_generic.go b/src/internal/bytealg/count_generic.go
index 13759ad496..6c0dccc0dd 100644
--- a/src/internal/bytealg/count_generic.go
+++ b/src/internal/bytealg/count_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !amd64,!arm,!arm64,!ppc64le,!ppc64
+// +build !amd64,!arm,!thumb,!arm64,!ppc64le,!ppc64
 
 package bytealg
 
diff --git a/src/internal/bytealg/count_native.go b/src/internal/bytealg/count_native.go
index 52b2a461a4..37d44cb013 100644
--- a/src/internal/bytealg/count_native.go
+++ b/src/internal/bytealg/count_native.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build amd64 arm arm64 ppc64le ppc64
+// +build amd64 arm thumb arm64 ppc64le ppc64
 
 package bytealg
 
diff --git a/src/internal/bytealg/count_thumb.s b/src/internal/bytealg/count_thumb.s
new file mode 100644
index 0000000000..1d7147f6b5
--- /dev/null
+++ b/src/internal/bytealg/count_thumb.s
@@ -0,0 +1,42 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+TEXT Count(SB),NOSPLIT,$0-20
+	MOVW   b_base+0(FP), R0
+	MOVW   b_len+4(FP), R1
+	MOVBU  c+12(FP), R2
+	MOVW   $ret+16(FP), R7
+	B      countbytebody<>(SB)
+
+TEXT CountString(SB),NOSPLIT,$0-16
+	MOVW   s_base+0(FP), R0
+	MOVW   s_len+4(FP), R1
+	MOVBU  c+8(FP), R2
+	MOVW   $ret+12(FP), R7
+	B      countbytebody<>(SB)
+
+// Input:
+// R0: data
+// R1: data length
+// R2: byte to find
+// R7: address to put result; be carefull: R7 is REGTMP on thumb
+//
+// On exit:
+// R4 and R8 are clobbered
+TEXT countbytebody<>(SB),NOSPLIT,$0
+	MOVW  $0, R8    // R8 = count of byte to search
+	CBZ   R1, done  // short path to handle 0-byte case
+	ADD   R0, R1    // R1 is the end of the range
+byte_loop:
+	MOVBU.P  1(R0), R4
+	CMP      R4, R2
+	ADD.EQ   $1, R8
+	CMP      R0, R1
+	BNE      byte_loop
+done:
+	MOVW  R8, (R7)
+	RET
diff --git a/src/internal/bytealg/equal_thumb.s b/src/internal/bytealg/equal_thumb.s
new file mode 100644
index 0000000000..66f75e09a5
--- /dev/null
+++ b/src/internal/bytealg/equal_thumb.s
@@ -0,0 +1,89 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+#define REGCTXT R11
+
+// memequal(a, b unsafe.Pointer, size uintptr) bool
+TEXT runtimememequal(SB),NOSPLIT|NOFRAME,$0-13
+	MOVW  a+0(FP), R0
+	MOVW  b+4(FP), R2
+	CMP   R0, R2
+	BEQ   eq
+	MOVW  size+8(FP), R1
+	CBZ   R1, eq  // short path to handle 0-byte case
+	MOVW  $ret+12(FP), R7
+	B     memeqbody<>(SB)
+eq:
+	MOVW  $1, R0
+	MOVB  R0, ret+12(FP)
+	RET
+
+// memequal_varlen(a, b unsafe.Pointer) bool
+TEXT runtimememequal_varlen(SB),NOSPLIT|NOFRAME,$0-9
+	MOVW  a+0(FP), R0
+	MOVW  b+4(FP), R2
+	CMP   R0, R2
+	BEQ   eq
+	MOVW  4(REGCTXT), R1  // compiler stores size at offset 4 in the closure
+	CBZ   R1, eq          // short path to handle 0-byte case
+	MOVW  $ret+8(FP), R7
+	B     memeqbody<>(SB)
+eq:
+	MOVW  $1, R0
+	MOVB  R0, ret+8(FP)
+	RET
+
+// Input:
+// R0: data of a
+// R1: length
+// R2: data of b
+// R7: points to return value
+//
+// On exit:
+// R4, R5 and R6 are clobbered
+TEXT memeqbody<>(SB),NOSPLIT|NOFRAME,$0-0
+	CMP  $1, R1
+	BEQ  one  // 1-byte special case for better performance
+
+	CMP   $4, R1
+	ADD   R0, R1     // R1 is the end of the range to compare
+	BLT   byte_loop  // length < 4
+	AND   $3, R0, R6
+	CBNZ  R6, byte_loop  // unaligned a, use byte-wise compare (TODO: try to align a)
+	AND   $3, R2, R6
+	CBNZ  R6, byte_loop  // unaligned b, use byte-wise compare
+	AND   $0xfffffffc, R1, R6
+	// length >= 4
+chunk4_loop:
+	MOVW.P  4(R0), R4
+	MOVW.P  4(R2), R5
+	CMP     R4, R5
+	BNE     notequal
+	CMP     R0, R6
+	BNE     chunk4_loop
+	CMP     R0, R1
+	BEQ     equal  // reached the end
+byte_loop:
+	MOVBU.P  1(R0), R4
+	MOVBU.P  1(R2), R5
+	CMP      R4, R5
+	BNE      notequal
+	CMP      R0, R1
+	BNE      byte_loop
+equal:
+	MOVW  $1, R0
+	MOVB  R0, (R7)
+	RET
+one:
+	MOVBU  (R0), R4
+	MOVBU  (R2), R5
+	CMP    R4, R5
+	BEQ    equal
+notequal:
+	MOVW  $0, R0
+	MOVB  R0, (R7)
+	RET
diff --git a/src/internal/bytealg/indexbyte_generic.go b/src/internal/bytealg/indexbyte_generic.go
index 6bff31ceee..1790077955 100644
--- a/src/internal/bytealg/indexbyte_generic.go
+++ b/src/internal/bytealg/indexbyte_generic.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !386,!amd64,!amd64p32,!s390x,!arm,!arm64,!ppc64,!ppc64le,!mips,!mipsle,!mips64,!mips64le,!wasm
+// +build !386,!amd64,!amd64p32,!s390x,!arm,!thumb,!arm64,!ppc64,!ppc64le,!mips,!mipsle,!mips64,!mips64le,!wasm
 
 package bytealg
 
diff --git a/src/internal/bytealg/indexbyte_native.go b/src/internal/bytealg/indexbyte_native.go
index b4ddc86ea9..d67be0c65e 100644
--- a/src/internal/bytealg/indexbyte_native.go
+++ b/src/internal/bytealg/indexbyte_native.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 amd64 amd64p32 s390x arm arm64 ppc64 ppc64le mips mipsle mips64 mips64le wasm
+// +build 386 amd64 amd64p32 s390x arm thumb arm64 ppc64 ppc64le mips mipsle mips64 mips64le wasm
 
 package bytealg
 
diff --git a/src/internal/bytealg/indexbyte_thumb.s b/src/internal/bytealg/indexbyte_thumb.s
new file mode 100644
index 0000000000..ba48b98bc7
--- /dev/null
+++ b/src/internal/bytealg/indexbyte_thumb.s
@@ -0,0 +1,46 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "textflag.h"
+
+TEXT IndexByte(SB),NOSPLIT,$0-20
+	MOVW   b_base+0(FP), R0
+	MOVW   b_len+4(FP), R1
+	MOVBU  c+12(FP), R2  // byte to find
+	MOVW   $ret+16(FP), R5
+	B      indexbytebody<>(SB)
+
+TEXT IndexByteString(SB),NOSPLIT,$0-16
+	MOVW   s_base+0(FP), R0
+	MOVW   s_len+4(FP), R1
+	MOVBU  c+8(FP), R2  // byte to find
+	MOVW   $ret+12(FP), R5
+	B      indexbytebody<>(SB)
+
+// input:
+//  R0: data
+//  R1: data length
+//  R2: byte to find
+//  R5: address to put result
+TEXT indexbytebody<>(SB),NOSPLIT,$0-0
+	MOVW  R0, R4  // store base for later
+	ADD   R0, R1  // end
+
+loop:
+	CMP      R0, R1
+	BEQ      notfound
+	MOVBU.P  1(R0), R3
+	CMP      R2, R3
+	BNE      loop
+
+	SUB   $1, R0  // R0 will be one beyond the position we want
+	SUB   R4, R0  // remove base
+	MOVW  R0, (R5)
+	RET   
+
+notfound:
+	MOVW  $-1, R0
+	MOVW  R0, (R5)
+	RET   
diff --git a/src/internal/cpu/cortexm/acc/doc.go b/src/internal/cpu/cortexm/acc/doc.go
new file mode 100644
index 0000000000..6c7a4456ba
--- /dev/null
+++ b/src/internal/cpu/cortexm/acc/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package acc gives access to the Access Control registers.
+package acc
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/acc/periph_thumb.go b/src/internal/cpu/cortexm/acc/periph_thumb.go
new file mode 100644
index 0000000000..0b5e45fbfe
--- /dev/null
+++ b/src/internal/cpu/cortexm/acc/periph_thumb.go
@@ -0,0 +1,94 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  ACC  0xE000EF90  -  -  Access Control registers
+// Registers:
+//  0x00  32  ITCMCR  Instruction Tightly-Coupled Memory Control Register
+//  0x04  32  DTCMCR  Data Tightly-Coupled Memory Control Register
+//	0x08  32  AHBPCR  AHBP Control Register
+//  0x0C  32  CACR    L1 Cache Control Register
+//  0x10  32  AHBSCR  AHB Slave Control Register
+//  0x18  32  ABFSR   Auxiliary Bus Fault Status Register
+package acc
+
+const (
+	ITCMEN    ITCMCR = 1 << 0   //+ TCM enable.
+	ITCMRMW   ITCMCR = 1 << 1   //+ Read-Modify-Write (RMW) enable.
+	ITCMRETEN ITCMCR = 1 << 2   //+ Retry phase enable.
+	ITCMSZ    ITCMCR = 0xF << 3 //+ TCM size: 0:0K, 3:4K, 4:8K, ..., 16:16M.
+)
+
+const (
+	ITCMENn    = 0
+	ITCMRMWn   = 1
+	ITCMRETENn = 2
+	ITCMSZn    = 3
+)
+
+const (
+	DTCMEN    DTCMCR = 1 << 0   //+ TCM enable.
+	DTCMRMW   DTCMCR = 1 << 1   //+ Read-Modify-Write (RMW) enable.
+	DTCMRETEN DTCMCR = 1 << 2   //+ Retry phase enable.
+	DTCMSZ    DTCMCR = 0xF << 3 //+ TCM size. 0:0K, 3:4K, 4:8K, ..., 16:16M.
+)
+
+const (
+	DTCMENn    = 0
+	DTCMRMWn   = 1
+	DTCMRETENn = 2
+	DTCMSZn    = 3
+)
+
+const (
+	AHBPEN AHBPCR = 1 << 0   //+ AHBP enable.
+	AHBPSZ AHBPCR = 0x7 << 1 //+ AHBP size. 1:64M, 2:128M, 3:256M, 4:512M.
+)
+
+const (
+	AHBPENn = 0
+	AHBPSZn = 1
+)
+
+const (
+	SIWT    CACR = 1 << 0 //+ Shared cacheable-is-WT for data cache.
+	ECCDIS  CACR = 1 << 1 //+ ECC in the instruction and data cache.
+	FORCEWT CACR = 1 << 2 //+ Force Write-Through in the data cache.
+)
+
+const (
+	SIWTn    = 0
+	ECCDISn  = 1
+	FORCEWTn = 2
+)
+
+const (
+	CTL       AHBSCR = 0x3 << 0   //+ AHBS prioritization control.
+	TPRI      AHBSCR = 0x1FF << 2 //+ Thresh. exec. prio. for traffic demotion.
+	INITCOUNT AHBSCR = 0x1F << 11 //+ Fairness counter initialization value.
+)
+
+const (
+	CTLn       = 0
+	TPRIn      = 2
+	INITCOUNTn = 11
+)
+
+const (
+	ITCM     ABFSR = 1 << 0   //+ Asynchronous fault on ITCM interface
+	DTCM     ABFSR = 1 << 1   //+ Asynchronous fault on DTCM interface.
+	AHBP     ABFSR = 1 << 2   //+ Asynchronous fault on AHBP interface.
+	AXIM     ABFSR = 1 << 3   //+ Asynchronous fault on AXIM interface.
+	EPPB     ABFSR = 1 << 4   //+ Asynchronous fault on EPPB interface.
+	AXIMTYPE ABFSR = 0x3 << 8 //+ The type of fault on the AXIM interface.
+)
+
+const (
+	ITCMn     = 0
+	DTCMn     = 1
+	AHBPn     = 2
+	AXIMn     = 3
+	EPPBn     = 4
+	AXIMTYPEn = 8
+)
diff --git a/src/internal/cpu/cortexm/acc/xperiph_thumb.go b/src/internal/cpu/cortexm/acc/xperiph_thumb.go
new file mode 100644
index 0000000000..2715ef4c05
--- /dev/null
+++ b/src/internal/cpu/cortexm/acc/xperiph_thumb.go
@@ -0,0 +1,208 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package acc
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	ITCMCR RITCMCR
+	DTCMCR RDTCMCR
+	AHBPCR RAHBPCR
+	CACR   RCACR
+	AHBSCR RAHBSCR
+	_      uint32
+	ABFSR  RABFSR
+}
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func ACC() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000EF90))) }
+
+type ITCMCR uint32
+
+type RITCMCR struct{ mmio.U32 }
+
+func (r *RITCMCR) LoadBits(mask ITCMCR) ITCMCR { return ITCMCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RITCMCR) StoreBits(mask, b ITCMCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RITCMCR) SetBits(mask ITCMCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RITCMCR) ClearBits(mask ITCMCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RITCMCR) Load() ITCMCR                { return ITCMCR(r.U32.Load()) }
+func (r *RITCMCR) Store(b ITCMCR)              { r.U32.Store(uint32(b)) }
+
+type RMITCMCR struct{ mmio.UM32 }
+
+func (rm RMITCMCR) Load() ITCMCR   { return ITCMCR(rm.UM32.Load()) }
+func (rm RMITCMCR) Store(b ITCMCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) ITCMEN() RMITCMCR {
+	return RMITCMCR{mmio.UM32{&p.ITCMCR.U32, uint32(ITCMEN)}}
+}
+
+func (p *Periph) ITCMRMW() RMITCMCR {
+	return RMITCMCR{mmio.UM32{&p.ITCMCR.U32, uint32(ITCMRMW)}}
+}
+
+func (p *Periph) ITCMRETEN() RMITCMCR {
+	return RMITCMCR{mmio.UM32{&p.ITCMCR.U32, uint32(ITCMRETEN)}}
+}
+
+func (p *Periph) ITCMSZ() RMITCMCR {
+	return RMITCMCR{mmio.UM32{&p.ITCMCR.U32, uint32(ITCMSZ)}}
+}
+
+type DTCMCR uint32
+
+type RDTCMCR struct{ mmio.U32 }
+
+func (r *RDTCMCR) LoadBits(mask DTCMCR) DTCMCR { return DTCMCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RDTCMCR) StoreBits(mask, b DTCMCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDTCMCR) SetBits(mask DTCMCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RDTCMCR) ClearBits(mask DTCMCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RDTCMCR) Load() DTCMCR                { return DTCMCR(r.U32.Load()) }
+func (r *RDTCMCR) Store(b DTCMCR)              { r.U32.Store(uint32(b)) }
+
+type RMDTCMCR struct{ mmio.UM32 }
+
+func (rm RMDTCMCR) Load() DTCMCR   { return DTCMCR(rm.UM32.Load()) }
+func (rm RMDTCMCR) Store(b DTCMCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) DTCMEN() RMDTCMCR {
+	return RMDTCMCR{mmio.UM32{&p.DTCMCR.U32, uint32(DTCMEN)}}
+}
+
+func (p *Periph) DTCMRMW() RMDTCMCR {
+	return RMDTCMCR{mmio.UM32{&p.DTCMCR.U32, uint32(DTCMRMW)}}
+}
+
+func (p *Periph) DTCMRETEN() RMDTCMCR {
+	return RMDTCMCR{mmio.UM32{&p.DTCMCR.U32, uint32(DTCMRETEN)}}
+}
+
+func (p *Periph) DTCMSZ() RMDTCMCR {
+	return RMDTCMCR{mmio.UM32{&p.DTCMCR.U32, uint32(DTCMSZ)}}
+}
+
+type AHBPCR uint32
+
+type RAHBPCR struct{ mmio.U32 }
+
+func (r *RAHBPCR) LoadBits(mask AHBPCR) AHBPCR { return AHBPCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RAHBPCR) StoreBits(mask, b AHBPCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RAHBPCR) SetBits(mask AHBPCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RAHBPCR) ClearBits(mask AHBPCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RAHBPCR) Load() AHBPCR                { return AHBPCR(r.U32.Load()) }
+func (r *RAHBPCR) Store(b AHBPCR)              { r.U32.Store(uint32(b)) }
+
+type RMAHBPCR struct{ mmio.UM32 }
+
+func (rm RMAHBPCR) Load() AHBPCR   { return AHBPCR(rm.UM32.Load()) }
+func (rm RMAHBPCR) Store(b AHBPCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) AHBPEN() RMAHBPCR {
+	return RMAHBPCR{mmio.UM32{&p.AHBPCR.U32, uint32(AHBPEN)}}
+}
+
+func (p *Periph) AHBPSZ() RMAHBPCR {
+	return RMAHBPCR{mmio.UM32{&p.AHBPCR.U32, uint32(AHBPSZ)}}
+}
+
+type CACR uint32
+
+type RCACR struct{ mmio.U32 }
+
+func (r *RCACR) LoadBits(mask CACR) CACR { return CACR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCACR) StoreBits(mask, b CACR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCACR) SetBits(mask CACR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RCACR) ClearBits(mask CACR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RCACR) Load() CACR              { return CACR(r.U32.Load()) }
+func (r *RCACR) Store(b CACR)            { r.U32.Store(uint32(b)) }
+
+type RMCACR struct{ mmio.UM32 }
+
+func (rm RMCACR) Load() CACR   { return CACR(rm.UM32.Load()) }
+func (rm RMCACR) Store(b CACR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) SIWT() RMCACR {
+	return RMCACR{mmio.UM32{&p.CACR.U32, uint32(SIWT)}}
+}
+
+func (p *Periph) ECCDIS() RMCACR {
+	return RMCACR{mmio.UM32{&p.CACR.U32, uint32(ECCDIS)}}
+}
+
+func (p *Periph) FORCEWT() RMCACR {
+	return RMCACR{mmio.UM32{&p.CACR.U32, uint32(FORCEWT)}}
+}
+
+type AHBSCR uint32
+
+type RAHBSCR struct{ mmio.U32 }
+
+func (r *RAHBSCR) LoadBits(mask AHBSCR) AHBSCR { return AHBSCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RAHBSCR) StoreBits(mask, b AHBSCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RAHBSCR) SetBits(mask AHBSCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RAHBSCR) ClearBits(mask AHBSCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RAHBSCR) Load() AHBSCR                { return AHBSCR(r.U32.Load()) }
+func (r *RAHBSCR) Store(b AHBSCR)              { r.U32.Store(uint32(b)) }
+
+type RMAHBSCR struct{ mmio.UM32 }
+
+func (rm RMAHBSCR) Load() AHBSCR   { return AHBSCR(rm.UM32.Load()) }
+func (rm RMAHBSCR) Store(b AHBSCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) CTL() RMAHBSCR {
+	return RMAHBSCR{mmio.UM32{&p.AHBSCR.U32, uint32(CTL)}}
+}
+
+func (p *Periph) TPRI() RMAHBSCR {
+	return RMAHBSCR{mmio.UM32{&p.AHBSCR.U32, uint32(TPRI)}}
+}
+
+func (p *Periph) INITCOUNT() RMAHBSCR {
+	return RMAHBSCR{mmio.UM32{&p.AHBSCR.U32, uint32(INITCOUNT)}}
+}
+
+type ABFSR uint32
+
+type RABFSR struct{ mmio.U32 }
+
+func (r *RABFSR) LoadBits(mask ABFSR) ABFSR { return ABFSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RABFSR) StoreBits(mask, b ABFSR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RABFSR) SetBits(mask ABFSR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RABFSR) ClearBits(mask ABFSR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RABFSR) Load() ABFSR               { return ABFSR(r.U32.Load()) }
+func (r *RABFSR) Store(b ABFSR)             { r.U32.Store(uint32(b)) }
+
+type RMABFSR struct{ mmio.UM32 }
+
+func (rm RMABFSR) Load() ABFSR   { return ABFSR(rm.UM32.Load()) }
+func (rm RMABFSR) Store(b ABFSR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) ITCM() RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(ITCM)}}
+}
+
+func (p *Periph) DTCM() RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(DTCM)}}
+}
+
+func (p *Periph) AHBP() RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(AHBP)}}
+}
+
+func (p *Periph) AXIM() RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(AXIM)}}
+}
+
+func (p *Periph) EPPB() RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(EPPB)}}
+}
+
+func (p *Periph) AXIMTYPE() RMABFSR {
+	return RMABFSR{mmio.UM32{&p.ABFSR.U32, uint32(AXIMTYPE)}}
+}
diff --git a/src/internal/cpu/cortexm/bitband/bitband_thumb.go b/src/internal/cpu/cortexm/bitband/bitband_thumb.go
new file mode 100644
index 0000000000..f2bfda8dcf
--- /dev/null
+++ b/src/internal/cpu/cortexm/bitband/bitband_thumb.go
@@ -0,0 +1,82 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package bitband
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Bit struct {
+	a *mmio.U32
+}
+
+func (b Bit) Load() int {
+	return int(b.a.Load())
+}
+
+func (b Bit) Store(v int) {
+	b.a.Store(uint32(v))
+}
+
+func (b Bit) Set() {
+	b.Store(1)
+}
+
+func (b Bit) Clear() {
+	b.Store(0)
+}
+
+// 0x20000000 - 0x200FFFFF: SRAM bit-band region.
+// 0x22000000 - 0x23FFFFFF: SRAM bit-band alias.
+//
+// 0x40000000 - 0x400FFFFF: peripheral bit-band region.
+// 0x42000000 - 0x43FFFFFF: peripheral bit-band alias.
+func bitAlias(addr unsafe.Pointer) unsafe.Pointer {
+	a := uintptr(addr)
+	base := a &^ 0xfffff
+	if base != 0x40000000 && base != 0x20000000 {
+		panic("bitband: not in region")
+	}
+	base += 0x2000000
+	offset := a & 0xfffff
+	return unsafe.Pointer(base + offset*32)
+}
+
+type Bits8 struct {
+	a *[8]mmio.U32
+}
+
+func (b Bits8) Bit(n int) Bit {
+	return Bit{&b.a[n]}
+}
+
+func Alias8(r *mmio.U8) Bits8 {
+	return Bits8{(*[8]mmio.U32)(bitAlias(unsafe.Pointer(r)))}
+}
+
+type Bits16 struct {
+	a *[16]mmio.U32
+}
+
+func (b Bits16) Bit(n int) Bit {
+	return Bit{&b.a[n]}
+}
+
+func Alias16(r *mmio.U16) Bits16 {
+	return Bits16{(*[16]mmio.U32)(bitAlias(unsafe.Pointer(r)))}
+}
+
+type Bits32 struct {
+	a *[32]mmio.U32
+}
+
+func (b Bits32) Bit(n int) Bit {
+	return Bit{&b.a[n]}
+}
+
+func Alias32(r *mmio.U32) Bits32 {
+	return Bits32{(*[32]mmio.U32)(bitAlias(unsafe.Pointer(r)))}
+}
diff --git a/src/internal/cpu/cortexm/bitband/doc.go b/src/internal/cpu/cortexm/bitband/doc.go
new file mode 100644
index 0000000000..5363525408
--- /dev/null
+++ b/src/internal/cpu/cortexm/bitband/doc.go
@@ -0,0 +1,7 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package bitband allows to use bit-banding feature of some Cortex-M
+// microcontrollers.
+package bitband
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/cmt/doc.go b/src/internal/cpu/cortexm/cmt/doc.go
new file mode 100644
index 0000000000..2539221cbd
--- /dev/null
+++ b/src/internal/cpu/cortexm/cmt/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package cmt provides access to the Cache maintenance registers.
+package cmt
diff --git a/src/internal/cpu/cortexm/cmt/periph_thumb.go b/src/internal/cpu/cortexm/cmt/periph_thumb.go
new file mode 100644
index 0000000000..33e7077927
--- /dev/null
+++ b/src/internal/cpu/cortexm/cmt/periph_thumb.go
@@ -0,0 +1,17 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  CMT  0xE000EF50  -  -  Cache maintenance registers
+// Registers:
+//  0x00 32  ICIALLU  Instr. cache invalidate all to the Point of Unification
+//  0x08 32  ICIMVAU  Instr. cache invalidate by address to the PoU
+//  0x0C 32  DCIMVAC  Data cache invalidate by address to the Point of Coherency
+//  0x10 32  DCISW    Data cache invalidate by set/way
+//  0x14 32  DCCMVAU  Data cache clean by address to the PoU
+//  0x18 32  DCCMVAC  Data cache clean by address to the PoC
+//  0x1c 32  DCCSW    Data cache clean by set/way
+//  0x20 32  DCCIMVAC Data cache clean and invalidate by address to the PoC
+//  0x24 32  DCCISW   Data cache clean and invalidate by set/way
+package cmt
diff --git a/src/internal/cpu/cortexm/cmt/xperiph_thumb.go b/src/internal/cpu/cortexm/cmt/xperiph_thumb.go
new file mode 100644
index 0000000000..76c67162c9
--- /dev/null
+++ b/src/internal/cpu/cortexm/cmt/xperiph_thumb.go
@@ -0,0 +1,171 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package cmt
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	ICIALLU  RICIALLU
+	_        uint32
+	ICIMVAU  RICIMVAU
+	DCIMVAC  RDCIMVAC
+	DCISW    RDCISW
+	DCCMVAU  RDCCMVAU
+	DCCMVAC  RDCCMVAC
+	DCCSW    RDCCSW
+	DCCIMVAC RDCCIMVAC
+	DCCISW   RDCCISW
+}
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func CMT() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000EF50))) }
+
+type ICIALLU uint32
+
+type RICIALLU struct{ mmio.U32 }
+
+func (r *RICIALLU) LoadBits(mask ICIALLU) ICIALLU { return ICIALLU(r.U32.LoadBits(uint32(mask))) }
+func (r *RICIALLU) StoreBits(mask, b ICIALLU)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICIALLU) SetBits(mask ICIALLU)          { r.U32.SetBits(uint32(mask)) }
+func (r *RICIALLU) ClearBits(mask ICIALLU)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RICIALLU) Load() ICIALLU                 { return ICIALLU(r.U32.Load()) }
+func (r *RICIALLU) Store(b ICIALLU)               { r.U32.Store(uint32(b)) }
+
+type RMICIALLU struct{ mmio.UM32 }
+
+func (rm RMICIALLU) Load() ICIALLU   { return ICIALLU(rm.UM32.Load()) }
+func (rm RMICIALLU) Store(b ICIALLU) { rm.UM32.Store(uint32(b)) }
+
+type ICIMVAU uint32
+
+type RICIMVAU struct{ mmio.U32 }
+
+func (r *RICIMVAU) LoadBits(mask ICIMVAU) ICIMVAU { return ICIMVAU(r.U32.LoadBits(uint32(mask))) }
+func (r *RICIMVAU) StoreBits(mask, b ICIMVAU)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICIMVAU) SetBits(mask ICIMVAU)          { r.U32.SetBits(uint32(mask)) }
+func (r *RICIMVAU) ClearBits(mask ICIMVAU)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RICIMVAU) Load() ICIMVAU                 { return ICIMVAU(r.U32.Load()) }
+func (r *RICIMVAU) Store(b ICIMVAU)               { r.U32.Store(uint32(b)) }
+
+type RMICIMVAU struct{ mmio.UM32 }
+
+func (rm RMICIMVAU) Load() ICIMVAU   { return ICIMVAU(rm.UM32.Load()) }
+func (rm RMICIMVAU) Store(b ICIMVAU) { rm.UM32.Store(uint32(b)) }
+
+type DCIMVAC uint32
+
+type RDCIMVAC struct{ mmio.U32 }
+
+func (r *RDCIMVAC) LoadBits(mask DCIMVAC) DCIMVAC { return DCIMVAC(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCIMVAC) StoreBits(mask, b DCIMVAC)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCIMVAC) SetBits(mask DCIMVAC)          { r.U32.SetBits(uint32(mask)) }
+func (r *RDCIMVAC) ClearBits(mask DCIMVAC)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCIMVAC) Load() DCIMVAC                 { return DCIMVAC(r.U32.Load()) }
+func (r *RDCIMVAC) Store(b DCIMVAC)               { r.U32.Store(uint32(b)) }
+
+type RMDCIMVAC struct{ mmio.UM32 }
+
+func (rm RMDCIMVAC) Load() DCIMVAC   { return DCIMVAC(rm.UM32.Load()) }
+func (rm RMDCIMVAC) Store(b DCIMVAC) { rm.UM32.Store(uint32(b)) }
+
+type DCISW uint32
+
+type RDCISW struct{ mmio.U32 }
+
+func (r *RDCISW) LoadBits(mask DCISW) DCISW { return DCISW(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCISW) StoreBits(mask, b DCISW)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCISW) SetBits(mask DCISW)        { r.U32.SetBits(uint32(mask)) }
+func (r *RDCISW) ClearBits(mask DCISW)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCISW) Load() DCISW               { return DCISW(r.U32.Load()) }
+func (r *RDCISW) Store(b DCISW)             { r.U32.Store(uint32(b)) }
+
+type RMDCISW struct{ mmio.UM32 }
+
+func (rm RMDCISW) Load() DCISW   { return DCISW(rm.UM32.Load()) }
+func (rm RMDCISW) Store(b DCISW) { rm.UM32.Store(uint32(b)) }
+
+type DCCMVAU uint32
+
+type RDCCMVAU struct{ mmio.U32 }
+
+func (r *RDCCMVAU) LoadBits(mask DCCMVAU) DCCMVAU { return DCCMVAU(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCMVAU) StoreBits(mask, b DCCMVAU)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCMVAU) SetBits(mask DCCMVAU)          { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCMVAU) ClearBits(mask DCCMVAU)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCMVAU) Load() DCCMVAU                 { return DCCMVAU(r.U32.Load()) }
+func (r *RDCCMVAU) Store(b DCCMVAU)               { r.U32.Store(uint32(b)) }
+
+type RMDCCMVAU struct{ mmio.UM32 }
+
+func (rm RMDCCMVAU) Load() DCCMVAU   { return DCCMVAU(rm.UM32.Load()) }
+func (rm RMDCCMVAU) Store(b DCCMVAU) { rm.UM32.Store(uint32(b)) }
+
+type DCCMVAC uint32
+
+type RDCCMVAC struct{ mmio.U32 }
+
+func (r *RDCCMVAC) LoadBits(mask DCCMVAC) DCCMVAC { return DCCMVAC(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCMVAC) StoreBits(mask, b DCCMVAC)     { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCMVAC) SetBits(mask DCCMVAC)          { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCMVAC) ClearBits(mask DCCMVAC)        { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCMVAC) Load() DCCMVAC                 { return DCCMVAC(r.U32.Load()) }
+func (r *RDCCMVAC) Store(b DCCMVAC)               { r.U32.Store(uint32(b)) }
+
+type RMDCCMVAC struct{ mmio.UM32 }
+
+func (rm RMDCCMVAC) Load() DCCMVAC   { return DCCMVAC(rm.UM32.Load()) }
+func (rm RMDCCMVAC) Store(b DCCMVAC) { rm.UM32.Store(uint32(b)) }
+
+type DCCSW uint32
+
+type RDCCSW struct{ mmio.U32 }
+
+func (r *RDCCSW) LoadBits(mask DCCSW) DCCSW { return DCCSW(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCSW) StoreBits(mask, b DCCSW)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCSW) SetBits(mask DCCSW)        { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCSW) ClearBits(mask DCCSW)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCSW) Load() DCCSW               { return DCCSW(r.U32.Load()) }
+func (r *RDCCSW) Store(b DCCSW)             { r.U32.Store(uint32(b)) }
+
+type RMDCCSW struct{ mmio.UM32 }
+
+func (rm RMDCCSW) Load() DCCSW   { return DCCSW(rm.UM32.Load()) }
+func (rm RMDCCSW) Store(b DCCSW) { rm.UM32.Store(uint32(b)) }
+
+type DCCIMVAC uint32
+
+type RDCCIMVAC struct{ mmio.U32 }
+
+func (r *RDCCIMVAC) LoadBits(mask DCCIMVAC) DCCIMVAC { return DCCIMVAC(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCIMVAC) StoreBits(mask, b DCCIMVAC)      { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCIMVAC) SetBits(mask DCCIMVAC)           { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCIMVAC) ClearBits(mask DCCIMVAC)         { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCIMVAC) Load() DCCIMVAC                  { return DCCIMVAC(r.U32.Load()) }
+func (r *RDCCIMVAC) Store(b DCCIMVAC)                { r.U32.Store(uint32(b)) }
+
+type RMDCCIMVAC struct{ mmio.UM32 }
+
+func (rm RMDCCIMVAC) Load() DCCIMVAC   { return DCCIMVAC(rm.UM32.Load()) }
+func (rm RMDCCIMVAC) Store(b DCCIMVAC) { rm.UM32.Store(uint32(b)) }
+
+type DCCISW uint32
+
+type RDCCISW struct{ mmio.U32 }
+
+func (r *RDCCISW) LoadBits(mask DCCISW) DCCISW { return DCCISW(r.U32.LoadBits(uint32(mask))) }
+func (r *RDCCISW) StoreBits(mask, b DCCISW)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RDCCISW) SetBits(mask DCCISW)         { r.U32.SetBits(uint32(mask)) }
+func (r *RDCCISW) ClearBits(mask DCCISW)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RDCCISW) Load() DCCISW                { return DCCISW(r.U32.Load()) }
+func (r *RDCCISW) Store(b DCCISW)              { r.U32.Store(uint32(b)) }
+
+type RMDCCISW struct{ mmio.UM32 }
+
+func (rm RMDCCISW) Load() DCCISW   { return DCCISW(rm.UM32.Load()) }
+func (rm RMDCCISW) Store(b DCCISW) { rm.UM32.Store(uint32(b)) }
diff --git a/src/internal/cpu/cortexm/debug/itm/doc.go b/src/internal/cpu/cortexm/debug/itm/doc.go
new file mode 100644
index 0000000000..b9833cab7a
--- /dev/null
+++ b/src/internal/cpu/cortexm/debug/itm/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package itm gives access to the Instrumentation Trace Macrocell registers.
+package itm
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/debug/itm/periph_thumb.go b/src/internal/cpu/cortexm/debug/itm/periph_thumb.go
new file mode 100644
index 0000000000..df77223e51
--- /dev/null
+++ b/src/internal/cpu/cortexm/debug/itm/periph_thumb.go
@@ -0,0 +1,31 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Peripheral: ITM_Periph
+// Instances:
+//  ITM  0xE0000000  -  -  Instrumentation Trace Macrocell registers
+// Registers:
+//  0x000  32  STIM[256]  Stimulus Port Registers
+//  0xE00  32  TER[8]     Trace Enable Register
+//  0xE40  32  TPR        Trace Privilege Register
+//  0xE80  32  TCR        Trace Control Register
+//  0xFD0  32  PID[8]     Peripheral Identification registers
+//  0xFF0  32  CID[4]     Component Identification registers
+package itm
+
+const (
+	ITMENA     TCR = 0x01 << 0  //+ enable ITM
+	TSENA      TCR = 0x01 << 1  //+ enable local timestamp generation
+	SYNCENA    TCR = 0x01 << 2  //+ enables packet transmission for a sync. TPIU
+	TXENA      TCR = 0x01 << 3  //+ enable forwarding packets from DWT to ITM
+	SWOENA     TCR = 0x01 << 4  //+ enable async. clocking of the timestamp cntr
+	TSPrescale TCR = 0x03 << 8  //+ local timestamp prescaler
+	TSP0       TCR = 0x00 << 8  //  no prescaling
+	TSP4       TCR = 0x01 << 8  //  divide by 4
+	TSP16      TCR = 0x02 << 8  //  divide by 16
+	TSP64      TCR = 0x03 << 8  //  divide by 64
+	GTSFREQ    TCR = 0x03 << 10 //+ global timestamp frequency
+	TraceBusID TCR = 0x7F << 16 //+ identifier for multi-source trace stream
+	BUSY       TCR = 0x01 << 23 //+ set if ITM is currently processing events
+)
diff --git a/src/internal/cpu/cortexm/debug/itm/xperiph_thumb.go b/src/internal/cpu/cortexm/debug/itm/xperiph_thumb.go
new file mode 100644
index 0000000000..4e21c386b5
--- /dev/null
+++ b/src/internal/cpu/cortexm/debug/itm/xperiph_thumb.go
@@ -0,0 +1,159 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package itm
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type ITM_Periph struct {
+	STIM [256]RSTIM
+	_    [640]uint32
+	TER  [8]RTER
+	_    [8]uint32
+	TPR  RTPR
+	_    [15]uint32
+	TCR  RTCR
+	_    [83]uint32
+	PID  [8]RPID
+	CID  [4]RCID
+}
+
+func (p *ITM_Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func ITM() *ITM_Periph { return (*ITM_Periph)(unsafe.Pointer(uintptr(0xE0000000))) }
+
+type STIM uint32
+
+type RSTIM struct{ mmio.U32 }
+
+func (r *RSTIM) LoadBits(mask STIM) STIM { return STIM(r.U32.LoadBits(uint32(mask))) }
+func (r *RSTIM) StoreBits(mask, b STIM)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSTIM) SetBits(mask STIM)       { r.U32.SetBits(uint32(mask)) }
+func (r *RSTIM) ClearBits(mask STIM)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RSTIM) Load() STIM              { return STIM(r.U32.Load()) }
+func (r *RSTIM) Store(b STIM)            { r.U32.Store(uint32(b)) }
+
+type RMSTIM struct{ mmio.UM32 }
+
+func (rm RMSTIM) Load() STIM   { return STIM(rm.UM32.Load()) }
+func (rm RMSTIM) Store(b STIM) { rm.UM32.Store(uint32(b)) }
+
+type TER uint32
+
+type RTER struct{ mmio.U32 }
+
+func (r *RTER) LoadBits(mask TER) TER { return TER(r.U32.LoadBits(uint32(mask))) }
+func (r *RTER) StoreBits(mask, b TER) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RTER) SetBits(mask TER)      { r.U32.SetBits(uint32(mask)) }
+func (r *RTER) ClearBits(mask TER)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RTER) Load() TER             { return TER(r.U32.Load()) }
+func (r *RTER) Store(b TER)           { r.U32.Store(uint32(b)) }
+
+type RMTER struct{ mmio.UM32 }
+
+func (rm RMTER) Load() TER   { return TER(rm.UM32.Load()) }
+func (rm RMTER) Store(b TER) { rm.UM32.Store(uint32(b)) }
+
+type TPR uint32
+
+type RTPR struct{ mmio.U32 }
+
+func (r *RTPR) LoadBits(mask TPR) TPR { return TPR(r.U32.LoadBits(uint32(mask))) }
+func (r *RTPR) StoreBits(mask, b TPR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RTPR) SetBits(mask TPR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RTPR) ClearBits(mask TPR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RTPR) Load() TPR             { return TPR(r.U32.Load()) }
+func (r *RTPR) Store(b TPR)           { r.U32.Store(uint32(b)) }
+
+type RMTPR struct{ mmio.UM32 }
+
+func (rm RMTPR) Load() TPR   { return TPR(rm.UM32.Load()) }
+func (rm RMTPR) Store(b TPR) { rm.UM32.Store(uint32(b)) }
+
+type TCR uint32
+
+type RTCR struct{ mmio.U32 }
+
+func (r *RTCR) LoadBits(mask TCR) TCR { return TCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RTCR) StoreBits(mask, b TCR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RTCR) SetBits(mask TCR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RTCR) ClearBits(mask TCR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RTCR) Load() TCR             { return TCR(r.U32.Load()) }
+func (r *RTCR) Store(b TCR)           { r.U32.Store(uint32(b)) }
+
+type RMTCR struct{ mmio.UM32 }
+
+func (rm RMTCR) Load() TCR   { return TCR(rm.UM32.Load()) }
+func (rm RMTCR) Store(b TCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *ITM_Periph) ITMENA() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(ITMENA)}}
+}
+
+func (p *ITM_Periph) TSENA() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(TSENA)}}
+}
+
+func (p *ITM_Periph) SYNCENA() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(SYNCENA)}}
+}
+
+func (p *ITM_Periph) TXENA() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(TXENA)}}
+}
+
+func (p *ITM_Periph) SWOENA() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(SWOENA)}}
+}
+
+func (p *ITM_Periph) TSPrescale() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(TSPrescale)}}
+}
+
+func (p *ITM_Periph) GTSFREQ() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(GTSFREQ)}}
+}
+
+func (p *ITM_Periph) TraceBusID() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(TraceBusID)}}
+}
+
+func (p *ITM_Periph) BUSY() RMTCR {
+	return RMTCR{mmio.UM32{&p.TCR.U32, uint32(BUSY)}}
+}
+
+type PID uint32
+
+type RPID struct{ mmio.U32 }
+
+func (r *RPID) LoadBits(mask PID) PID { return PID(r.U32.LoadBits(uint32(mask))) }
+func (r *RPID) StoreBits(mask, b PID) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RPID) SetBits(mask PID)      { r.U32.SetBits(uint32(mask)) }
+func (r *RPID) ClearBits(mask PID)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RPID) Load() PID             { return PID(r.U32.Load()) }
+func (r *RPID) Store(b PID)           { r.U32.Store(uint32(b)) }
+
+type RMPID struct{ mmio.UM32 }
+
+func (rm RMPID) Load() PID   { return PID(rm.UM32.Load()) }
+func (rm RMPID) Store(b PID) { rm.UM32.Store(uint32(b)) }
+
+type CID uint32
+
+type RCID struct{ mmio.U32 }
+
+func (r *RCID) LoadBits(mask CID) CID { return CID(r.U32.LoadBits(uint32(mask))) }
+func (r *RCID) StoreBits(mask, b CID) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCID) SetBits(mask CID)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCID) ClearBits(mask CID)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCID) Load() CID             { return CID(r.U32.Load()) }
+func (r *RCID) Store(b CID)           { r.U32.Store(uint32(b)) }
+
+type RMCID struct{ mmio.UM32 }
+
+func (rm RMCID) Load() CID   { return CID(rm.UM32.Load()) }
+func (rm RMCID) Store(b CID) { rm.UM32.Store(uint32(b)) }
diff --git a/src/internal/cpu/cortexm/doc.go b/src/internal/cpu/cortexm/doc.go
new file mode 100644
index 0000000000..a8525c9f3b
--- /dev/null
+++ b/src/internal/cpu/cortexm/doc.go
@@ -0,0 +1,7 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package cortexm defines types and constants specific to the ARMv7-M and
+// ARMv8-M ISA.
+package cortexm
diff --git a/src/internal/cpu/cortexm/exce_thumb.go b/src/internal/cpu/cortexm/exce_thumb.go
new file mode 100644
index 0000000000..9e0f637dd2
--- /dev/null
+++ b/src/internal/cpu/cortexm/exce_thumb.go
@@ -0,0 +1,37 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package cortexm
+
+// Cortex-M exception numbers.
+const (
+	Reset       = 1  // IRQ number:
+	NMI         = 2  // -14
+	HardFault   = 3  // -13
+	MemManage   = 4  // -12
+	BusFault    = 5  // -11
+	UsageFault  = 6  // -10
+	SecureFault = 7  //  -9
+	_           = 8  //  -8
+	_           = 9  //  -7
+	_           = 10 //  -6
+	SVCall      = 11 //  -5
+	DebugMon    = 12 //  -4
+	_           = 13 //  -3
+	PendSV      = 14 //  -2
+	SysTick     = 15 //  -1
+)
+
+// Exception number for the first external interrupt.
+const IRQ0 = 16
+
+// EXC_RETURN fields
+const (
+	ExcReturnBase    = 0xFFFFFFE0 // EXC_RETURN base value
+	ExcReturnMode    = 0xF        // Selects bits responsible for return mode:
+	ExcReturnHandler = 0x01       // - return to handler mode, use MSP
+	ExcReturnMSP     = 0x09       // - return to thread mode, use MSP
+	ExcReturnPSP     = 0x0D       // - return to thread mode, use PSP
+	ExcReturnNoFPU   = 0x10       // Basic frame on the stack (no FPU state)
+)
diff --git a/src/internal/cpu/cortexm/fpu/doc.go b/src/internal/cpu/cortexm/fpu/doc.go
new file mode 100644
index 0000000000..7d2c6433c5
--- /dev/null
+++ b/src/internal/cpu/cortexm/fpu/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package fpu gives access to the Floating Point Unit registers.
+package fpu
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/fpu/periph_thumb.go b/src/internal/cpu/cortexm/fpu/periph_thumb.go
new file mode 100644
index 0000000000..612d8394d8
--- /dev/null
+++ b/src/internal/cpu/cortexm/fpu/periph_thumb.go
@@ -0,0 +1,49 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  FPU  0xe000ED88  -  -  Floating Point Unit registers
+// Registers:
+//  0x000  32  CPACR   Coprocessor Access Control Register
+//  0x1AC  32  FPCCR   Floating-point Context Control Register
+//  0x1B0  32  FPCAR   Floating-point Context Address Register
+//  0x1B4  32  FPDSCR  Floating-point Default Status Control Register
+package fpu
+
+const (
+	CP10 CPACR = 3 << 20 //+ Access privileges for coprocessor 10.
+	CP11 CPACR = 3 << 22 //+ Access privileges for coprocessor 11.
+
+	CPACDENY CPACR = 0
+	CPACPRIV CPACR = 1
+	CPACFULL CPACR = 3
+)
+
+const (
+	CP10n = 20
+	CP11n = 22
+)
+
+const (
+	LSPACT FPCCR = 1 << 0  //+
+	USER   FPCCR = 1 << 1  //+
+	THREAD FPCCR = 1 << 3  //+
+	HFRDY  FPCCR = 1 << 4  //+
+	MMRDY  FPCCR = 1 << 5  //+
+	BFRDY  FPCCR = 1 << 6  //+
+	MONRDY FPCCR = 1 << 8  //+
+	LSPEN  FPCCR = 1 << 30 //+
+	ASPEN  FPCCR = 1 << 31 //+
+)
+
+const (
+	ADDRESS FPCAR = 0x3fffffff << 2 //+
+)
+
+const (
+	RMode FPDSCR = 3 << 22 //+
+	FZ    FPDSCR = 1 << 24 //+
+	DN    FPDSCR = 1 << 25 //+
+	AHP   FPDSCR = 1 << 26 //+
+)
diff --git a/src/internal/cpu/cortexm/fpu/xperiph_thumb.go b/src/internal/cpu/cortexm/fpu/xperiph_thumb.go
new file mode 100644
index 0000000000..5bb8d1c174
--- /dev/null
+++ b/src/internal/cpu/cortexm/fpu/xperiph_thumb.go
@@ -0,0 +1,150 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package fpu
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	CPACR  RCPACR
+	_      [106]uint32
+	FPCCR  RFPCCR
+	FPCAR  RFPCAR
+	FPDSCR RFPDSCR
+}
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func FPU() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xe000ED88))) }
+
+type CPACR uint32
+
+type RCPACR struct{ mmio.U32 }
+
+func (r *RCPACR) LoadBits(mask CPACR) CPACR { return CPACR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCPACR) StoreBits(mask, b CPACR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCPACR) SetBits(mask CPACR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RCPACR) ClearBits(mask CPACR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RCPACR) Load() CPACR               { return CPACR(r.U32.Load()) }
+func (r *RCPACR) Store(b CPACR)             { r.U32.Store(uint32(b)) }
+
+type RMCPACR struct{ mmio.UM32 }
+
+func (rm RMCPACR) Load() CPACR   { return CPACR(rm.UM32.Load()) }
+func (rm RMCPACR) Store(b CPACR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) CP10() RMCPACR {
+	return RMCPACR{mmio.UM32{&p.CPACR.U32, uint32(CP10)}}
+}
+
+func (p *Periph) CP11() RMCPACR {
+	return RMCPACR{mmio.UM32{&p.CPACR.U32, uint32(CP11)}}
+}
+
+type FPCCR uint32
+
+type RFPCCR struct{ mmio.U32 }
+
+func (r *RFPCCR) LoadBits(mask FPCCR) FPCCR { return FPCCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RFPCCR) StoreBits(mask, b FPCCR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RFPCCR) SetBits(mask FPCCR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RFPCCR) ClearBits(mask FPCCR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RFPCCR) Load() FPCCR               { return FPCCR(r.U32.Load()) }
+func (r *RFPCCR) Store(b FPCCR)             { r.U32.Store(uint32(b)) }
+
+type RMFPCCR struct{ mmio.UM32 }
+
+func (rm RMFPCCR) Load() FPCCR   { return FPCCR(rm.UM32.Load()) }
+func (rm RMFPCCR) Store(b FPCCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) LSPACT() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(LSPACT)}}
+}
+
+func (p *Periph) USER() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(USER)}}
+}
+
+func (p *Periph) THREAD() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(THREAD)}}
+}
+
+func (p *Periph) HFRDY() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(HFRDY)}}
+}
+
+func (p *Periph) MMRDY() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(MMRDY)}}
+}
+
+func (p *Periph) BFRDY() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(BFRDY)}}
+}
+
+func (p *Periph) MONRDY() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(MONRDY)}}
+}
+
+func (p *Periph) LSPEN() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(LSPEN)}}
+}
+
+func (p *Periph) ASPEN() RMFPCCR {
+	return RMFPCCR{mmio.UM32{&p.FPCCR.U32, uint32(ASPEN)}}
+}
+
+type FPCAR uint32
+
+type RFPCAR struct{ mmio.U32 }
+
+func (r *RFPCAR) LoadBits(mask FPCAR) FPCAR { return FPCAR(r.U32.LoadBits(uint32(mask))) }
+func (r *RFPCAR) StoreBits(mask, b FPCAR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RFPCAR) SetBits(mask FPCAR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RFPCAR) ClearBits(mask FPCAR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RFPCAR) Load() FPCAR               { return FPCAR(r.U32.Load()) }
+func (r *RFPCAR) Store(b FPCAR)             { r.U32.Store(uint32(b)) }
+
+type RMFPCAR struct{ mmio.UM32 }
+
+func (rm RMFPCAR) Load() FPCAR   { return FPCAR(rm.UM32.Load()) }
+func (rm RMFPCAR) Store(b FPCAR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) ADDRESS() RMFPCAR {
+	return RMFPCAR{mmio.UM32{&p.FPCAR.U32, uint32(ADDRESS)}}
+}
+
+type FPDSCR uint32
+
+type RFPDSCR struct{ mmio.U32 }
+
+func (r *RFPDSCR) LoadBits(mask FPDSCR) FPDSCR { return FPDSCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RFPDSCR) StoreBits(mask, b FPDSCR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RFPDSCR) SetBits(mask FPDSCR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RFPDSCR) ClearBits(mask FPDSCR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RFPDSCR) Load() FPDSCR                { return FPDSCR(r.U32.Load()) }
+func (r *RFPDSCR) Store(b FPDSCR)              { r.U32.Store(uint32(b)) }
+
+type RMFPDSCR struct{ mmio.UM32 }
+
+func (rm RMFPDSCR) Load() FPDSCR   { return FPDSCR(rm.UM32.Load()) }
+func (rm RMFPDSCR) Store(b FPDSCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) RMode() RMFPDSCR {
+	return RMFPDSCR{mmio.UM32{&p.FPDSCR.U32, uint32(RMode)}}
+}
+
+func (p *Periph) FZ() RMFPDSCR {
+	return RMFPDSCR{mmio.UM32{&p.FPDSCR.U32, uint32(FZ)}}
+}
+
+func (p *Periph) DN() RMFPDSCR {
+	return RMFPDSCR{mmio.UM32{&p.FPDSCR.U32, uint32(DN)}}
+}
+
+func (p *Periph) AHP() RMFPDSCR {
+	return RMFPDSCR{mmio.UM32{&p.FPDSCR.U32, uint32(AHP)}}
+}
diff --git a/src/internal/cpu/cortexm/mpu/doc.go b/src/internal/cpu/cortexm/mpu/doc.go
new file mode 100644
index 0000000000..ee9e171bef
--- /dev/null
+++ b/src/internal/cpu/cortexm/mpu/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package mpu provides interface to configure Cortex-M Memory Protection Unit
+package mpu
diff --git a/src/internal/cpu/cortexm/mpu/mpu_thumb.go b/src/internal/cpu/cortexm/mpu/mpu_thumb.go
new file mode 100644
index 0000000000..4bbb3e73df
--- /dev/null
+++ b/src/internal/cpu/cortexm/mpu/mpu_thumb.go
@@ -0,0 +1,119 @@
+package mpu
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type regs struct {
+	typ  mmio.U32
+	ctrl mmio.U32
+	rnr  mmio.U32
+	rbar mmio.U32
+	rasr mmio.U32
+}
+
+func p() *regs { return (*regs)(unsafe.Pointer(uintptr(0xE000ED90))) }
+
+// Type returns information about MPU unit:
+// i - number of supported instruction regions,
+// d - number of supported data regions.
+// s - true if separate instruction and data regions are supported.
+func Type() (i, d int, s bool) {
+	typ := p().typ.Load()
+	i = int(typ>>16) & 0xf
+	d = int(typ>>8) & 0xf
+	s = (typ&1 != 0)
+	return
+}
+
+type Flags uint32
+
+const (
+	// If ENABLE is set MPU is enabled.
+	ENABLE Flags = 1 << 0
+	// If HFNMIENA is not set the MPU will be disabled during HardFault, NMI
+	// and FAULTMASK handlers.
+	HFNMIENA Flags = 1 << 1
+	// If PRIVDEF is set the default memory map is used as background region for
+	// privileged software access.
+	PRIVDEFENA Flags = 1 << 2
+)
+
+// Set sets the flags specified by fl.
+func Set(fl Flags) { p().ctrl.SetBits(uint32(fl)) }
+
+// Clear clears the flags specified by fl.
+func Clear(fl Flags) { p().ctrl.ClearBits(uint32(fl)) }
+
+// State returns the current state.
+func State() Flags { return Flags(p().ctrl.Load()) }
+
+// Select selects the region number n.
+func Select(n int) { p().rnr.Store(uint32(n)) }
+
+type Attr uint32
+
+const (
+	ENA Attr = 1 << 0 // Enables region
+
+	B Attr = 1 << 16 // Bufferable
+	C Attr = 1 << 17 // Cacheable
+	S Attr = 1 << 18 // Shareable
+
+	// Access permissons.
+	Amask Attr = 7 << 24 // Use to extract access permission bits
+	A____ Attr = 0 << 24 // No access
+	Ar___ Attr = 5 << 24 // Priv-RO
+	Arw__ Attr = 1 << 24 // Priv-RW
+	Ar_r_ Attr = 6 << 24 // Priv-RO, Unpriv-RO
+	Arwr_ Attr = 2 << 24 // Priv-RW, Unpriv-RO
+	Arwrw Attr = 3 << 24 // Priv-RW, Unpriv-RW
+
+	XN Attr = 1 << 28 // Instruction access disable
+)
+
+func SIZE(exp int) Attr {
+	return Attr(exp-1) & 0x1f << 1
+}
+
+func (a Attr) SIZE() (exp int) {
+	return int(a>>1)&0x1f + 1
+}
+
+func SRD(srd uint8) Attr {
+	return Attr(srd) << 8
+}
+
+func (a Attr) SRD() uint8 {
+	return uint8(a >> 8)
+}
+
+func TEX(tex int) Attr {
+	return Attr(tex&7) << 19
+}
+
+func (a Attr) TEX() int {
+	return int(a>>19) & 7
+}
+
+const VALID = 1 << 4
+
+func SetRegion(base uintptr, attr Attr) {
+	p().rbar.Store(uint32(base))
+	p().rasr.Store(uint32(attr))
+}
+
+func Region() (base uintptr, attr Attr) {
+	return uintptr(p().rbar.Load()), Attr(p().rasr.Load())
+}
+
+/*
+TODO: Implement SetRegions using STM instrucion.
+type BaseAttr struct {
+	RBAR uintptr
+	RASR Attr
+}
+
+func SetRegions(bas [4]BaseAttr)
+*/
diff --git a/src/internal/cpu/cortexm/nvic/doc.go b/src/internal/cpu/cortexm/nvic/doc.go
new file mode 100644
index 0000000000..a5118b7bf2
--- /dev/null
+++ b/src/internal/cpu/cortexm/nvic/doc.go
@@ -0,0 +1,31 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package nvic gives access to the registers of the Nested Vectored Interrupt
+// Controller
+//
+// This package can not manage system exceptions. Use scb package instead.
+//
+// NVIC combines level and pulse sensing of interrupt signals. It is important
+// to understand this behavior to avoid subtle bugs in device drivers code.
+//
+// An interrupt can be in four states:
+//	1. Inactive: the interrupt is not active and not pending.
+//	2. Pending: the interrupt is waiting to be serviced by the processor.
+//	3. Active: the interrupt is being serviced by the processor.
+//	4. Active and pending: the interrupt is being serviced by the processor
+//	   and there is a pending interrupt from the same source.
+//
+// In simple terms, it can be assumed that level sensing is used in inactive
+// state, pulse sensing is used in active state. This behavior has two
+// important consequences:
+//
+// If the interrupt signal remains active after return from the interrupt
+// handler, the interrupt state becomes pending and the handler will be
+// executed again.
+//
+// If the interrupt signal was deasserted and next asserted again in active
+// state, the interrupt state changes to active and pending, the handler will
+// be executed again after return.
+package nvic
diff --git a/src/internal/cpu/cortexm/nvic/periph_thumb.go b/src/internal/cpu/cortexm/nvic/periph_thumb.go
new file mode 100644
index 0000000000..b802c68b35
--- /dev/null
+++ b/src/internal/cpu/cortexm/nvic/periph_thumb.go
@@ -0,0 +1,21 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// BUG: Cortex-M0 supports only 32-bit access on the Private Peripheral Bus but
+// the IPRs are defined as 8-bit registers. We prefer 8-bit access because it
+// allows changing the priority of different interrupts concurrently. If we
+// ever will support ARMv6-M all code that uses IPRs need to be fixed.
+
+// Instances:
+//  NVIC  0xE000E100  -  -  Nested Vectored Interrupt Controller
+// Registers:
+//  0x000  32  ISER[16]  Interrupt Set-Enable Registers
+//  0x080  32  ICER[16]  Interrupt Clear-Enable Registers
+//  0x100  32  ISPR[16]  Interrupt Set-Pending Registers
+//  0x180  32  ICPR[16]  Interrupt Clear-Pending Registers
+//  0x200  32  IABR[16]  Interrupt Active Bit Registers
+//  0x280  32  ITNS[16]  Interrupt Target Non-secure Registers
+//  0x300   8  IPR[496]  Interrupt Priority Registers
+//  0xE00  32  STIR      Software Trigger Interrupt Register
+package nvic
diff --git a/src/internal/cpu/cortexm/nvic/xperiph_thumb.go b/src/internal/cpu/cortexm/nvic/xperiph_thumb.go
new file mode 100644
index 0000000000..e83a78c72f
--- /dev/null
+++ b/src/internal/cpu/cortexm/nvic/xperiph_thumb.go
@@ -0,0 +1,160 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package nvic
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	ISER [16]RISER
+	_    [16]uint32
+	ICER [16]RICER
+	_    [16]uint32
+	ISPR [16]RISPR
+	_    [16]uint32
+	ICPR [16]RICPR
+	_    [16]uint32
+	IABR [16]RIABR
+	_    [16]uint32
+	ITNS [16]RITNS
+	_    [16]uint32
+	IPR  [496]RIPR
+	_    [580]uint32
+	STIR RSTIR
+}
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func NVIC() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000E100))) }
+
+type ISER uint32
+
+type RISER struct{ mmio.U32 }
+
+func (r *RISER) LoadBits(mask ISER) ISER { return ISER(r.U32.LoadBits(uint32(mask))) }
+func (r *RISER) StoreBits(mask, b ISER)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RISER) SetBits(mask ISER)       { r.U32.SetBits(uint32(mask)) }
+func (r *RISER) ClearBits(mask ISER)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RISER) Load() ISER              { return ISER(r.U32.Load()) }
+func (r *RISER) Store(b ISER)            { r.U32.Store(uint32(b)) }
+
+type RMISER struct{ mmio.UM32 }
+
+func (rm RMISER) Load() ISER   { return ISER(rm.UM32.Load()) }
+func (rm RMISER) Store(b ISER) { rm.UM32.Store(uint32(b)) }
+
+type ICER uint32
+
+type RICER struct{ mmio.U32 }
+
+func (r *RICER) LoadBits(mask ICER) ICER { return ICER(r.U32.LoadBits(uint32(mask))) }
+func (r *RICER) StoreBits(mask, b ICER)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICER) SetBits(mask ICER)       { r.U32.SetBits(uint32(mask)) }
+func (r *RICER) ClearBits(mask ICER)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RICER) Load() ICER              { return ICER(r.U32.Load()) }
+func (r *RICER) Store(b ICER)            { r.U32.Store(uint32(b)) }
+
+type RMICER struct{ mmio.UM32 }
+
+func (rm RMICER) Load() ICER   { return ICER(rm.UM32.Load()) }
+func (rm RMICER) Store(b ICER) { rm.UM32.Store(uint32(b)) }
+
+type ISPR uint32
+
+type RISPR struct{ mmio.U32 }
+
+func (r *RISPR) LoadBits(mask ISPR) ISPR { return ISPR(r.U32.LoadBits(uint32(mask))) }
+func (r *RISPR) StoreBits(mask, b ISPR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RISPR) SetBits(mask ISPR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RISPR) ClearBits(mask ISPR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RISPR) Load() ISPR              { return ISPR(r.U32.Load()) }
+func (r *RISPR) Store(b ISPR)            { r.U32.Store(uint32(b)) }
+
+type RMISPR struct{ mmio.UM32 }
+
+func (rm RMISPR) Load() ISPR   { return ISPR(rm.UM32.Load()) }
+func (rm RMISPR) Store(b ISPR) { rm.UM32.Store(uint32(b)) }
+
+type ICPR uint32
+
+type RICPR struct{ mmio.U32 }
+
+func (r *RICPR) LoadBits(mask ICPR) ICPR { return ICPR(r.U32.LoadBits(uint32(mask))) }
+func (r *RICPR) StoreBits(mask, b ICPR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICPR) SetBits(mask ICPR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RICPR) ClearBits(mask ICPR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RICPR) Load() ICPR              { return ICPR(r.U32.Load()) }
+func (r *RICPR) Store(b ICPR)            { r.U32.Store(uint32(b)) }
+
+type RMICPR struct{ mmio.UM32 }
+
+func (rm RMICPR) Load() ICPR   { return ICPR(rm.UM32.Load()) }
+func (rm RMICPR) Store(b ICPR) { rm.UM32.Store(uint32(b)) }
+
+type IABR uint32
+
+type RIABR struct{ mmio.U32 }
+
+func (r *RIABR) LoadBits(mask IABR) IABR { return IABR(r.U32.LoadBits(uint32(mask))) }
+func (r *RIABR) StoreBits(mask, b IABR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RIABR) SetBits(mask IABR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RIABR) ClearBits(mask IABR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RIABR) Load() IABR              { return IABR(r.U32.Load()) }
+func (r *RIABR) Store(b IABR)            { r.U32.Store(uint32(b)) }
+
+type RMIABR struct{ mmio.UM32 }
+
+func (rm RMIABR) Load() IABR   { return IABR(rm.UM32.Load()) }
+func (rm RMIABR) Store(b IABR) { rm.UM32.Store(uint32(b)) }
+
+type ITNS uint32
+
+type RITNS struct{ mmio.U32 }
+
+func (r *RITNS) LoadBits(mask ITNS) ITNS { return ITNS(r.U32.LoadBits(uint32(mask))) }
+func (r *RITNS) StoreBits(mask, b ITNS)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RITNS) SetBits(mask ITNS)       { r.U32.SetBits(uint32(mask)) }
+func (r *RITNS) ClearBits(mask ITNS)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RITNS) Load() ITNS              { return ITNS(r.U32.Load()) }
+func (r *RITNS) Store(b ITNS)            { r.U32.Store(uint32(b)) }
+
+type RMITNS struct{ mmio.UM32 }
+
+func (rm RMITNS) Load() ITNS   { return ITNS(rm.UM32.Load()) }
+func (rm RMITNS) Store(b ITNS) { rm.UM32.Store(uint32(b)) }
+
+type IPR uint8
+
+type RIPR struct{ mmio.U8 }
+
+func (r *RIPR) LoadBits(mask IPR) IPR { return IPR(r.U8.LoadBits(uint8(mask))) }
+func (r *RIPR) StoreBits(mask, b IPR) { r.U8.StoreBits(uint8(mask), uint8(b)) }
+func (r *RIPR) SetBits(mask IPR)      { r.U8.SetBits(uint8(mask)) }
+func (r *RIPR) ClearBits(mask IPR)    { r.U8.ClearBits(uint8(mask)) }
+func (r *RIPR) Load() IPR             { return IPR(r.U8.Load()) }
+func (r *RIPR) Store(b IPR)           { r.U8.Store(uint8(b)) }
+
+type RMIPR struct{ mmio.UM8 }
+
+func (rm RMIPR) Load() IPR   { return IPR(rm.UM8.Load()) }
+func (rm RMIPR) Store(b IPR) { rm.UM8.Store(uint8(b)) }
+
+type STIR uint32
+
+type RSTIR struct{ mmio.U32 }
+
+func (r *RSTIR) LoadBits(mask STIR) STIR { return STIR(r.U32.LoadBits(uint32(mask))) }
+func (r *RSTIR) StoreBits(mask, b STIR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSTIR) SetBits(mask STIR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RSTIR) ClearBits(mask STIR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RSTIR) Load() STIR              { return STIR(r.U32.Load()) }
+func (r *RSTIR) Store(b STIR)            { r.U32.Store(uint32(b)) }
+
+type RMSTIR struct{ mmio.UM32 }
+
+func (rm RMSTIR) Load() STIR   { return STIR(rm.UM32.Load()) }
+func (rm RMSTIR) Store(b STIR) { rm.UM32.Store(uint32(b)) }
diff --git a/src/internal/cpu/cortexm/pft/doc.go b/src/internal/cpu/cortexm/pft/doc.go
new file mode 100644
index 0000000000..730d0d07c6
--- /dev/null
+++ b/src/internal/cpu/cortexm/pft/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package pft provides an access to the Processor features registers.
+package pft
diff --git a/src/internal/cpu/cortexm/pft/periph_thumb.go b/src/internal/cpu/cortexm/pft/periph_thumb.go
new file mode 100644
index 0000000000..8e24cb41a5
--- /dev/null
+++ b/src/internal/cpu/cortexm/pft/periph_thumb.go
@@ -0,0 +1,112 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  PFT  0xE000ED78  -  -  Processor features registers
+// Registers:
+//  0x00 32  CLIDR   Cache Level ID
+//  0x00 32  CTR     Cache Type
+//  0x00 32  CCSIDR  Cache Size ID
+//  0x00 32  CSSELR  Cache Size Selection
+package pft
+
+const (
+	CL1I  CLIDR = 1 << 0    //+ Instruction L1 cache implemented.
+	CL1D  CLIDR = 1 << 1    //+ Data cache L1 implemented.
+	CL1U  CLIDR = 1 << 2    //+ Unified L1 cache.
+	CL2I  CLIDR = 1 << 3    //+ Instruction L2 cache implemented.
+	CL2D  CLIDR = 1 << 4    //+ Data cache L2 implemented.
+	CL2U  CLIDR = 1 << 5    //+ Unified L2 cache.
+	CL3I  CLIDR = 1 << 6    //+ Instruction L3 cache implemented.
+	CL3D  CLIDR = 1 << 7    //+ Data cache L3 implemented.
+	CL3U  CLIDR = 1 << 8    //+ Unified L3 cache.
+	CL4I  CLIDR = 1 << 9    //+ Instruction L4 cache implemented.
+	CL4D  CLIDR = 1 << 10   //+ Data cache L4 implemented.
+	CL4U  CLIDR = 1 << 11   //+ Unified L4 cache.
+	CL5I  CLIDR = 1 << 12   //+ Instruction L5 cache implemented.
+	CL5D  CLIDR = 1 << 13   //+ Data cache L5 implemented.
+	CL5U  CLIDR = 1 << 14   //+ Unified L5 cache.
+	CL6I  CLIDR = 1 << 15   //+ Instruction L6 cache implemented.
+	CL6D  CLIDR = 1 << 16   //+ Data cache L6 implemented.
+	CL6U  CLIDR = 1 << 17   //+ Unified L6 cache.
+	CL7I  CLIDR = 1 << 18   //+ Instruction L7 cache implemented.
+	CL7D  CLIDR = 1 << 19   //+ Data cache L7 implemented.
+	CL7U  CLIDR = 1 << 20   //+ Unified L7 cache.
+	LoUIS CLIDR = 0x7 << 21 //+
+	LoC   CLIDR = 0x7 << 24 //+ Level of Coherency.
+	LoU   CLIDR = 0x7 << 27 //+ Level of Unification.
+)
+
+const (
+	CL1In  = 0
+	CL1Dn  = 1
+	CL1Un  = 2
+	CL2In  = 3
+	CL2Dn  = 4
+	CL2Un  = 5
+	CL3In  = 6
+	CL3Dn  = 7
+	CL3Un  = 8
+	CL4In  = 9
+	CL4Dn  = 10
+	CL4Un  = 11
+	CL5In  = 12
+	CL5Dn  = 13
+	CL5Un  = 14
+	CL6In  = 15
+	CL6Dn  = 16
+	CL6Un  = 17
+	CL7In  = 18
+	CL7Dn  = 19
+	CL7Un  = 20
+	LoUISn = 21
+	LoCn   = 24
+	LoUn   = 27
+)
+
+const (
+	IMinLine CTR = 0xf << 0  //+ Smallest cache line of all the I-caches.
+	DMinLine CTR = 0xf << 16 //+ Smallest cache line of all the D/U-caches.
+	ERG      CTR = 0xf << 20 //+ Exclusives Reservation Granule.
+	CWG      CTR = 0xf << 24 //+ Cache Writeback Granule.
+	Format   CTR = 0x7 << 29 //+ Register format (4: ARMv7 format).
+)
+
+const (
+	IMinLinen = 0
+	DMinLinen = 16
+	ERGn      = 20
+	CWGn      = 24
+	Formatn   = 29
+)
+
+const (
+	LineSize      CCSIDR = 0x7 << 0     //+ Number of words in cache line (log2(n)-2).
+	Associativity CCSIDR = 0x3ff << 3   //+ Number of ways - 1.
+	NumSets       CCSIDR = 0x7fff << 13 //+ Number of sets - 1.
+	WA            CCSIDR = 1 << 28      //+ Write allocation support.
+	RA            CCSIDR = 1 << 29      //+ Read allocation support.
+	WB            CCSIDR = 1 << 30      //+ Write-Back support.
+	WT            CCSIDR = 1 << 31      //+ Write-Through support.
+)
+
+const (
+	LineSizen      = 0
+	Associativityn = 3
+	NumSetsn       = 13
+	WAn            = 28
+	RAn            = 29
+	WBn            = 30
+	WTn            = 31
+)
+
+const (
+	InD   CSSELR = 1 << 0   //+ Selection of 1:instruction or 0:data cache.
+	Level CSSELR = 0x7 << 1 //+ Cache level selected (0: level1).
+)
+
+const (
+	InDn   = 0
+	Leveln = 1
+)
diff --git a/src/internal/cpu/cortexm/pft/xperiph_thumb.go b/src/internal/cpu/cortexm/pft/xperiph_thumb.go
new file mode 100644
index 0000000000..357a5b66d3
--- /dev/null
+++ b/src/internal/cpu/cortexm/pft/xperiph_thumb.go
@@ -0,0 +1,237 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package pft
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	CLIDR  RCLIDR
+	CTR    RCTR
+	CCSIDR RCCSIDR
+	CSSELR RCSSELR
+}
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func PFT() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000ED78))) }
+
+type CLIDR uint32
+
+type RCLIDR struct{ mmio.U32 }
+
+func (r *RCLIDR) LoadBits(mask CLIDR) CLIDR { return CLIDR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCLIDR) StoreBits(mask, b CLIDR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCLIDR) SetBits(mask CLIDR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RCLIDR) ClearBits(mask CLIDR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RCLIDR) Load() CLIDR               { return CLIDR(r.U32.Load()) }
+func (r *RCLIDR) Store(b CLIDR)             { r.U32.Store(uint32(b)) }
+
+type RMCLIDR struct{ mmio.UM32 }
+
+func (rm RMCLIDR) Load() CLIDR   { return CLIDR(rm.UM32.Load()) }
+func (rm RMCLIDR) Store(b CLIDR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) CL1I() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL1I)}}
+}
+
+func (p *Periph) CL1D() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL1D)}}
+}
+
+func (p *Periph) CL1U() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL1U)}}
+}
+
+func (p *Periph) CL2I() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL2I)}}
+}
+
+func (p *Periph) CL2D() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL2D)}}
+}
+
+func (p *Periph) CL2U() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL2U)}}
+}
+
+func (p *Periph) CL3I() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL3I)}}
+}
+
+func (p *Periph) CL3D() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL3D)}}
+}
+
+func (p *Periph) CL3U() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL3U)}}
+}
+
+func (p *Periph) CL4I() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL4I)}}
+}
+
+func (p *Periph) CL4D() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL4D)}}
+}
+
+func (p *Periph) CL4U() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL4U)}}
+}
+
+func (p *Periph) CL5I() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL5I)}}
+}
+
+func (p *Periph) CL5D() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL5D)}}
+}
+
+func (p *Periph) CL5U() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL5U)}}
+}
+
+func (p *Periph) CL6I() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL6I)}}
+}
+
+func (p *Periph) CL6D() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL6D)}}
+}
+
+func (p *Periph) CL6U() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL6U)}}
+}
+
+func (p *Periph) CL7I() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL7I)}}
+}
+
+func (p *Periph) CL7D() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL7D)}}
+}
+
+func (p *Periph) CL7U() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(CL7U)}}
+}
+
+func (p *Periph) LoUIS() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(LoUIS)}}
+}
+
+func (p *Periph) LoC() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(LoC)}}
+}
+
+func (p *Periph) LoU() RMCLIDR {
+	return RMCLIDR{mmio.UM32{&p.CLIDR.U32, uint32(LoU)}}
+}
+
+type CTR uint32
+
+type RCTR struct{ mmio.U32 }
+
+func (r *RCTR) LoadBits(mask CTR) CTR { return CTR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCTR) StoreBits(mask, b CTR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCTR) SetBits(mask CTR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCTR) ClearBits(mask CTR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCTR) Load() CTR             { return CTR(r.U32.Load()) }
+func (r *RCTR) Store(b CTR)           { r.U32.Store(uint32(b)) }
+
+type RMCTR struct{ mmio.UM32 }
+
+func (rm RMCTR) Load() CTR   { return CTR(rm.UM32.Load()) }
+func (rm RMCTR) Store(b CTR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) IMinLine() RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(IMinLine)}}
+}
+
+func (p *Periph) DMinLine() RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(DMinLine)}}
+}
+
+func (p *Periph) ERG() RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(ERG)}}
+}
+
+func (p *Periph) CWG() RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(CWG)}}
+}
+
+func (p *Periph) Format() RMCTR {
+	return RMCTR{mmio.UM32{&p.CTR.U32, uint32(Format)}}
+}
+
+type CCSIDR uint32
+
+type RCCSIDR struct{ mmio.U32 }
+
+func (r *RCCSIDR) LoadBits(mask CCSIDR) CCSIDR { return CCSIDR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCCSIDR) StoreBits(mask, b CCSIDR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCCSIDR) SetBits(mask CCSIDR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RCCSIDR) ClearBits(mask CCSIDR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RCCSIDR) Load() CCSIDR                { return CCSIDR(r.U32.Load()) }
+func (r *RCCSIDR) Store(b CCSIDR)              { r.U32.Store(uint32(b)) }
+
+type RMCCSIDR struct{ mmio.UM32 }
+
+func (rm RMCCSIDR) Load() CCSIDR   { return CCSIDR(rm.UM32.Load()) }
+func (rm RMCCSIDR) Store(b CCSIDR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) LineSize() RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(LineSize)}}
+}
+
+func (p *Periph) Associativity() RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(Associativity)}}
+}
+
+func (p *Periph) NumSets() RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(NumSets)}}
+}
+
+func (p *Periph) WA() RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(WA)}}
+}
+
+func (p *Periph) RA() RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(RA)}}
+}
+
+func (p *Periph) WB() RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(WB)}}
+}
+
+func (p *Periph) WT() RMCCSIDR {
+	return RMCCSIDR{mmio.UM32{&p.CCSIDR.U32, uint32(WT)}}
+}
+
+type CSSELR uint32
+
+type RCSSELR struct{ mmio.U32 }
+
+func (r *RCSSELR) LoadBits(mask CSSELR) CSSELR { return CSSELR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCSSELR) StoreBits(mask, b CSSELR)    { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCSSELR) SetBits(mask CSSELR)         { r.U32.SetBits(uint32(mask)) }
+func (r *RCSSELR) ClearBits(mask CSSELR)       { r.U32.ClearBits(uint32(mask)) }
+func (r *RCSSELR) Load() CSSELR                { return CSSELR(r.U32.Load()) }
+func (r *RCSSELR) Store(b CSSELR)              { r.U32.Store(uint32(b)) }
+
+type RMCSSELR struct{ mmio.UM32 }
+
+func (rm RMCSSELR) Load() CSSELR   { return CSSELR(rm.UM32.Load()) }
+func (rm RMCSSELR) Store(b CSSELR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) InD() RMCSSELR {
+	return RMCSSELR{mmio.UM32{&p.CSSELR.U32, uint32(InD)}}
+}
+
+func (p *Periph) Level() RMCSSELR {
+	return RMCSSELR{mmio.UM32{&p.CSSELR.U32, uint32(Level)}}
+}
diff --git a/src/internal/cpu/cortexm/scb/doc.go b/src/internal/cpu/cortexm/scb/doc.go
new file mode 100644
index 0000000000..990699bad6
--- /dev/null
+++ b/src/internal/cpu/cortexm/scb/doc.go
@@ -0,0 +1,14 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package scb gives access to the registers of the System Control Block.
+//
+// Notes:
+//
+// 1. Cortex-M0 does not implement SHPR1, CFSR, HFSR, MMFR, BFAR, AFSR
+// registers.
+//
+// 2. Cortex-M0 supports only word access to SHPR2, SHPR3 so this package does
+// not provide a byte access to the individual fields.
+package scb
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/scb/periph_thumb.go b/src/internal/cpu/cortexm/scb/periph_thumb.go
new file mode 100644
index 0000000000..a9de6b48f3
--- /dev/null
+++ b/src/internal/cpu/cortexm/scb/periph_thumb.go
@@ -0,0 +1,172 @@
+// Instances:
+//  SCB  0xE000ED00  -  -  System Control Block
+// Registers:
+//  0x00  32  CPUID  CPUID Base Register
+//  0x04  32  ICSR   Interrupt Control and State Register
+//  0x08  32  VTOR   Vector Table Offset Register
+//  0x0C  32  AIRCR  Application Interrupt and Reset Control Register
+//  0x10  32  SCR    System Control Register
+//  0x14  32  CCR    Configuration and Control Register
+//  0x18  32  SHPR1  System Handler Priority Register 1
+//  0x1C  32  SHPR2  System Handler Priority Register 2
+//  0x20  32  SHPR3  System Handler Priority Register 3
+//  0x24  32  SHCSR  System Handler Control and State Register
+//  0x28  32  CFSR   Configurable Fault Status Register
+//  0x2C  32  HFSR   HardFault Status Register
+//  0x34  32  MMFR   MemManage Fault Address Register
+//  0x38  32  BFAR   BusFault Address Register
+//  0x3C  32  AFSR   Auxiliary Fault Status Register
+package scb
+
+const (
+	Revision    CPUID = 0xf << 0   //+
+	PartNo      CPUID = 0xfff << 4 //+
+	Constant    CPUID = 0xf << 16  //+
+	Variant     CPUID = 0xf << 20  //+
+	Implementer CPUID = 0xff << 24 //+
+)
+
+const (
+	Revisionn    = 0
+	PartNon      = 4
+	Constantn    = 16
+	Variantn     = 20
+	Implementern = 24
+)
+
+const (
+	VECTACTIVE  ICSR = 0x1ff << 0  //+ Active exception number (0: thread mode)
+	RETTOBASE   ICSR = 1 << 11     //+ No preempted active exceptions
+	VECTPENDING ICSR = 0x1ff << 12 //+ Highest priority pending exception number
+	ISRPENDING  ICSR = 1 << 22     //+ Int. pending flag, excluding NMI, Faults
+	PENDSTCLR   ICSR = 1 << 25     //+ SysTick exception clear-pending bit
+	PENDSTSET   ICSR = 1 << 26     //+ SysTick exception set-pending bit
+	PENDSVCLR   ICSR = 1 << 27     //+ PendSV clear-pending bit
+	PENDSVSET   ICSR = 1 << 28     //+ PendSV set-pending bit
+	NMIPENDSET  ICSR = 1 << 31     //+ NMI set-pending bit
+)
+
+const (
+	VECTACTIVEn  = 0
+	VECTPENDINGn = 12
+)
+
+const (
+	TBLOFF VTOR = 0x1ffffff << 7 //+
+)
+
+const (
+	VECTRESET     AIRCR = 1 << 0       //+
+	VECTCLRACTIVE AIRCR = 1 << 1       //+
+	SYSRESETREQ   AIRCR = 1 << 2       //+
+	PRIGROUP      AIRCR = 7 << 8       //+
+	ENDIANNESS    AIRCR = 1 << 15      //+
+	VECTKEY       AIRCR = 0xffff << 16 //+
+)
+
+const (
+	VECTRESETn     = 0
+	VECTCLRACTIVEn = 1
+	SYSRESETREQn   = 2
+	PRIGROUPn      = 8
+	ENDIANNESSn    = 15
+	VECTKEYn       = 16
+)
+
+const (
+	SLEEPONEXIT SCR = 1 << 1 //+
+	SLEEPDEEP   SCR = 1 << 2 //+
+	SEVONPEND   SCR = 1 << 4 //+
+)
+
+const (
+	NONBASETHRDENA CCR = 1 << 0  //+
+	USERSETMPEND   CCR = 1 << 1  //+
+	UNALIGN_TRP    CCR = 1 << 3  //+
+	DIV_0_TRP      CCR = 1 << 4  //+
+	BFHFNMIGN      CCR = 1 << 8  //+
+	STKALIGN       CCR = 1 << 9  //+ Stack 8 byte aligned on exception entry
+	DC             CCR = 1 << 16 //+ Enable data cache
+	IC             CCR = 1 << 17 //+ Enable instruction cache
+	BP             CCR = 1 << 18 //+ Branch prediction is enabled
+)
+
+const (
+	PRI_MemManage  SHPR1 = 0xff << 0  //+
+	PRI_BusFault   SHPR1 = 0xff << 8  //+
+	PRI_UsageFault SHPR1 = 0xff << 16 //+
+)
+
+const (
+	PRI_MemManagen  = 0
+	PRI_BusFaultn   = 8
+	PRI_UsageFaultn = 16
+)
+
+const (
+	PRI_SVCall SHPR2 = 0xff << 24 //+
+)
+
+const (
+	PRI_SVCalln = 24
+)
+
+const (
+	PRI_PendSV  SHPR3 = 0xff << 16 //+
+	PRI_SysTick SHPR3 = 0xff << 24 //+
+)
+
+const (
+	PRI_PendSVn  = 16
+	PRI_SysTickn = 24
+)
+
+const (
+	MEMFAULTACT    SHCSR = 1 << 0  //+
+	BUSFAULTACT    SHCSR = 1 << 1  //+
+	USGFAULTACT    SHCSR = 1 << 3  //+
+	SVCALLACT      SHCSR = 1 << 7  //+
+	MONITORACT     SHCSR = 1 << 8  //+
+	PENDSVACT      SHCSR = 1 << 10 //+
+	SYSTICKACT     SHCSR = 1 << 11 //+
+	USGFAULTPENDED SHCSR = 1 << 12 //+
+	MEMFAULTPENDED SHCSR = 1 << 13 //+
+	BUSFAULTPENDED SHCSR = 1 << 14 //+
+	SVCALLPENDED   SHCSR = 1 << 15 //+
+	MEMFAULTENA    SHCSR = 1 << 16 //+
+	BUSFAULTENA    SHCSR = 1 << 17 //+
+	USGFAULTENA    SHCSR = 1 << 18 //+
+)
+
+const (
+	// MFSR
+	IACCVIOL  CFSR = 1 << 0 //+
+	DACCVIOL  CFSR = 1 << 1 //+
+	MUNSTKERR CFSR = 1 << 3 //+
+	MSTKERR   CFSR = 1 << 4 //+
+	MLSPERR   CFSR = 1 << 5 //+
+	MMARVALID CFSR = 1 << 7 //+
+
+	// BFSR
+	IBUSERR     CFSR = 1 << 8  //+
+	PRECISERR   CFSR = 1 << 9  //+
+	IMPRECISERR CFSR = 1 << 10 //+
+	UNSTKERR    CFSR = 1 << 11 //+
+	STKERR      CFSR = 1 << 12 //+
+	LSPERR      CFSR = 1 << 13 //+
+	BFARVALID   CFSR = 1 << 15 //+
+
+	// UFSR
+	UNDEFINSTR CFSR = 1 << 16 //+
+	INVSTATE   CFSR = 1 << 17 //+
+	INVPC      CFSR = 1 << 18 //+
+	NOCP       CFSR = 1 << 19 //+
+	UNALIGNED  CFSR = 1 << 24 //+
+	DIVBYZERO  CFSR = 1 << 25 //+
+)
+
+const (
+	VECTTBL  HFSR = 1 << 1  //+
+	FORCED   HFSR = 1 << 30 //+
+	DEBUGEVT HFSR = 1 << 31 //+
+)
diff --git a/src/internal/cpu/cortexm/scb/xperiph_thumb.go b/src/internal/cpu/cortexm/scb/xperiph_thumb.go
new file mode 100644
index 0000000000..e20d109fa4
--- /dev/null
+++ b/src/internal/cpu/cortexm/scb/xperiph_thumb.go
@@ -0,0 +1,573 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package scb
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	CPUID RCPUID
+	ICSR  RICSR
+	VTOR  RVTOR
+	AIRCR RAIRCR
+	SCR   RSCR
+	CCR   RCCR
+	SHPR1 RSHPR1
+	SHPR2 RSHPR2
+	SHPR3 RSHPR3
+	SHCSR RSHCSR
+	CFSR  RCFSR
+	HFSR  RHFSR
+	_     uint32
+	MMFR  RMMFR
+	BFAR  RBFAR
+	AFSR  RAFSR
+}
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func SCB() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000ED00))) }
+
+type CPUID uint32
+
+type RCPUID struct{ mmio.U32 }
+
+func (r *RCPUID) LoadBits(mask CPUID) CPUID { return CPUID(r.U32.LoadBits(uint32(mask))) }
+func (r *RCPUID) StoreBits(mask, b CPUID)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCPUID) SetBits(mask CPUID)        { r.U32.SetBits(uint32(mask)) }
+func (r *RCPUID) ClearBits(mask CPUID)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RCPUID) Load() CPUID               { return CPUID(r.U32.Load()) }
+func (r *RCPUID) Store(b CPUID)             { r.U32.Store(uint32(b)) }
+
+type RMCPUID struct{ mmio.UM32 }
+
+func (rm RMCPUID) Load() CPUID   { return CPUID(rm.UM32.Load()) }
+func (rm RMCPUID) Store(b CPUID) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) Revision() RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(Revision)}}
+}
+
+func (p *Periph) PartNo() RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(PartNo)}}
+}
+
+func (p *Periph) Constant() RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(Constant)}}
+}
+
+func (p *Periph) Variant() RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(Variant)}}
+}
+
+func (p *Periph) Implementer() RMCPUID {
+	return RMCPUID{mmio.UM32{&p.CPUID.U32, uint32(Implementer)}}
+}
+
+type ICSR uint32
+
+type RICSR struct{ mmio.U32 }
+
+func (r *RICSR) LoadBits(mask ICSR) ICSR { return ICSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RICSR) StoreBits(mask, b ICSR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICSR) SetBits(mask ICSR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RICSR) ClearBits(mask ICSR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RICSR) Load() ICSR              { return ICSR(r.U32.Load()) }
+func (r *RICSR) Store(b ICSR)            { r.U32.Store(uint32(b)) }
+
+type RMICSR struct{ mmio.UM32 }
+
+func (rm RMICSR) Load() ICSR   { return ICSR(rm.UM32.Load()) }
+func (rm RMICSR) Store(b ICSR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) VECTACTIVE() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(VECTACTIVE)}}
+}
+
+func (p *Periph) RETTOBASE() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(RETTOBASE)}}
+}
+
+func (p *Periph) VECTPENDING() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(VECTPENDING)}}
+}
+
+func (p *Periph) ISRPENDING() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(ISRPENDING)}}
+}
+
+func (p *Periph) PENDSTCLR() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(PENDSTCLR)}}
+}
+
+func (p *Periph) PENDSTSET() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(PENDSTSET)}}
+}
+
+func (p *Periph) PENDSVCLR() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(PENDSVCLR)}}
+}
+
+func (p *Periph) PENDSVSET() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(PENDSVSET)}}
+}
+
+func (p *Periph) NMIPENDSET() RMICSR {
+	return RMICSR{mmio.UM32{&p.ICSR.U32, uint32(NMIPENDSET)}}
+}
+
+type VTOR uint32
+
+type RVTOR struct{ mmio.U32 }
+
+func (r *RVTOR) LoadBits(mask VTOR) VTOR { return VTOR(r.U32.LoadBits(uint32(mask))) }
+func (r *RVTOR) StoreBits(mask, b VTOR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RVTOR) SetBits(mask VTOR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RVTOR) ClearBits(mask VTOR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RVTOR) Load() VTOR              { return VTOR(r.U32.Load()) }
+func (r *RVTOR) Store(b VTOR)            { r.U32.Store(uint32(b)) }
+
+type RMVTOR struct{ mmio.UM32 }
+
+func (rm RMVTOR) Load() VTOR   { return VTOR(rm.UM32.Load()) }
+func (rm RMVTOR) Store(b VTOR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) TBLOFF() RMVTOR {
+	return RMVTOR{mmio.UM32{&p.VTOR.U32, uint32(TBLOFF)}}
+}
+
+type AIRCR uint32
+
+type RAIRCR struct{ mmio.U32 }
+
+func (r *RAIRCR) LoadBits(mask AIRCR) AIRCR { return AIRCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RAIRCR) StoreBits(mask, b AIRCR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RAIRCR) SetBits(mask AIRCR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RAIRCR) ClearBits(mask AIRCR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RAIRCR) Load() AIRCR               { return AIRCR(r.U32.Load()) }
+func (r *RAIRCR) Store(b AIRCR)             { r.U32.Store(uint32(b)) }
+
+type RMAIRCR struct{ mmio.UM32 }
+
+func (rm RMAIRCR) Load() AIRCR   { return AIRCR(rm.UM32.Load()) }
+func (rm RMAIRCR) Store(b AIRCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) VECTRESET() RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(VECTRESET)}}
+}
+
+func (p *Periph) VECTCLRACTIVE() RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(VECTCLRACTIVE)}}
+}
+
+func (p *Periph) SYSRESETREQ() RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(SYSRESETREQ)}}
+}
+
+func (p *Periph) PRIGROUP() RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(PRIGROUP)}}
+}
+
+func (p *Periph) ENDIANNESS() RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(ENDIANNESS)}}
+}
+
+func (p *Periph) VECTKEY() RMAIRCR {
+	return RMAIRCR{mmio.UM32{&p.AIRCR.U32, uint32(VECTKEY)}}
+}
+
+type SCR uint32
+
+type RSCR struct{ mmio.U32 }
+
+func (r *RSCR) LoadBits(mask SCR) SCR { return SCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RSCR) StoreBits(mask, b SCR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSCR) SetBits(mask SCR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RSCR) ClearBits(mask SCR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RSCR) Load() SCR             { return SCR(r.U32.Load()) }
+func (r *RSCR) Store(b SCR)           { r.U32.Store(uint32(b)) }
+
+type RMSCR struct{ mmio.UM32 }
+
+func (rm RMSCR) Load() SCR   { return SCR(rm.UM32.Load()) }
+func (rm RMSCR) Store(b SCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) SLEEPONEXIT() RMSCR {
+	return RMSCR{mmio.UM32{&p.SCR.U32, uint32(SLEEPONEXIT)}}
+}
+
+func (p *Periph) SLEEPDEEP() RMSCR {
+	return RMSCR{mmio.UM32{&p.SCR.U32, uint32(SLEEPDEEP)}}
+}
+
+func (p *Periph) SEVONPEND() RMSCR {
+	return RMSCR{mmio.UM32{&p.SCR.U32, uint32(SEVONPEND)}}
+}
+
+type CCR uint32
+
+type RCCR struct{ mmio.U32 }
+
+func (r *RCCR) LoadBits(mask CCR) CCR { return CCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCCR) StoreBits(mask, b CCR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCCR) SetBits(mask CCR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCCR) ClearBits(mask CCR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCCR) Load() CCR             { return CCR(r.U32.Load()) }
+func (r *RCCR) Store(b CCR)           { r.U32.Store(uint32(b)) }
+
+type RMCCR struct{ mmio.UM32 }
+
+func (rm RMCCR) Load() CCR   { return CCR(rm.UM32.Load()) }
+func (rm RMCCR) Store(b CCR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) NONBASETHRDENA() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(NONBASETHRDENA)}}
+}
+
+func (p *Periph) USERSETMPEND() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(USERSETMPEND)}}
+}
+
+func (p *Periph) UNALIGN_TRP() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(UNALIGN_TRP)}}
+}
+
+func (p *Periph) DIV_0_TRP() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(DIV_0_TRP)}}
+}
+
+func (p *Periph) BFHFNMIGN() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(BFHFNMIGN)}}
+}
+
+func (p *Periph) STKALIGN() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(STKALIGN)}}
+}
+
+func (p *Periph) DC() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(DC)}}
+}
+
+func (p *Periph) IC() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(IC)}}
+}
+
+func (p *Periph) BP() RMCCR {
+	return RMCCR{mmio.UM32{&p.CCR.U32, uint32(BP)}}
+}
+
+type SHPR1 uint32
+
+type RSHPR1 struct{ mmio.U32 }
+
+func (r *RSHPR1) LoadBits(mask SHPR1) SHPR1 { return SHPR1(r.U32.LoadBits(uint32(mask))) }
+func (r *RSHPR1) StoreBits(mask, b SHPR1)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSHPR1) SetBits(mask SHPR1)        { r.U32.SetBits(uint32(mask)) }
+func (r *RSHPR1) ClearBits(mask SHPR1)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RSHPR1) Load() SHPR1               { return SHPR1(r.U32.Load()) }
+func (r *RSHPR1) Store(b SHPR1)             { r.U32.Store(uint32(b)) }
+
+type RMSHPR1 struct{ mmio.UM32 }
+
+func (rm RMSHPR1) Load() SHPR1   { return SHPR1(rm.UM32.Load()) }
+func (rm RMSHPR1) Store(b SHPR1) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) PRI_MemManage() RMSHPR1 {
+	return RMSHPR1{mmio.UM32{&p.SHPR1.U32, uint32(PRI_MemManage)}}
+}
+
+func (p *Periph) PRI_BusFault() RMSHPR1 {
+	return RMSHPR1{mmio.UM32{&p.SHPR1.U32, uint32(PRI_BusFault)}}
+}
+
+func (p *Periph) PRI_UsageFault() RMSHPR1 {
+	return RMSHPR1{mmio.UM32{&p.SHPR1.U32, uint32(PRI_UsageFault)}}
+}
+
+type SHPR2 uint32
+
+type RSHPR2 struct{ mmio.U32 }
+
+func (r *RSHPR2) LoadBits(mask SHPR2) SHPR2 { return SHPR2(r.U32.LoadBits(uint32(mask))) }
+func (r *RSHPR2) StoreBits(mask, b SHPR2)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSHPR2) SetBits(mask SHPR2)        { r.U32.SetBits(uint32(mask)) }
+func (r *RSHPR2) ClearBits(mask SHPR2)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RSHPR2) Load() SHPR2               { return SHPR2(r.U32.Load()) }
+func (r *RSHPR2) Store(b SHPR2)             { r.U32.Store(uint32(b)) }
+
+type RMSHPR2 struct{ mmio.UM32 }
+
+func (rm RMSHPR2) Load() SHPR2   { return SHPR2(rm.UM32.Load()) }
+func (rm RMSHPR2) Store(b SHPR2) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) PRI_SVCall() RMSHPR2 {
+	return RMSHPR2{mmio.UM32{&p.SHPR2.U32, uint32(PRI_SVCall)}}
+}
+
+type SHPR3 uint32
+
+type RSHPR3 struct{ mmio.U32 }
+
+func (r *RSHPR3) LoadBits(mask SHPR3) SHPR3 { return SHPR3(r.U32.LoadBits(uint32(mask))) }
+func (r *RSHPR3) StoreBits(mask, b SHPR3)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSHPR3) SetBits(mask SHPR3)        { r.U32.SetBits(uint32(mask)) }
+func (r *RSHPR3) ClearBits(mask SHPR3)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RSHPR3) Load() SHPR3               { return SHPR3(r.U32.Load()) }
+func (r *RSHPR3) Store(b SHPR3)             { r.U32.Store(uint32(b)) }
+
+type RMSHPR3 struct{ mmio.UM32 }
+
+func (rm RMSHPR3) Load() SHPR3   { return SHPR3(rm.UM32.Load()) }
+func (rm RMSHPR3) Store(b SHPR3) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) PRI_PendSV() RMSHPR3 {
+	return RMSHPR3{mmio.UM32{&p.SHPR3.U32, uint32(PRI_PendSV)}}
+}
+
+func (p *Periph) PRI_SysTick() RMSHPR3 {
+	return RMSHPR3{mmio.UM32{&p.SHPR3.U32, uint32(PRI_SysTick)}}
+}
+
+type SHCSR uint32
+
+type RSHCSR struct{ mmio.U32 }
+
+func (r *RSHCSR) LoadBits(mask SHCSR) SHCSR { return SHCSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RSHCSR) StoreBits(mask, b SHCSR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RSHCSR) SetBits(mask SHCSR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RSHCSR) ClearBits(mask SHCSR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RSHCSR) Load() SHCSR               { return SHCSR(r.U32.Load()) }
+func (r *RSHCSR) Store(b SHCSR)             { r.U32.Store(uint32(b)) }
+
+type RMSHCSR struct{ mmio.UM32 }
+
+func (rm RMSHCSR) Load() SHCSR   { return SHCSR(rm.UM32.Load()) }
+func (rm RMSHCSR) Store(b SHCSR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) MEMFAULTACT() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(MEMFAULTACT)}}
+}
+
+func (p *Periph) BUSFAULTACT() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(BUSFAULTACT)}}
+}
+
+func (p *Periph) USGFAULTACT() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(USGFAULTACT)}}
+}
+
+func (p *Periph) SVCALLACT() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(SVCALLACT)}}
+}
+
+func (p *Periph) MONITORACT() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(MONITORACT)}}
+}
+
+func (p *Periph) PENDSVACT() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(PENDSVACT)}}
+}
+
+func (p *Periph) SYSTICKACT() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(SYSTICKACT)}}
+}
+
+func (p *Periph) USGFAULTPENDED() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(USGFAULTPENDED)}}
+}
+
+func (p *Periph) MEMFAULTPENDED() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(MEMFAULTPENDED)}}
+}
+
+func (p *Periph) BUSFAULTPENDED() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(BUSFAULTPENDED)}}
+}
+
+func (p *Periph) SVCALLPENDED() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(SVCALLPENDED)}}
+}
+
+func (p *Periph) MEMFAULTENA() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(MEMFAULTENA)}}
+}
+
+func (p *Periph) BUSFAULTENA() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(BUSFAULTENA)}}
+}
+
+func (p *Periph) USGFAULTENA() RMSHCSR {
+	return RMSHCSR{mmio.UM32{&p.SHCSR.U32, uint32(USGFAULTENA)}}
+}
+
+type CFSR uint32
+
+type RCFSR struct{ mmio.U32 }
+
+func (r *RCFSR) LoadBits(mask CFSR) CFSR { return CFSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCFSR) StoreBits(mask, b CFSR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCFSR) SetBits(mask CFSR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RCFSR) ClearBits(mask CFSR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RCFSR) Load() CFSR              { return CFSR(r.U32.Load()) }
+func (r *RCFSR) Store(b CFSR)            { r.U32.Store(uint32(b)) }
+
+type RMCFSR struct{ mmio.UM32 }
+
+func (rm RMCFSR) Load() CFSR   { return CFSR(rm.UM32.Load()) }
+func (rm RMCFSR) Store(b CFSR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) IACCVIOL() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(IACCVIOL)}}
+}
+
+func (p *Periph) DACCVIOL() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(DACCVIOL)}}
+}
+
+func (p *Periph) MUNSTKERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(MUNSTKERR)}}
+}
+
+func (p *Periph) MSTKERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(MSTKERR)}}
+}
+
+func (p *Periph) MLSPERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(MLSPERR)}}
+}
+
+func (p *Periph) MMARVALID() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(MMARVALID)}}
+}
+
+func (p *Periph) IBUSERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(IBUSERR)}}
+}
+
+func (p *Periph) PRECISERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(PRECISERR)}}
+}
+
+func (p *Periph) IMPRECISERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(IMPRECISERR)}}
+}
+
+func (p *Periph) UNSTKERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(UNSTKERR)}}
+}
+
+func (p *Periph) STKERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(STKERR)}}
+}
+
+func (p *Periph) LSPERR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(LSPERR)}}
+}
+
+func (p *Periph) BFARVALID() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(BFARVALID)}}
+}
+
+func (p *Periph) UNDEFINSTR() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(UNDEFINSTR)}}
+}
+
+func (p *Periph) INVSTATE() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(INVSTATE)}}
+}
+
+func (p *Periph) INVPC() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(INVPC)}}
+}
+
+func (p *Periph) NOCP() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(NOCP)}}
+}
+
+func (p *Periph) UNALIGNED() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(UNALIGNED)}}
+}
+
+func (p *Periph) DIVBYZERO() RMCFSR {
+	return RMCFSR{mmio.UM32{&p.CFSR.U32, uint32(DIVBYZERO)}}
+}
+
+type HFSR uint32
+
+type RHFSR struct{ mmio.U32 }
+
+func (r *RHFSR) LoadBits(mask HFSR) HFSR { return HFSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RHFSR) StoreBits(mask, b HFSR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RHFSR) SetBits(mask HFSR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RHFSR) ClearBits(mask HFSR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RHFSR) Load() HFSR              { return HFSR(r.U32.Load()) }
+func (r *RHFSR) Store(b HFSR)            { r.U32.Store(uint32(b)) }
+
+type RMHFSR struct{ mmio.UM32 }
+
+func (rm RMHFSR) Load() HFSR   { return HFSR(rm.UM32.Load()) }
+func (rm RMHFSR) Store(b HFSR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) VECTTBL() RMHFSR {
+	return RMHFSR{mmio.UM32{&p.HFSR.U32, uint32(VECTTBL)}}
+}
+
+func (p *Periph) FORCED() RMHFSR {
+	return RMHFSR{mmio.UM32{&p.HFSR.U32, uint32(FORCED)}}
+}
+
+func (p *Periph) DEBUGEVT() RMHFSR {
+	return RMHFSR{mmio.UM32{&p.HFSR.U32, uint32(DEBUGEVT)}}
+}
+
+type MMFR uint32
+
+type RMMFR struct{ mmio.U32 }
+
+func (r *RMMFR) LoadBits(mask MMFR) MMFR { return MMFR(r.U32.LoadBits(uint32(mask))) }
+func (r *RMMFR) StoreBits(mask, b MMFR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RMMFR) SetBits(mask MMFR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RMMFR) ClearBits(mask MMFR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RMMFR) Load() MMFR              { return MMFR(r.U32.Load()) }
+func (r *RMMFR) Store(b MMFR)            { r.U32.Store(uint32(b)) }
+
+type RMMMFR struct{ mmio.UM32 }
+
+func (rm RMMMFR) Load() MMFR   { return MMFR(rm.UM32.Load()) }
+func (rm RMMMFR) Store(b MMFR) { rm.UM32.Store(uint32(b)) }
+
+type BFAR uint32
+
+type RBFAR struct{ mmio.U32 }
+
+func (r *RBFAR) LoadBits(mask BFAR) BFAR { return BFAR(r.U32.LoadBits(uint32(mask))) }
+func (r *RBFAR) StoreBits(mask, b BFAR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RBFAR) SetBits(mask BFAR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RBFAR) ClearBits(mask BFAR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RBFAR) Load() BFAR              { return BFAR(r.U32.Load()) }
+func (r *RBFAR) Store(b BFAR)            { r.U32.Store(uint32(b)) }
+
+type RMBFAR struct{ mmio.UM32 }
+
+func (rm RMBFAR) Load() BFAR   { return BFAR(rm.UM32.Load()) }
+func (rm RMBFAR) Store(b BFAR) { rm.UM32.Store(uint32(b)) }
+
+type AFSR uint32
+
+type RAFSR struct{ mmio.U32 }
+
+func (r *RAFSR) LoadBits(mask AFSR) AFSR { return AFSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RAFSR) StoreBits(mask, b AFSR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RAFSR) SetBits(mask AFSR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RAFSR) ClearBits(mask AFSR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RAFSR) Load() AFSR              { return AFSR(r.U32.Load()) }
+func (r *RAFSR) Store(b AFSR)            { r.U32.Store(uint32(b)) }
+
+type RMAFSR struct{ mmio.UM32 }
+
+func (rm RMAFSR) Load() AFSR   { return AFSR(rm.UM32.Load()) }
+func (rm RMAFSR) Store(b AFSR) { rm.UM32.Store(uint32(b)) }
diff --git a/src/internal/cpu/cortexm/scid/doc.go b/src/internal/cpu/cortexm/scid/doc.go
new file mode 100644
index 0000000000..1a851fad8b
--- /dev/null
+++ b/src/internal/cpu/cortexm/scid/doc.go
@@ -0,0 +1,7 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package scid gives access to the System Control and ID registers. Note:
+// Cortex-M0 does not implement ACTLR.
+package scid
\ No newline at end of file
diff --git a/src/internal/cpu/cortexm/scid/periph_thumb.go b/src/internal/cpu/cortexm/scid/periph_thumb.go
new file mode 100644
index 0000000000..3736a9cd8e
--- /dev/null
+++ b/src/internal/cpu/cortexm/scid/periph_thumb.go
@@ -0,0 +1,19 @@
+// Instances:
+//  SCID  0xE000E000  -  -  System Control and ID registers
+// Registers:
+//  0x00  32  MCR    Master Control register, Reserved
+//  0x04  32  ICTR   Interrupt Controller Type Register
+//  0x08  32  ACTLR  Auxiliary Control Register
+package scid
+
+const (
+	INTLINESNUM ICTR = 15 << 0 //+ The number of IRQs = 32*(INTLINESNUM+1)
+)
+
+const (
+	DISMCYCINT ACTLR = 1 << 0 //+
+	DISDEFWBUF ACTLR = 1 << 1 //+
+	DISFOLD    ACTLR = 1 << 2 //+
+	DISFPCA    ACTLR = 1 << 8 //+
+	DISOOFP    ACTLR = 1 << 9 //+
+)
diff --git a/src/internal/cpu/cortexm/scid/xperiph_thumb.go b/src/internal/cpu/cortexm/scid/xperiph_thumb.go
new file mode 100644
index 0000000000..52278fc4b0
--- /dev/null
+++ b/src/internal/cpu/cortexm/scid/xperiph_thumb.go
@@ -0,0 +1,92 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package scid
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	MCR   RMCR
+	ICTR  RICTR
+	ACTLR RACTLR
+}
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func SCID() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000E000))) }
+
+type MCR uint32
+
+type RMCR struct{ mmio.U32 }
+
+func (r *RMCR) LoadBits(mask MCR) MCR { return MCR(r.U32.LoadBits(uint32(mask))) }
+func (r *RMCR) StoreBits(mask, b MCR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RMCR) SetBits(mask MCR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RMCR) ClearBits(mask MCR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RMCR) Load() MCR             { return MCR(r.U32.Load()) }
+func (r *RMCR) Store(b MCR)           { r.U32.Store(uint32(b)) }
+
+type RMMCR struct{ mmio.UM32 }
+
+func (rm RMMCR) Load() MCR   { return MCR(rm.UM32.Load()) }
+func (rm RMMCR) Store(b MCR) { rm.UM32.Store(uint32(b)) }
+
+type ICTR uint32
+
+type RICTR struct{ mmio.U32 }
+
+func (r *RICTR) LoadBits(mask ICTR) ICTR { return ICTR(r.U32.LoadBits(uint32(mask))) }
+func (r *RICTR) StoreBits(mask, b ICTR)  { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RICTR) SetBits(mask ICTR)       { r.U32.SetBits(uint32(mask)) }
+func (r *RICTR) ClearBits(mask ICTR)     { r.U32.ClearBits(uint32(mask)) }
+func (r *RICTR) Load() ICTR              { return ICTR(r.U32.Load()) }
+func (r *RICTR) Store(b ICTR)            { r.U32.Store(uint32(b)) }
+
+type RMICTR struct{ mmio.UM32 }
+
+func (rm RMICTR) Load() ICTR   { return ICTR(rm.UM32.Load()) }
+func (rm RMICTR) Store(b ICTR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) INTLINESNUM() RMICTR {
+	return RMICTR{mmio.UM32{&p.ICTR.U32, uint32(INTLINESNUM)}}
+}
+
+type ACTLR uint32
+
+type RACTLR struct{ mmio.U32 }
+
+func (r *RACTLR) LoadBits(mask ACTLR) ACTLR { return ACTLR(r.U32.LoadBits(uint32(mask))) }
+func (r *RACTLR) StoreBits(mask, b ACTLR)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RACTLR) SetBits(mask ACTLR)        { r.U32.SetBits(uint32(mask)) }
+func (r *RACTLR) ClearBits(mask ACTLR)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RACTLR) Load() ACTLR               { return ACTLR(r.U32.Load()) }
+func (r *RACTLR) Store(b ACTLR)             { r.U32.Store(uint32(b)) }
+
+type RMACTLR struct{ mmio.UM32 }
+
+func (rm RMACTLR) Load() ACTLR   { return ACTLR(rm.UM32.Load()) }
+func (rm RMACTLR) Store(b ACTLR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) DISMCYCINT() RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISMCYCINT)}}
+}
+
+func (p *Periph) DISDEFWBUF() RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISDEFWBUF)}}
+}
+
+func (p *Periph) DISFOLD() RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISFOLD)}}
+}
+
+func (p *Periph) DISFPCA() RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISFPCA)}}
+}
+
+func (p *Periph) DISOOFP() RMACTLR {
+	return RMACTLR{mmio.UM32{&p.ACTLR.U32, uint32(DISOOFP)}}
+}
diff --git a/src/internal/cpu/cortexm/stackframe_thumb.go b/src/internal/cpu/cortexm/stackframe_thumb.go
new file mode 100644
index 0000000000..6986e4bb37
--- /dev/null
+++ b/src/internal/cpu/cortexm/stackframe_thumb.go
@@ -0,0 +1,46 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package cortexm
+
+// Masks to select PSR subregisters/bits
+const (
+	APSR  = 0xF80F0000 // Application Program Status Register:
+	GE    = 0x000F0000 // - greater than or equal flags (SIMD instructions)
+	Q     = 0x08000000 // - saturation flag (DSP instructions)
+	V     = 0x10000000 // - overflow condition flag
+	C     = 0x20000000 // - carry condition flag
+	Z     = 0x40000000 // - zero condition flag
+	N     = 0x80000000 // - negative condition flag
+	IPSR  = 0x000001FF // Interrupt Program Status Register (exception number)
+	EPSR  = 0x0700FC00 // Execution Program Status Register:
+	SR    = 0x00000200 // - stack realigned (in stacked PSR only)
+	ITICI = 0x0600FC00 // - interrupt-continue for LDM/STM or IT block
+	T     = 0x01000000 // - execution mode (0: ARM, 1: Thumb)
+)
+
+// StackFrame represents the ARMv7-M stack frame
+type StackFrame struct {
+	R   [4]uintptr
+	R12 uintptr
+	LR  uintptr
+	PC  uintptr
+	PSR uint32
+}
+
+// StackFrameF represents the ARMv7-M extended stack frame with
+// single-precision floating-point registers
+type StackFrameF struct {
+	StackFrame
+	S     [16]float32
+	FPSCR uint32
+}
+
+// StackFrameD represents the ARMv7-M extended stack frame with
+// double-precision floating-point registers
+type StackFrameD struct {
+	StackFrame
+	D     [8]float64
+	FPSCR uint32
+}
diff --git a/src/internal/cpu/cortexm/systick/doc.go b/src/internal/cpu/cortexm/systick/doc.go
new file mode 100644
index 0000000000..8edbbb905f
--- /dev/null
+++ b/src/internal/cpu/cortexm/systick/doc.go
@@ -0,0 +1,6 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Package systick gives access to the registers of the System Timer.
+package systick
diff --git a/src/internal/cpu/cortexm/systick/periph_thumb.go b/src/internal/cpu/cortexm/systick/periph_thumb.go
new file mode 100644
index 0000000000..ee8abb6f84
--- /dev/null
+++ b/src/internal/cpu/cortexm/systick/periph_thumb.go
@@ -0,0 +1,33 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Instances:
+//  SYSTICK  0xE000E010  -  -  System Timer registers
+// Registers:
+//	0x00  32  CSR   Control and Status Register (any read clears COUNTFLAG)
+//	0x04  32  RVR   Reload Value Register.
+//	0x08  32  CVR   Current Value Register.
+//	0x0C  32  CALIB Calibration Value Register.
+package systick
+
+const (
+	ENABLE    CSR = 1 << 0  //+ Enable counter.
+	TICKINT   CSR = 1 << 1  //+ Generate exceptions.
+	CLKSOURCE CSR = 1 << 2  //+ Clock source: 0:external, 1:CPU.
+	COUNTFLAG CSR = 1 << 16 //+ 1:Timer counted to 0 since last read of CSR
+)
+
+const (
+	RELOAD RVR = 1<<24 - 1 //+ Loaded into CVR when the counter reaches 0.
+)
+
+const (
+	CURRENT CVR = 1<<24 - 1 //+ Read: counter value, write: clears to zero.
+)
+
+const (
+	TENMS CALIB = 1<<24 - 1 //+
+	SKEW  CALIB = 1 << 30   //+
+	NOREF CALIB = 1 << 31   //+
+)
diff --git a/src/internal/cpu/cortexm/systick/xperiph_thumb.go b/src/internal/cpu/cortexm/systick/xperiph_thumb.go
new file mode 100644
index 0000000000..25001bd530
--- /dev/null
+++ b/src/internal/cpu/cortexm/systick/xperiph_thumb.go
@@ -0,0 +1,121 @@
+// DO NOT EDIT THIS FILE. GENERATED BY xgen.
+
+package systick
+
+import (
+	"embedded/mmio"
+	"unsafe"
+)
+
+type Periph struct {
+	CSR   RCSR
+	RVR   RRVR
+	CVR   RCVR
+	CALIB RCALIB
+}
+
+func (p *Periph) BaseAddr() uintptr {
+	return uintptr(unsafe.Pointer(p))
+}
+
+func SYSTICK() *Periph { return (*Periph)(unsafe.Pointer(uintptr(0xE000E010))) }
+
+type CSR uint32
+
+type RCSR struct{ mmio.U32 }
+
+func (r *RCSR) LoadBits(mask CSR) CSR { return CSR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCSR) StoreBits(mask, b CSR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCSR) SetBits(mask CSR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCSR) ClearBits(mask CSR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCSR) Load() CSR             { return CSR(r.U32.Load()) }
+func (r *RCSR) Store(b CSR)           { r.U32.Store(uint32(b)) }
+
+type RMCSR struct{ mmio.UM32 }
+
+func (rm RMCSR) Load() CSR   { return CSR(rm.UM32.Load()) }
+func (rm RMCSR) Store(b CSR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) ENABLE() RMCSR {
+	return RMCSR{mmio.UM32{&p.CSR.U32, uint32(ENABLE)}}
+}
+
+func (p *Periph) TICKINT() RMCSR {
+	return RMCSR{mmio.UM32{&p.CSR.U32, uint32(TICKINT)}}
+}
+
+func (p *Periph) CLKSOURCE() RMCSR {
+	return RMCSR{mmio.UM32{&p.CSR.U32, uint32(CLKSOURCE)}}
+}
+
+func (p *Periph) COUNTFLAG() RMCSR {
+	return RMCSR{mmio.UM32{&p.CSR.U32, uint32(COUNTFLAG)}}
+}
+
+type RVR uint32
+
+type RRVR struct{ mmio.U32 }
+
+func (r *RRVR) LoadBits(mask RVR) RVR { return RVR(r.U32.LoadBits(uint32(mask))) }
+func (r *RRVR) StoreBits(mask, b RVR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RRVR) SetBits(mask RVR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RRVR) ClearBits(mask RVR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RRVR) Load() RVR             { return RVR(r.U32.Load()) }
+func (r *RRVR) Store(b RVR)           { r.U32.Store(uint32(b)) }
+
+type RMRVR struct{ mmio.UM32 }
+
+func (rm RMRVR) Load() RVR   { return RVR(rm.UM32.Load()) }
+func (rm RMRVR) Store(b RVR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) RELOAD() RMRVR {
+	return RMRVR{mmio.UM32{&p.RVR.U32, uint32(RELOAD)}}
+}
+
+type CVR uint32
+
+type RCVR struct{ mmio.U32 }
+
+func (r *RCVR) LoadBits(mask CVR) CVR { return CVR(r.U32.LoadBits(uint32(mask))) }
+func (r *RCVR) StoreBits(mask, b CVR) { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCVR) SetBits(mask CVR)      { r.U32.SetBits(uint32(mask)) }
+func (r *RCVR) ClearBits(mask CVR)    { r.U32.ClearBits(uint32(mask)) }
+func (r *RCVR) Load() CVR             { return CVR(r.U32.Load()) }
+func (r *RCVR) Store(b CVR)           { r.U32.Store(uint32(b)) }
+
+type RMCVR struct{ mmio.UM32 }
+
+func (rm RMCVR) Load() CVR   { return CVR(rm.UM32.Load()) }
+func (rm RMCVR) Store(b CVR) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) CURRENT() RMCVR {
+	return RMCVR{mmio.UM32{&p.CVR.U32, uint32(CURRENT)}}
+}
+
+type CALIB uint32
+
+type RCALIB struct{ mmio.U32 }
+
+func (r *RCALIB) LoadBits(mask CALIB) CALIB { return CALIB(r.U32.LoadBits(uint32(mask))) }
+func (r *RCALIB) StoreBits(mask, b CALIB)   { r.U32.StoreBits(uint32(mask), uint32(b)) }
+func (r *RCALIB) SetBits(mask CALIB)        { r.U32.SetBits(uint32(mask)) }
+func (r *RCALIB) ClearBits(mask CALIB)      { r.U32.ClearBits(uint32(mask)) }
+func (r *RCALIB) Load() CALIB               { return CALIB(r.U32.Load()) }
+func (r *RCALIB) Store(b CALIB)             { r.U32.Store(uint32(b)) }
+
+type RMCALIB struct{ mmio.UM32 }
+
+func (rm RMCALIB) Load() CALIB   { return CALIB(rm.UM32.Load()) }
+func (rm RMCALIB) Store(b CALIB) { rm.UM32.Store(uint32(b)) }
+
+func (p *Periph) TENMS() RMCALIB {
+	return RMCALIB{mmio.UM32{&p.CALIB.U32, uint32(TENMS)}}
+}
+
+func (p *Periph) SKEW() RMCALIB {
+	return RMCALIB{mmio.UM32{&p.CALIB.U32, uint32(SKEW)}}
+}
+
+func (p *Periph) NOREF() RMCALIB {
+	return RMCALIB{mmio.UM32{&p.CALIB.U32, uint32(NOREF)}}
+}
diff --git a/src/internal/cpu/cpu_arm.go b/src/internal/cpu/cpu_armt.go
similarity index 97%
rename from src/internal/cpu/cpu_arm.go
rename to src/internal/cpu/cpu_armt.go
index 772b67147c..9f1f2ace4c 100644
--- a/src/internal/cpu/cpu_arm.go
+++ b/src/internal/cpu/cpu_armt.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build arm thumb,!noos
+
 package cpu
 
 const CacheLinePadSize = 32
diff --git a/src/internal/cpu/cpu_no_init.go b/src/internal/cpu/cpu_no_init.go
index 777ea9de8b..51ddd8195f 100644
--- a/src/internal/cpu/cpu_no_init.go
+++ b/src/internal/cpu/cpu_no_init.go
@@ -6,6 +6,7 @@
 // +build !amd64
 // +build !amd64p32
 // +build !arm
+// +build !thumb
 // +build !arm64
 // +build !ppc64
 // +build !ppc64le
diff --git a/src/internal/cpu/cpu_noos_thumb.go b/src/internal/cpu/cpu_noos_thumb.go
new file mode 100644
index 0000000000..3b6b0474ea
--- /dev/null
+++ b/src/internal/cpu/cpu_noos_thumb.go
@@ -0,0 +1,9 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package cpu
+
+const CacheLinePadSize = 32 // Cortex-M7
+
+func doinit() {}
diff --git a/src/internal/poll/fd_noos.go b/src/internal/poll/fd_noos.go
new file mode 100644
index 0000000000..059021af14
--- /dev/null
+++ b/src/internal/poll/fd_noos.go
@@ -0,0 +1,15 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package poll
+
+type FD struct {
+	// Lock sysfd and serialize access to Read and Write methods.
+	fdmu fdMutex
+
+	// Whether this is a file rather than a network socket.
+	isFile bool
+}
+
+func (fd *FD) destroy() error { return nil }
diff --git a/src/internal/syscall/unix/at_sysnum_fstatat64_linux.go b/src/internal/syscall/unix/at_sysnum_fstatat64_linux.go
index c6ea206c12..ab2f05e88c 100644
--- a/src/internal/syscall/unix/at_sysnum_fstatat64_linux.go
+++ b/src/internal/syscall/unix/at_sysnum_fstatat64_linux.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build arm mips mipsle 386
+// +build arm thumb mips mipsle 386
 
 package unix
 
diff --git a/src/internal/syscall/unix/getrandom_linux_arm.go b/src/internal/syscall/unix/getrandom_linux_armt.go
similarity index 88%
rename from src/internal/syscall/unix/getrandom_linux_arm.go
rename to src/internal/syscall/unix/getrandom_linux_armt.go
index 92e2492cd0..53c7141dfa 100644
--- a/src/internal/syscall/unix/getrandom_linux_arm.go
+++ b/src/internal/syscall/unix/getrandom_linux_armt.go
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build linux
+// +build arm thumb
+
 package unix
 
 // Linux getrandom system call number.
diff --git a/src/math/big/arith_thumb.s b/src/math/big/arith_thumb.s
new file mode 100644
index 0000000000..1d38b9cf17
--- /dev/null
+++ b/src/math/big/arith_thumb.s
@@ -0,0 +1,294 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !math_big_pure_go
+
+#include "/home/michal/P/go/goroot/src/runtime/textflag.h"
+
+// This file provides fast assembly versions for the elementary
+// arithmetic operations on vectors implemented in arith.go.
+
+// func addVV(z, x, y []Word) (c Word)
+TEXT addVV(SB),NOSPLIT,$0
+	MOVW   z+0(FP), R1
+	MOVW   z_len+4(FP), R4
+	MOVW   x+12(FP), R2
+	MOVW   y+24(FP), R3
+	ADD    R4<<2, R1, R4
+	ADD.S  $0, R0  // clear carry flag
+	B      E1
+L1:
+	MOVW.P  4(R2), R5
+	MOVW.P  4(R3), R6
+	ADC.S   R6, R5
+	MOVW.P  R5, 4(R1)
+E1:
+	TEQ  R1, R4
+	BNE  L1
+
+	MOVW.CC  $0, R0
+	MOVW.CS  $1, R0
+	MOVW     R0, c+36(FP)
+	RET      
+
+
+// func subVV(z, x, y []Word) (c Word)
+// (same as addVV except for SBC instead of ADC and label names)
+TEXT subVV(SB),NOSPLIT,$0
+	MOVW   z+0(FP), R1
+	MOVW   z_len+4(FP), R4
+	MOVW   x+12(FP), R2
+	MOVW   y+24(FP), R3
+	ADD    R4<<2, R1, R4
+	SUB.S  $0, R0  // clear borrow flag
+	B      E2
+L2:
+	MOVW.P  4(R2), R5
+	MOVW.P  4(R3), R6
+	SBC.S   R6, R5
+	MOVW.P  R5, 4(R1)
+E2:
+	TEQ  R1, R4
+	BNE  L2
+
+	MOVW.CS  $0, R0
+	MOVW.CC  $1, R0
+	MOVW     R0, c+36(FP)
+	RET      
+
+
+// func addVW(z, x []Word, y Word) (c Word)
+TEXT addVW(SB),NOSPLIT,$0
+	MOVW  z+0(FP), R1
+	MOVW  z_len+4(FP), R4
+	MOVW  x+12(FP), R2
+	MOVW  y+24(FP), R3
+	ADD   R4<<2, R1, R4
+	TEQ   R1, R4
+	BNE   L3a
+	MOVW  R3, c+28(FP)
+	RET   
+L3a:
+	MOVW.P  4(R2), R5
+	ADD.S   R3, R5
+	MOVW.P  R5, 4(R1)
+	B       E3
+L3:
+	MOVW.P  4(R2), R5
+	ADC.S   $0, R5
+	MOVW.P  R5, 4(R1)
+E3:
+	TEQ  R1, R4
+	BNE  L3
+
+	MOVW.CC  $0, R0
+	MOVW.CS  $1, R0
+	MOVW     R0, c+28(FP)
+	RET      
+
+
+// func subVW(z, x []Word, y Word) (c Word)
+TEXT subVW(SB),NOSPLIT,$0
+	MOVW  z+0(FP), R1
+	MOVW  z_len+4(FP), R4
+	MOVW  x+12(FP), R2
+	MOVW  y+24(FP), R3
+	ADD   R4<<2, R1, R4
+	TEQ   R1, R4
+	BNE   L4a
+	MOVW  R3, c+28(FP)
+	RET   
+L4a:
+	MOVW.P  4(R2), R5
+	SUB.S   R3, R5
+	MOVW.P  R5, 4(R1)
+	B       E4
+L4:
+	MOVW.P  4(R2), R5
+	SBC.S   $0, R5
+	MOVW.P  R5, 4(R1)
+E4:
+	TEQ  R1, R4
+	BNE  L4
+
+	MOVW.CS  $0, R0
+	MOVW.CC  $1, R0
+	MOVW     R0, c+28(FP)
+	RET      
+
+
+// func shlVU(z, x []Word, s uint) (c Word)
+TEXT shlVU(SB),NOSPLIT,$0
+	MOVW  z_len+4(FP), R5
+	TEQ   $0, R5
+	BEQ   X7
+
+	MOVW  z+0(FP), R1
+	MOVW  x+12(FP), R2
+	ADD   R5<<2, R2, R2
+	ADD   R5<<2, R1, R5
+	MOVW  s+24(FP), R3
+	TEQ   $0, R3  // shift 0 is special
+	BEQ   Y7
+	ADD   $4, R1  // stop one word early
+	MOVW  $32, R4
+	SUB   R3, R4
+
+	MOVW.W  -4(R2), R6
+	MOVW    R6<<R3, R7
+	MOVW    R6>>R4, R6
+	MOVW    R6, c+28(FP)
+	B       E7
+
+L7:
+	MOVW.W  -4(R2), R6
+	MOVW    R6>>R4, R0
+	ORR     R0, R7
+	MOVW.W  R7, -4(R5)
+	MOVW    R6<<R3, R7
+E7:
+	TEQ  R1, R5
+	BNE  L7
+
+	MOVW  R7, -4(R5)
+	RET   
+
+Y7: // copy loop, because shift 0 == shift 32
+	MOVW.W  -4(R2), R6
+	MOVW.W  R6, -4(R5)
+	TEQ     R1, R5
+	BNE     Y7
+
+X7:
+	MOVW  $0, R1
+	MOVW  R1, c+28(FP)
+	RET   
+
+
+// func shrVU(z, x []Word, s uint) (c Word)
+TEXT shrVU(SB),NOSPLIT,$0
+	MOVW  z_len+4(FP), R5
+	TEQ   $0, R5
+	BEQ   X6
+
+	MOVW  z+0(FP), R1
+	MOVW  x+12(FP), R2
+	ADD   R5<<2, R1, R5
+	MOVW  s+24(FP), R3
+	TEQ   $0, R3  // shift 0 is special
+	BEQ   Y6
+	SUB   $4, R5  // stop one word early
+	MOVW  $32, R4
+	SUB   R3, R4
+
+	// first word
+	MOVW.P  4(R2), R6
+	MOVW    R6>>R3, R7
+	MOVW    R6<<R4, R6
+	MOVW    R6, c+28(FP)
+	B       E6
+
+	// word loop
+L6:
+	MOVW.P  4(R2), R6
+	MOVW    R6<<R4, R0
+	ORR     R0, R7
+	MOVW.P  R7, 4(R1)
+	MOVW    R6>>R3, R7
+E6:
+	TEQ  R1, R5
+	BNE  L6
+
+	MOVW  R7, 0(R1)
+	RET   
+
+Y6: // copy loop, because shift 0 == shift 32
+	MOVW.P  4(R2), R6
+	MOVW.P  R6, 4(R1)
+	TEQ     R1, R5
+	BNE     Y6
+
+X6:
+	MOVW  $0, R1
+	MOVW  R1, c+28(FP)
+	RET   
+
+
+// func mulAddVWW(z, x []Word, y, r Word) (c Word)
+TEXT mulAddVWW(SB),NOSPLIT,$0
+	MOVW  $0, R0
+	MOVW  z+0(FP), R1
+	MOVW  z_len+4(FP), R5
+	MOVW  x+12(FP), R2
+	MOVW  y+24(FP), R3
+	MOVW  r+28(FP), R4
+	ADD   R5<<2, R1, R5
+	B     E8
+
+	// word loop
+L8:
+	MOVW.P  4(R2), R6
+	MULLU   R6, R3, (R7, R6)
+	ADD.S   R4, R6
+	ADC     R0, R7
+	MOVW.P  R6, 4(R1)
+	MOVW    R7, R4
+E8:
+	TEQ  R1, R5
+	BNE  L8
+
+	MOVW  R4, c+32(FP)
+	RET   
+
+
+// func addMulVVW(z, x []Word, y Word) (c Word)
+TEXT addMulVVW(SB),NOSPLIT,$0
+	MOVW  $0, R0
+	MOVW  z+0(FP), R1
+	MOVW  z_len+4(FP), R5
+	MOVW  x+12(FP), R2
+	MOVW  y+24(FP), R3
+	ADD   R5<<2, R1, R5
+	MOVW  $0, R4
+	B     E9
+
+	// word loop
+L9:
+	MOVW.P  4(R2), R6
+	MULLU   R6, R3, (R7, R6)
+	ADD.S   R4, R6
+	ADC     R0, R7
+	MOVW    0(R1), R4
+	ADD.S   R4, R6
+	ADC     R0, R7
+	MOVW.P  R6, 4(R1)
+	MOVW    R7, R4
+E9:
+	TEQ  R1, R5
+	BNE  L9
+
+	MOVW  R4, c+28(FP)
+	RET   
+
+
+// func divWVW(z* Word, xn Word, x []Word, y Word) (r Word)
+TEXT divWVW(SB),NOSPLIT,$0
+	// ARM has no multiword division, so use portable code.
+	B   divWVW_g(SB)
+
+
+// func divWW(x1, x0, y Word) (q, r Word)
+TEXT divWW(SB),NOSPLIT,$0
+	// ARM has no multiword division, so use portable code.
+	B   divWW_g(SB)
+
+
+// func mulWW(x, y Word) (z1, z0 Word)
+TEXT mulWW(SB),NOSPLIT,$0
+	MOVW   x+0(FP), R1
+	MOVW   y+4(FP), R2
+	MULLU  R1, R2, (R4, R3)
+	MOVW   R4, z1+8(FP)
+	MOVW   R3, z0+12(FP)
+	RET    
diff --git a/src/math/sqrt_thumb.s b/src/math/sqrt_thumb.s
new file mode 100644
index 0000000000..b3fd31250e
--- /dev/null
+++ b/src/math/sqrt_thumb.s
@@ -0,0 +1,22 @@
+// Copyright 2011 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+#define REGTMP R7
+
+// func Sqrt(x float64) float64
+TEXT Sqrt(SB),NOSPLIT,$0
+	MOVB	runtimegoarm(SB), REGTMP
+	CMP	$0x7D, REGTMP
+	BNE	softfloat
+	MOVD	x+0(FP),F0
+	SQRTD	F0,F0
+	MOVD	F0,ret+8(FP)
+	RET
+softfloat:
+	// Tail call to Go implementation.
+	// Can't use JMP, as in softfloat mode SQRTD is rewritten
+	// to a CALL, which makes this function have a frame.
+	RET	sqrt(SB)
diff --git a/src/math/stubs_thumb.s b/src/math/stubs_thumb.s
new file mode 100644
index 0000000000..31bf872e43
--- /dev/null
+++ b/src/math/stubs_thumb.s
@@ -0,0 +1,110 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT Acos(SB), NOSPLIT, $0
+	B acos(SB)
+
+TEXT Acosh(SB), NOSPLIT, $0
+	B acosh(SB)
+
+TEXT Asin(SB), NOSPLIT, $0
+	B asin(SB)
+
+TEXT Asinh(SB), NOSPLIT, $0
+	B asinh(SB)
+
+TEXT Atan(SB), NOSPLIT, $0
+	B atan(SB)
+
+TEXT Atan2(SB), NOSPLIT, $0
+	B atan2(SB)
+
+TEXT Atanh(SB), NOSPLIT, $0
+	B atanh(SB)
+
+TEXT Cbrt(SB), NOSPLIT, $0
+	B cbrt(SB)
+
+TEXT Cos(SB), NOSPLIT, $0
+	B cos(SB)
+
+TEXT Cosh(SB), NOSPLIT, $0
+	B cosh(SB)
+
+TEXT Erf(SB), NOSPLIT, $0
+	B erf(SB)
+
+TEXT Erfc(SB), NOSPLIT, $0
+	B erfc(SB)
+
+TEXT Exp2(SB), NOSPLIT, $0
+	B exp2(SB)
+
+TEXT Exp(SB), NOSPLIT, $0
+	B exp(SB)
+
+TEXT Expm1(SB), NOSPLIT, $0
+	B expm1(SB)
+
+TEXT Floor(SB), NOSPLIT, $0
+	B floor(SB)
+
+TEXT Ceil(SB), NOSPLIT, $0
+	B ceil(SB)
+
+TEXT Trunc(SB), NOSPLIT, $0
+	B trunc(SB)
+
+TEXT Frexp(SB), NOSPLIT, $0
+	B frexp(SB)
+
+TEXT Hypot(SB), NOSPLIT, $0
+	B hypot(SB)
+
+TEXT Ldexp(SB), NOSPLIT, $0
+	B ldexp(SB)
+
+TEXT Log10(SB), NOSPLIT, $0
+	B log10(SB)
+
+TEXT Log2(SB), NOSPLIT, $0
+	B log2(SB)
+
+TEXT Log1p(SB), NOSPLIT, $0
+	B log1p(SB)
+
+TEXT Log(SB), NOSPLIT, $0
+	B log(SB)
+
+TEXT Max(SB), NOSPLIT, $0
+	B max(SB)
+
+TEXT Min(SB), NOSPLIT, $0
+	B min(SB)
+
+TEXT Mod(SB), NOSPLIT, $0
+	B mod(SB)
+
+TEXT Modf(SB), NOSPLIT, $0
+	B modf(SB)
+
+TEXT Pow(SB), NOSPLIT, $0
+	JMP pow(SB)
+
+TEXT Remainder(SB), NOSPLIT, $0
+	B remainder(SB)
+
+TEXT Sin(SB), NOSPLIT, $0
+	B sin(SB)
+
+TEXT Sinh(SB), NOSPLIT, $0
+	B sinh(SB)
+
+TEXT Tan(SB), NOSPLIT, $0
+	B tan(SB)
+
+TEXT Tanh(SB), NOSPLIT, $0
+	B tanh(SB)
diff --git a/src/os/dir_noos.go b/src/os/dir_noos.go
new file mode 100644
index 0000000000..2f74995f27
--- /dev/null
+++ b/src/os/dir_noos.go
@@ -0,0 +1,13 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import "syscall"
+
+type dirInfo struct{}
+
+func (f *File) readdirnames(n int) (names []string, err error) {
+	return nil, syscall.ENOTSUP
+}
diff --git a/src/os/exec_noos.go b/src/os/exec_noos.go
new file mode 100644
index 0000000000..1ddd762514
--- /dev/null
+++ b/src/os/exec_noos.go
@@ -0,0 +1,34 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import (
+	"syscall"
+	"time"
+)
+
+type ProcessState struct{}
+
+func findProcess(pid int) (p *Process, err error) {
+	return &Process{Pid: pid}, nil
+}
+
+func startProcess(name string, argv []string, attr *ProcAttr) (p *Process, err error) {
+	return nil, &PathError{"fork/exec", name, syscall.ENOTSUP}
+}
+
+func (p *Process) release() error               { return syscall.ENOTSUP }
+func (p *Process) kill() error                  { return syscall.ENOTSUP }
+func (p *Process) wait() (*ProcessState, error) { return nil, syscall.ENOTSUP }
+func (p *Process) signal(sig Signal) error      { return syscall.ENOTSUP }
+
+func (p *ProcessState) userTime() time.Duration   { return 0 }
+func (p *ProcessState) systemTime() time.Duration { return 0 }
+func (p *ProcessState) exited() bool              { return false }
+func (p *ProcessState) success() bool             { return false }
+func (p *ProcessState) sys() interface{}          { return nil }
+func (p *ProcessState) sysUsage() interface{}     { return nil }
+
+func executable() (string, error) { return "", ErrNotExist }
diff --git a/src/os/file_noos.go b/src/os/file_noos.go
new file mode 100644
index 0000000000..81cbb5dfe3
--- /dev/null
+++ b/src/os/file_noos.go
@@ -0,0 +1,131 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import (
+	"syscall"
+	"time"
+)
+
+type file struct {
+	name       string
+	dirinfo    *dirInfo // nil unless directory being read
+	appendMode bool
+}
+
+func NewFile(fd uintptr, name string) *File {
+	return nil
+}
+
+func (f *File) readdir(n int) (fi []FileInfo, err error) {
+	return nil, syscall.EINVAL
+}
+
+func (f *File) checkValid(op string) error {
+	if f == nil {
+		return ErrInvalid
+	}
+	return nil
+}
+
+func (f *File) read(b []byte) (n int, err error) {
+	return 0, f.wrapErr("read", syscall.ENOTSUP)
+}
+
+func (f *File) pread(b []byte, off int64) (n int, err error) {
+	return 0, f.wrapErr("pread", syscall.ENOTSUP)
+}
+
+func (f *File) write(b []byte) (n int, err error) {
+	return 0, f.wrapErr("write", syscall.ENOTSUP)
+}
+
+func (f *File) pwrite(b []byte, off int64) (n int, err error) {
+	return 0, f.wrapErr("pwrite", syscall.ENOTSUP)
+}
+
+func (f *File) seek(offset int64, whence int) (ret int64, err error) {
+	return 0, f.wrapErr("seek", syscall.ENOTSUP)
+}
+
+// See docs in file.go:(*File).Chmod.
+func (f *File) chmod(mode FileMode) error {
+	return f.wrapErr("chmod", syscall.ENOTSUP)
+}
+
+func (f *File) setDeadline(t time.Time) error {
+	return f.wrapErr("setDeadline", syscall.ENOTSUP)
+}
+
+func (f *File) setReadDeadline(t time.Time) error {
+	return f.wrapErr("setDeadline", syscall.ENOTSUP)
+}
+
+func (f *File) setWriteDeadline(t time.Time) error {
+	return f.wrapErr("setDeadline", syscall.ENOTSUP)
+}
+
+func (f *File) Close() error {
+	return f.wrapErr("close", syscall.ENOTSUP)
+}
+
+func (f *File) Stat() (FileInfo, error) {
+	return nil, f.wrapErr("setDeadline", syscall.ENOTSUP)
+
+}
+
+func epipecheck(file *File, e error) {}
+
+// fixLongPath is a noop on non-Windows platforms.
+func fixLongPath(path string) string {
+	return path
+}
+
+func syscallMode(i FileMode) uint32 {
+	return 0
+}
+
+func tempDir() string {
+	return "/tmp"
+}
+
+func openFileNolog(name string, flag int, perm FileMode) (*File, error) {
+	return nil, &PathError{"open", name, syscall.ENOTSUP}
+}
+
+func rename(oldname, newname string) error {
+	return &PathError{"rename", oldname, syscall.ENOTSUP}
+}
+
+// See docs in file.go:Chmod.
+func chmod(name string, mode FileMode) error {
+	return &PathError{"chmod", name, syscall.ENOTSUP}
+}
+
+func Remove(name string) error {
+	return &PathError{"remove", name, syscall.ENOTSUP}
+}
+
+type rawConn struct{}
+
+func (c *rawConn) Control(f func(uintptr)) error {
+	return syscall.ENOTSUP
+}
+
+func (c *rawConn) Read(f func(uintptr) bool) error {
+	return syscall.ENOTSUP
+}
+
+func (c *rawConn) Write(f func(uintptr) bool) error {
+	return syscall.ENOTSUP
+}
+
+func newRawConn(file *File) (*rawConn, error) {
+	return nil, syscall.ENOTSUP
+}
+
+func hostname() (name string, err error) {
+	return "", syscall.ENOTSUP
+}
diff --git a/src/os/path_unix.go b/src/os/path_unix.go
index df423d2c9d..f0f3b784e6 100644
--- a/src/os/path_unix.go
+++ b/src/os/path_unix.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build aix darwin dragonfly freebsd js,wasm linux nacl netbsd openbsd solaris
+// +build aix darwin dragonfly freebsd js,wasm linux nacl netbsd noos openbsd solaris
 
 package os
 
diff --git a/src/os/rawconn.go b/src/os/rawconn.go
index 9e11cda8c9..9d8f4dc501 100644
--- a/src/os/rawconn.go
+++ b/src/os/rawconn.go
@@ -3,6 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build !plan9
+// +build !noos
 
 package os
 
diff --git a/src/os/stat_noos.go b/src/os/stat_noos.go
new file mode 100644
index 0000000000..bde6b353e3
--- /dev/null
+++ b/src/os/stat_noos.go
@@ -0,0 +1,15 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import "syscall"
+
+func statNolog(name string) (FileInfo, error) {
+	return nil, &PathError{"stat", name, syscall.ENOTSUP}
+}
+
+func lstatNolog(name string) (FileInfo, error) {
+	return nil, &PathError{"lstat", name, syscall.ENOTSUP}
+}
diff --git a/src/os/types_noos.go b/src/os/types_noos.go
new file mode 100644
index 0000000000..51089d8cd8
--- /dev/null
+++ b/src/os/types_noos.go
@@ -0,0 +1,31 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package os
+
+import (
+	"syscall"
+	"time"
+)
+
+type fileStat struct {
+	name    string
+	size    int64
+	mode    FileMode
+	modTime time.Time
+	sys     interface{}
+}
+
+func (fs *fileStat) Size() int64        { return fs.size }
+func (fs *fileStat) Mode() FileMode     { return fs.mode }
+func (fs *fileStat) ModTime() time.Time { return fs.modTime }
+func (fs *fileStat) Sys() interface{}   { return fs.sys }
+
+func sameFile(fs1, fs2 *fileStat) bool {
+	a := fs1.sys.(*syscall.Dir)
+	b := fs2.sys.(*syscall.Dir)
+	return a.Qid.Path == b.Qid.Path && a.Type == b.Type && a.Dev == b.Dev
+}
+
+const badFd = -1
diff --git a/src/os/types_unix.go b/src/os/types_unix.go
index c0259ae0e8..8a595aaa98 100644
--- a/src/os/types_unix.go
+++ b/src/os/types_unix.go
@@ -4,6 +4,7 @@
 
 // +build !windows
 // +build !plan9
+// +build !noos
 
 package os
 
diff --git a/src/reflect/asm_thumb.s b/src/reflect/asm_thumb.s
new file mode 100644
index 0000000000..16b4b45d97
--- /dev/null
+++ b/src/reflect/asm_thumb.s
@@ -0,0 +1,40 @@
+// Copyright 2012 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "funcdata.h"
+
+#define REGCTXT R11
+
+// makeFuncStub is jumped to by the code generated by MakeFunc.
+// See the comment on the declaration of makeFuncStub in makefunc.go
+// for more details.
+// No argsize here, gc generates argsize info at call site.
+TEXT makeFuncStub(SB),(NOSPLIT|WRAPPER),$16
+	NO_LOCAL_POINTERS
+	MOVW	REGCTXT, 4(R13)
+	MOVW	$argframe+0(FP), R1
+	MOVW	R1, 8(R13)
+	MOVW	$0, R1
+	MOVB	R1, 16(R13)
+	ADD	$16, R13, R1
+	MOVW	R1, 12(R13)
+	BL	callReflect(SB)
+	RET
+
+// methodValueCall is the code half of the function returned by makeMethodValue.
+// See the comment on the declaration of methodValueCall in makefunc.go
+// for more details.
+// No argsize here, gc generates argsize info at call site.
+TEXT methodValueCall(SB),(NOSPLIT|WRAPPER),$16
+	NO_LOCAL_POINTERS
+	MOVW	REGCTXT, 4(R13)
+	MOVW	$argframe+0(FP), R1
+	MOVW	R1, 8(R13)
+	MOVW	$0, R1
+	MOVB	R1, 16(R13)
+	ADD	$16, R13, R1
+	MOVW	R1, 12(R13)
+	BL	callMethod(SB)
+	RET
diff --git a/src/runtime/asm_thumb.s b/src/runtime/asm_thumb.s
new file mode 100644
index 0000000000..786660507d
--- /dev/null
+++ b/src/runtime/asm_thumb.s
@@ -0,0 +1,776 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "funcdata.h"
+#include "textflag.h"
+
+// using NOFRAME means do not save LR on stack.
+// argc is in R0, argv is in R1.
+TEXT runtimert0_go(SB),NOSPLIT|NOFRAME,$0
+	MOVW  $0xcafebabe, R12
+
+	// copy arguments forward on an even stack
+	// use R13 instead of SP to avoid linker rewriting the offsets
+	SUB   $64, R13  // plenty of scratch
+	AND   $~7, R13
+	MOVW  R0, 60(R13)  // save argc, argv away
+	MOVW  R1, 64(R13)
+
+	// set up g register
+	// g is R10
+	MOVW  $runtimeg0(SB), g
+	MOVW  $runtimem0(SB), R8
+
+	// save m->g0 = g0
+	MOVW  g, m_g0(R8)
+	// save g->m = m0
+	MOVW  R8, g_m(g)
+
+	// create istack out of the OS stack
+	// (1MB of system stack is available on iOS and Android)
+	MOVW  $(-64*1024+104)(R13), R0
+	MOVW  R0, (g_stack+stack_lo)(g)
+	MOVW  R13, (g_stack+stack_hi)(g)
+	ADD   $const__StackGuard, R0
+	MOVW  R0, g_stackguard0(g)
+	MOVW  R0, g_stackguard1(g)
+
+	BL  runtimeemptyfunc(SB)  // fault if stack check is wrong
+
+	BL  runtimecheck(SB)
+
+	// saved argc, argv
+	MOVW  60(R13), R0
+	MOVW  R0, 4(R13)
+	MOVW  64(R13), R1
+	MOVW  R1, 8(R13)
+	BL    runtimeargs(SB)
+	BL    runtimecheckgoarm(SB)
+	BL    runtimeosinit(SB)
+	BL    runtimeschedinit(SB)
+
+	// create a new goroutine to start program
+	MOVW    $runtimemainPC(SB), R0
+	MOVW.W  R0, -4(R13)
+	MOVW    $8, R0
+	MOVW.W  R0, -4(R13)
+	MOVW    $0, R0
+	MOVW.W  R0, -4(R13)  // push $0 as guard
+	BL      runtimenewproc(SB)
+	MOVW    $12(R13), R13  // pop args and LR
+
+	// start this M
+	BL  runtimemstart(SB)
+
+	MOVW  $1234, R0
+	MOVW  $1000, R1
+	MOVW  R0, (R1)  // fail hard
+
+DATA runtimemainPC+0(SB)/4,$runtimemain(SB)
+GLOBL runtimemainPC(SB),RODATA,$4
+
+TEXT runtimebreakpoint(SB),NOSPLIT,$0-0
+	// gdb won't skip this breakpoint instruction automatically,
+	// so you must manually "set $pc+=4" to skip it and continue.
+	UNDEF  $1  // undefined instruction that gdb understands is a software breakpoint
+	//WORD 0xA000F7F0 // T32 UNDEF $0
+	RET
+
+TEXT runtimeasminit(SB),NOSPLIT,$0-0
+	// disable runfast (flush-to-zero) mode of vfp if runtime.goarm > 5
+	// MOVB	runtimegoarm(SB), REGTMP
+	// CMP	$5, REGTMP
+	// BLE	4(PC)
+	// WORD	$0xeef1ba10	// vmrs REGTMP, fpscr
+	// BIC	$(1<<24), REGTMP
+	// WORD	$0xeee1ba10	// vmsr fpscr, REGTMP
+	RET
+
+/*
+	*   go-routine
+	*/
+
+// void gosave(Gobuf*)
+// save state in Gobuf; setjmp
+TEXT runtimegosave(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  buf+0(FP), R0
+	MOVW  R13, gobuf_sp(R0)
+	MOVW  LR, gobuf_pc(R0)
+	MOVW  g, gobuf_g(R0)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, gobuf_lr(R0)
+	MOVW  REGTMP, gobuf_ret(R0)
+	// Assert ctxt is zero. See func save.
+	MOVW  gobuf_ctxt(R0), R0
+	CMP   R0, REGTMP
+	BEQ   2(PC)
+	CALL  runtimebadctxt(SB)
+	RET
+
+TEXT asmcgocall(SB),NOSPLIT,$0-12
+	BKPT
+	B   -1(PC)
+
+// void gogo(Gobuf*)
+// restore state from Gobuf; longjmp
+TEXT runtimegogo(SB),NOSPLIT,$8-4
+	MOVW  buf+0(FP), R1
+	MOVW  gobuf_g(R1), R0
+	BL    setg<>(SB)
+
+	// NOTE: We updated g above, and we are about to update SP.
+	// Until LR and PC are also updated, the g/SP/LR/PC quadruple
+	// are out of sync and must not be used as the basis of a traceback.
+	// Sigprof skips the traceback when SP is not within g's bounds,
+	// and when the PC is inside this function, runtime.gogo.
+	// Since we are about to update SP, until we complete runtime.gogo
+	// we must not leave this function. In particular, no calls
+	// after this point: it must be straight-line code until the
+	// final B instruction.
+	// See large comment in sigprof for more details.
+	MOVW  gobuf_sp(R1), R13  // restore SP==R13
+	MOVW  gobuf_lr(R1), LR
+	MOVW  gobuf_ret(R1), R0
+	MOVW  gobuf_ctxt(R1), REGCTXT
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, gobuf_sp(R1)  // clear to help garbage collector
+	MOVW  REGTMP, gobuf_ret(R1)
+	MOVW  REGTMP, gobuf_lr(R1)
+	MOVW  REGTMP, gobuf_ctxt(R1)
+	MOVW  gobuf_pc(R1), REGTMP
+	CMP   REGTMP, REGTMP  // set condition codes for == test, needed by stack split
+	B     (REGTMP)
+
+// func mcall(fn func(*g))
+// Switch to m->g0's stack, call fn(g).
+// Fn must never return. It should gogo(&g->sched)
+// to keep running g.
+TEXT runtimemcall(SB),NOSPLIT|NOFRAME,$0-4
+	// Save caller state in g->sched.
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, (g_sched+gobuf_lr)(g)
+	MOVW  g, (g_sched+gobuf_g)(g)
+
+	// Switch to m->g0 & its stack, call fn.
+	MOVW  g, R1
+	MOVW  g_m(g), R8
+	MOVW  m_g0(R8), R0
+	BL    setg<>(SB)
+	CMP   g, R1
+	B.NE  2(PC)
+	B     runtimebadmcall(SB)
+	MOVW  fn+0(FP), R0
+	MOVW  (g_sched+gobuf_sp)(g), R13
+	SUB   $8, R13
+	MOVW  R1, 4(R13)
+	MOVW  R0, REGCTXT
+	MOVW  0(R0), R0
+	BL    (R0)
+	B     runtimebadmcall2(SB)
+	RET
+
+// systemstack_switch is a dummy routine that systemstack leaves at the bottom
+// of the G stack. We need to distinguish the routine that
+// lives at the bottom of the G stack from the one that lives
+// at the top of the system stack because the one at the top of
+// the system stack terminates the stack walk (see topofstack()).
+TEXT runtimesystemstack_switch(SB),NOSPLIT,$0-0
+	MOVW  $0, R0
+	BL    (R0)  // clobber lr to ensure push {lr} is kept
+	RET
+
+// func systemstack(fn func())
+TEXT runtimesystemstack(SB),NOSPLIT,$0-4
+	MOVW  fn+0(FP), R0  // R0 = fn
+	MOVW  g_m(g), R1    // R1 = m
+
+	MOVW  m_gsignal(R1), R2  // R2 = gsignal
+	CMP   g, R2
+	B.EQ  noswitch
+
+	MOVW  m_g0(R1), R2  // R2 = g0
+	CMP   g, R2
+	B.EQ  noswitch
+
+	MOVW  m_curg(R1), R3
+	CMP   g, R3
+	B.EQ  switch
+
+	// Bad: g is not gsignal, not g0, not curg. What is it?
+	// Hide call from linker nosplit analysis.
+	MOVW  $runtimebadsystemstack(SB), R0
+	BL    (R0)
+	B     runtimeabort(SB)
+
+switch:
+	// save our state in g->sched. Pretend to
+	// be systemstack_switch if the G stack is scanned.
+	MOVW  $runtimesystemstack_switch(SB), R3
+#ifdef GOOS_nacl
+	ADD  $4, R3, R3  // get past nacl-insert bic instruction
+#endif
+	ADD   $4, R3, R3  // get past push {lr}
+	MOVW  R3, (g_sched+gobuf_pc)(g)
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_lr)(g)
+	MOVW  g, (g_sched+gobuf_g)(g)
+
+	// switch to g0
+	MOVW  R0, R5
+	MOVW  R2, R0
+	BL    setg<>(SB)
+	MOVW  R5, R0
+	MOVW  (g_sched+gobuf_sp)(R2), R3
+	// make it look like mstart called systemstack on g0, to stop traceback
+	SUB   $4, R3, R3
+	MOVW  $runtimemstart(SB), R4
+	MOVW  R4, 0(R3)
+	MOVW  R3, R13
+
+	// call target function
+	MOVW  R0, REGCTXT
+	MOVW  0(R0), R0
+	BL    (R0)
+
+	// switch back to g
+	MOVW  g_m(g), R1
+	MOVW  m_curg(R1), R0
+	BL    setg<>(SB)
+	MOVW  (g_sched+gobuf_sp)(g), R13
+	MOVW  $0, R3
+	MOVW  R3, (g_sched+gobuf_sp)(g)
+	RET
+
+noswitch:
+	// Using a tail call here cleans up tracebacks since we won't stop
+	// at an intermediate systemstack.
+	MOVW    R0, REGCTXT
+	MOVW    0(R0), R0
+	MOVW.P  4(R13), R14  // restore LR
+	B       (R0)
+
+/*
+	*   support for morestack
+	*/
+
+// Called during function prolog when more stack is needed.
+// R3 prolog's LR
+// using NOFRAME means do not save LR on stack.
+
+// The traceback routines see morestack on a g0 as being
+// the top of a stack (for example, morestack calling newstack
+// calling the scheduler calling newm calling gc), so we must
+// record an argument size. For that purpose, it has no arguments.
+TEXT runtimemorestack(SB),NOSPLIT|NOFRAME,$0-0
+	// Cannot grow scheduler stack (m->g0).
+	MOVW  g_m(g), R8
+	MOVW  m_g0(R8), R4
+	CMP   g, R4
+	BNE   3(PC)
+	BL    runtimebadmorestackg0(SB)
+	B     runtimeabort(SB)
+
+	// Cannot grow signal stack (m->gsignal).
+	MOVW  m_gsignal(R8), R4
+	CMP   g, R4
+	BNE   3(PC)
+	BL    runtimebadmorestackgsignal(SB)
+	B     runtimeabort(SB)
+
+	// Called from f.
+	// Set g->sched to context in f.
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  R3, (g_sched+gobuf_lr)(g)
+	MOVW  REGCTXT, (g_sched+gobuf_ctxt)(g)
+
+	// Called from f.
+	// Set m->morebuf to f's caller.
+	MOVW  R3, (m_morebuf+gobuf_pc)(R8)   // f's caller's PC
+	MOVW  R13, (m_morebuf+gobuf_sp)(R8)  // f's caller's SP
+	MOVW  g, (m_morebuf+gobuf_g)(R8)
+
+	// Call newstack on m->g0's stack.
+	MOVW    m_g0(R8), R0
+	BL      setg<>(SB)
+	MOVW    (g_sched+gobuf_sp)(g), R13
+	MOVW    $0, R0
+	MOVW.W  R0, -4(R13)  // create a call frame on g0 (saved LR)
+	BL      runtimenewstack(SB)
+
+	// Not reached, but make sure the return PC from the call to newstack
+	// is still in this function, and not the beginning of the next.
+	RET
+
+TEXT runtimemorestack_noctxt(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, REGCTXT
+	B     runtimemorestack(SB)
+
+// reflectcall: call a function with the given argument list
+// func call(argtype *_type, f *FuncVal, arg *byte, argsize, retoffset uint32).
+// we don't have variable-sized frames, so we use a small number
+// of constant-sized-frame functions to encode a few bits of size in the pc.
+// Caution: ugly multiline assembly macros in your future!
+
+#define DISPATCH(NAME,MAXSIZE) \
+	CMP   $MAXSIZE, R0; \
+	B.HI  3(PC); \
+	MOVW  $NAME(SB), R1; \
+	B     (R1)
+
+TEXT reflectcall(SB),NOSPLIT|NOFRAME,$0-20
+	MOVW                              argsize+12(FP), R0
+	DISPATCH(runtimecall16,          16)
+	DISPATCH(runtimecall32,          32)
+	DISPATCH(runtimecall64,          64)
+	DISPATCH(runtimecall128,         128)
+	DISPATCH(runtimecall256,         256)
+	DISPATCH(runtimecall512,         512)
+	DISPATCH(runtimecall1024,        1024)
+	DISPATCH(runtimecall2048,        2048)
+	DISPATCH(runtimecall4096,        4096)
+	DISPATCH(runtimecall8192,        8192)
+	DISPATCH(runtimecall16384,       16384)
+	DISPATCH(runtimecall32768,       32768)
+	DISPATCH(runtimecall65536,       65536)
+	DISPATCH(runtimecall131072,      131072)
+	DISPATCH(runtimecall262144,      262144)
+	DISPATCH(runtimecall524288,      524288)
+	DISPATCH(runtimecall1048576,     1048576)
+	DISPATCH(runtimecall2097152,     2097152)
+	DISPATCH(runtimecall4194304,     4194304)
+	DISPATCH(runtimecall8388608,     8388608)
+	DISPATCH(runtimecall16777216,    16777216)
+	DISPATCH(runtimecall33554432,    33554432)
+	DISPATCH(runtimecall67108864,    67108864)
+	DISPATCH(runtimecall134217728,   134217728)
+	DISPATCH(runtimecall268435456,   268435456)
+	DISPATCH(runtimecall536870912,   536870912)
+	DISPATCH(runtimecall1073741824,  1073741824)
+	MOVW                              $runtimebadreflectcall(SB), R1
+	B                                 (R1)
+
+#define CALLFN(NAME,MAXSIZE) \
+TEXT NAME(SB), WRAPPER, $MAXSIZE-20; \
+	NO_LOCAL_POINTERS;  \
+/*                  copy arguments to stack */ \
+	MOVW     argptr+8(FP), R0; \
+	MOVW     argsize+12(FP), R2; \
+	ADD      $4, R13, R1; \
+	CMP      $0, R2; \
+	B.EQ     5(PC); \
+	MOVBU.P  1(R0), R5; \
+	MOVB.P   R5, 1(R1); \
+	SUB      $1, R2, R2; \
+	B        -5(PC); \
+/*                  call function */ \
+	MOVW    f+4(FP), REGCTXT; \
+	MOVW    (REGCTXT), R0; \
+	PCDATA  $PCDATA_StackMapIndex, $0; \
+	BL      (R0); \
+/*                  copy return values back */ \
+	MOVW  argtype+0(FP), R4; \
+	MOVW  argptr+8(FP), R0; \
+	MOVW  argsize+12(FP), R2; \
+	MOVW  retoffset+16(FP), R3; \
+	ADD   $4, R13, R1; \
+	ADD   R3, R1; \
+	ADD   R3, R0; \
+	SUB   R3, R2; \
+	BL    callRet<>(SB); \
+	RET
+
+// callRet copies return values back at the end of call*. This is a
+// separate function so it can allocate stack space for the arguments
+// to reflectcallmove. It does not follow the Go ABI; it expects its
+// arguments in registers.
+TEXT callRet<>(SB), NOSPLIT, $16-0
+	MOVW  R4, 4(R13)
+	MOVW  R0, 8(R13)
+	MOVW  R1, 12(R13)
+	MOVW  R2, 16(R13)
+	BL    runtimereflectcallmove(SB)
+	RET
+
+	CALLFN(call16,          16)
+	CALLFN(call32,          32)
+	CALLFN(call64,          64)
+	CALLFN(call128,         128)
+	CALLFN(call256,         256)
+	CALLFN(call512,         512)
+	CALLFN(call1024,        1024)
+	CALLFN(call2048,        2048)
+	CALLFN(call4096,        4096)
+	CALLFN(call8192,        8192)
+	CALLFN(call16384,       16384)
+	CALLFN(call32768,       32768)
+	CALLFN(call65536,       65536)
+	CALLFN(call131072,      131072)
+	CALLFN(call262144,      262144)
+	CALLFN(call524288,      524288)
+	CALLFN(call1048576,     1048576)
+	CALLFN(call2097152,     2097152)
+	CALLFN(call4194304,     4194304)
+	CALLFN(call8388608,     8388608)
+	CALLFN(call16777216,    16777216)
+	CALLFN(call33554432,    33554432)
+	CALLFN(call67108864,    67108864)
+	CALLFN(call134217728,   134217728)
+	CALLFN(call268435456,   268435456)
+	CALLFN(call536870912,   536870912)
+	CALLFN(call1073741824,  1073741824)
+
+// void jmpdefer(fn, sp);
+// called from deferreturn.
+// 1. grab stored LR for caller
+// 2. sub 4 bytes to get back to BL deferreturn
+// 3. B to fn
+// TODO(rsc): Push things on stack and then use pop
+// to load all registers simultaneously, so that a profiling
+// interrupt can never see mismatched SP/LR/PC.
+// (And double-check that pop is atomic in that way.)
+TEXT runtimejmpdefer(SB),NOSPLIT,$0-8
+	MOVW  0(R13), LR
+	MOVW  $-4(LR), LR  // BL deferreturn
+	MOVW  fv+0(FP), REGCTXT
+	MOVW  argp+4(FP), R13
+	MOVW  $-4(R13), R13  // SP is 4 below argp, due to saved LR
+	MOVW  0(REGCTXT), R1
+	B     (R1)
+
+// Save state of caller into g->sched. Smashes REGTMP.
+TEXT gosave<>(SB),NOSPLIT|NOFRAME,$0
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, (g_sched+gobuf_lr)(g)
+	MOVW  REGTMP, (g_sched+gobuf_ret)(g)
+	MOVW  REGTMP, (g_sched+gobuf_ctxt)(g)
+	// Assert ctxt is zero. See func save.
+	MOVW  (g_sched+gobuf_ctxt)(g), REGTMP
+	CMP   $0, REGTMP
+	B.EQ  2(PC)
+	CALL  runtimebadctxt(SB)
+	RET
+
+// void setg(G*); set g. for use by needm.
+TEXT runtimesetg(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  gg+0(FP), R0
+	B     setg<>(SB)
+
+TEXT setg<>(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  R0, g
+	RET
+
+TEXT runtimeemptyfunc(SB),0,$0-0
+	RET
+
+TEXT runtimeabort(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW     $1, R0
+	MOVM.IA  (R0), [R0,R1]  // fails even in the absence or not configured MMU/MPU
+
+// AES hashing not implemented for ARM
+TEXT runtimeaeshash(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+TEXT runtimeaeshash32(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+TEXT runtimeaeshash64(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+TEXT runtimeaeshashstr(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+
+TEXT runtimereturn0(SB),NOSPLIT,$0
+	MOVW  $0, R0
+	RET
+
+TEXT runtimeprocyield(SB),NOSPLIT|NOFRAME,$0
+	MOVW  cycles+0(FP), R1
+yieldloop:
+	YIELD
+	CMP  $0, R1
+	RET.EQ
+	SUB  $1, R1
+	B    yieldloop
+
+// Called from cgo wrappers, this function returns g->m->curg.stack.hi.
+// Must obey the gcc calling convention.
+TEXT _cgo_topofstack(SB),NOSPLIT,$8
+	// R11 (REGCTXT) and g register are clobbered by load_g. They are
+	// callee-save in the gcc calling convention, so save them here.
+	MOVW  REGCTXT, saveR11-4(SP)
+	MOVW  g, saveG-8(SP)
+
+	BL    runtimeload_g(SB)
+	MOVW  g_m(g), R0
+	MOVW  m_curg(R0), R0
+	MOVW  (g_stack+stack_hi)(R0), R0
+
+	MOVW  saveG-8(SP), g
+	MOVW  saveR11-4(SP), REGCTXT
+	RET
+
+// The top-most function running on a goroutine
+// returns to goexit+PCQuantum.
+TEXT runtimegoexit(SB),NOSPLIT|NOFRAME|TOPFRAME,$0-0
+	NOP2
+	BL  runtimegoexit1(SB)  // does not return
+	// traceback from goexit1 must hit code range of goexit
+	NOP2
+
+// x -> x/1000000, x%1000000, called from Go with args, results on stack.
+TEXT runtimeusplit(SB),NOSPLIT,$0-12
+	MOVW  x+0(FP), R0
+	CALL  runtimeusplitR0(SB)
+	MOVW  R0, q+4(FP)
+	MOVW  R1, r+8(FP)
+	RET
+
+// R0, R1 = R0/1000000, R0%1000000
+TEXT runtimeusplitR0(SB),NOSPLIT,$0
+	// magic multiply to avoid software divide without available m.
+	// see output of go tool compile -S for x/1000000.
+	MOVW   R0, R3
+	MOVW   $1125899907, R1
+	MULLU  R1, R0, (R0, R1)
+	MOVW   R0>>18, R0
+	MOVW   $1000000, R1
+	MUL    R0, R1
+	SUB    R1, R3, R1
+	RET
+
+// This is called from .init_array and follows the platform, not Go, ABI.
+TEXT runtimeaddmoduledata(SB),NOSPLIT,$0-0
+	MOVW  R9, saver9-4(SP)        // The access to global variables below implicitly uses R9, which is callee-save
+	MOVW  REGCTXT, saver11-8(SP)  // Likewise, R11 is the REGCTXT register, but callee-save in C ABI
+	MOVW  runtimelastmoduledatap(SB), R1
+	MOVW  R0, moduledata_next(R1)
+	MOVW  R0, runtimelastmoduledatap(SB)
+	MOVW  saver11-8(SP), REGCTXT
+	MOVW  saver9-4(SP), R9
+	RET
+
+TEXT checkASM(SB),NOSPLIT,$0-1
+	MOVW  $1, R3
+	MOVB  R3, ret+0(FP)
+	RET
+
+// gcWriteBarrier performs a heap pointer write and informs the GC.
+
+// gcWriteBarrier does NOT follow the Go ABI. It takes two arguments:
+// - R2 is the destination of the write
+// - R3 is the value being written at R2
+// It clobbers condition codes.
+// It does not clobber any other general-purpose registers,
+// but may clobber others (e.g., floating point registers).
+// The act of CALLing gcWriteBarrier will clobber R14 (LR).
+TEXT runtimegcWriteBarrier(SB),NOSPLIT|NOFRAME,$0
+	// Save the registers clobbered by the fast path.
+	MOVM.DB.W  [R0,R1], (R13)
+	MOVW       g_m(g), R0
+	MOVW       m_p(R0), R0
+	MOVW       (p_wbBuf+wbBuf_next)(R0), R1
+	// Increment wbBuf.next position.
+	ADD   $8, R1
+	MOVW  R1, (p_wbBuf+wbBuf_next)(R0)
+	MOVW  (p_wbBuf+wbBuf_end)(R0), R0
+	CMP   R1, R0
+	// Record the write.
+	MOVW  R3, -8(R1)  // Record value
+	MOVW  (R2), R0    // TODO: This turns bad writes into bad reads.
+	MOVW  R0, -4(R1)  // Record *slot
+	// Is the buffer full? (flags set in CMP above)
+	B.EQ  flush
+ret:
+	MOVM.IA.W  (R13), [R0,R1]
+	// Do the write.
+	MOVW  R3, (R2)
+	// Normally RET on nacl clobbers R12, but because this
+	// function has no frame it doesn't have to usual epilogue.
+	RET
+
+flush:
+	// Save all general purpose registers since these could be
+	// clobbered by wbBufFlush and were not saved by the caller.
+
+	// R0 and R1 were saved at entry.
+	// R7 is linker temp, so no need to save.
+	// R10 is g, so preserved.
+	// R13 is stack pointer.
+	// R15 is PC.
+
+	// This also sets up R2 and R3 as the arguments to wbBufFlush.
+	MOVM.DB.W  [R2-R6,R8-R9,R11-R12], (R13)
+	// Save R14 (LR) because the fast path above doesn't save it,
+	// but needs it to RET. This is after the MOVM so it appears below
+	// the arguments in the stack frame.
+	MOVW.W  LR, -4(R13)
+
+	// This takes arguments R2 and R3.
+	CALL  runtimewbBufFlush(SB)
+
+	MOVW.P     4(R13), LR
+	MOVM.IA.W  (R13), [R2-R6,R8-R9,R11-R12]
+	JMP        ret
+
+// Note: these functions use a special calling convention to save generated code space.
+// Arguments are passed in registers, but the space for those arguments are allocated
+// in the caller's stack frame. These stubs write the args into that stack space and
+// then tail call to the corresponding runtime handler.
+// The tail call makes these stubs disappear in backtraces.
+TEXT runtimepanicIndex(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicIndex(SB)
+TEXT runtimepanicIndexU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicIndexU(SB)
+TEXT runtimepanicSliceAlen(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSliceAlen(SB)
+TEXT runtimepanicSliceAlenU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSliceAlenU(SB)
+TEXT runtimepanicSliceAcap(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSliceAcap(SB)
+TEXT runtimepanicSliceAcapU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSliceAcapU(SB)
+TEXT runtimepanicSliceB(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicSliceB(SB)
+TEXT runtimepanicSliceBU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicSliceBU(SB)
+TEXT runtimepanicSlice3Alen(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtimegoPanicSlice3Alen(SB)
+TEXT runtimepanicSlice3AlenU(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtimegoPanicSlice3AlenU(SB)
+TEXT runtimepanicSlice3Acap(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtimegoPanicSlice3Acap(SB)
+TEXT runtimepanicSlice3AcapU(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtimegoPanicSlice3AcapU(SB)
+TEXT runtimepanicSlice3B(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSlice3B(SB)
+TEXT runtimepanicSlice3BU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSlice3BU(SB)
+TEXT runtimepanicSlice3C(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicSlice3C(SB)
+TEXT runtimepanicSlice3CU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicSlice3CU(SB)
+
+// Extended versions for 64-bit indexes.
+TEXT runtimepanicExtendIndex(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendIndex(SB)
+TEXT runtimepanicExtendIndexU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendIndexU(SB)
+TEXT runtimepanicExtendSliceAlen(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSliceAlen(SB)
+TEXT runtimepanicExtendSliceAlenU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSliceAlenU(SB)
+TEXT runtimepanicExtendSliceAcap(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSliceAcap(SB)
+TEXT runtimepanicExtendSliceAcapU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSliceAcapU(SB)
+TEXT runtimepanicExtendSliceB(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendSliceB(SB)
+TEXT runtimepanicExtendSliceBU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendSliceBU(SB)
+TEXT runtimepanicExtendSlice3Alen(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3Alen(SB)
+TEXT runtimepanicExtendSlice3AlenU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3AlenU(SB)
+TEXT runtimepanicExtendSlice3Acap(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3Acap(SB)
+TEXT runtimepanicExtendSlice3AcapU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3AcapU(SB)
+TEXT runtimepanicExtendSlice3B(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3B(SB)
+TEXT runtimepanicExtendSlice3BU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3BU(SB)
+TEXT runtimepanicExtendSlice3C(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3C(SB)
+TEXT runtimepanicExtendSlice3CU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3CU(SB)
diff --git a/src/runtime/cgo.go b/src/runtime/cgo.go
index 395d54a66e..538cf91e28 100644
--- a/src/runtime/cgo.go
+++ b/src/runtime/cgo.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build !noos
+
 package runtime
 
 import "unsafe"
diff --git a/src/runtime/cgo_noos.go b/src/runtime/cgo_noos.go
new file mode 100644
index 0000000000..9ab9bf1f2e
--- /dev/null
+++ b/src/runtime/cgo_noos.go
@@ -0,0 +1,30 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+const iscgo = false
+
+var (
+	_cgo_thread_start             unsafe.Pointer
+	_cgo_notify_runtime_init_done unsafe.Pointer
+	_cgo_set_context_function     unsafe.Pointer
+	_cgo_yield                    unsafe.Pointer
+)
+
+var (
+	cgo_yield      = &_cgo_yield
+	cgoAlwaysFalse bool
+	cgoHasExtraM   bool
+)
+
+type cgoCallers [1]uintptr
+
+func cgoCheckSliceCopy(typ *_type, dst slice, src slice, n int) {}
+func cgoCheckWriteBarrier(dst *uintptr, src uintptr)            {}
+func cgocall(fn unsafe.Pointer, arg unsafe.Pointer) int32       { return 0 }
+
+func cgoCheckMemmove(typ *_type, dst unsafe.Pointer, src unsafe.Pointer, off uintptr, size uintptr) {}
diff --git a/src/runtime/cgocall.go b/src/runtime/cgocall.go
index a881ae1489..b6b2093d4b 100644
--- a/src/runtime/cgocall.go
+++ b/src/runtime/cgocall.go
@@ -77,6 +77,8 @@
 // _cgoexp_GoF immediately returns to crosscall2, which restores the
 // callee-save registers for gcc and returns to GoF, which returns to f.
 
+// +build !noos
+
 package runtime
 
 import (
diff --git a/src/runtime/cgocheck.go b/src/runtime/cgocheck.go
index ed854e5e2b..a4234e6365 100644
--- a/src/runtime/cgocheck.go
+++ b/src/runtime/cgocheck.go
@@ -5,6 +5,8 @@
 // Code to check that pointer writes follow the cgo rules.
 // These functions are invoked via the write barrier when debug.cgocheck > 1.
 
+// +build !noos
+
 package runtime
 
 import (
diff --git a/src/runtime/cpumtx.go b/src/runtime/cpumtx.go
new file mode 100644
index 0000000000..f3698ca054
--- /dev/null
+++ b/src/runtime/cpumtx.go
@@ -0,0 +1,28 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos !thumb
+
+package runtime
+
+import "runtime/internal/atomic"
+
+// simple spinlock
+type cpumtx struct {
+	v uint32
+}
+
+//go:nosplit
+func (l *cpumtx) lock() {
+	for {
+		if atomic.Cas(&l.v, 0, 1) {
+			return
+		}
+	}
+}
+
+//go:nosplit
+func (l *cpumtx) unlock() {
+	atomic.Store(&l.v, 0)
+}
diff --git a/src/runtime/cpumtx_noos_thumb.go b/src/runtime/cpumtx_noos_thumb.go
new file mode 100644
index 0000000000..1f27582c88
--- /dev/null
+++ b/src/runtime/cpumtx_noos_thumb.go
@@ -0,0 +1,8 @@
+package runtime
+
+// for now noos/thumb supports only single CPU
+
+type cpumtx struct{}
+
+func (mt *cpumtx) lock()   {}
+func (mt *cpumtx) unlock() {}
diff --git a/src/runtime/cputicks.go b/src/runtime/cputicks.go
index 7beb57ea12..969a73f895 100644
--- a/src/runtime/cputicks.go
+++ b/src/runtime/cputicks.go
@@ -3,6 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build !arm
+// +build !thumb
 // +build !arm64
 // +build !mips64
 // +build !mips64le
diff --git a/src/runtime/debug.go b/src/runtime/debug.go
index af5c3a1170..7f66cb63cb 100644
--- a/src/runtime/debug.go
+++ b/src/runtime/debug.go
@@ -46,6 +46,9 @@ func NumCPU() int {
 
 // NumCgoCall returns the number of cgo calls made by the current process.
 func NumCgoCall() int64 {
+	if _MCU != 0 {
+		return 0
+	}
 	var n int64
 	for mp := (*m)(atomic.Loadp(unsafe.Pointer(&allm))); mp != nil; mp = mp.alllink {
 		n += int64(mp.ncgocall)
diff --git a/src/runtime/debug/stubs.go b/src/runtime/debug/stubs.go
index 2cba136044..82ab5ba0fd 100644
--- a/src/runtime/debug/stubs.go
+++ b/src/runtime/debug/stubs.go
@@ -15,3 +15,4 @@ func setMaxStack(int) int
 func setGCPercent(int32) int32
 func setPanicOnFault(bool) bool
 func setMaxThreads(int) int
+func setGCTrace(int32)
diff --git a/src/runtime/defs_linux_arm.go b/src/runtime/defs_linux_armt.go
similarity index 98%
rename from src/runtime/defs_linux_arm.go
rename to src/runtime/defs_linux_armt.go
index 9d10d664e1..be08ee731d 100644
--- a/src/runtime/defs_linux_arm.go
+++ b/src/runtime/defs_linux_armt.go
@@ -1,3 +1,6 @@
+// +build linux
+// +build arm thumb
+
 package runtime
 
 // Constants
diff --git a/src/runtime/duff_thumb.s b/src/runtime/duff_thumb.s
new file mode 100644
index 0000000000..ba8235b740
--- /dev/null
+++ b/src/runtime/duff_thumb.s
@@ -0,0 +1,523 @@
+// Code generated by mkduff.go; DO NOT EDIT.
+// Run go generate from src/runtime to update.
+// See mkduff.go for comments.
+
+#include "textflag.h"
+
+TEXT runtimeduffzero(SB), NOSPLIT, $0-0
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	MOVW.P	R0, 4(R1)
+	RET
+
+TEXT runtimeduffcopy(SB), NOSPLIT, $0-0
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	MOVW.P	4(R1), R0
+	MOVW.P	R0, 4(R2)
+
+	RET
diff --git a/src/runtime/env_noos.go b/src/runtime/env_noos.go
new file mode 100644
index 0000000000..2a8c6701c5
--- /dev/null
+++ b/src/runtime/env_noos.go
@@ -0,0 +1,30 @@
+// Copyright 2012 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/atomic"
+	"unsafe"
+)
+
+var _cgo_setenv unsafe.Pointer   // pointer to C function
+var _cgo_unsetenv unsafe.Pointer // pointer to C function
+
+//go:nosplit
+func gogetenv(key string) string { return "" }
+
+// The following functions should be defined in the runtime/debug package but
+// for now they are here because the runtime/debug imports time package, not
+// ported yet.
+
+// SetGCTrace is the alternate way to set GODEBUG=gctrace=n at the runtime.
+func SetGCTrace(in int32) {
+	atomic.Store((*uint32)(unsafe.Pointer(&debug.gctrace)), uint32(in))
+}
+
+// SetGCPercent is the alternate way to set GOGC=percent at the runtime.
+func SetGCPercent(percent int) int {
+	return int(setGCPercent(int32(percent)))
+}
diff --git a/src/runtime/export_thumb_test.go b/src/runtime/export_thumb_test.go
new file mode 100644
index 0000000000..b8a89fc0d2
--- /dev/null
+++ b/src/runtime/export_thumb_test.go
@@ -0,0 +1,9 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Export guts for testing.
+
+package runtime
+
+var Usplit = usplit
diff --git a/src/runtime/gcinfo_test.go b/src/runtime/gcinfo_test.go
index 0741f6361c..8306da30f7 100644
--- a/src/runtime/gcinfo_test.go
+++ b/src/runtime/gcinfo_test.go
@@ -171,7 +171,7 @@ type BigStruct struct {
 
 func infoBigStruct() []byte {
 	switch runtime.GOARCH {
-	case "386", "arm", "mips", "mipsle":
+	case "386", "arm", "thumb", "mips", "mipsle":
 		return []byte{
 			typePointer,                                                // q *int
 			typeScalar, typeScalar, typeScalar, typeScalar, typeScalar, // w byte; e [17]byte
diff --git a/src/runtime/go_tls.h b/src/runtime/go_tls.h
index 61f7dbef3c..13a7c0beb5 100644
--- a/src/runtime/go_tls.h
+++ b/src/runtime/go_tls.h
@@ -6,6 +6,12 @@
 #define LR R14
 #endif
 
+#ifdef GOARCH_thumb
+#define REGTMP  R7
+#define REGCTXT R11
+#define LR      R14
+#endif
+
 #ifdef GOARCH_amd64
 #define	get_tls(r)	MOVQ TLS, r
 #define	g(r)	0(r)(TLS*1)
diff --git a/src/runtime/hash32.go b/src/runtime/hash32.go
index 5574923911..f7bd96bbae 100644
--- a/src/runtime/hash32.go
+++ b/src/runtime/hash32.go
@@ -6,7 +6,7 @@
 //   xxhash: https://code.google.com/p/xxhash/
 // cityhash: https://code.google.com/p/cityhash/
 
-// +build 386 arm mips mipsle
+// +build 386 arm thumb mips mipsle
 
 package runtime
 
diff --git a/src/runtime/heapdump.go b/src/runtime/heapdump.go
index 992df6391e..007b53c3ed 100644
--- a/src/runtime/heapdump.go
+++ b/src/runtime/heapdump.go
@@ -9,6 +9,8 @@
 // The format of the dumped file is described at
 // https://golang.org/s/go15heapdump.
 
+// +build !noos
+
 package runtime
 
 import (
@@ -560,8 +562,8 @@ func dumpmemstats() {
 	dumpint(memstats.next_gc)
 	dumpint(memstats.last_gc_unix)
 	dumpint(memstats.pause_total_ns)
-	for i := 0; i < 256; i++ {
-		dumpint(memstats.pause_ns[i])
+	for _, ns := range memstats.pause_ns {
+		dumpint(ns)
 	}
 	dumpint(uint64(memstats.numgc))
 }
diff --git a/src/runtime/iface.go b/src/runtime/iface.go
index bb4eccc9bd..62b60f9699 100644
--- a/src/runtime/iface.go
+++ b/src/runtime/iface.go
@@ -10,7 +10,7 @@ import (
 	"unsafe"
 )
 
-const itabInitSize = 512
+const itabInitSize = 512 * (1 - _MCU)
 
 var (
 	itabLock      mutex                               // lock for accessing itab table
@@ -239,6 +239,11 @@ imethods:
 
 func itabsinit() {
 	lock(&itabLock)
+	if _MCU != 0 {
+		// allocate starter table, it will fit into two 256-byte pages
+		itabTable = (*itabTableType)(mallocgc((2+126)*sys.PtrSize, nil, true))
+		itabTable.size = 126
+	}
 	for _, md := range activeModules() {
 		for _, i := range md.itablinks {
 			itabAdd(i)
diff --git a/src/runtime/internal/atomic/asm_thumb.s b/src/runtime/internal/atomic/asm_thumb.s
new file mode 100644
index 0000000000..80e855a4f3
--- /dev/null
+++ b/src/runtime/internal/atomic/asm_thumb.s
@@ -0,0 +1,107 @@
+// Copyright 2015 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT Load(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R0
+	MOVW  (R0), R1
+	DMB   MB_ISH
+	MOVW  R1, ret+4(FP)
+	RET
+
+TEXT Store(SB),NOSPLIT,$0-8
+	MOVW  addr+0(FP), R1
+	MOVW  v+4(FP), R2
+	DMB   MB_ISH
+	MOVW  R2, (R1)
+	DMB   MB_ISH
+	RET
+
+TEXT Cas(SB),NOSPLIT|NOFRAME,$0
+	MOVW  ptr+0(FP), R1
+	MOVW  old+4(FP), R2
+	MOVW  new+8(FP), R3
+casl:
+	LDREX  (R1), R0
+	CMP    R0, R2
+	BNE    end
+	DMB    MB_ISHST
+	STREX  R3, (R1), R0
+	CMP    $0, R0
+	BNE    casl
+end:
+	MOVW.NE    $0, R0
+	MOVW.P.EQ  $1, R0
+	DMB.EQ     MB_ISH
+	MOVB       R0, ret+12(FP)
+	RET
+
+TEXT Xchg(SB),NOSPLIT|NOFRAME,$0-12
+	MOVW  addr+0(FP), R1
+	MOVW  v+4(FP), R2
+loop:
+	LDREX  (R1), R0
+	DMB    MB_ISHST
+	STREX  R2, (R1), R3
+	CMP    $0, R3
+	BNE    loop
+	DMB    MB_ISH
+	MOVW   R0, ret+8(FP)
+	RET
+
+TEXT Xadd(SB),NOSPLIT|NOFRAME,$0-12
+	MOVW  val+0(FP), R1
+	MOVW  delta+4(FP), R2
+loop:
+	LDREX  (R1), R0
+	DMB    MB_ISHST
+	ADD    R2, R0
+	STREX  R0, (R1), R3
+	CMP    $0, R3
+	BNE    loop
+	DMB    MB_ISH
+	MOVW   R0, ret+8(FP)
+	RET
+
+// stubs
+
+TEXT Loadp(SB),NOSPLIT|NOFRAME,$0-8
+	B   Load(SB)
+
+TEXT LoadAcq(SB),NOSPLIT|NOFRAME,$0-8
+	B   Load(SB)
+
+TEXT Casp1(SB),NOSPLIT,$0-13
+	B   Cas(SB)
+
+TEXT CasRel(SB),NOSPLIT,$0-13
+	B   Cas(SB)
+
+TEXT StorepNoWB(SB),NOSPLIT,$0-8
+	B   Store(SB)
+
+TEXT StoreRel(SB),NOSPLIT,$0-8
+	B   Store(SB)
+
+TEXT Loadint64(SB),NOSPLIT,$0-12
+	B   Load64(SB)
+
+TEXT Xaddint64(SB),NOSPLIT,$0-20
+	B   Xadd64(SB)
+
+TEXT Cas64(SB),NOSPLIT,$0-21
+	B   goCas64(SB)
+
+TEXT Xadd64(SB),NOSPLIT,$0-20
+	B   goXadd64(SB)
+
+TEXT Xchg64(SB),NOSPLIT,$0-20
+	B   goXchg64(SB)
+
+TEXT Load64(SB),NOSPLIT,$0-12
+	B   goLoad64(SB)
+
+TEXT Store64(SB),NOSPLIT,$0-12
+	B   goStore64(SB)
diff --git a/src/runtime/internal/atomic/atomic_arm.go b/src/runtime/internal/atomic/atomic_arm.go
index c1fc1f727f..cf92a2d159 100644
--- a/src/runtime/internal/atomic/atomic_arm.go
+++ b/src/runtime/internal/atomic/atomic_arm.go
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build arm
-
 package atomic
 
 import (
diff --git a/src/runtime/internal/atomic/atomic_thumb.go b/src/runtime/internal/atomic/atomic_thumb.go
new file mode 100644
index 0000000000..db9f54d1b5
--- /dev/null
+++ b/src/runtime/internal/atomic/atomic_thumb.go
@@ -0,0 +1,233 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package atomic
+
+import (
+	"internal/cpu"
+	"runtime/internal/sys"
+	"unsafe"
+)
+
+const _MCU = sys.GoosNoos
+
+type spinlock struct {
+	v uint32
+}
+
+//go:nosplit
+func (l *spinlock) lock() {
+	for {
+		if Cas(&l.v, 0, 1) {
+			return
+		}
+	}
+}
+
+//go:nosplit
+func (l *spinlock) unlock() {
+	Store(&l.v, 0)
+}
+
+var locktab [57*(1-_MCU) + 29*_MCU]struct {
+	l   spinlock
+	pad [cpu.CacheLinePadSize - unsafe.Sizeof(spinlock{})]byte
+}
+
+func addrLock(addr *uint64) *spinlock {
+	return &locktab[(uintptr(unsafe.Pointer(addr))>>3)%uintptr(len(locktab))].l
+}
+
+//go:nosplit
+func goCas64(addr *uint64, old, new uint64) bool {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	var ok bool
+	addrLock(addr).lock()
+	if *addr == old {
+		*addr = new
+		ok = true
+	}
+	addrLock(addr).unlock()
+	return ok
+}
+
+//go:nosplit
+func goXadd64(addr *uint64, delta int64) uint64 {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	var r uint64
+	addrLock(addr).lock()
+	r = *addr + uint64(delta)
+	*addr = r
+	addrLock(addr).unlock()
+	return r
+}
+
+//go:nosplit
+func goXchg64(addr *uint64, v uint64) uint64 {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	var r uint64
+	addrLock(addr).lock()
+	r = *addr
+	*addr = v
+	addrLock(addr).unlock()
+	return r
+}
+
+//go:nosplit
+func goLoad64(addr *uint64) uint64 {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	var r uint64
+	addrLock(addr).lock()
+	r = *addr
+	addrLock(addr).unlock()
+	return r
+}
+
+//go:nosplit
+func goStore64(addr *uint64, v uint64) {
+	if uintptr(unsafe.Pointer(addr))&7 != 0 {
+		*(*int)(nil) = 0 // crash on unaligned uint64
+	}
+	_ = *addr // if nil, fault before taking the lock
+	addrLock(addr).lock()
+	*addr = v
+	addrLock(addr).unlock()
+}
+
+//go:nosplit
+func Or8(addr *uint8, v uint8) {
+	// Align down to 4 bytes and use 32-bit CAS.
+	uaddr := uintptr(unsafe.Pointer(addr))
+	addr32 := (*uint32)(unsafe.Pointer(uaddr &^ 3))
+	word := uint32(v) << ((uaddr & 3) * 8) // little endian
+	for {
+		old := *addr32
+		if Cas(addr32, old, old|word) {
+			return
+		}
+	}
+}
+
+//go:nosplit
+func And8(addr *uint8, v uint8) {
+	// Align down to 4 bytes and use 32-bit CAS.
+	uaddr := uintptr(unsafe.Pointer(addr))
+	addr32 := (*uint32)(unsafe.Pointer(uaddr &^ 3))
+	word := uint32(v) << ((uaddr & 3) * 8)    // little endian
+	mask := uint32(0xFF) << ((uaddr & 3) * 8) // little endian
+	word |= ^mask
+	for {
+		old := *addr32
+		if Cas(addr32, old, old&word) {
+			return
+		}
+	}
+}
+
+//go:noescape
+func Load(addr *uint32) uint32
+
+// NO go:noescape annotation; *addr escapes if result escapes (#31525)
+func Loadp(addr unsafe.Pointer) unsafe.Pointer
+
+//go:noescape
+func Loadint64(ptr *int64) int64
+
+//go:noescape
+func Load8(addr *uint8) uint8
+
+//go:noescape
+func LoadAcq(addr *uint32) uint32
+
+//go:noescape
+func Cas(ptr *uint32, old, new uint32) bool
+
+// NO go:noescape annotation; see atomic_pointer.go.
+func Casp1(ptr *unsafe.Pointer, old, new unsafe.Pointer) bool
+
+// Not noescape -- it installs a pointer to addr.
+func StorepNoWB(addr unsafe.Pointer, v unsafe.Pointer)
+
+//go:noescape
+func Store(addr *uint32, v uint32)
+
+//go:noescape
+func StoreRel(addr *uint32, v uint32)
+
+//go:noescape
+func Store64(addr *uint64, v uint64)
+
+//go:noescape
+func Xaddint64(ptr *int64, delta int64) int64
+
+//go:noescape
+func Cas64(addr *uint64, old, new uint64) bool
+
+//go:noescape
+func CasRel(addr *uint32, old, new uint32) bool
+
+//go:noescape
+func Xchg(addr *uint32, v uint32) uint32
+
+//go:noescape
+func Xadd(val *uint32, delta int32) uint32
+
+//go:noescape
+func Xadd64(addr *uint64, delta int64) uint64
+
+//go:noescape
+func Xchg64(addr *uint64, v uint64) uint64
+
+//go:noescape
+func Load64(addr *uint64) uint64
+
+// Export the following wrapper function via linkname to assembly in sync/atomic.
+//go:linkname Loaduint
+//go:linkname Loaduintptr
+//go:linkname Storeuintptr
+//go:linkname Xchguintptr
+//go:linkname Xadduintptr
+//go:linkname Casuintptr
+
+//go:nosplit
+func Loaduint(ptr *uint) uint {
+	return uint(Load((*uint32)(unsafe.Pointer(ptr))))
+}
+
+//go:nosplit
+func Loaduintptr(ptr *uintptr) uintptr {
+	return uintptr(Load((*uint32)(unsafe.Pointer(ptr))))
+}
+
+//go:nosplit
+func Storeuintptr(ptr *uintptr, new uintptr) {
+	Store((*uint32)(unsafe.Pointer(ptr)), uint32(new))
+}
+
+//go:nosplit
+func Xchguintptr(addr *uintptr, v uintptr) uintptr {
+	return uintptr(Xchg((*uint32)(unsafe.Pointer(addr)), uint32(v)))
+}
+
+//go:nosplit
+func Xadduintptr(val *uintptr, delta uintptr) uintptr {
+	return uintptr(Xadd((*uint32)(unsafe.Pointer(val)), int32(delta)))
+}
+
+//go:nosplit
+func Casuintptr(addr *uintptr, old, new uintptr) bool {
+	return Cas((*uint32)(unsafe.Pointer(addr)), uint32(old), uint32(new))
+}
diff --git a/src/runtime/internal/atomic/stubs.go b/src/runtime/internal/atomic/stubs.go
index 62e30d1788..6ed6e53b81 100644
--- a/src/runtime/internal/atomic/stubs.go
+++ b/src/runtime/internal/atomic/stubs.go
@@ -3,6 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build !wasm
+// +build !thumb
 
 package atomic
 
diff --git a/src/runtime/internal/sys/arch_thumb.go b/src/runtime/internal/sys/arch_thumb.go
new file mode 100644
index 0000000000..556236be89
--- /dev/null
+++ b/src/runtime/internal/sys/arch_thumb.go
@@ -0,0 +1,17 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package sys
+
+const (
+	ArchFamily          = ARM
+	BigEndian           = false
+	DefaultPhysPageSize = 1024
+	PCQuantum           = 2
+	Int64Align          = 4
+	HugePageSize        = 0
+	MinFrameSize        = 4
+)
+
+type Uintreg uint32
diff --git a/src/runtime/internal/sys/zgoarch_386.go b/src/runtime/internal/sys/zgoarch_386.go
index c286d0df2b..85dfdb6b44 100644
--- a/src/runtime/internal/sys/zgoarch_386.go
+++ b/src/runtime/internal/sys/zgoarch_386.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_amd64.go b/src/runtime/internal/sys/zgoarch_amd64.go
index d21c1d7d2a..8ed3ec6899 100644
--- a/src/runtime/internal/sys/zgoarch_amd64.go
+++ b/src/runtime/internal/sys/zgoarch_amd64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_amd64p32.go b/src/runtime/internal/sys/zgoarch_amd64p32.go
index 13dc2e756d..ce5028652a 100644
--- a/src/runtime/internal/sys/zgoarch_amd64p32.go
+++ b/src/runtime/internal/sys/zgoarch_amd64p32.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_arm.go b/src/runtime/internal/sys/zgoarch_arm.go
index 9085fb0ea8..b23027e402 100644
--- a/src/runtime/internal/sys/zgoarch_arm.go
+++ b/src/runtime/internal/sys/zgoarch_arm.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_arm64.go b/src/runtime/internal/sys/zgoarch_arm64.go
index ed7ef2ebcb..1f90f77914 100644
--- a/src/runtime/internal/sys/zgoarch_arm64.go
+++ b/src/runtime/internal/sys/zgoarch_arm64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_arm64be.go b/src/runtime/internal/sys/zgoarch_arm64be.go
index faf3111053..3f717d4bd8 100644
--- a/src/runtime/internal/sys/zgoarch_arm64be.go
+++ b/src/runtime/internal/sys/zgoarch_arm64be.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_armbe.go b/src/runtime/internal/sys/zgoarch_armbe.go
index cb28301e0b..0393a4d176 100644
--- a/src/runtime/internal/sys/zgoarch_armbe.go
+++ b/src/runtime/internal/sys/zgoarch_armbe.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips.go b/src/runtime/internal/sys/zgoarch_mips.go
index 315dea1c84..b5b85f2337 100644
--- a/src/runtime/internal/sys/zgoarch_mips.go
+++ b/src/runtime/internal/sys/zgoarch_mips.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips64.go b/src/runtime/internal/sys/zgoarch_mips64.go
index 5258cbfbe7..f6be8bfb3c 100644
--- a/src/runtime/internal/sys/zgoarch_mips64.go
+++ b/src/runtime/internal/sys/zgoarch_mips64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips64le.go b/src/runtime/internal/sys/zgoarch_mips64le.go
index 1721698518..640550a5e1 100644
--- a/src/runtime/internal/sys/zgoarch_mips64le.go
+++ b/src/runtime/internal/sys/zgoarch_mips64le.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips64p32.go b/src/runtime/internal/sys/zgoarch_mips64p32.go
index 44c4624da9..181fd59061 100644
--- a/src/runtime/internal/sys/zgoarch_mips64p32.go
+++ b/src/runtime/internal/sys/zgoarch_mips64p32.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mips64p32le.go b/src/runtime/internal/sys/zgoarch_mips64p32le.go
index eb63225430..f35e05b89d 100644
--- a/src/runtime/internal/sys/zgoarch_mips64p32le.go
+++ b/src/runtime/internal/sys/zgoarch_mips64p32le.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_mipsle.go b/src/runtime/internal/sys/zgoarch_mipsle.go
index e0ebfbf038..b4a8fc7ea9 100644
--- a/src/runtime/internal/sys/zgoarch_mipsle.go
+++ b/src/runtime/internal/sys/zgoarch_mipsle.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_ppc.go b/src/runtime/internal/sys/zgoarch_ppc.go
index ef26aa3201..e8dd9c90c0 100644
--- a/src/runtime/internal/sys/zgoarch_ppc.go
+++ b/src/runtime/internal/sys/zgoarch_ppc.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_ppc64.go b/src/runtime/internal/sys/zgoarch_ppc64.go
index 32c2d46d4c..a3c812c93f 100644
--- a/src/runtime/internal/sys/zgoarch_ppc64.go
+++ b/src/runtime/internal/sys/zgoarch_ppc64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_ppc64le.go b/src/runtime/internal/sys/zgoarch_ppc64le.go
index 3a6e56763c..20ebdbe286 100644
--- a/src/runtime/internal/sys/zgoarch_ppc64le.go
+++ b/src/runtime/internal/sys/zgoarch_ppc64le.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_riscv.go b/src/runtime/internal/sys/zgoarch_riscv.go
index d8f6b49093..944b1f7704 100644
--- a/src/runtime/internal/sys/zgoarch_riscv.go
+++ b/src/runtime/internal/sys/zgoarch_riscv.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_riscv64.go b/src/runtime/internal/sys/zgoarch_riscv64.go
index 0ba843b5ac..74ac8c102b 100644
--- a/src/runtime/internal/sys/zgoarch_riscv64.go
+++ b/src/runtime/internal/sys/zgoarch_riscv64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_s390.go b/src/runtime/internal/sys/zgoarch_s390.go
index 20a1b234a6..8f0777d2b5 100644
--- a/src/runtime/internal/sys/zgoarch_s390.go
+++ b/src/runtime/internal/sys/zgoarch_s390.go
@@ -28,4 +28,5 @@ const GoarchS390 = 1
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_s390x.go b/src/runtime/internal/sys/zgoarch_s390x.go
index ffdda0c827..c88d6d82fe 100644
--- a/src/runtime/internal/sys/zgoarch_s390x.go
+++ b/src/runtime/internal/sys/zgoarch_s390x.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 1
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_sparc.go b/src/runtime/internal/sys/zgoarch_sparc.go
index b4949510d5..487d43b557 100644
--- a/src/runtime/internal/sys/zgoarch_sparc.go
+++ b/src/runtime/internal/sys/zgoarch_sparc.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 1
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_sparc64.go b/src/runtime/internal/sys/zgoarch_sparc64.go
index 0f6df411ce..0924b1a031 100644
--- a/src/runtime/internal/sys/zgoarch_sparc64.go
+++ b/src/runtime/internal/sys/zgoarch_sparc64.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 1
+const GoarchThumb = 0
 const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_thumb.go b/src/runtime/internal/sys/zgoarch_thumb.go
new file mode 100644
index 0000000000..2cc7172b31
--- /dev/null
+++ b/src/runtime/internal/sys/zgoarch_thumb.go
@@ -0,0 +1,32 @@
+// Code generated by gengoos.go using 'go generate'. DO NOT EDIT.
+
+// +build thumb
+
+package sys
+
+const GOARCH = `thumb`
+
+const Goarch386 = 0
+const GoarchAmd64 = 0
+const GoarchAmd64p32 = 0
+const GoarchArm = 0
+const GoarchArmbe = 0
+const GoarchArm64 = 0
+const GoarchArm64be = 0
+const GoarchPpc64 = 0
+const GoarchPpc64le = 0
+const GoarchMips = 0
+const GoarchMipsle = 0
+const GoarchMips64 = 0
+const GoarchMips64le = 0
+const GoarchMips64p32 = 0
+const GoarchMips64p32le = 0
+const GoarchPpc = 0
+const GoarchRiscv = 0
+const GoarchRiscv64 = 0
+const GoarchS390 = 0
+const GoarchS390x = 0
+const GoarchSparc = 0
+const GoarchSparc64 = 0
+const GoarchThumb = 1
+const GoarchWasm = 0
diff --git a/src/runtime/internal/sys/zgoarch_wasm.go b/src/runtime/internal/sys/zgoarch_wasm.go
index e69afb0cb3..06a191ee74 100644
--- a/src/runtime/internal/sys/zgoarch_wasm.go
+++ b/src/runtime/internal/sys/zgoarch_wasm.go
@@ -28,4 +28,5 @@ const GoarchS390 = 0
 const GoarchS390x = 0
 const GoarchSparc = 0
 const GoarchSparc64 = 0
+const GoarchThumb = 0
 const GoarchWasm = 1
diff --git a/src/runtime/internal/sys/zgoos_aix.go b/src/runtime/internal/sys/zgoos_aix.go
index d97485c43c..37a02883d0 100644
--- a/src/runtime/internal/sys/zgoos_aix.go
+++ b/src/runtime/internal/sys/zgoos_aix.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_android.go b/src/runtime/internal/sys/zgoos_android.go
index eec970b064..d973ae4689 100644
--- a/src/runtime/internal/sys/zgoos_android.go
+++ b/src/runtime/internal/sys/zgoos_android.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_darwin.go b/src/runtime/internal/sys/zgoos_darwin.go
index c40819ee55..a2a91c2853 100644
--- a/src/runtime/internal/sys/zgoos_darwin.go
+++ b/src/runtime/internal/sys/zgoos_darwin.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_dragonfly.go b/src/runtime/internal/sys/zgoos_dragonfly.go
index 3dc4edcc31..2cb9ee4f47 100644
--- a/src/runtime/internal/sys/zgoos_dragonfly.go
+++ b/src/runtime/internal/sys/zgoos_dragonfly.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_freebsd.go b/src/runtime/internal/sys/zgoos_freebsd.go
index 6c98b342f9..57d6e7e804 100644
--- a/src/runtime/internal/sys/zgoos_freebsd.go
+++ b/src/runtime/internal/sys/zgoos_freebsd.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_hurd.go b/src/runtime/internal/sys/zgoos_hurd.go
index d6dcc7bad4..7e35c7e184 100644
--- a/src/runtime/internal/sys/zgoos_hurd.go
+++ b/src/runtime/internal/sys/zgoos_hurd.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_illumos.go b/src/runtime/internal/sys/zgoos_illumos.go
index 17f4ecc34e..10e30eff54 100644
--- a/src/runtime/internal/sys/zgoos_illumos.go
+++ b/src/runtime/internal/sys/zgoos_illumos.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_js.go b/src/runtime/internal/sys/zgoos_js.go
index 74c9943d9b..fe66e70963 100644
--- a/src/runtime/internal/sys/zgoos_js.go
+++ b/src/runtime/internal/sys/zgoos_js.go
@@ -17,6 +17,7 @@ const GoosJs = 1
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_linux.go b/src/runtime/internal/sys/zgoos_linux.go
index 1d5fcb0685..2a70be2100 100644
--- a/src/runtime/internal/sys/zgoos_linux.go
+++ b/src/runtime/internal/sys/zgoos_linux.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 1
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_nacl.go b/src/runtime/internal/sys/zgoos_nacl.go
index 9e65b6f185..5d27cebffa 100644
--- a/src/runtime/internal/sys/zgoos_nacl.go
+++ b/src/runtime/internal/sys/zgoos_nacl.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 1
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_netbsd.go b/src/runtime/internal/sys/zgoos_netbsd.go
index 194fa6e432..9416cb0b12 100644
--- a/src/runtime/internal/sys/zgoos_netbsd.go
+++ b/src/runtime/internal/sys/zgoos_netbsd.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 1
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_noos.go b/src/runtime/internal/sys/zgoos_noos.go
new file mode 100644
index 0000000000..fd1b435b2e
--- /dev/null
+++ b/src/runtime/internal/sys/zgoos_noos.go
@@ -0,0 +1,24 @@
+// Code generated by gengoos.go using 'go generate'. DO NOT EDIT.
+
+// +build noos
+
+package sys
+
+const GOOS = `noos`
+
+const GoosAix = 0
+const GoosAndroid = 0
+const GoosDarwin = 0
+const GoosDragonfly = 0
+const GoosFreebsd = 0
+const GoosHurd = 0
+const GoosJs = 0
+const GoosLinux = 0
+const GoosNacl = 0
+const GoosNetbsd = 0
+const GoosNoos = 1
+const GoosOpenbsd = 0
+const GoosPlan9 = 0
+const GoosSolaris = 0
+const GoosWindows = 0
+const GoosZos = 0
diff --git a/src/runtime/internal/sys/zgoos_openbsd.go b/src/runtime/internal/sys/zgoos_openbsd.go
index 2108691679..3ff416049b 100644
--- a/src/runtime/internal/sys/zgoos_openbsd.go
+++ b/src/runtime/internal/sys/zgoos_openbsd.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 1
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_plan9.go b/src/runtime/internal/sys/zgoos_plan9.go
index e632a90b2e..7cb4882510 100644
--- a/src/runtime/internal/sys/zgoos_plan9.go
+++ b/src/runtime/internal/sys/zgoos_plan9.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 1
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_solaris.go b/src/runtime/internal/sys/zgoos_solaris.go
index 67b2ffbfcd..627e9ad850 100644
--- a/src/runtime/internal/sys/zgoos_solaris.go
+++ b/src/runtime/internal/sys/zgoos_solaris.go
@@ -18,6 +18,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 1
diff --git a/src/runtime/internal/sys/zgoos_windows.go b/src/runtime/internal/sys/zgoos_windows.go
index cf2d6f4fb0..395e2b3ca6 100644
--- a/src/runtime/internal/sys/zgoos_windows.go
+++ b/src/runtime/internal/sys/zgoos_windows.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/internal/sys/zgoos_zos.go b/src/runtime/internal/sys/zgoos_zos.go
index e5d79accb4..0849c1900c 100644
--- a/src/runtime/internal/sys/zgoos_zos.go
+++ b/src/runtime/internal/sys/zgoos_zos.go
@@ -17,6 +17,7 @@ const GoosJs = 0
 const GoosLinux = 0
 const GoosNacl = 0
 const GoosNetbsd = 0
+const GoosNoos = 0
 const GoosOpenbsd = 0
 const GoosPlan9 = 0
 const GoosSolaris = 0
diff --git a/src/runtime/lfstack_32bit.go b/src/runtime/lfstack_32bit.go
index d36ca50971..af9882c356 100644
--- a/src/runtime/lfstack_32bit.go
+++ b/src/runtime/lfstack_32bit.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 arm nacl mips mipsle
+// +build 386 arm thumb nacl mips mipsle
 
 package runtime
 
diff --git a/src/runtime/lock_futex.go b/src/runtime/lock_futex.go
index d2828b138a..27d4f9440c 100644
--- a/src/runtime/lock_futex.go
+++ b/src/runtime/lock_futex.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build dragonfly freebsd linux
+// +build dragonfly freebsd linux noos
 
 package runtime
 
diff --git a/src/runtime/malloc.go b/src/runtime/malloc.go
index d768054198..a5cff5e51c 100644
--- a/src/runtime/malloc.go
+++ b/src/runtime/malloc.go
@@ -133,6 +133,9 @@ const (
 	// _64bit = 1 on 64-bit systems, 0 on 32-bit systems
 	_64bit = 1 << (^uintptr(0) >> 63) / 2
 
+	_MCU    = sys.GoosNoos
+	_ARMv7M = sys.GoosNoos * sys.GoarchThumb
+
 	// Tiny allocator parameters, see "Tiny allocator" comment in malloc.go.
 	_TinySize      = 16
 	_TinySizeClass = int8(2)
@@ -140,7 +143,7 @@ const (
 	_FixAllocChunk = 16 << 10 // Chunk size for FixAlloc
 
 	// Per-P, per order stack segment cache size.
-	_StackCacheSize = 32 * 1024
+	_StackCacheSize = 32*1024*(1-_MCU) + 4*1024*_MCU
 
 	// Number of orders that get caching. Order 0 is FixedStack
 	// and each successive order is twice as large.
@@ -154,7 +157,8 @@ const (
 	//   windows/32       | 4KB        | 3
 	//   windows/64       | 8KB        | 2
 	//   plan9            | 4KB        | 3
-	_NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9
+	//   MCU              | 1KB        | 2
+	_NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9 - 2*_MCU
 
 	// heapAddrBits is the number of bits in a heap address. On
 	// amd64, addresses are sign-extended beyond heapAddrBits. On
@@ -207,7 +211,13 @@ const (
 	// we further limit it to 31 bits.
 	//
 	// WebAssembly currently has a limit of 4GB linear memory.
-	heapAddrBits = (_64bit*(1-sys.GoarchWasm)*(1-sys.GoosAix))*48 + (1-_64bit+sys.GoarchWasm)*(32-(sys.GoarchMips+sys.GoarchMipsle)) + 60*sys.GoosAix
+	//
+	// ARMv7-M defines 512MB SRAM region at 0x20000000 and 1GB External RAM
+	// region at 0x60000000. This limits the maximum value of memory address
+	// to 0x9FFFFFFF. arenaBaseOffset is used to offset addresses by
+	// -0x20000000 so all possible RAM fits into 2GB (offsetted). For now
+	// the supported address space is further limited to only 1 MB.
+	heapAddrBits = (_64bit*(1-sys.GoarchWasm)*(1-sys.GoosAix))*48 + (1-_64bit+sys.GoarchWasm)*(32-(sys.GoarchMips+sys.GoarchMipsle)-12*_ARMv7M) + 60*sys.GoosAix
 
 	// maxAlloc is the maximum size of an allocation. On 64-bit,
 	// it's theoretically possible to allocate 1<<heapAddrBits bytes. On
@@ -248,7 +258,7 @@ const (
 	// logHeapArenaBytes is log_2 of heapArenaBytes. For clarity,
 	// prefer using heapArenaBytes where possible (we need the
 	// constant to compute some other constants).
-	logHeapArenaBytes = (6+20)*(_64bit*(1-sys.GoosWindows)*(1-sys.GoosAix)*(1-sys.GoarchWasm)) + (2+20)*(_64bit*sys.GoosWindows) + (2+20)*(1-_64bit) + (8+20)*sys.GoosAix + (2+20)*sys.GoarchWasm
+	logHeapArenaBytes = (6+20)*(_64bit*(1-sys.GoosWindows)*(1-sys.GoosAix)*(1-sys.GoarchWasm)) + (2+20)*(_64bit*sys.GoosWindows) + (2+20)*(1-_64bit)*(1-_MCU) + 13*_MCU + (8+20)*sys.GoosAix + (2+20)*sys.GoarchWasm
 
 	// heapArenaBitmapBytes is the size of each heap arena's bitmap.
 	heapArenaBitmapBytes = heapArenaBytes / (sys.PtrSize * 8 / 2)
@@ -300,7 +310,9 @@ const (
 	//
 	// On other platforms, the user address space is contiguous
 	// and starts at 0, so no offset is necessary.
-	arenaBaseOffset uintptr = sys.GoarchAmd64 * (1 << 47)
+	//
+	// In case of ARMv7-M the SRAM region starts at 0x20000000.
+	arenaBaseOffset uintptr = sys.GoarchAmd64*(1<<47) + _ARMv7M*(1<<32-0x20000000)
 
 	// Max number of threads to run garbage collection.
 	// 2, 3, and 4 are all plausible maximums depending
@@ -521,6 +533,8 @@ func mallocinit() {
 			hint.addr = p
 			hint.next, mheap_.arenaHints = mheap_.arenaHints, hint
 		}
+	} else if _MCU == 1 {
+		mheap_.arena.init(sysReserveMaxArena())
 	} else {
 		// On a 32-bit machine, we're much more concerned
 		// about keeping the usable heap contiguous.
@@ -609,6 +623,9 @@ func (h *mheap) sysAlloc(n uintptr) (v unsafe.Pointer, size uintptr) {
 		size = n
 		goto mapped
 	}
+	if _MCU != 0 {
+		return nil, 0
+	}
 
 	// Try to grow the heap at a hint address.
 	for h.arenaHints != nil {
@@ -1283,6 +1300,10 @@ func persistentalloc(size, align uintptr, sysStat *uint64) unsafe.Pointer {
 // See issue 9174.
 //go:systemstack
 func persistentalloc1(size, align uintptr, sysStat *uint64) *notInHeap {
+	if _MCU != 0 {
+		return sysPersistentAlloc(size, align, sysStat)
+	}
+
 	const (
 		maxBlock = 64 << 10 // VM reservation granularity is 64K on windows
 	)
diff --git a/src/runtime/mbitmap.go b/src/runtime/mbitmap.go
index 30ec5f1cc9..6f8da43b72 100644
--- a/src/runtime/mbitmap.go
+++ b/src/runtime/mbitmap.go
@@ -859,39 +859,39 @@ func (h heapBits) clearCheckmarkSpan(size, n, total uintptr) {
 // oneBitCount is indexed by byte and produces the
 // number of 1 bits in that byte. For example 128 has 1 bit set
 // and oneBitCount[128] will holds 1.
-var oneBitCount = [256]uint8{
-	0, 1, 1, 2, 1, 2, 2, 3,
-	1, 2, 2, 3, 2, 3, 3, 4,
-	1, 2, 2, 3, 2, 3, 3, 4,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	1, 2, 2, 3, 2, 3, 3, 4,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	1, 2, 2, 3, 2, 3, 3, 4,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	4, 5, 5, 6, 5, 6, 6, 7,
-	1, 2, 2, 3, 2, 3, 3, 4,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	4, 5, 5, 6, 5, 6, 6, 7,
-	2, 3, 3, 4, 3, 4, 4, 5,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	4, 5, 5, 6, 5, 6, 6, 7,
-	3, 4, 4, 5, 4, 5, 5, 6,
-	4, 5, 5, 6, 5, 6, 6, 7,
-	4, 5, 5, 6, 5, 6, 6, 7,
-	5, 6, 6, 7, 6, 7, 7, 8}
+const oneBitCount = "" +
+	"\x00\x01\x01\x02\x01\x02\x02\x03" +
+	"\x01\x02\x02\x03\x02\x03\x03\x04" +
+	"\x01\x02\x02\x03\x02\x03\x03\x04" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x01\x02\x02\x03\x02\x03\x03\x04" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x01\x02\x02\x03\x02\x03\x03\x04" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x04\x05\x05\x06\x05\x06\x06\x07" +
+	"\x01\x02\x02\x03\x02\x03\x03\x04" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x04\x05\x05\x06\x05\x06\x06\x07" +
+	"\x02\x03\x03\x04\x03\x04\x04\x05" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x04\x05\x05\x06\x05\x06\x06\x07" +
+	"\x03\x04\x04\x05\x04\x05\x05\x06" +
+	"\x04\x05\x05\x06\x05\x06\x06\x07" +
+	"\x04\x05\x05\x06\x05\x06\x06\x07" +
+	"\x05\x06\x06\x07\x06\x07\x07\x08"
 
 // countAlloc returns the number of objects allocated in span s by
 // scanning the allocation bitmap.
diff --git a/src/runtime/mem_noos.go b/src/runtime/mem_noos.go
new file mode 100644
index 0000000000..958c78fb47
--- /dev/null
+++ b/src/runtime/mem_noos.go
@@ -0,0 +1,130 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+// Simple memory allocator that emulates OS allocator
+//
+// There are sysReserveMaxArena and sysPersistentAlloc functions specific to
+// noos target.
+//
+// sysReserveMaxArena allocates maximum arena space heapArenaBytes aligned for
+// mheap. It is intended to be run only once by mallocinit.
+//
+// sysReserve allocates down from sysMem.end always returning _PageSize aligned
+// memory.
+//
+// sysPersistentAlloc is fast and memory efficient implementation of
+// persistentalloc1. It uses sysReserve for _PageSize alignned allocations.
+// Otherwise it allocates from sysMem.start.
+
+var sysMem struct {
+	free, nodma           pamem
+	arenaStart, arenaSize uintptr
+	mx                    mutex
+}
+
+type pamem struct {
+	start, end uintptr
+}
+
+//go:nosplit
+func (m *pamem) allocPages(size uintptr) unsafe.Pointer {
+	var p uintptr
+	if m.end-m.start >= size {
+		p = m.end - size
+		m.end = p
+	}
+	return unsafe.Pointer(p)
+}
+
+//go:nosplit
+func (m *pamem) alloc(size, align uintptr) unsafe.Pointer {
+	var p uintptr
+	align-- // align must be power of two
+	astart := (m.start + align) &^ align
+	if m.end-astart >= size {
+		p = astart
+		m.start = astart + size
+	}
+	return unsafe.Pointer(p)
+}
+
+// sysReserveMaxArena is for initial reservation, must not be called
+// concurrently with any other reservation. TODO: Consider replace this by
+// direct initialization of mheap_.arena.
+//go:nosplit
+func sysReserveMaxArena() (addr, size uintptr) {
+	return sysMem.arenaStart, sysMem.arenaSize
+}
+
+//go:nosplit
+func sysReserve1(size uintptr) unsafe.Pointer {
+	lock(&sysMem.mx)
+	p := sysMem.free.allocPages(size)
+	if p == nil {
+		p = sysMem.nodma.allocPages(size)
+	}
+	unlock(&sysMem.mx)
+	return p
+}
+
+//go:nosplit
+func sysReserve(v unsafe.Pointer, size uintptr) unsafe.Pointer {
+	size += (_PageSize - 1)
+	size &^= (_PageSize - 1)
+	return sysReserve1(size)
+}
+
+//go:nosplit
+func sysAlloc(size uintptr, sysStat *uint64) unsafe.Pointer {
+	size += (_PageSize - 1)
+	size &^= (_PageSize - 1)
+	p := sysReserve1(size)
+	if p != nil {
+		mSysStatInc(sysStat, size)
+	}
+	return p
+}
+
+// align must be power of two
+//go:nosplit
+func sysPersistentAlloc(size, align uintptr, sysStat *uint64) (p *notInHeap) {
+	if size&(_PageSize-1) == 0 {
+		p = (*notInHeap)(sysReserve1(size))
+	} else {
+		if align == 0 {
+			align = 8
+		}
+		lock(&sysMem.mx)
+		p = (*notInHeap)(sysMem.free.alloc(size, align))
+		if p == nil {
+			p = (*notInHeap)(sysMem.nodma.alloc(size, align))
+		}
+		unlock(&sysMem.mx)
+	}
+	if p != nil {
+		mSysStatInc(sysStat, size)
+	}
+	return
+}
+
+//go:nosplit
+func sysMap(v unsafe.Pointer, n uintptr, sysStat *uint64) {
+	mSysStatInc(sysStat, n)
+}
+
+//go:nosplit
+func sysFree(v unsafe.Pointer, n uintptr, sysStat *uint64) {
+	mSysStatDec(sysStat, n)
+}
+
+func sysUnused(v unsafe.Pointer, n uintptr)   {}
+func sysUsed(v unsafe.Pointer, n uintptr)     {}
+func sysFault(v unsafe.Pointer, n uintptr)    {}
+func sysHugePage(v unsafe.Pointer, n uintptr) {}
+
+const mspanSize = unsafe.Sizeof(mspan{}) // used by assembly
diff --git a/src/runtime/memclr_thumb.s b/src/runtime/memclr_thumb.s
new file mode 100644
index 0000000000..ea3c67afc5
--- /dev/null
+++ b/src/runtime/memclr_thumb.s
@@ -0,0 +1,88 @@
+// Inferno's libkern/memset-arm.s
+// https://bitbucket.org/inferno-os/inferno-os/src/default/libkern/memset-arm.s
+//
+//         Copyright  1994-1999 Lucent Technologies Inc. All rights reserved.
+//         Revisions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com).  All rights reserved.
+//         Portions Copyright 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+#include "textflag.h"
+
+#define TO	R8
+#define TOE	R11
+#define N	R12
+#define TMP	R12				/* N and TMP don't overlap */
+
+// func memclrNoHeapPointers(ptr unsafe.Pointer, n uintptr)
+TEXT runtimememclrNoHeapPointers(SB),NOSPLIT,$0-8
+	MOVW	ptr+0(FP), TO
+	MOVW	n+4(FP), N
+	MOVW	$0, R0
+
+	ADD	N, TO, TOE	/* to end pointer */
+
+	CMP	$4, N		/* need at least 4 bytes to copy */
+	BLT	_1tail
+
+_4align:				/* align on 4 */
+	AND.S	$3, TO, TMP
+	BEQ	_4aligned
+
+	MOVBU.P	R0, 1(TO)		/* implicit write back */
+	B	_4align
+
+_4aligned:
+	SUB	$31, TOE, TMP	/* do 32-byte chunks if possible */
+	CMP	TMP, TO
+	BHS	_4tail
+
+	MOVW	R0, R1			/* replicate */
+	MOVW	R0, R2
+	MOVW	R0, R3
+	MOVW	R0, R4
+	MOVW	R0, R5
+	MOVW	R0, R6
+	MOVW	R0, R7
+
+_f32loop:
+	CMP	TMP, TO
+	BHS	_4tail
+
+	MOVM.IA.W [R0-R7], (TO)
+	B	_f32loop
+
+_4tail:
+	SUB	$3, TOE, TMP	/* do remaining words if possible */
+_4loop:
+	CMP	TMP, TO
+	BHS	_1tail
+
+	MOVW.P	R0, 4(TO)		/* implicit write back */
+	B	_4loop
+
+_1tail:
+	CMP	TO, TOE
+	BEQ	_return
+
+	MOVBU.P	R0, 1(TO)		/* implicit write back */
+	B	_1tail
+
+_return:
+	RET
diff --git a/src/runtime/memmove_thumb.s b/src/runtime/memmove_thumb.s
new file mode 100644
index 0000000000..1790cd583d
--- /dev/null
+++ b/src/runtime/memmove_thumb.s
@@ -0,0 +1,108 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+// The ARMv7-M architecture supports unaligned MOVW, MOVH. An implementation can
+// be configured to force alignment faults but no real one does this.
+
+// Writes are often buffered so this code aligns reads and allows unaligned
+// writes. Ensure CCR.UNALIGN_TRP=0.
+
+// TODO: better performance (eg. full aligned copy using MOVM)
+
+// func memmove(to, from unsafe.Pointer, n uintptr)
+TEXT runtimememmove(SB), NOSPLIT|NOFRAME, $0-12
+	MOVW  to+0(FP), R0
+	MOVW  from+4(FP), R1
+	MOVW  n+8(FP), R2
+
+	CMP  $4, R2
+	BHI  more
+
+// Fast path for 0, 1, 2, 3, 4 bytes.
+
+	TBB    R2, R15
+	HWORD  $0x0305
+	HWORD  $0x0906
+	HWORD  $0x0E
+
+	MOVBU  (R1), R3  // n == 1 (TBB offset 0x03)
+	MOVBU  R3, (R0)
+	RET        // n == 0 (TBB offset 0x05)
+
+	MOVHU  (R1), R3  // n == 2 (TBB offset 0x06)
+	MOVHU  R3, (R0)
+	RET    
+
+	MOVHU  (R1), R3  // n == 3 (TBB offset 0x09)
+	MOVBU  2(R1),R2
+	MOVHU  R3, (R0)
+	MOVBU  R2, 2(R0)
+	RET    
+
+	MOVW  (R1), R3  // n == 4 (TBB offset 0x0E)
+	MOVW  R3, (R0)
+end:
+	RET  
+
+more: // now we are sure that n > 4
+	CMP  R0, R1
+	BLO  backward
+
+// Forward copy
+
+fhead: // head copy (up to 3 bytes) to make src (R1) word aligned
+	TST      $3, R1
+	BEQ      fwords
+	MOVBU.P  1(R1), R3
+	MOVBU.P  R3, 1(R0)
+	SUB      $1, R2
+	B        fhead
+
+fwords: // copy words
+	SUB.S   $4, R2
+	BLT     fwend
+	MOVW.P  4(R1), R3
+	MOVW.P  R3, 4(R0)
+	B       fwords
+fwend:
+	ADD.S  $4, R2
+
+ftail: // tail copy (up to 3 bytes)
+	BEQ      end
+	MOVBU.P  1(R1), R3
+	MOVBU.P  R3, 1(R0)
+	SUB.S    $1, R2
+	B        ftail
+
+// Backward copy
+
+backward:
+	ADD  R2, R0
+	ADD  R2, R1
+
+btail: // tail copy (up to 3 bytes) to make src (R1) word aligned
+	TST      $3, R1
+	BEQ      bwords
+	MOVBU.W  -1(R1), R3
+	MOVBU.W  R3, -1(R0)
+	SUB      $1, R2
+	B        btail
+
+bwords: // copy words
+	SUB.S   $4, R2
+	BLT     bwend
+	MOVW.W  -4(R1), R3
+	MOVW.W  R3, -4(R0)
+	B       bwords
+bwend:
+	ADD.S  $4, R2
+
+bhead: // head copy (up to 3 bytes)
+	BEQ      end
+	MOVBU.W  -1(R1), R3
+	MOVBU.W  R3, -1(R0)
+	SUB.S    $1, R2
+	B        bhead
diff --git a/src/runtime/mfixalloc.go b/src/runtime/mfixalloc.go
index f9dd6ca474..4741f7ee19 100644
--- a/src/runtime/mfixalloc.go
+++ b/src/runtime/mfixalloc.go
@@ -77,8 +77,19 @@ func (f *fixalloc) alloc() unsafe.Pointer {
 		return v
 	}
 	if uintptr(f.nchunk) < f.size {
-		f.chunk = uintptr(persistentalloc(_FixAllocChunk, 0, f.stat))
-		f.nchunk = _FixAllocChunk
+		if _MCU != 0 {
+			nchunk := f.size
+			if nchunk == unsafe.Sizeof(mspan{}) {
+				nchunk *= 16 // five 256 byte pages for 80 byte mspan
+			} else {
+				nchunk *= 2
+			}
+			f.nchunk = uint32(nchunk)
+			f.chunk = uintptr(persistentalloc(nchunk, 0, f.stat))
+		} else {
+			f.chunk = uintptr(persistentalloc(_FixAllocChunk, 0, f.stat))
+			f.nchunk = _FixAllocChunk
+		}
 	}
 
 	v := unsafe.Pointer(f.chunk)
diff --git a/src/runtime/mgc.go b/src/runtime/mgc.go
index 8a6b860c96..e85a8d258e 100644
--- a/src/runtime/mgc.go
+++ b/src/runtime/mgc.go
@@ -137,12 +137,12 @@ import (
 const (
 	_DebugGC         = 0
 	_ConcurrentSweep = true
-	_FinBlockSize    = 4 * 1024
+	_FinBlockSize    = 4*1024*(1-_MCU) + 1024*_MCU
 
 	// sweepMinHeapDistance is a lower bound on the heap distance
 	// (in bytes) reserved for concurrent sweeping between GC
 	// cycles.
-	sweepMinHeapDistance = 1024 * 1024
+	sweepMinHeapDistance = 1024*1024*(1-_MCU) + 1024*_MCU
 )
 
 // heapminimum is the minimum heap size at which to trigger GC.
@@ -160,7 +160,7 @@ const (
 var heapminimum uint64 = defaultHeapMinimum
 
 // defaultHeapMinimum is the value of heapminimum for GOGC==100.
-const defaultHeapMinimum = 4 << 20
+const defaultHeapMinimum = 4<<20*(1-_MCU) + 8<<10*_MCU
 
 // Initialized from $GOGC.  GOGC=off means no GC.
 var gcpercent int32
@@ -197,7 +197,11 @@ func readgogc() int32 {
 	if n, ok := atoi32(p); ok {
 		return n
 	}
-	return 100
+	if _MCU != 0 {
+		return 10
+	} else {
+		return 100
+	}
 }
 
 // gcenable is called after the bulk of the runtime initialization,
@@ -420,8 +424,8 @@ func (c *gcControllerState) startCycle() {
 	// GOGC. Assist is proportional to this distance, so enforce a
 	// minimum distance, even if it means going over the GOGC goal
 	// by a tiny bit.
-	if memstats.next_gc < memstats.heap_live+1024*1024 {
-		memstats.next_gc = memstats.heap_live + 1024*1024
+	if memstats.next_gc < memstats.heap_live+1024*1024*(1-_MCU)+1024*_MCU {
+		memstats.next_gc = memstats.heap_live + 1024*1024*(1-_MCU) + 1024*_MCU
 	}
 
 	// Compute the background mark utilization goal. In general,
@@ -1758,10 +1762,17 @@ func gcMarkTermination(nextTriggerRatio float64) {
 			}
 			print(string(fmtNSAsMS(sbuf[:], uint64(ns))))
 		}
-		print(" ms cpu, ",
-			work.heap0>>20, "->", work.heap1>>20, "->", work.heap2>>20, " MB, ",
-			work.heapGoal>>20, " MB goal, ",
-			work.maxprocs, " P")
+		if _MCU == 0 {
+			print(" ms cpu, ",
+				work.heap0>>20, "->", work.heap1>>20, "->", work.heap2>>20, " MB, ",
+				work.heapGoal>>20, " MB goal, ",
+				work.maxprocs, " P")
+		} else {
+			print(" ms cpu, ",
+				work.heap0>>10, "->", work.heap1>>10, "->", work.heap2>>10, " KB, ",
+				work.heapGoal>>10, " KB goal, ",
+				work.maxprocs, " P")
+		}
 		if work.userForced {
 			print(" (forced)")
 		}
diff --git a/src/runtime/mgcsweepbuf.go b/src/runtime/mgcsweepbuf.go
index 0491f7ccf6..c5b9e9eb3b 100644
--- a/src/runtime/mgcsweepbuf.go
+++ b/src/runtime/mgcsweepbuf.go
@@ -43,8 +43,8 @@ type gcSweepBuf struct {
 }
 
 const (
-	gcSweepBlockEntries    = 512 // 4KB on 64-bit
-	gcSweepBufInitSpineCap = 256 // Enough for 1GB heap on 64-bit
+	gcSweepBlockEntries    = 512 - 448*_MCU // 4KB on 64-bit
+	gcSweepBufInitSpineCap = 256 - 192*_MCU // Enough for 1GB heap on 64-bit
 )
 
 type gcSweepBlock struct {
diff --git a/src/runtime/mgcwork.go b/src/runtime/mgcwork.go
index f2c16d7d8c..a40f51a6e8 100644
--- a/src/runtime/mgcwork.go
+++ b/src/runtime/mgcwork.go
@@ -11,7 +11,7 @@ import (
 )
 
 const (
-	_WorkbufSize = 2048 // in bytes; larger values result in less contention
+	_WorkbufSize = 2048*(1-_MCU) + _PageSize*_MCU // in bytes; larger values result in less contention
 
 	// workbufAlloc is the number of bytes to allocate at a time
 	// for new workbufs. This must be a multiple of pageSize and
@@ -19,7 +19,7 @@ const (
 	//
 	// Larger values reduce workbuf allocation overhead. Smaller
 	// values reduce heap fragmentation.
-	workbufAlloc = 32 << 10
+	workbufAlloc = 32<<10*(1-_MCU) + 2*_WorkbufSize*_MCU
 )
 
 // throwOnGCWork causes any operations that add pointers to a gcWork
diff --git a/src/runtime/mheap.go b/src/runtime/mheap.go
index 3807050cbe..c23d03876b 100644
--- a/src/runtime/mheap.go
+++ b/src/runtime/mheap.go
@@ -18,7 +18,7 @@ import (
 // minPhysPageSize is a lower-bound on the physical page size. The
 // true physical page size may be larger than this. In contrast,
 // sys.PhysPageSize is an upper-bound on the physical page size.
-const minPhysPageSize = 4096
+const minPhysPageSize = 4096*(1-_MCU) + 256*_MCU
 
 // Main malloc heap.
 // The heap itself is the "free" and "scav" treaps,
@@ -622,7 +622,7 @@ func recordspan(vh unsafe.Pointer, p unsafe.Pointer) {
 	h := (*mheap)(vh)
 	s := (*mspan)(p)
 	if len(h.allspans) >= cap(h.allspans) {
-		n := 64 * 1024 / sys.PtrSize
+		n := 8 * _PageSize / sys.PtrSize
 		if n < cap(h.allspans)*3/2 {
 			n = cap(h.allspans) * 3 / 2
 		}
@@ -866,7 +866,7 @@ func (h *mheap) reclaim(npage uintptr) {
 	// locking/unlocking the heap (even with low contention). We
 	// could make the slow path here several times faster by
 	// batching heap frees.
-	const pagesPerChunk = 512
+	const pagesPerChunk = 512*(1-_MCU) + pagesPerArena*_MCU
 
 	// Bail early if there's no more reclaim work.
 	if atomic.Load64(&h.reclaimIndex) >= 1<<63 {
@@ -1888,7 +1888,7 @@ func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8) {
 	return b.bytep(n / 8), 1 << (n % 8)
 }
 
-const gcBitsChunkBytes = uintptr(64 << 10)
+const gcBitsChunkBytes = uintptr((64 - 62*_MCU) << 10)
 const gcBitsHeaderBytes = unsafe.Sizeof(gcBitsHeader{})
 
 type gcBitsHeader struct {
diff --git a/src/runtime/mkduff.go b/src/runtime/mkduff.go
index b6fe701497..3d04ebd762 100644
--- a/src/runtime/mkduff.go
+++ b/src/runtime/mkduff.go
@@ -35,6 +35,7 @@ func main() {
 	gen("amd64", notags, zeroAMD64, copyAMD64)
 	gen("386", notags, zero386, copy386)
 	gen("arm", notags, zeroARM, copyARM)
+	gen("thumb", notags, zeroARM, copyARM)
 	gen("arm64", notags, zeroARM64, copyARM64)
 	gen("ppc64x", tagsPPC64x, zeroPPC64x, copyPPC64x)
 	gen("mips64x", tagsMIPS64x, zeroMIPS64x, copyMIPS64x)
diff --git a/src/runtime/mksizeclasses.go b/src/runtime/mksizeclasses.go
index cacbb64207..a8d66ffa17 100644
--- a/src/runtime/mksizeclasses.go
+++ b/src/runtime/mksizeclasses.go
@@ -51,6 +51,8 @@ func main() {
 	fmt.Fprintln(&b, "// Code generated by mksizeclasses.go; DO NOT EDIT.")
 	fmt.Fprintln(&b, "//go:generate go run mksizeclasses.go")
 	fmt.Fprintln(&b)
+	fmt.Fprintln(&b, "// +build !noos")
+	fmt.Fprintln(&b)
 	fmt.Fprintln(&b, "package runtime")
 	classes := makeClasses()
 
diff --git a/src/runtime/mksizeclasses_noos.go b/src/runtime/mksizeclasses_noos.go
new file mode 100644
index 0000000000..0d1780ecd1
--- /dev/null
+++ b/src/runtime/mksizeclasses_noos.go
@@ -0,0 +1,328 @@
+// Copyright 2016 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build ignore
+
+// Generate tables for small malloc size classes.
+//
+// See malloc.go for overview.
+//
+// The size classes are chosen so that rounding an allocation
+// request up to the next size class wastes at most 12.5% (1.125x).
+//
+// Each size class has its own page count that gets allocated
+// and chopped up when new objects of the size class are needed.
+// That page count is chosen so that chopping up the run of
+// pages into objects of the given size wastes at most 12.5% (1.125x)
+// of the memory. It is not necessary that the cutoff here be
+// the same as above.
+//
+// The two sources of waste multiply, so the worst possible case
+// for the above constraints would be that allocations of some
+// size might have a 26.6% (1.266x) overhead.
+// In practice, only one of the wastes comes into play for a
+// given size (sizes < 512 waste mainly on the round-up,
+// sizes > 512 waste mainly on the page chopping).
+// For really small sizes, alignment constraints force the
+// overhead higher.
+
+package main
+
+import (
+	"bytes"
+	"flag"
+	"fmt"
+	"go/format"
+	"io"
+	"io/ioutil"
+	"log"
+	"os"
+)
+
+// Generate msize.go
+
+var stdout = flag.Bool("stdout", false, "write to stdout instead of sizeclasses.go")
+
+func main() {
+	flag.Parse()
+
+	var b bytes.Buffer
+	fmt.Fprintln(&b, "// Code generated by mksizeclasses_noos.go; DO NOT EDIT.")
+	fmt.Fprintln(&b, "//go:generate go run mksizeclasses_noos.go")
+	fmt.Fprintln(&b)
+	fmt.Fprintln(&b, "package runtime")
+	classes := makeClasses()
+
+	printComment(&b, classes)
+
+	printClasses(&b, classes)
+
+	out, err := format.Source(b.Bytes())
+	if err != nil {
+		log.Fatal(err)
+	}
+	if *stdout {
+		_, err = os.Stdout.Write(out)
+	} else {
+		err = ioutil.WriteFile("sizeclasses_noos.go", out, 0666)
+	}
+	if err != nil {
+		log.Fatal(err)
+	}
+}
+
+const (
+	// Constants that we use and will transfer to the runtime.
+	maxSmallSize = 1 << 9
+	smallSizeDiv = 8
+	smallSizeMax = 256
+	largeSizeDiv = 128
+	pageShift    = 8
+
+	// Derived constants.
+	pageSize = 1 << pageShift
+)
+
+type class struct {
+	size   int // max size
+	npages int // number of pages
+
+	mul    int
+	shift  uint
+	shift2 uint
+	mask   int
+}
+
+func powerOfTwo(x int) bool {
+	return x != 0 && x&(x-1) == 0
+}
+
+func makeClasses() []class {
+	var classes []class
+
+	classes = append(classes, class{}) // class #0 is a dummy entry
+
+	align := 8
+	for size := align; size <= maxSmallSize; size += align {
+		if powerOfTwo(size) { // bump alignment once in a while
+			if size >= 2048 {
+				align = 256
+			} else if size >= 128 {
+				align = size / 8
+			} else if size >= 16 {
+				align = 16 // required for x86 SSE instructions, if we want to use them
+			}
+		}
+		if !powerOfTwo(align) {
+			panic("incorrect alignment")
+		}
+
+		// Make the allocnpages big enough that
+		// the leftover is less than 1/8 of the total,
+		// so wasted space is at most 12.5%.
+		allocsize := pageSize
+		for allocsize%size > allocsize/8 {
+			allocsize += pageSize
+		}
+		npages := allocsize / pageSize
+
+		// If the previous sizeclass chose the same
+		// allocation size and fit the same number of
+		// objects into the page, we might as well
+		// use just this size instead of having two
+		// different sizes.
+		if len(classes) > 1 && npages == classes[len(classes)-1].npages && allocsize/size == allocsize/classes[len(classes)-1].size {
+			classes[len(classes)-1].size = size
+			continue
+		}
+		classes = append(classes, class{size: size, npages: npages})
+	}
+
+	// Increase object sizes if we can fit the same number of larger objects
+	// into the same number of pages. For example, we choose size 8448 above
+	// with 6 objects in 7 pages. But we can well use object size 9472,
+	// which is also 6 objects in 7 pages but +1024 bytes (+12.12%).
+	// We need to preserve at least largeSizeDiv alignment otherwise
+	// sizeToClass won't work.
+	for i := range classes {
+		if i == 0 {
+			continue
+		}
+		c := &classes[i]
+		psize := c.npages * pageSize
+		new_size := (psize / (psize / c.size)) &^ (largeSizeDiv - 1)
+		if new_size > c.size {
+			c.size = new_size
+		}
+	}
+
+	if len(classes) != 19 {
+		panic("number of size classes has changed")
+	}
+
+	for i := range classes {
+		computeDivMagic(&classes[i])
+	}
+
+	return classes
+}
+
+// computeDivMagic computes some magic constants to implement
+// the division required to compute object number from span offset.
+// n / c.size is implemented as n >> c.shift * c.mul >> c.shift2
+// for all 0 <= n < c.npages * pageSize
+func computeDivMagic(c *class) {
+	// divisor
+	d := c.size
+	if d == 0 {
+		return
+	}
+
+	// maximum input value for which the formula needs to work.
+	max := c.npages*pageSize - 1
+
+	if powerOfTwo(d) {
+		// If the size is a power of two, heapBitsForObject can divide even faster by masking.
+		// Compute this mask.
+		if max >= 1<<16 {
+			panic("max too big for power of two size")
+		}
+		c.mask = 1<<16 - d
+	}
+
+	// Compute pre-shift by factoring power of 2 out of d.
+	for d%2 == 0 {
+		c.shift++
+		d >>= 1
+		max >>= 1
+	}
+
+	// Find the smallest k that works.
+	// A small k allows us to fit the math required into 32 bits
+	// so we can use 32-bit multiplies and shifts on 32-bit platforms.
+nextk:
+	for k := uint(0); ; k++ {
+		mul := (int(1)<<k + d - 1) / d //  2^k / d
+
+		// Test to see if mul works.
+		for n := 0; n <= max; n++ {
+			if n*mul>>k != n/d {
+				continue nextk
+			}
+		}
+		if mul >= 1<<16 {
+			panic("mul too big")
+		}
+		if uint64(mul)*uint64(max) >= 1<<32 {
+			panic("mul*max too big")
+		}
+		c.mul = mul
+		c.shift2 = k
+		break
+	}
+
+	// double-check.
+	for n := 0; n <= max; n++ {
+		if n*c.mul>>c.shift2 != n/d {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad multiply magic")
+		}
+		// Also check the exact computations that will be done by the runtime,
+		// for both 32 and 64 bit operations.
+		if uint32(n)*uint32(c.mul)>>uint8(c.shift2) != uint32(n/d) {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad 32-bit multiply magic")
+		}
+		if uint64(n)*uint64(c.mul)>>uint8(c.shift2) != uint64(n/d) {
+			fmt.Printf("d=%d max=%d mul=%d shift2=%d n=%d\n", d, max, c.mul, c.shift2, n)
+			panic("bad 64-bit multiply magic")
+		}
+	}
+}
+
+func printComment(w io.Writer, classes []class) {
+	fmt.Fprintf(w, "// %-5s  %-9s  %-10s  %-7s  %-10s  %-9s\n", "class", "bytes/obj", "bytes/span", "objects", "tail waste", "max waste")
+	prevSize := 0
+	for i, c := range classes {
+		if i == 0 {
+			continue
+		}
+		spanSize := c.npages * pageSize
+		objects := spanSize / c.size
+		tailWaste := spanSize - c.size*(spanSize/c.size)
+		maxWaste := float64((c.size-prevSize-1)*objects+tailWaste) / float64(spanSize)
+		prevSize = c.size
+		fmt.Fprintf(w, "// %5d  %9d  %10d  %7d  %10d  %8.2f%%\n", i, c.size, spanSize, objects, tailWaste, 100*maxWaste)
+	}
+	fmt.Fprintf(w, "\n")
+}
+
+func printClasses(w io.Writer, classes []class) {
+	fmt.Fprintln(w, "const (")
+	fmt.Fprintf(w, "_MaxSmallSize = %d\n", maxSmallSize)
+	fmt.Fprintf(w, "smallSizeDiv = %d\n", smallSizeDiv)
+	fmt.Fprintf(w, "smallSizeMax = %d\n", smallSizeMax)
+	fmt.Fprintf(w, "largeSizeDiv = %d\n", largeSizeDiv)
+	fmt.Fprintf(w, "_NumSizeClasses = %d\n", len(classes))
+	fmt.Fprintf(w, "_PageShift = %d\n", pageShift)
+	fmt.Fprintln(w, ")")
+
+	fmt.Fprint(w, "var class_to_size = [_NumSizeClasses]uint16 {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "%d,", c.size)
+	}
+	fmt.Fprintln(w, "}")
+
+	fmt.Fprint(w, "var class_to_allocnpages = [_NumSizeClasses]uint8 {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "%d,", c.npages)
+	}
+	fmt.Fprintln(w, "}")
+
+	fmt.Fprintln(w, "type divMagic struct {")
+	fmt.Fprintln(w, "  shift uint8")
+	fmt.Fprintln(w, "  shift2 uint8")
+	fmt.Fprintln(w, "  mul uint16")
+	fmt.Fprintln(w, "  baseMask uint16")
+	fmt.Fprintln(w, "}")
+	fmt.Fprint(w, "var class_to_divmagic = [_NumSizeClasses]divMagic {")
+	for _, c := range classes {
+		fmt.Fprintf(w, "{%d,%d,%d,%d},", c.shift, c.shift2, c.mul, c.mask)
+	}
+	fmt.Fprintln(w, "}")
+
+	// map from size to size class, for small sizes.
+	sc := make([]int, smallSizeMax/smallSizeDiv+1)
+	for i := range sc {
+		size := i * smallSizeDiv
+		for j, c := range classes {
+			if c.size >= size {
+				sc[i] = j
+				break
+			}
+		}
+	}
+	fmt.Fprint(w, "var size_to_class8 = [smallSizeMax/smallSizeDiv+1]uint8 {")
+	for _, v := range sc {
+		fmt.Fprintf(w, "%d,", v)
+	}
+	fmt.Fprintln(w, "}")
+
+	// map from size to size class, for large sizes.
+	sc = make([]int, (maxSmallSize-smallSizeMax)/largeSizeDiv+1)
+	for i := range sc {
+		size := smallSizeMax + i*largeSizeDiv
+		for j, c := range classes {
+			if c.size >= size {
+				sc[i] = j
+				break
+			}
+		}
+	}
+	fmt.Fprint(w, "var size_to_class128 = [(_MaxSmallSize-smallSizeMax)/largeSizeDiv+1]uint8 {")
+	for _, v := range sc {
+		fmt.Fprintf(w, "%d,", v)
+	}
+	fmt.Fprintln(w, "}")
+}
diff --git a/src/runtime/mprof.go b/src/runtime/mprof.go
index 2bd41b650f..e74463eefb 100644
--- a/src/runtime/mprof.go
+++ b/src/runtime/mprof.go
@@ -5,6 +5,8 @@
 // Malloc profiling.
 // Patterned after tcmalloc's algorithms; shorter code.
 
+// +build !noos
+
 package runtime
 
 import (
@@ -490,7 +492,7 @@ func (r *StackRecord) Stack() []uintptr {
 // memory profiling rate should do so just once, as early as
 // possible in the execution of the program (for example,
 // at the beginning of main).
-var MemProfileRate int = 512 * 1024
+var MemProfileRate int = 512 * 1024 * (1 - _MCU)
 
 // A MemProfileRecord describes the live objects allocated
 // by a particular call sequence (stack trace).
diff --git a/src/runtime/mprof_noos.go b/src/runtime/mprof_noos.go
new file mode 100644
index 0000000000..87f4ef5770
--- /dev/null
+++ b/src/runtime/mprof_noos.go
@@ -0,0 +1,28 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+const (
+	blockprofilerate = 0
+	mutexprofilerate = 0
+)
+
+var MemProfileRate int = 0
+
+//go:notinheap
+type bucket struct{}
+
+func blockevent(cycles int64, skip int)                     {}
+func tracealloc(p unsafe.Pointer, size uintptr, typ *_type) {}
+func mProf_Malloc(p unsafe.Pointer, size uintptr)           {}
+func mProf_PostSweep()                                      {}
+func mProf_NextCycle()                                      {}
+func mProf_Flush()                                          {}
+func tracegc()                                              {}
+func tracefree(p unsafe.Pointer, size uintptr)              {}
+func mProf_Free(b *bucket, size uintptr)                    {}
+func mutexevent(cycles int64, skip int)                     {}
diff --git a/src/runtime/mq.go b/src/runtime/mq.go
new file mode 100644
index 0000000000..94893d1b87
--- /dev/null
+++ b/src/runtime/mq.go
@@ -0,0 +1,245 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/atomic"
+)
+
+// A mq represents a queue of threads.
+type mq struct {
+	_mqprivate
+}
+
+type _mqprivate struct {
+	first muintptr
+	last  muintptr
+	n     uint
+	mx    cpumtx
+}
+
+//go:nosplit
+func (q *mq) lock() { q.mx.lock() }
+
+//go:nosplit
+func (q *mq) unlock() { q.mx.unlock() }
+
+// atomicLen returns the approximate number of elements in the q. It returns an
+// exact value if called by the only mutator of q.
+//go:nosplit
+func (q *mq) atomicLen() int { return int(atomic.Loaduint(&q.n)) }
+
+// push inserts m at the end of q
+//go:nosplit
+func (q *mq) push(m *m) {
+	if q.n == 0 {
+		q.first.set(m)
+	} else {
+		msetnext1(q.last.ptr(), m)
+	}
+	q.last.set(m)
+	q.n++
+}
+
+// pop removes the first m from the beginning of q and returns it
+//go:nosplit
+func (q *mq) pop() *m {
+	var ret *m
+	if q.n != 0 {
+		q.n--
+		ret = q.first.ptr()
+		q.first.set(mnext1(ret))
+	}
+	return ret
+}
+
+// A mcl represents a circular list of threads.
+type mcl struct {
+	_mclprivate
+	//_ [(cpu.CacheLinePadSize - unsafe.Sizeof(_mclprivate{}))]byte
+}
+
+type _mclprivate struct {
+	cur muintptr
+	n   uint
+	mx  cpumtx
+}
+
+//go:nosplit
+func (q *mcl) lock() { q.mx.lock() }
+
+//go:nosplit
+func (q *mcl) unlock() { q.mx.unlock() }
+
+// push inserts m just before the current m.
+//go:nosplit
+func (q *mcl) push(m *m) {
+	if q.n == 0 {
+		msetnext1(m, m)
+		msetprev1(m, m)
+		q.cur.set(m)
+	} else {
+		cur := q.cur.ptr()
+		prev := mprev1(cur)
+		msetprev1(m, prev)
+		msetnext1(m, cur)
+		msetnext1(prev, m)
+		msetprev1(cur, m)
+	}
+	q.n++
+}
+
+// find finds and returns the pointer to the first m in q that matches the
+// provided key. As a side effect it rotates the q so the m.next becomes the
+// current element.
+//go:nosplit
+func (q *mcl) find(key uintptr) *m {
+	if q.n == 0 {
+		return nil
+	}
+	cur := q.cur.ptr()
+	first := cur
+	for {
+		if mkey(cur) == key {
+			q.cur.set(mnext1(cur))
+			return cur
+		}
+		cur = mnext1(cur)
+		if cur == first {
+			return nil
+		}
+	}
+}
+
+// remove removes m from q. Remove is fast (O(1)) but the caller must ensure
+// that m belongs to q, othervise the effect of remove is unpredictable.
+//go:nosplit
+func (q *mcl) remove(m *m) {
+	q.n--
+	if q.n == 0 {
+		return
+	}
+	prev := mprev1(m)
+	next := mnext1(m)
+	if m == q.cur.ptr() {
+		q.cur.set(next)
+	}
+	msetnext1(prev, next)
+	msetprev1(next, prev)
+}
+
+// A msl represents a sorted list of threads.
+type msl struct {
+	_mslprivate
+}
+
+type _mslprivate struct {
+	head muintptr
+	n    uint
+	mx   cpumtx
+}
+
+//go:nosplit
+func (q *msl) lock() { q.mx.lock() }
+
+//go:nosplit
+func (q *msl) unlock() { q.mx.unlock() }
+
+// insertbyval inserts m into q just before the first item with greater value
+//go:nosplit
+func (q *msl) insertbyval(m *m) {
+	if q.n == 0 {
+		msetnext2(m, m)
+		msetprev2(m, m)
+		q.head.set(m)
+	} else {
+		val := mval(m)
+		// search from the last to the first because mval(m) is strongly
+		// corelated with the current (monotonic) time
+		last := mprev2(q.head.ptr())
+		cur := last
+		for {
+			if mval(cur) <= val {
+				break
+			}
+			cur = mprev2(cur)
+			if cur == last {
+				q.head.set(m)
+				break
+			}
+		}
+		next := mnext2(cur)
+		msetprev2(m, cur)
+		msetnext2(m, next)
+		msetnext2(cur, m)
+		msetprev2(next, m)
+
+	}
+	q.n++
+}
+
+// first returns the pointer to the first m in q.
+//go:nosplit
+func (q *msl) first() *m {
+	if q.n == 0 {
+		return nil
+	}
+	return q.head.ptr()
+}
+
+// remove removes m from q. Remove is fast (O(1)) but the caller must ensure
+// that m belongs to q, othervise the effect of remove is unpredictable.
+//go:nosplit
+func (q *msl) remove(m *m) {
+	q.n--
+	if q.n == 0 {
+		return
+	}
+	prev := mprev2(m)
+	next := mnext2(m)
+	if m == q.head.ptr() {
+		q.head.set(next)
+	}
+	msetnext2(prev, next)
+	msetprev2(next, prev)
+}
+
+// m fields used
+
+//go:nosplit
+func mprev1(m *m) *m { return muintptr(m.tls[0]).ptr() }
+
+//go:nosplit
+func mnext1(m *m) *m { return muintptr(m.tls[1]).ptr() }
+
+//go:nosplit
+func mprev2(m *m) *m { return muintptr(m.tls[2]).ptr() }
+
+//go:nosplit
+func mnext2(m *m) *m { return muintptr(m.tls[3]).ptr() }
+
+//go:nosplit
+func mval(m *m) int64 { return int64(m.ncgocall) }
+
+//go:nosplit
+func mkey(m *m) uintptr { return m.thread }
+
+//go:nosplit
+func msetprev1(m, prev *m) { (*muintptr)(&m.tls[0]).set(prev) }
+
+//go:nosplit
+func msetnext1(m, next *m) { (*muintptr)(&m.tls[1]).set(next) }
+
+//go:nosplit
+func msetprev2(m, prev *m) { (*muintptr)(&m.tls[2]).set(prev) }
+
+//go:nosplit
+func msetnext2(m, next *m) { (*muintptr)(&m.tls[3]).set(next) }
+
+//go:nosplit
+func msetval(m *m, val int64) { m.ncgocall = uint64(val) }
+
+//go:nosplit
+func msetkey(m *m, key uintptr) { m.thread = key }
diff --git a/src/runtime/mq_test.go b/src/runtime/mq_test.go
new file mode 100644
index 0000000000..dd76fb46b0
--- /dev/null
+++ b/src/runtime/mq_test.go
@@ -0,0 +1,312 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "runtime/internal/atomic"
+
+const fillkeys = "ABCDEFGHIJ"
+
+var (
+	testms [len(fillkeys)]m // global to avoid GC (mq uses uintptr pointers)
+	testm  m
+	run    uint32
+)
+
+func mqfill(q *mq) {
+	for i := range testms {
+		m := &testms[i]
+		msetkey(m, uintptr(fillkeys[i]))
+		q.push(m)
+	}
+}
+
+func mclfill(q *mcl) {
+	for i := range testms {
+		m := &testms[i]
+		msetkey(m, uintptr(fillkeys[i]))
+		q.push(m)
+	}
+}
+
+func (q *mcl) removebykey(key uintptr) *m {
+	m := q.find(key)
+	if m != nil {
+		q.remove(m)
+	}
+	return m
+}
+
+func (q *msl) pople(val int64) *m {
+	m := q.first()
+	if m == nil || mval(m) > val {
+		return nil
+	}
+	q.remove(m)
+	return m
+}
+
+func (q *mq) str() string {
+	if q.n == 0 {
+		return ""
+	}
+	var buf [len(testms)]byte
+	n := 0
+	for m := q.first.ptr(); ; m = mnext1(m) {
+		buf[n] = byte(mkey(m))
+		n++
+		if m == q.last.ptr() {
+			break
+		}
+	}
+	return string(buf[:n])
+}
+
+func (q *mcl) str() string {
+	if q.n == 0 {
+		return ""
+	}
+	var buf [len(testms)]byte
+	cur := q.cur.ptr()
+	m := cur
+	n := 0
+	for {
+		buf[n] = byte(mkey(m))
+		n++
+		m = mnext1(m)
+		if m == cur {
+			break
+		}
+	}
+	return string(buf[:n])
+}
+
+func (q *msl) str() string {
+	if q.n == 0 {
+		return ""
+	}
+	var buf [len(testms)]byte
+	first := q.head.ptr()
+	m := first
+	n := 0
+	for {
+		buf[n] = byte(mval(m) >> 30)
+		n++
+		m = mnext2(m)
+		if m == first {
+			break
+		}
+	}
+	return string(buf[:n])
+}
+
+var mvi int
+
+func mv(val byte) *m {
+	m := &testms[mvi]
+	mvi++
+	msetval(m, int64(val)<<30)
+	return m
+}
+
+func MQTest() string {
+	if !atomic.Cas(&run, 0, 1) {
+		return "" // run only once
+	}
+
+	q := new(mq)
+
+	mqfill(q)
+	if q.str() != fillkeys {
+		return "mq: fill"
+	}
+	for i := 0; i < len(fillkeys); i++ {
+		m := q.pop()
+		if mkey(m) != uintptr(fillkeys[i]) || q.str() != fillkeys[i+1:] {
+			return "mq: pop"
+		}
+	}
+	if m := q.pop(); m != nil && q.str() != "" {
+		return "mq: pop from empty"
+	}
+	mqfill(q)
+	if q.str() != fillkeys {
+		return "mq fill"
+	}
+	order := fillkeys
+	for i := 0; i < len(fillkeys); i++ {
+		q.push(q.pop())
+		order = order[1:] + order[:1]
+		if q.str() != order {
+			return "mq: push(pop)"
+		}
+	}
+	if order != fillkeys {
+		return "mq: order != fillkeys"
+	}
+
+	cl := new(mcl)
+
+	mclfill(cl)
+	if cl.str() != fillkeys {
+		return "mcl fill"
+	}
+
+	var m *m
+	if m = cl.removebykey('x'); m != nil || cl.str() != "ABCDEFGHIJ" {
+		return "mcl: removebykey, key unknown"
+	}
+	if m = cl.removebykey('A'); mkey(m) != 'A' || cl.str() != "BCDEFGHIJ" {
+		return "mcl: removebykey A"
+	}
+	if m = cl.removebykey('E'); mkey(m) != 'E' || cl.str() != "FGHIJBCD" {
+		return "mcl: removebykey E"
+	}
+	if m = cl.removebykey('D'); mkey(m) != 'D' || cl.str() != "FGHIJBC" {
+		return "mcl: removebykey D"
+	}
+	if cl.push(m); cl.str() != "FGHIJBCD" {
+		return "mcl: push D"
+	}
+	msetkey(&testm, 'G')
+	if cl.push(&testm); cl.str() != "FGHIJBCDG" {
+		return "mcl: push G"
+	}
+	if m = cl.removebykey('G'); mkey(m) != 'G' || cl.str() != "HIJBCDGF" {
+		return "mcl: removebykey G"
+	}
+	if m = cl.removebykey('G'); mkey(m) != 'G' || cl.str() != "FHIJBCD" {
+		return "mcl: removebykey G"
+	}
+	if cl.push(&testm); cl.str() != "FHIJBCDG" {
+		return "mcl: push G"
+	}
+	if m = cl.removebykey('D'); mkey(m) != 'D' || cl.str() != "GFHIJBC" {
+		return "mcl: removebykey D"
+	}
+	if cl.remove(&testm); cl.str() != "FHIJBC" {
+		return "mcl: remove G"
+	}
+	if cl.push(m); cl.str() != "FHIJBCD" {
+		return "mcl: push D"
+	}
+	if m = cl.removebykey('H'); mkey(m) != 'H' || cl.str() != "IJBCDF" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('I'); mkey(m) != 'I' || cl.str() != "JBCDF" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('C'); mkey(m) != 'C' || cl.str() != "DFJB" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('B'); mkey(m) != 'B' || cl.str() != "DFJ" {
+		return "mcl: removebykey"
+	}
+	if cl.push(m); cl.str() != "DFJB" {
+		return "mcl: push"
+	}
+	if cl.push(&testm); cl.str() != "DFJBG" {
+		return "mcl: push G"
+	}
+	if cl.remove(&testm); cl.str() != "DFJB" {
+		return "mcl: remove G"
+	}
+	if m = cl.removebykey('F'); mkey(m) != 'F' || cl.str() != "JBD" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('B'); mkey(m) != 'B' || cl.str() != "DJ" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('J'); mkey(m) != 'J' || cl.str() != "D" {
+		return "mcl: removebykey"
+	}
+	if cl.push(m); cl.str() != "DJ" {
+		return "mcl: push"
+	}
+	if m = cl.removebykey('D'); mkey(m) != 'D' || cl.str() != "J" {
+		return "mcl: removebykey"
+	}
+	if cl.push(m); cl.str() != "JD" {
+		return "mq: push"
+	}
+	if m = cl.removebykey('x'); m != nil || cl.str() != "JD" {
+		return "mcl: removebykey, key unknown"
+	}
+	if m = cl.removebykey('J'); mkey(m) != 'J' || cl.str() != "D" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('x'); m != nil || cl.str() != "D" {
+		return "mcl: removebykey, key unknown"
+	}
+	if m = cl.removebykey('D'); mkey(m) != 'D' || cl.str() != "" {
+		return "mcl: removebykey"
+	}
+	if m = cl.removebykey('D'); m != nil || cl.str() != "" {
+		return "mcl: removebykey, key unknown"
+	}
+
+	sl := new(msl)
+
+	sl.insertbyval(mv('3'))
+	sl.insertbyval(mv('0'))
+	sl.insertbyval(mv('9'))
+	sl.insertbyval(mv('1'))
+	sl.insertbyval(mv('7'))
+	sl.insertbyval(mv('5'))
+	sl.insertbyval(mv('2'))
+	sl.insertbyval(mv('8'))
+	sl.insertbyval(mv('4'))
+	sl.insertbyval(mv('6'))
+	if sl.str() != "0123456789" {
+		return "msl: insertbyval"
+	}
+	if m = sl.pople('2' << 30); mval(m) != '0'<<30 || sl.str() != "123456789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('2' << 30); mval(m) != '1'<<30 || sl.str() != "23456789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('2' << 30); mval(m) != '2'<<30 || sl.str() != "3456789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('2' << 30); m != nil || sl.str() != "3456789" {
+		println(m, sl.str())
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '3'<<30 || sl.str() != "456789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '4'<<30 || sl.str() != "56789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '5'<<30 || sl.str() != "6789" {
+		return "msl: pople"
+	}
+	msetval(&testm, '7'<<30)
+	if sl.insertbyval(&testm); sl.str() != "67789" {
+		return "msl: insertbyval 7"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '6'<<30 || sl.str() != "7789" {
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '7'<<30 || sl.str() != "789" {
+		return "msl: pople"
+	}
+	if sl.remove(&testm); sl.str() != "89" {
+		return "msl: remove 7"
+	}
+	if m = sl.pople('8' << 30); mval(m) != '8'<<30 || sl.str() != "9" {
+		return "msl: pople"
+	}
+	if m = sl.pople('8' << 30); m != nil || sl.str() != "9" {
+		return "msl: pople"
+	}
+	if m = sl.pople('9' << 30); mval(m) != '9'<<30 || sl.str() != "" {
+		return "msl: pople"
+	}
+	if m = sl.pople('9' << 30); m != nil || sl.str() != "" {
+		return "msl: pople"
+	}
+	return ""
+}
diff --git a/src/runtime/mstats.go b/src/runtime/mstats.go
index 09dbb26735..b5883b55bb 100644
--- a/src/runtime/mstats.go
+++ b/src/runtime/mstats.go
@@ -59,8 +59,8 @@ type mstats struct {
 	next_gc         uint64 // goal heap_live for when next GC ends; ^0 if disabled
 	last_gc_unix    uint64 // last gc (in unix time)
 	pause_total_ns  uint64
-	pause_ns        [256]uint64 // circular buffer of recent gc pause lengths
-	pause_end       [256]uint64 // circular buffer of recent gc end times (nanoseconds since 1970)
+	pause_ns        [256*(1-_MCU) + 16*_MCU]uint64 // circular buffer of recent gc pause lengths
+	pause_end       [256*(1-_MCU) + 16*_MCU]uint64 // circular buffer of recent gc end times (nanoseconds since 1970)
 	numgc           uint32
 	numforcedgc     uint32  // number of user-forced GCs
 	gc_cpu_fraction float64 // fraction of CPU time used by GC
@@ -354,7 +354,7 @@ type MemStats struct {
 	// general, PauseNs[N%256] records the time paused in the most
 	// recent N%256th GC cycle. There may be multiple pauses per
 	// GC cycle; this is the sum of all pauses during a cycle.
-	PauseNs [256]uint64
+	PauseNs [256*(1-_MCU) + 16*_MCU]uint64
 
 	// PauseEnd is a circular buffer of recent GC pause end times,
 	// as nanoseconds since 1970 (the UNIX epoch).
@@ -362,7 +362,7 @@ type MemStats struct {
 	// This buffer is filled the same way as PauseNs. There may be
 	// multiple pauses per GC cycle; this records the end of the
 	// last pause in a cycle.
-	PauseEnd [256]uint64
+	PauseEnd [256*(1-_MCU) + 16*_MCU]uint64
 
 	// NumGC is the number of completed GC cycles.
 	NumGC uint32
diff --git a/src/runtime/netpoll_stub.go b/src/runtime/netpoll_stub.go
index f585333579..bd728919ef 100644
--- a/src/runtime/netpoll_stub.go
+++ b/src/runtime/netpoll_stub.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build plan9
+// +build noos plan9
 
 package runtime
 
diff --git a/src/runtime/os_linux_noauxv.go b/src/runtime/os_linux_noauxv.go
index 895b4cd5f4..f2415e811b 100644
--- a/src/runtime/os_linux_noauxv.go
+++ b/src/runtime/os_linux_noauxv.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build !arm,!arm64,!mips,!mipsle,!mips64,!mips64le,!s390x,!ppc64,!ppc64le
+// +build !arm,!thumb,!arm64,!mips,!mipsle,!mips64,!mips64le,!s390x,!ppc64,!ppc64le
 
 package runtime
 
diff --git a/src/runtime/os_linux_novdso.go b/src/runtime/os_linux_novdso.go
index e54c1c4dc1..304ffc4607 100644
--- a/src/runtime/os_linux_novdso.go
+++ b/src/runtime/os_linux_novdso.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build !386,!amd64,!arm,!arm64,!ppc64,!ppc64le
+// +build !386,!amd64,!arm,!thumb,!arm64,!ppc64,!ppc64le
 
 package runtime
 
diff --git a/src/runtime/os_linux_thumb.go b/src/runtime/os_linux_thumb.go
new file mode 100644
index 0000000000..fa1b62ef6b
--- /dev/null
+++ b/src/runtime/os_linux_thumb.go
@@ -0,0 +1,49 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "internal/cpu"
+
+const (
+	_HWCAP_VFPv3 = 1 << 13 // introduced in 2.6.30
+	_HWCAP_IDIVT = 1 << 18
+)
+
+var randomNumber uint32
+
+func checkgoarm() {
+	if cpu.HWCap&_HWCAP_IDIVT == 0 {
+		print("runtime: hardware division not supported, cannot run GOARCH=thumb binary")
+		exit(1)
+	}
+	if goarm&0xF >= 0xD && cpu.HWCap&_HWCAP_VFPv3 == 0 {
+		print("runtime: VFPv3 not supported, cannot run GOARM=7F/7D binary\n")
+		exit(1)
+	}
+}
+
+func archauxv(tag, val uintptr) {
+	switch tag {
+	case _AT_RANDOM:
+		// sysargs filled in startupRandomData, but that
+		// pointer may not be word aligned, so we must treat
+		// it as a byte array.
+		randomNumber = uint32(startupRandomData[4]) | uint32(startupRandomData[5])<<8 |
+			uint32(startupRandomData[6])<<16 | uint32(startupRandomData[7])<<24
+
+	case _AT_HWCAP:
+		cpu.HWCap = uint(val)
+	case _AT_HWCAP2:
+		cpu.HWCap2 = uint(val)
+	}
+}
+
+//go:nosplit
+func cputicks() int64 {
+	// Currently cputicks() is used in blocking profiler and to seed fastrand().
+	// nanotime() is a poor approximation of CPU ticks that is enough for the profiler.
+	// randomNumber provides better seeding of fastrand.
+	return nanotime() + int64(randomNumber)
+}
diff --git a/src/runtime/os_noos.go b/src/runtime/os_noos.go
new file mode 100644
index 0000000000..6db0c802f1
--- /dev/null
+++ b/src/runtime/os_noos.go
@@ -0,0 +1,111 @@
+// Copyright 2018 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+const _NSIG = 0
+
+type gsignalStack struct{}
+type sigset struct{}
+
+//go:nosplit
+func setThreadCPUProfiler(hz int32) {
+	for {
+		breakpoint()
+	}
+}
+
+//go:nosplit
+func unminit() {
+	for {
+		breakpoint()
+	}
+}
+
+func clearSignalHandlers()           {}
+func sigdisable(uint32)              {}
+func sigenable(uint32)               {}
+func sigignore(uint32)               {}
+func signame(sig uint32) string      { return "" }
+func sigblock()                      {}
+func msigrestore(sigmask sigset)     {}
+func initsig(preinit bool)           {}
+func setProcessCPUProfiler(hz int32) {}
+func mpreinit(mp *m)                 {}
+func msigsave(mp *m)                 {}
+func goenvs()                        {}
+func minit()                         {}
+
+//go:nosplit
+func crash() {
+	for {
+		breakpoint()
+	}
+}
+
+//go:nosplit
+func setsystim(nanotime func() int64, setalarm func(ns int64) bool) {
+	if nanotime != nil {
+		thetasker.newnanotime = nanotime
+	} else {
+		thetasker.newnanotime = dummyNanotime
+	}
+	if setalarm != nil {
+		thetasker.newsetalarm = setalarm
+	} else {
+		thetasker.newsetalarm = dummySetalarm
+
+	}
+	setsystim1()
+}
+
+//go:nosplit
+func usleep(usec uint32) {
+	nanosleep(int64(usec) * 1000)
+}
+
+//go:nosplit
+func getRandomData(r []byte) {
+	// BUG: true random data required
+	extendRandom(r, 0)
+}
+
+//go:nosplit
+func osinit() {
+	ncpu = 1 // for now only single CPU is supported (see identcurcpu, cpuid)
+	physPageSize = _PageSize
+}
+
+//go:nosplit
+func isr() bool {
+	return getg() == &thetasker.allcpu[cpuid()].gh
+}
+
+func setsystim1()
+func newosproc(mp *m)
+func exit(code int32)
+func osyield()
+
+//go:noescape
+func write(fd uintptr, p unsafe.Pointer, n int32) int32
+
+//go:noescape
+func futexsleep(addr *uint32, val uint32, ns int64)
+
+//go:noescape
+func futexwakeup(addr *uint32, cnt uint32)
+
+//go:noescape
+func exitThread(wait *uint32)
+
+// syscalls not used by runtime
+
+func setwalltime(sec int64, nsec int32)
+func setprivlevel(newlevel int) (oldlevel, errno int)
+func irqenabled(irq int) (enabled, errno int)
+func setirqenabled(irq, enabled int) (errno int)
+func irqctl(irq, ctl int) (enabled, prio, errno int)
+func nanosleep(ns int64)
diff --git a/src/runtime/os_noos_thumb.go b/src/runtime/os_noos_thumb.go
new file mode 100644
index 0000000000..34c95fac83
--- /dev/null
+++ b/src/runtime/os_noos_thumb.go
@@ -0,0 +1,18 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+//go:nosplit
+func cputicks() int64 {
+	// Currently cputicks() is used in blocking profiler and to seed fastrand().
+	// nanotime() is a poor approximation of CPU ticks that is enough for the profiler.
+	return nanotime()
+}
+
+//go:nosplit
+func cpuid() int {
+	// for now only single CPU is supported (see also identcurcpu, osinit)
+	return 0
+}
diff --git a/src/runtime/panic.go b/src/runtime/panic.go
index 5f33cd7c0c..7db15cf78b 100644
--- a/src/runtime/panic.go
+++ b/src/runtime/panic.go
@@ -1066,5 +1066,5 @@ func shouldPushSigpanic(gp *g, pc, lr uintptr) bool {
 //
 //go:nosplit
 func isAbortPC(pc uintptr) bool {
-	return pc == funcPC(abort) || ((GOARCH == "arm" || GOARCH == "arm64") && pc == funcPC(abort)+sys.PCQuantum)
+	return pc == funcAddr(abort) || ((GOARCH == "arm" || GOARCH == "arm64" || GOARCH == "thumb") && pc == funcAddr(abort)+sys.PCQuantum)
 }
diff --git a/src/runtime/panic32.go b/src/runtime/panic32.go
index b89ce9d563..1b23ff1a29 100644
--- a/src/runtime/panic32.go
+++ b/src/runtime/panic32.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 amd64p32 arm mips mipsle
+// +build 386 amd64p32 arm thumb mips mipsle
 
 package runtime
 
diff --git a/src/runtime/print.go b/src/runtime/print.go
index e605eb34cb..62e0312b79 100644
--- a/src/runtime/print.go
+++ b/src/runtime/print.go
@@ -26,7 +26,7 @@ func bytes(s string) (ret []byte) {
 var (
 	// printBacklog is a circular buffer of messages written with the builtin
 	// print* functions, for use in postmortem analysis of core dumps.
-	printBacklog      [512]byte
+	printBacklog      [512 - 256*_MCU]byte
 	printBacklogIndex int
 )
 
@@ -64,6 +64,9 @@ var debuglock mutex
 // For both these reasons, let a thread acquire the printlock 'recursively'.
 
 func printlock() {
+	if isr() {
+		return
+	}
 	mp := getg().m
 	mp.locks++ // do not reschedule between printlock++ and lock(&debuglock).
 	mp.printlock++
@@ -74,6 +77,9 @@ func printlock() {
 }
 
 func printunlock() {
+	if isr() {
+		return
+	}
 	mp := getg().m
 	mp.printlock--
 	if mp.printlock == 0 {
@@ -196,7 +202,7 @@ func printcomplex(c complex128) {
 }
 
 func printuint(v uint64) {
-	var buf [100]byte
+	var buf [24]byte
 	i := len(buf)
 	for i--; i > 0; i-- {
 		buf[i] = byte(v%10 + '0')
@@ -218,7 +224,7 @@ func printint(v int64) {
 
 func printhex(v uint64) {
 	const dig = "0123456789abcdef"
-	var buf [100]byte
+	var buf [20]byte
 	i := len(buf)
 	for i--; i > 0; i-- {
 		buf[i] = dig[v%16]
diff --git a/src/runtime/proc.go b/src/runtime/proc.go
index 93d329d15e..6e19d4cbce 100644
--- a/src/runtime/proc.go
+++ b/src/runtime/proc.go
@@ -122,6 +122,8 @@ func main() {
 	// they look nicer in the stack overflow failure message.
 	if sys.PtrSize == 8 {
 		maxstacksize = 1000000000
+	} else if _MCU != 0 {
+		maxstacksize = 16000
 	} else {
 		maxstacksize = 250000000
 	}
@@ -410,12 +412,30 @@ func releaseSudog(s *sudog) {
 // for the same function (because there are actually multiple copies of
 // the same function in the address space). To be safe, don't use the
 // results of this function in any == expression. It is only safe to
-// use the result as an address at which to start executing code.
+// use the result as an address at which to start executing code. For some
+// architectures (eg. thumb) it can differ from address returned by funcAddr.
 //go:nosplit
 func funcPC(f interface{}) uintptr {
 	return **(**uintptr)(add(unsafe.Pointer(&f), sys.PtrSize))
 }
 
+// funcAddr returns the address of the function f in memory.
+// It assumes that f is a func value. Otherwise the behavior is undefined.
+// CAREFUL: In programs with plugins, funcPC can return different values
+// for the same function (because there are actually multiple copies of
+// the same function in the address space). To be safe, don't use the
+// results of this function in any == expression. It is only safe to
+// use the result as an address at which to start executing code. For some
+// architectures (eg. thumb) it can differ from address returned by funcPC.
+//go:nosplit
+func funcAddr(f interface{}) uintptr {
+	fpc := **(**uintptr)(add(unsafe.Pointer(&f), sys.PtrSize))
+	if GOARCH == "thumb" {
+		fpc &^= 1
+	}
+	return fpc
+}
+
 // called from assembly
 func badmcall(fn func(*g)) {
 	throw("runtime: mcall called on m->g0 stack")
@@ -1239,7 +1259,7 @@ func mexit(osStack bool) {
 	g := getg()
 	m := g.m
 
-	if m == &m0 {
+	if m == &m0 && _MCU == 0 {
 		// This is the main thread. Just wedge it.
 		//
 		// On Linux, exiting the main thread puts the process
@@ -1478,7 +1498,15 @@ func allocm(_p_ *p, fn func()) *m {
 				continue
 			}
 			stackfree(freem.g0.stack)
+			cleanm := freem
 			freem = freem.freelink
+			if cleanm == &m0 {
+				// ensure nothing will hang on m0
+				cleanm.g0 = nil
+				cleanm.curg = nil
+				cleanm.freelink = nil
+				// TODO: can we zero allink? any other pointer fields?
+			}
 		}
 		sched.freem = newList
 		unlock(&sched.lock)
@@ -1492,6 +1520,8 @@ func allocm(_p_ *p, fn func()) *m {
 	// Windows and Plan 9 will layout sched stack on OS stack.
 	if iscgo || GOOS == "solaris" || GOOS == "illumos" || GOOS == "windows" || GOOS == "plan9" || GOOS == "darwin" {
 		mp.g0 = malg(-1)
+	} else if _MCU != 0 {
+		mp.g0 = malg(2 * _FixedStack)
 	} else {
 		mp.g0 = malg(8192 * sys.StackGuardMultiplier)
 	}
@@ -3418,9 +3448,9 @@ func gfput(_p_ *p, gp *g) {
 
 	_p_.gFree.push(gp)
 	_p_.gFree.n++
-	if _p_.gFree.n >= 64 {
+	if _p_.gFree.n >= 64-59*_MCU {
 		lock(&sched.gFree.lock)
-		for _p_.gFree.n >= 32 {
+		for _p_.gFree.n >= 32-29*_MCU {
 			_p_.gFree.n--
 			gp = _p_.gFree.pop()
 			if gp.stack.lo == 0 {
@@ -3441,7 +3471,7 @@ retry:
 	if _p_.gFree.empty() && (!sched.gFree.stack.empty() || !sched.gFree.noStack.empty()) {
 		lock(&sched.gFree.lock)
 		// Move a batch of free Gs to the P.
-		for _p_.gFree.n < 32 {
+		for _p_.gFree.n < 32-29*_MCU {
 			// Prefer Gs with stacks.
 			gp := sched.gFree.stack.pop()
 			if gp == nil {
@@ -4862,7 +4892,7 @@ func runqget(_p_ *p) (gp *g, inheritTime bool) {
 // Batch is a ring buffer starting at batchHead.
 // Returns number of grabbed goroutines.
 // Can be executed by any P.
-func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32 {
+func runqgrab(_p_ *p, batch *[256*(1-_MCU) + 64*_MCU]guintptr, batchHead uint32, stealRunNextG bool) uint32 {
 	for {
 		h := atomic.LoadAcq(&_p_.runqhead) // load-acquire, synchronize with other consumers
 		t := atomic.LoadAcq(&_p_.runqtail) // load-acquire, synchronize with the producer
diff --git a/src/runtime/rt0_linux_thumb.s b/src/runtime/rt0_linux_thumb.s
new file mode 100644
index 0000000000..6cdeb88294
--- /dev/null
+++ b/src/runtime/rt0_linux_thumb.s
@@ -0,0 +1,29 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+
+TEXT _rt0_thumb_linux(SB),NOSPLIT|NOFRAME,$0
+	MOVW  (R13), R0    // argc
+	MOVW  $4(R13), R1  // argv
+	MOVW  $_rt0_thumb_linux1(SB), R4
+	B     (R4)
+
+TEXT _rt0_thumb_linux1(SB),NOSPLIT|NOFRAME,$0
+	// We first need to detect the kernel ABI, and warn the user
+	// if the system only supports OABI.
+	// The strategy here is to call some EABI syscall to see if
+	// SIGILL is received.
+	// If you get a SIGILL here, you have the wrong kernel.
+
+	// Save argc and argv (syscall will clobber at least R0).
+	MOVM.DB.W  [R0-R1], (R13)
+
+	// do an EABI syscall
+	MOVW  $20, R7  // sys_getpid
+	SWI   $0       // this will trigger SIGILL on OABI systems
+
+	MOVM.IA.W  (R13), [R0-R1]
+	B          runtimert0_go(SB)
+
diff --git a/src/runtime/rt0_noos_thumb.s b/src/runtime/rt0_noos_thumb.s
new file mode 100644
index 0000000000..7f495ea97d
--- /dev/null
+++ b/src/runtime/rt0_noos_thumb.s
@@ -0,0 +1,832 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "funcdata.h"
+#include "textflag.h"
+
+
+#define PALLOC_MIN 20*1024
+
+
+TEXT _rt0_thumb_noos(SB),NOSPLIT|NOFRAME,$0
+	// TODO: check does we have to disable interrupts for initialization.
+
+	// initialize data and BSS
+
+	MOVW       R13, R0
+	MOVW       $runtimeromdata(SB), R1
+	MOVW       $runtimebss(SB), R3
+	MOVW       $runtimeramend(SB), R4
+	SUB        R0, R3, R2
+	SUB        R3, R4
+	MOVM.DB.W  [R0-R4], (R13)  // push: to,from,n for memmove, ptr,n for memclr
+	SUB        $4, R13
+	BL         runtimememmove(SB)  // copy data to RAM
+	ADD        $12, R13
+	BL         runtimememclrNoHeapPointers(SB)  // clear BSS and unallocated memory
+	MOVW       $runtimenodmastart(SB), R0
+	MOVW       $runtimenodmaend(SB), R1
+	SUB        R0, R1
+	MOVW       R0, 4(R13)
+	MOVW       R1, 8(R13)
+	BL         runtimememclrNoHeapPointers(SB)  // clear non-DMA memory
+	ADD        $12, R13
+
+	// setup main stack in cpu0.gh
+
+	MOVW  $runtimecpu0(SB), R0      // gh is the first field of the cpuctx struct
+	MOVW  $runtimeramstart(SB), R1  // main stack starts at the beggining of memory
+	MOVW  R1, (g_stack+stack_lo)(R0)
+	MOVW  R13, (g_stack+stack_hi)(R0)
+	ADD   $const__StackGuard, R1
+	MOVW  R1, g_stackguard0(R0)
+	MOVW  R1, g_stackguard1(R0)
+
+	// set up m0 (bootstrap thread), temporarily use cpu0.gh as g
+	MOVW  $runtimem0(SB), R1
+	MOVW  R0, m_g0(R1)  // m0.g0 = cpu0.gh
+	MOVW  R1, g_m(R0)   // cpu0.gh.m = m0
+
+	MOVW  R0, g  // using R0 instead of g above gives shorter encoding
+
+	BL  runtimeemptyfunc(SB)  // fault if stack check is wrong
+	//BL  runtimecheck(SB)
+	BL  runtimeosinit(SB)
+
+	// initialize sysMem
+
+	MOVW  $runtimeend(SB), R0
+	MOVW  $runtimeramend(SB), R1
+	SUB   R0, R1, R5  // size of available memory (DMA capable)
+
+	// estimate the space need for non-heap allocations
+	MOVW  R5>>(const__PageShift+2), R4
+	MOVW  $const_mspanSize, R2
+	MUL   R2, R4
+	ADD   $PALLOC_MIN, R4
+
+	MOVW  $runtimenodmastart(SB), R2
+	MOVW  $runtimenodmaend(SB), R3
+	SUB   R2, R3, R6  // size of non-DMA memory
+
+	// we prefer the non-DMA memory for non-heap objects to preserve as much as
+	// possible of the DMA capable memory for heap allocations
+	SUB.S  R6, R4
+
+	// reduce the arena by the remain of the non-heap space that did not fit in
+	// the non-DMA memory, properly align the arena
+	SUB.HI  R4, R5
+	BIC     $(const_heapArenaBytes-1), R5
+	SUB     R5, R1
+	MOVW    R1, R4
+
+	// save {free.start,free.end,nodma.start,nodma.end,arenaStart,arenaSize}
+	MOVW     $runtimesysMem(SB), R6
+	MOVM.IA  [R0-R5], (R6)
+
+	// initialize noos tasker and Go scheduler
+
+	BL  runtimetaskerpreinit(SB)
+	BL  runtimeschedinit(SB)
+	BL  runtimetaskerinit(SB)
+
+	// allocate g0 for m0 and leave gh
+
+	SUB        $4, R13
+	MOVW       $0, R0
+	MOVW       $(2*const__FixedStack), R1
+	MOVM.DB.W  [R0-R1], (R13)
+	BL         runtimemalg(SB)
+	MOVW       8(R13), R0  // newg in R0
+	ADD        $12, R13
+
+	// stackguard check during newproc requires valid stackguard1 but
+	// malg sets it to 0xFFFFFFFF (mstart fixes this but is called later)
+	MOVW  g_stackguard0(R0), R1
+	MOVW  R1, g_stackguard1(R0)
+
+	MOVW  $runtimem0(SB), R1
+	MOVW  R0, m_g0(R1)  // m0.g0 = newg
+	MOVW  R1, g_m(R0)   // newg.m = m0
+
+	MOVW  (g_stack+stack_hi)(R0), R1
+	MOVW  R1, PSP
+
+	MOVW  g, R2
+	MOVW  R0, g
+
+	// fix cpu0.gh, cpu0.mh
+
+	ADD   $cpuctx_mh, R2, R1  // R2 points to cpu0 (and to cpu0.gh at the same time)
+	MOVW  R2, m_g0(R1)        // cpu0.mh.g0 = cpu0.gh
+	MOVW  R1, g_m(R2)         // cpu0.gh.m = cpu0.mh
+
+	// leave the main stack and the privileged mode
+	DSB
+	MOVW  CONTROL, R0
+	ORR   $2, R0  // use PSP as stack pointer
+	MOVW  R0, CONTROL
+	ISB
+	ORR   $1, R0  // go to unprivileged mode
+	MOVW  R0, CONTROL
+	ISB
+
+	// create a new goroutine to start program
+	MOVW       $0, R0
+	MOVW       $8, R1
+	MOVW       $runtimemainPC(SB), R2
+	MOVM.DB.W  [R0-R2], (R13)
+	BL         runtimenewproc(SB)
+	MOVW       $12(R13), R13
+
+	// start this M
+	BL  runtimemstart(SB)
+
+	UNDEF  // fail
+
+DATA runtimemainPC+0(SB)/4,$runtimemain(SB)
+GLOBL runtimemainPC(SB),RODATA,$4
+
+TEXT runtimebreakpoint(SB),NOSPLIT,$0-0
+	BKPT
+	RET
+
+// asminit is called by mstart1 for every new thread
+TEXT runtimeasminit(SB),NOSPLIT,$0-0
+	RET
+
+//
+// go-routine
+//
+
+// void gosave(Gobuf*)
+// save state in Gobuf; setjmp
+TEXT runtimegosave(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  buf+0(FP), R0
+	MOVW  R13, gobuf_sp(R0)
+	MOVW  LR, gobuf_pc(R0)
+	MOVW  g, gobuf_g(R0)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, gobuf_lr(R0)
+	MOVW  REGTMP, gobuf_ret(R0)
+	// Assert ctxt is zero. See func save.
+	MOVW  gobuf_ctxt(R0), R0
+	CMP   R0, REGTMP
+	B.EQ  2(PC)
+	CALL  runtimebadctxt(SB)
+	RET
+
+// void gogo(Gobuf*)
+// restore state from Gobuf; longjmp
+TEXT runtimegogo(SB),NOSPLIT,$8-4
+	MOVW  buf+0(FP), R1
+	MOVW  gobuf_g(R1), R0
+	BL    setg<>(SB)
+
+	// NOTE: We updated g above, and we are about to update SP.
+	// Until LR and PC are also updated, the g/SP/LR/PC quadruple
+	// are out of sync and must not be used as the basis of a traceback.
+	// Sigprof skips the traceback when SP is not within g's bounds,
+	// and when the PC is inside this function, runtime.gogo.
+	// Since we are about to update SP, until we complete runtime.gogo
+	// we must not leave this function. In particular, no calls
+	// after this point: it must be straight-line code until the
+	// final B instruction.
+	// See large comment in sigprof for more details.
+	MOVW  gobuf_sp(R1), R13  // restore SP==R13
+	MOVW  gobuf_lr(R1), LR
+	MOVW  gobuf_ret(R1), R0
+	MOVW  gobuf_ctxt(R1), REGCTXT
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, gobuf_sp(R1)  // clear to help garbage collector
+	MOVW  REGTMP, gobuf_ret(R1)
+	MOVW  REGTMP, gobuf_lr(R1)
+	MOVW  REGTMP, gobuf_ctxt(R1)
+	MOVW  gobuf_pc(R1), REGTMP
+	CMP   REGTMP, REGTMP  // set condition codes for == test, needed by stack split
+	B     (REGTMP)
+
+// func mcall(fn func(*g))
+// Switch to m->g0's stack, call fn(g).
+// Fn must never return. It should gogo(&g->sched)
+// to keep running g.
+TEXT runtimemcall(SB),NOSPLIT|NOFRAME,$0-4
+	// Save caller state in g->sched.
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, (g_sched+gobuf_lr)(g)
+	MOVW  g, (g_sched+gobuf_g)(g)
+
+	// Switch to m->g0 & its stack, call fn.
+	MOVW  g, R1
+	MOVW  g_m(g), R8
+	MOVW  m_g0(R8), R0
+	BL    setg<>(SB)
+	CMP   g, R1
+	B.NE  2(PC)
+	B     runtimebadmcall(SB)
+	MOVW  fn+0(FP), R0
+	MOVW  (g_sched+gobuf_sp)(g), R13
+	SUB   $8, R13
+	MOVW  R1, 4(R13)
+	MOVW  R0, REGCTXT
+	MOVW  0(R0), R0
+	BL    (R0)
+	B     runtimebadmcall2(SB)
+	RET
+
+// systemstack_switch is a dummy routine that systemstack leaves at the bottom
+// of the G stack. We need to distinguish the routine that
+// lives at the bottom of the G stack from the one that lives
+// at the top of the system stack because the one at the top of
+// the system stack terminates the stack walk (see topofstack()).
+TEXT runtimesystemstack_switch(SB),NOSPLIT,$0-0
+	MOVW  $0, R0
+	BL    (R0)  // clobber lr to ensure push {lr} is kept
+	RET
+
+// func systemstack(fn func())
+TEXT runtimesystemstack(SB),NOSPLIT,$0-4
+	MOVW  fn+0(FP), R0  // R0 = fn
+	MOVW  g_m(g), R1    // R1 = m
+
+	MOVW  m_g0(R1), R2  // R2 = g0
+	CMP   g, R2
+	B.EQ  noswitch
+
+	MOVW  m_curg(R1), R3
+	CMP   g, R3
+	B.EQ  switch
+
+	// Bad: g is not g0, not curg. What is it?
+	// Hide call from linker nosplit analysis.
+	MOVW  $runtimebadsystemstack(SB), R0
+	BL    (R0)
+	B     runtimeabort(SB)
+
+switch:
+	// save our state in g->sched. Pretend to
+	// be systemstack_switch if the G stack is scanned.
+	MOVW  $runtimesystemstack_switch(SB), R3
+	ADD   $4, R3, R3  // get past push {lr}
+	MOVW  R3, (g_sched+gobuf_pc)(g)
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_lr)(g)
+	MOVW  g, (g_sched+gobuf_g)(g)
+
+	// switch to g0
+	MOVW  R0, R5
+	MOVW  R2, R0
+	BL    setg<>(SB)
+	MOVW  R5, R0
+	MOVW  (g_sched+gobuf_sp)(R2), R3
+	// make it look like mstart called systemstack on g0, to stop traceback
+	SUB   $4, R3, R3
+	MOVW  $runtimemstart(SB), R4
+	MOVW  R4, 0(R3)
+	MOVW  R3, R13
+
+	// call target function
+	MOVW  R0, REGCTXT
+	MOVW  0(R0), R0
+	BL    (R0)
+
+	// switch back to g
+	MOVW  g_m(g), R1
+	MOVW  m_curg(R1), R0
+	BL    setg<>(SB)
+	MOVW  (g_sched+gobuf_sp)(g), R13
+	MOVW  $0, R3
+	MOVW  R3, (g_sched+gobuf_sp)(g)
+	RET
+
+noswitch:
+	// Using a tail call here cleans up tracebacks since we won't stop
+	// at an intermediate systemstack.
+	MOVW    R0, REGCTXT
+	MOVW    0(R0), R0
+	MOVW.P  4(R13), R14  // restore LR
+	B       (R0)
+
+/*
+	*   support for morestack
+	*/
+
+// Called during function prolog when more stack is needed.
+// R3 prolog's LR
+// using NOFRAME means do not save LR on stack.
+
+// The traceback routines see morestack on a g0 as being
+// the top of a stack (for example, morestack calling newstack
+// calling the scheduler calling newm calling gc), so we must
+// record an argument size. For that purpose, it has no arguments.
+TEXT runtimemorestack(SB),NOSPLIT|NOFRAME,$0-0
+	// Cannot grow scheduler stack (m->g0).
+	MOVW  g_m(g), R8
+	MOVW  m_g0(R8), R4
+	CMP   g, R4
+	BNE   3(PC)
+	BL    runtimebadmorestackg0(SB)
+	B     runtimeabort(SB)
+
+	// Called from f.
+	// Set g->sched to context in f.
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  R3, (g_sched+gobuf_lr)(g)
+	MOVW  REGCTXT, (g_sched+gobuf_ctxt)(g)
+
+	// Called from f.
+	// Set m->morebuf to f's caller.
+	MOVW  R3, (m_morebuf+gobuf_pc)(R8)   // f's caller's PC
+	MOVW  R13, (m_morebuf+gobuf_sp)(R8)  // f's caller's SP
+	MOVW  g, (m_morebuf+gobuf_g)(R8)
+
+	// Call newstack on m->g0's stack.
+	MOVW    m_g0(R8), R0
+	BL      setg<>(SB)
+	MOVW    (g_sched+gobuf_sp)(g), R13
+	MOVW    $0, R0
+	MOVW.W  R0, -4(R13)  // create a call frame on g0 (saved LR)
+	BL      runtimenewstack(SB)
+
+	// Not reached, but make sure the return PC from the call to newstack
+	// is still in this function, and not the beginning of the next.
+	RET
+
+TEXT runtimemorestack_noctxt(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, REGCTXT
+	B     runtimemorestack(SB)
+
+// reflectcall: call a function with the given argument list
+// func call(argtype *_type, f *FuncVal, arg *byte, argsize, retoffset uint32).
+// we don't have variable-sized frames, so we use a small number
+// of constant-sized-frame functions to encode a few bits of size in the pc.
+// Caution: ugly multiline assembly macros in your future!
+
+#define DISPATCH(NAME,MAXSIZE) \
+	CMP   $MAXSIZE, R0; \
+	B.HI  3(PC); \
+	MOVW  $NAME(SB), R1; \
+	B     (R1)
+
+TEXT reflectcall(SB),NOSPLIT|NOFRAME,$0-20
+	MOVW                              argsize+12(FP), R0
+	DISPATCH(runtimecall16,          16)
+	DISPATCH(runtimecall32,          32)
+	DISPATCH(runtimecall64,          64)
+	DISPATCH(runtimecall128,         128)
+	DISPATCH(runtimecall256,         256)
+	DISPATCH(runtimecall512,         512)
+	DISPATCH(runtimecall1024,        1024)
+	DISPATCH(runtimecall2048,        2048)
+	DISPATCH(runtimecall4096,        4096)
+	DISPATCH(runtimecall8192,        8192)
+	DISPATCH(runtimecall16384,       16384)
+	DISPATCH(runtimecall32768,       32768)
+	DISPATCH(runtimecall65536,       65536)
+	DISPATCH(runtimecall131072,      131072)
+	DISPATCH(runtimecall262144,      262144)
+	DISPATCH(runtimecall524288,      524288)
+	DISPATCH(runtimecall1048576,     1048576)
+	DISPATCH(runtimecall2097152,     2097152)
+	DISPATCH(runtimecall4194304,     4194304)
+	DISPATCH(runtimecall8388608,     8388608)
+	DISPATCH(runtimecall16777216,    16777216)
+	DISPATCH(runtimecall33554432,    33554432)
+	DISPATCH(runtimecall67108864,    67108864)
+	DISPATCH(runtimecall134217728,   134217728)
+	DISPATCH(runtimecall268435456,   268435456)
+	DISPATCH(runtimecall536870912,   536870912)
+	DISPATCH(runtimecall1073741824,  1073741824)
+	MOVW                              $runtimebadreflectcall(SB), R1
+	B                                 (R1)
+
+#define CALLFN(NAME,MAXSIZE) \
+TEXT NAME(SB), WRAPPER, $MAXSIZE-20; \
+	NO_LOCAL_POINTERS;  \
+/*                  copy arguments to stack */ \
+	MOVW     argptr+8(FP), R0; \
+	MOVW     argsize+12(FP), R2; \
+	ADD      $4, R13, R1; \
+	CMP      $0, R2; \
+	B.EQ     5(PC); \
+	MOVBU.P  1(R0), R5; \
+	MOVB.P   R5, 1(R1); \
+	SUB      $1, R2, R2; \
+	B        -5(PC); \
+/*                  call function */ \
+	MOVW    f+4(FP), REGCTXT; \
+	MOVW    (REGCTXT), R0; \
+	PCDATA  $PCDATA_StackMapIndex, $0; \
+	BL      (R0); \
+/*                  copy return values back */ \
+	MOVW  argtype+0(FP), R4; \
+	MOVW  argptr+8(FP), R0; \
+	MOVW  argsize+12(FP), R2; \
+	MOVW  retoffset+16(FP), R3; \
+	ADD   $4, R13, R1; \
+	ADD   R3, R1; \
+	ADD   R3, R0; \
+	SUB   R3, R2; \
+	BL    callRet<>(SB); \
+	RET
+
+// callRet copies return values back at the end of call*. This is a
+// separate function so it can allocate stack space for the arguments
+// to reflectcallmove. It does not follow the Go ABI; it expects its
+// arguments in registers.
+TEXT callRet<>(SB), NOSPLIT, $16-0
+	MOVW  R4, 4(R13)
+	MOVW  R0, 8(R13)
+	MOVW  R1, 12(R13)
+	MOVW  R2, 16(R13)
+	BL    runtimereflectcallmove(SB)
+	RET
+
+	CALLFN(call16,          16)
+	CALLFN(call32,          32)
+	CALLFN(call64,          64)
+	CALLFN(call128,         128)
+	CALLFN(call256,         256)
+	CALLFN(call512,         512)
+	CALLFN(call1024,        1024)
+	CALLFN(call2048,        2048)
+	CALLFN(call4096,        4096)
+	CALLFN(call8192,        8192)
+	CALLFN(call16384,       16384)
+	CALLFN(call32768,       32768)
+	CALLFN(call65536,       65536)
+	CALLFN(call131072,      131072)
+	CALLFN(call262144,      262144)
+	CALLFN(call524288,      524288)
+	CALLFN(call1048576,     1048576)
+	CALLFN(call2097152,     2097152)
+	CALLFN(call4194304,     4194304)
+	CALLFN(call8388608,     8388608)
+	CALLFN(call16777216,    16777216)
+	CALLFN(call33554432,    33554432)
+	CALLFN(call67108864,    67108864)
+	CALLFN(call134217728,   134217728)
+	CALLFN(call268435456,   268435456)
+	CALLFN(call536870912,   536870912)
+	CALLFN(call1073741824,  1073741824)
+
+// void jmpdefer(fn, sp);
+// called from deferreturn.
+// 1. grab stored LR for caller
+// 2. sub 4 bytes to get back to BL deferreturn
+// 3. B to fn
+// TODO(rsc): Push things on stack and then use pop
+// to load all registers simultaneously, so that a profiling
+// interrupt can never see mismatched SP/LR/PC.
+// (And double-check that pop is atomic in that way.)
+TEXT runtimejmpdefer(SB),NOSPLIT,$0-8
+	MOVW  0(R13), LR
+	MOVW  $-4(LR), LR  // BL deferreturn
+	MOVW  fv+0(FP), REGCTXT
+	MOVW  argp+4(FP), R13
+	MOVW  $-4(R13), R13  // SP is 4 below argp, due to saved LR
+	MOVW  0(REGCTXT), R1
+	B     (R1)
+
+// Save state of caller into g->sched. Smashes REGTMP.
+TEXT gosave<>(SB),NOSPLIT|NOFRAME,$0
+	MOVW  LR, (g_sched+gobuf_pc)(g)
+	MOVW  R13, (g_sched+gobuf_sp)(g)
+	MOVW  $0, REGTMP
+	MOVW  REGTMP, (g_sched+gobuf_lr)(g)
+	MOVW  REGTMP, (g_sched+gobuf_ret)(g)
+	MOVW  REGTMP, (g_sched+gobuf_ctxt)(g)
+	// Assert ctxt is zero. See func save.
+	MOVW  (g_sched+gobuf_ctxt)(g), REGTMP
+	CMP   $0, REGTMP
+	B.EQ  2(PC)
+	CALL  runtimebadctxt(SB)
+	RET
+
+TEXT asmcgocall(SB),NOSPLIT|NOFRAME,$0-12
+	BKPT
+	B   -1(PC)
+
+// void setg(G*); set g. for use by needm.
+TEXT runtimesetg(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  gg+0(FP), R0
+	B     setg<>(SB)
+
+TEXT setg<>(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  R0, g
+	RET
+
+TEXT runtimeemptyfunc(SB),0,$0-0
+	RET
+
+TEXT runtimeabort(SB),NOSPLIT|NOFRAME,$0-0
+	BKPT
+	B   -1(PC)
+
+// AES hashing not implemented for ARM
+TEXT runtimeaeshash(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+TEXT runtimeaeshash32(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+TEXT runtimeaeshash64(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+TEXT runtimeaeshashstr(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $0, R0
+	MOVW  (R0), R1
+
+TEXT runtimereturn0(SB),NOSPLIT,$0
+	MOVW  $0, R0
+	RET
+
+TEXT runtimeprocyield(SB),NOSPLIT|NOFRAME,$0
+	MOVW  cycles+0(FP), R1
+yieldloop:
+	YIELD
+	CMP  $0, R1
+	RET.EQ
+	SUB  $1, R1
+	B    yieldloop
+
+// Called from cgo wrappers, this function returns g->m->curg.stack.hi.
+// Must obey the gcc calling convention.
+TEXT _cgo_topofstack(SB),NOSPLIT,$8
+	// R11 (REGCTXT) and g register are clobbered by load_g. They are
+	// callee-save in the gcc calling convention, so save them here.
+	MOVW  REGCTXT, saveR11-4(SP)
+	MOVW  g, saveG-8(SP)
+
+	BL    runtimeload_g(SB)
+	MOVW  g_m(g), R0
+	MOVW  m_curg(R0), R0
+	MOVW  (g_stack+stack_hi)(R0), R0
+
+	MOVW  saveG-8(SP), g
+	MOVW  saveR11-4(SP), REGCTXT
+	RET
+
+// The top-most function running on a goroutine
+// returns to goexit+PCQuantum.
+TEXT runtimegoexit(SB),NOSPLIT|NOFRAME|TOPFRAME,$0-0
+	NOP2
+	BL  runtimegoexit1(SB)  // does not return
+	// traceback from goexit1 must hit code range of goexit
+	NOP2
+
+// x -> x/1000000, x%1000000, called from Go with args, results on stack.
+TEXT runtimeusplit(SB),NOSPLIT,$0-12
+	MOVW  x+0(FP), R0
+	CALL  runtimeusplitR0(SB)
+	MOVW  R0, q+4(FP)
+	MOVW  R1, r+8(FP)
+	RET
+
+// R0, R1 = R0/1000000, R0%1000000
+TEXT runtimeusplitR0(SB),NOSPLIT,$0
+	// magic multiply to avoid software divide without available m.
+	// see output of go tool compile -S for x/1000000.
+	MOVW   R0, R3
+	MOVW   $1125899907, R1
+	MULLU  R1, R0, (R0, R1)
+	MOVW   R0>>18, R0
+	MOVW   $1000000, R1
+	MUL    R0, R1
+	SUB    R1, R3, R1
+	RET
+
+TEXT runtimesigreturn(SB),NOSPLIT,$0-0
+	RET
+
+// This is called from .init_array and follows the platform, not Go, ABI.
+TEXT runtimeaddmoduledata(SB),NOSPLIT,$0-0
+	MOVW  R9, saver9-4(SP)        // The access to global variables below implicitly uses R9, which is callee-save
+	MOVW  REGCTXT, saver11-8(SP)  // Likewise, R11 is the REGCTXT register, but callee-save in C ABI
+	MOVW  runtimelastmoduledatap(SB), R1
+	MOVW  R0, moduledata_next(R1)
+	MOVW  R0, runtimelastmoduledatap(SB)
+	MOVW  saver11-8(SP), REGCTXT
+	MOVW  saver9-4(SP), R9
+	RET
+
+TEXT checkASM(SB),NOSPLIT,$0-1
+	MOVW  $1, R3
+	MOVB  R3, ret+0(FP)
+	RET
+
+// gcWriteBarrier performs a heap pointer write and informs the GC.
+
+// gcWriteBarrier does NOT follow the Go ABI. It takes two arguments:
+// - R2 is the destination of the write
+// - R3 is the value being written at R2
+// It clobbers condition codes.
+// It does not clobber any other general-purpose registers,
+// but may clobber others (e.g., floating point registers).
+// The act of CALLing gcWriteBarrier will clobber R14 (LR).
+TEXT runtimegcWriteBarrier(SB),NOSPLIT|NOFRAME,$0
+	// Save the registers clobbered by the fast path.
+	MOVM.DB.W  [R0,R1], (R13)
+	MOVW       g_m(g), R0
+	MOVW       m_p(R0), R0
+	MOVW       (p_wbBuf+wbBuf_next)(R0), R1
+	// Increment wbBuf.next position.
+	ADD   $8, R1
+	MOVW  R1, (p_wbBuf+wbBuf_next)(R0)
+	MOVW  (p_wbBuf+wbBuf_end)(R0), R0
+	CMP   R1, R0
+	// Record the write.
+	MOVW  R3, -8(R1)  // Record value
+	MOVW  (R2), R0    // TODO: This turns bad writes into bad reads.
+	MOVW  R0, -4(R1)  // Record *slot
+	// Is the buffer full? (flags set in CMP above)
+	B.EQ  flush
+ret:
+	MOVM.IA.W  (R13), [R0,R1]
+	// Do the write.
+	MOVW  R3, (R2)
+	// Normally RET on nacl clobbers R12, but because this
+	// function has no frame it doesn't have to usual epilogue.
+	RET
+
+flush:
+	// Save all general purpose registers since these could be
+	// clobbered by wbBufFlush and were not saved by the caller.
+
+	// R0 and R1 were saved at entry.
+	// R7 is linker temp, so no need to save.
+	// R10 is g, so preserved.
+	// R13 is stack pointer.
+	// R15 is PC.
+
+	// This also sets up R2 and R3 as the arguments to wbBufFlush.
+	MOVM.DB.W  [R2-R6,R8-R9,R11-R12], (R13)
+	// Save R14 (LR) because the fast path above doesn't save it,
+	// but needs it to RET. This is after the MOVM so it appears below
+	// the arguments in the stack frame.
+	MOVW.W  LR, -4(R13)
+
+	// This takes arguments R2 and R3.
+	CALL  runtimewbBufFlush(SB)
+
+	MOVW.P     4(R13), LR
+	MOVM.IA.W  (R13), [R2-R6,R8-R9,R11-R12]
+	JMP        ret
+
+// Note: these functions use a special calling convention to save generated code space.
+// Arguments are passed in registers, but the space for those arguments are allocated
+// in the caller's stack frame. These stubs write the args into that stack space and
+// then tail call to the corresponding runtime handler.
+// The tail call makes these stubs disappear in backtraces.
+TEXT runtimepanicIndex(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicIndex(SB)
+TEXT runtimepanicIndexU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicIndexU(SB)
+TEXT runtimepanicSliceAlen(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSliceAlen(SB)
+TEXT runtimepanicSliceAlenU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSliceAlenU(SB)
+TEXT runtimepanicSliceAcap(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSliceAcap(SB)
+TEXT runtimepanicSliceAcapU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSliceAcapU(SB)
+TEXT runtimepanicSliceB(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicSliceB(SB)
+TEXT runtimepanicSliceBU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicSliceBU(SB)
+TEXT runtimepanicSlice3Alen(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtimegoPanicSlice3Alen(SB)
+TEXT runtimepanicSlice3AlenU(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtimegoPanicSlice3AlenU(SB)
+TEXT runtimepanicSlice3Acap(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtimegoPanicSlice3Acap(SB)
+TEXT runtimepanicSlice3AcapU(SB),NOSPLIT,$0-8
+	MOVW  R2, x+0(FP)
+	MOVW  R3, y+4(FP)
+	JMP   runtimegoPanicSlice3AcapU(SB)
+TEXT runtimepanicSlice3B(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSlice3B(SB)
+TEXT runtimepanicSlice3BU(SB),NOSPLIT,$0-8
+	MOVW  R1, x+0(FP)
+	MOVW  R2, y+4(FP)
+	JMP   runtimegoPanicSlice3BU(SB)
+TEXT runtimepanicSlice3C(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicSlice3C(SB)
+TEXT runtimepanicSlice3CU(SB),NOSPLIT,$0-8
+	MOVW  R0, x+0(FP)
+	MOVW  R1, y+4(FP)
+	JMP   runtimegoPanicSlice3CU(SB)
+
+// Extended versions for 64-bit indexes.
+TEXT runtimepanicExtendIndex(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendIndex(SB)
+TEXT runtimepanicExtendIndexU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendIndexU(SB)
+TEXT runtimepanicExtendSliceAlen(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSliceAlen(SB)
+TEXT runtimepanicExtendSliceAlenU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSliceAlenU(SB)
+TEXT runtimepanicExtendSliceAcap(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSliceAcap(SB)
+TEXT runtimepanicExtendSliceAcapU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSliceAcapU(SB)
+TEXT runtimepanicExtendSliceB(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendSliceB(SB)
+TEXT runtimepanicExtendSliceBU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendSliceBU(SB)
+TEXT runtimepanicExtendSlice3Alen(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3Alen(SB)
+TEXT runtimepanicExtendSlice3AlenU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3AlenU(SB)
+TEXT runtimepanicExtendSlice3Acap(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3Acap(SB)
+TEXT runtimepanicExtendSlice3AcapU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R2, lo+4(FP)
+	MOVW  R3, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3AcapU(SB)
+TEXT runtimepanicExtendSlice3B(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3B(SB)
+TEXT runtimepanicExtendSlice3BU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R1, lo+4(FP)
+	MOVW  R2, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3BU(SB)
+TEXT runtimepanicExtendSlice3C(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3C(SB)
+TEXT runtimepanicExtendSlice3CU(SB),NOSPLIT,$0-12
+	MOVW  R4, hi+0(FP)
+	MOVW  R0, lo+4(FP)
+	MOVW  R1, y+8(FP)
+	JMP   runtimegoPanicExtendSlice3CU(SB)
diff --git a/src/runtime/runtime2.go b/src/runtime/runtime2.go
index 16c02cd1ed..3d6b63c95b 100644
--- a/src/runtime/runtime2.go
+++ b/src/runtime/runtime2.go
@@ -532,7 +532,7 @@ type p struct {
 	raceprocctx uintptr
 
 	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
-	deferpoolbuf [5][32]*_defer
+	deferpoolbuf [5][32 - 16*_MCU]*_defer
 
 	// Cache of goroutine ids, amortizes accesses to runtimesched.goidgen.
 	goidcache    uint64
@@ -541,7 +541,7 @@ type p struct {
 	// Queue of runnable goroutines. Accessed without lock.
 	runqhead uint32
 	runqtail uint32
-	runq     [256]guintptr
+	runq     [256*(1-_MCU) + 64*_MCU]guintptr
 	// runnext, if non-nil, is a runnable G that was ready'd by
 	// the current G and should be run next instead of what's in
 	// runq if there's time remaining in the running G's time
@@ -560,7 +560,7 @@ type p struct {
 	}
 
 	sudogcache []*sudog
-	sudogbuf   [128]*sudog
+	sudogbuf   [128 - 64*_MCU]*sudog
 
 	tracebuf traceBufPtr
 
diff --git a/src/runtime/sema.go b/src/runtime/sema.go
index 30c8959473..55b6df8a72 100644
--- a/src/runtime/sema.go
+++ b/src/runtime/sema.go
@@ -44,7 +44,7 @@ type semaRoot struct {
 }
 
 // Prime to not correlate with any user patterns.
-const semTabSize = 251
+const semTabSize = 251*(1-_MCU) + 31*_MCU
 
 var semtable [semTabSize]struct {
 	root semaRoot
diff --git a/src/runtime/signal_arm.go b/src/runtime/signal_armt.go
similarity index 99%
rename from src/runtime/signal_arm.go
rename to src/runtime/signal_armt.go
index bb597c5608..6b8d7d4c32 100644
--- a/src/runtime/signal_arm.go
+++ b/src/runtime/signal_armt.go
@@ -3,6 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build darwin dragonfly freebsd linux nacl netbsd openbsd
+// +build arm thumb
 
 package runtime
 
diff --git a/src/runtime/signal_linux_arm.go b/src/runtime/signal_linux_armt.go
similarity index 98%
rename from src/runtime/signal_linux_arm.go
rename to src/runtime/signal_linux_armt.go
index 876b505917..41da819863 100644
--- a/src/runtime/signal_linux_arm.go
+++ b/src/runtime/signal_linux_armt.go
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build linux
+// +build arm thumb
+
 package runtime
 
 import (
diff --git a/src/runtime/sizeclasses.go b/src/runtime/sizeclasses.go
index 9c1b44fe0b..3bb12fb09a 100644
--- a/src/runtime/sizeclasses.go
+++ b/src/runtime/sizeclasses.go
@@ -1,6 +1,8 @@
 // Code generated by mksizeclasses.go; DO NOT EDIT.
 //go:generate go run mksizeclasses.go
 
+// +build !noos
+
 package runtime
 
 // class  bytes/obj  bytes/span  objects  tail waste  max waste
diff --git a/src/runtime/sizeclasses_noos.go b/src/runtime/sizeclasses_noos.go
new file mode 100644
index 0000000000..2c5a60e5c9
--- /dev/null
+++ b/src/runtime/sizeclasses_noos.go
@@ -0,0 +1,47 @@
+// Code generated by mksizeclasses_noos.go; DO NOT EDIT.
+//go:generate go run mksizeclasses_noos.go
+
+package runtime
+
+// class  bytes/obj  bytes/span  objects  tail waste  max waste
+//     1          8         256       32           0     87.50%
+//     2         16         256       16           0     43.75%
+//     3         32         256        8           0     46.88%
+//     4         48         256        5          16     35.55%
+//     5         64         256        4           0     23.44%
+//     6         80         256        3          16     23.83%
+//     7         96         512        5          32     20.90%
+//     8        128         256        2           0     24.22%
+//     9        144         768        5          48     16.02%
+//    10        160         512        3          32     15.04%
+//    11        192         768        4           0     16.15%
+//    12        208        1280        6          32      9.53%
+//    13        256         256        1           0     18.36%
+//    14        288        1280        4         128     19.69%
+//    15        320        1024        3          64     15.33%
+//    16        384         768        2           0     16.41%
+//    17        416        1280        3          32      9.77%
+//    18        512         512        1           0     18.55%
+
+const (
+	_MaxSmallSize   = 512
+	smallSizeDiv    = 8
+	smallSizeMax    = 256
+	largeSizeDiv    = 128
+	_NumSizeClasses = 19
+	_PageShift      = 8
+)
+
+var class_to_size = [_NumSizeClasses]uint16{0, 8, 16, 32, 48, 64, 80, 96, 128, 144, 160, 192, 208, 256, 288, 320, 384, 416, 512}
+var class_to_allocnpages = [_NumSizeClasses]uint8{0, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 3, 5, 1, 5, 4, 3, 5, 2}
+
+type divMagic struct {
+	shift    uint8
+	shift2   uint8
+	mul      uint16
+	baseMask uint16
+}
+
+var class_to_divmagic = [_NumSizeClasses]divMagic{{0, 0, 0, 0}, {3, 0, 1, 65528}, {4, 0, 1, 65520}, {5, 0, 1, 65504}, {4, 5, 11, 0}, {6, 0, 1, 65472}, {4, 6, 13, 0}, {5, 5, 11, 0}, {7, 0, 1, 65408}, {4, 8, 29, 0}, {5, 6, 13, 0}, {6, 5, 11, 0}, {4, 10, 79, 0}, {8, 0, 1, 65280}, {5, 8, 29, 0}, {6, 6, 13, 0}, {7, 3, 3, 0}, {5, 6, 5, 0}, {9, 0, 1, 65024}}
+var size_to_class8 = [smallSizeMax/smallSizeDiv + 1]uint8{0, 1, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 8, 8, 9, 9, 10, 10, 11, 11, 11, 11, 12, 12, 13, 13, 13, 13, 13, 13}
+var size_to_class128 = [(_MaxSmallSize-smallSizeMax)/largeSizeDiv + 1]uint8{13, 16, 18}
diff --git a/src/runtime/stack.go b/src/runtime/stack.go
index 7ae3eeef83..ff32f61a93 100644
--- a/src/runtime/stack.go
+++ b/src/runtime/stack.go
@@ -65,10 +65,11 @@ const (
 	// to each stack below the usual guard area for OS-specific
 	// purposes like signal handling. Used on Windows, Plan 9,
 	// and iOS because they do not use a separate stack.
-	_StackSystem = sys.GoosWindows*512*sys.PtrSize + sys.GoosPlan9*512 + sys.GoosDarwin*sys.GoarchArm*1024 + sys.GoosDarwin*sys.GoarchArm64*1024
+	// Used on ARMv7M due to register stacking at exception entry.
+	_StackSystem = sys.GoosWindows*512*sys.PtrSize + sys.GoosPlan9*512 + sys.GoosDarwin*sys.GoarchArm*1024 + sys.GoosDarwin*sys.GoarchArm64*1024 + _ARMv7M*27*4
 
 	// The minimum size of stack used by Go code
-	_StackMin = 2048
+	_StackMin = 2048*(1-_MCU) + 512*_MCU // actual round2(_StackMin+_StackSystem) - _StackSystem
 
 	// The minimum stack size to allocate.
 	// The hackery here rounds FixedStack0 up to a power of 2.
@@ -90,7 +91,7 @@ const (
 
 	// The stack guard is a pointer this many bytes above the
 	// bottom of the stack.
-	_StackGuard = 880*sys.StackGuardMultiplier + _StackSystem
+	_StackGuard = 880*sys.StackGuardMultiplier*(1-_ARMv7M) + 440*_ARMv7M + _StackSystem
 
 	// After a stack split check the SP is allowed to be this
 	// many bytes below the stack guard. This saves an instruction
diff --git a/src/runtime/stubs2.go b/src/runtime/stubs2.go
index 57134f7354..945123b3ec 100644
--- a/src/runtime/stubs2.go
+++ b/src/runtime/stubs2.go
@@ -9,6 +9,7 @@
 // +build !js
 // +build !darwin
 // +build !aix
+// +build !noos
 
 package runtime
 
diff --git a/src/runtime/stubs32.go b/src/runtime/stubs32.go
index 149560fd93..01d0bb2b61 100644
--- a/src/runtime/stubs32.go
+++ b/src/runtime/stubs32.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build 386 arm amd64p32 mips mipsle
+// +build 386 arm thumb amd64p32 mips mipsle
 
 package runtime
 
diff --git a/src/runtime/stubs_arm.go b/src/runtime/stubs_armt.go
similarity index 95%
rename from src/runtime/stubs_arm.go
rename to src/runtime/stubs_armt.go
index c13bf16de2..1181174589 100644
--- a/src/runtime/stubs_arm.go
+++ b/src/runtime/stubs_armt.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+//+build arm thumb
+
 package runtime
 
 // Called from compiler-generated code; declared for go vet.
diff --git a/src/runtime/stubs_os.go b/src/runtime/stubs_os.go
new file mode 100644
index 0000000000..68ebc4ebfc
--- /dev/null
+++ b/src/runtime/stubs_os.go
@@ -0,0 +1,19 @@
+// Copyright 2014 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// +build !noos
+
+package runtime
+
+func isr() bool {
+	return false
+}
+
+func sysReserveMaxArena() (addr, size uintptr) {
+	return 0, 0
+}
+
+func sysPersistentAlloc(size, align uintptr, sysStat *uint64) *notInHeap {
+	return nil
+}
diff --git a/src/runtime/sys_arm.go b/src/runtime/sys_armt.go
similarity index 96%
rename from src/runtime/sys_arm.go
rename to src/runtime/sys_armt.go
index 730b9c918f..86118ab3e6 100644
--- a/src/runtime/sys_arm.go
+++ b/src/runtime/sys_armt.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build arm thumb
+
 package runtime
 
 import "unsafe"
diff --git a/src/runtime/sys_linux_thumb.s b/src/runtime/sys_linux_thumb.s
new file mode 100644
index 0000000000..16461e44f9
--- /dev/null
+++ b/src/runtime/sys_linux_thumb.s
@@ -0,0 +1,589 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+//
+// System calls and other sys.stuff for arm, Linux
+//
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+
+#define CLOCK_REALTIME 0
+#define CLOCK_MONOTONIC 1
+
+// for EABI, as we don't support OABI
+#define SYS_BASE 0x0
+
+#define SYS_exit (SYS_BASE + 1)
+#define SYS_read (SYS_BASE + 3)
+#define SYS_write (SYS_BASE + 4)
+#define SYS_open (SYS_BASE + 5)
+#define SYS_close (SYS_BASE + 6)
+#define SYS_getpid (SYS_BASE + 20)
+#define SYS_kill (SYS_BASE + 37)
+#define SYS_clone (SYS_BASE + 120)
+#define SYS_rt_sigreturn (SYS_BASE + 173)
+#define SYS_rt_sigaction (SYS_BASE + 174)
+#define SYS_rt_sigprocmask (SYS_BASE + 175)
+#define SYS_sigaltstack (SYS_BASE + 186)
+#define SYS_mmap2 (SYS_BASE + 192)
+#define SYS_futex (SYS_BASE + 240)
+#define SYS_exit_group (SYS_BASE + 248)
+#define SYS_munmap (SYS_BASE + 91)
+#define SYS_madvise (SYS_BASE + 220)
+#define SYS_setitimer (SYS_BASE + 104)
+#define SYS_mincore (SYS_BASE + 219)
+#define SYS_gettid (SYS_BASE + 224)
+#define SYS_tgkill (SYS_BASE + 268)
+#define SYS_sched_yield (SYS_BASE + 158)
+#define SYS_nanosleep (SYS_BASE + 162)
+#define SYS_sched_getaffinity (SYS_BASE + 242)
+#define SYS_clock_gettime (SYS_BASE + 263)
+#define SYS_epoll_create (SYS_BASE + 250)
+#define SYS_epoll_ctl (SYS_BASE + 251)
+#define SYS_epoll_wait (SYS_BASE + 252)
+#define SYS_epoll_create1 (SYS_BASE + 357)
+#define SYS_fcntl (SYS_BASE + 55)
+#define SYS_access (SYS_BASE + 33)
+#define SYS_connect (SYS_BASE + 283)
+#define SYS_socket (SYS_BASE + 281)
+#define SYS_brk (SYS_BASE + 45)
+
+#define ARM_BASE (SYS_BASE + 0x0f0000)
+
+TEXT runtimeopen(SB),NOSPLIT,$0
+	MOVW     name+0(FP), R0
+	MOVW     mode+4(FP), R1
+	MOVW     perm+8(FP), R2
+	MOVW     $SYS_open, R7
+	SWI      $0
+	MOVW     $0xfffff001, R1
+	CMP      R1, R0
+	MOVW.HI  $-1, R0
+	MOVW     R0, ret+12(FP)
+	RET      
+
+TEXT runtimeclosefd(SB),NOSPLIT,$0
+	MOVW     fd+0(FP), R0
+	MOVW     $SYS_close, R7
+	SWI      $0
+	MOVW     $0xfffff001, R1
+	CMP      R1, R0
+	MOVW.HI  $-1, R0
+	MOVW     R0, ret+4(FP)
+	RET      
+
+TEXT runtimewrite(SB),NOSPLIT,$0
+	MOVW     fd+0(FP), R0
+	MOVW     p+4(FP), R1
+	MOVW     n+8(FP), R2
+	MOVW     $SYS_write, R7
+	SWI      $0
+	MOVW     $0xfffff001, R1
+	CMP      R1, R0
+	MOVW.HI  $-1, R0
+	MOVW     R0, ret+12(FP)
+	RET      
+
+TEXT runtimeread(SB),NOSPLIT,$0
+	MOVW     fd+0(FP), R0
+	MOVW     p+4(FP), R1
+	MOVW     n+8(FP), R2
+	MOVW     $SYS_read, R7
+	SWI      $0
+	MOVW     $0xfffff001, R1
+	CMP      R1, R0
+	MOVW.HI  $-1, R0
+	MOVW     R0, ret+12(FP)
+	RET      
+
+TEXT runtimeexit(SB),NOSPLIT|NOFRAME,$0
+	MOVW  code+0(FP), R0
+	MOVW  $SYS_exit_group, R7
+	SWI   $0
+	MOVW  $1234, R0
+	MOVW  $1002, R1
+	MOVW  R0, (R1)  // fail hard
+
+TEXT exit1<>(SB),NOSPLIT|NOFRAME,$0
+	MOVW  code+0(FP), R0
+	MOVW  $SYS_exit, R7
+	SWI   $0
+	MOVW  $1234, R0
+	MOVW  $1003, R1
+	MOVW  R0, (R1)  // fail hard
+
+// func exitThread(wait *uint32)
+TEXT runtimeexitThread(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  wait+0(FP), R0
+	// We're done using the stack.
+	// Alas, there's no reliable way to make this write atomic
+	// without potentially using the stack. So it goes.
+	MOVW  $0, R1
+	MOVW  R1, (R0)
+	MOVW  $0, R0  // exit code
+	MOVW  $SYS_exit, R7
+	SWI   $0
+	MOVW  $1234, R0
+	MOVW  $1004, R1
+	MOVW  R0, (R1)  // fail hard
+	JMP   0(PC)
+
+TEXT runtimegettid(SB),NOSPLIT,$0-4
+	MOVW  $SYS_gettid, R7
+	SWI   $0
+	MOVW  R0, ret+0(FP)
+	RET   
+
+TEXT runtimeraise(SB),NOSPLIT|NOFRAME,$0
+	MOVW  $SYS_getpid, R7
+	SWI   $0
+	MOVW  R0, R4
+	MOVW  $SYS_gettid, R7
+	SWI   $0
+	MOVW  R0, R1         // arg 2 tid
+	MOVW  R4, R0         // arg 1 pid
+	MOVW  sig+0(FP), R2  // arg 3
+	MOVW  $SYS_tgkill, R7
+	SWI   $0
+	RET   
+
+TEXT runtimeraiseproc(SB),NOSPLIT|NOFRAME,$0
+	MOVW  $SYS_getpid, R7
+	SWI   $0
+	// arg 1 tid already in R0 from getpid
+	MOVW  sig+0(FP), R1  // arg 2 - signal
+	MOVW  $SYS_kill, R7
+	SWI   $0
+	RET   
+
+TEXT runtimemmap(SB),NOSPLIT,$0
+	MOVW       addr+0(FP), R0
+	MOVW       n+4(FP), R1
+	MOVW       prot+8(FP), R2
+	MOVW       flags+12(FP), R3
+	MOVW       fd+16(FP), R4
+	MOVW       off+20(FP), R5
+	MOVW       $SYS_mmap2, R7
+	SWI        $0
+	MOVW       $0xfffff001, R6
+	MOVW       $0, R1
+	CMP        R6, R0
+	RSB.P.HI   $0, R0
+	MOVW.P.HI  R0, R1  // if error, put in R1
+	MOVW.HI    $0, R0
+	MOVW       R0, p+24(FP)
+	MOVW       R1, err+28(FP)
+	RET        
+
+TEXT runtimemunmap(SB),NOSPLIT,$0
+	MOVW       addr+0(FP), R0
+	MOVW       n+4(FP), R1
+	MOVW       $SYS_munmap, R7
+	SWI        $0
+	MOVW       $0xfffff001, R6
+	CMP        R6, R0
+	MOVW.P.HI  $0, R8  // crash on syscall failure
+	MOVW.HI    R8, (R8)
+	RET        
+
+TEXT runtimemadvise(SB),NOSPLIT,$0
+	MOVW  addr+0(FP), R0
+	MOVW  n+4(FP), R1
+	MOVW  flags+8(FP), R2
+	MOVW  $SYS_madvise, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET   
+
+TEXT runtimesetitimer(SB),NOSPLIT,$0
+	MOVW  mode+0(FP), R0
+	MOVW  new+4(FP), R1
+	MOVW  old+8(FP), R2
+	MOVW  $SYS_setitimer, R7
+	SWI   $0
+	RET   
+
+TEXT runtimemincore(SB),NOSPLIT,$0
+	MOVW  addr+0(FP), R0
+	MOVW  n+4(FP), R1
+	MOVW  dst+8(FP), R2
+	MOVW  $SYS_mincore, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET   
+
+TEXT runtimewalltime(SB),NOSPLIT,$0-12
+	// We don't know how much stack space the VDSO code will need,
+	// so switch to g0.
+
+	// Save old SP. Use R13 instead of SP to avoid linker rewriting the offsets.
+	MOVW  R13, R4  // R4 is unchanged by C code.
+
+	MOVW  g_m(g), R5  // R5 is unchanged by C code.
+
+	// Set vdsoPC and vdsoSP for SIGPROF traceback.
+	MOVW  LR, m_vdsoPC(R5)
+	MOVW  R13, m_vdsoSP(R5)
+
+	MOVW  m_curg(R5), R0
+
+	CMP   g, R0  // Only switch if on curg.
+	B.NE  noswitch
+
+	MOVW  m_g0(R5), R0
+	MOVW  (g_sched+gobuf_sp)(R0), R13  // Set SP to g0 stack
+
+noswitch:
+	SUB  $24, R13   // Space for results
+	BIC  $0x7, R13  // Align for C code
+
+	MOVW  $CLOCK_REALTIME, R0
+	MOVW  $8(R13), R1  // timespec
+	MOVW  runtimevdsoClockgettimeSym(SB), REGTMP
+	CMP   $0, REGTMP
+	B.EQ  fallback
+
+	BL   (REGTMP)
+	JMP  finish
+
+fallback:
+	MOVW  $SYS_clock_gettime, R7
+	SWI   $0
+
+finish:
+	MOVW  8(R13), R0   // sec
+	MOVW  12(R13), R2  // nsec
+
+	MOVW  R4, R13  // Restore real SP
+	MOVW  $0, R1
+	MOVW  R1, m_vdsoSP(R5)
+
+	MOVW  R0, sec_lo+0(FP)
+	MOVW  R1, sec_hi+4(FP)
+	MOVW  R2, nsec+8(FP)
+	RET   
+
+// int64 nanotime(void)
+TEXT runtimenanotime(SB),NOSPLIT,$0-8
+	// Switch to g0 stack. See comment above in runtimewalltime.
+
+	// Save old SP. Use R13 instead of SP to avoid linker rewriting the offsets.
+	MOVW  R13, R4  // R4 is unchanged by C code.
+
+	MOVW  g_m(g), R5  // R5 is unchanged by C code.
+
+	// Set vdsoPC and vdsoSP for SIGPROF traceback.
+	MOVW  LR, m_vdsoPC(R5)
+	MOVW  R13, m_vdsoSP(R5)
+
+	MOVW  m_curg(R5), R0
+
+	CMP   g, R0  // Only switch if on curg.
+	B.NE  noswitch
+
+	MOVW  m_g0(R5), R0
+	MOVW  (g_sched+gobuf_sp)(R0), R13  // Set SP to g0 stack
+
+noswitch:
+	SUB  $24, R13   // Space for results
+	BIC  $0x7, R13  // Align for C code
+
+	MOVW  $CLOCK_MONOTONIC, R0
+	MOVW  $8(R13), R1  // timespec
+	MOVW  runtimevdsoClockgettimeSym(SB), REGTMP
+	CMP   $0, REGTMP
+	B.EQ  fallback
+
+	BL   (REGTMP)
+	JMP  finish
+
+fallback:
+	MOVW  $SYS_clock_gettime, R7
+	SWI   $0
+
+finish:
+	MOVW  8(R13), R0   // sec
+	MOVW  12(R13), R2  // nsec
+
+	MOVW  R4, R13  // Restore real SP
+	MOVW  $0, R4
+	MOVW  R4, m_vdsoSP(R5)
+
+	MOVW   $1000000000, R3
+	MULLU  R0, R3, (R1, R0)
+	ADD.S  R2, R0
+	ADC    R4, R1
+
+	MOVW  R0, ret_lo+0(FP)
+	MOVW  R1, ret_hi+4(FP)
+	RET   
+
+// int32 futex(int32 *uaddr, int32 op, int32 val,
+//	struct timespec *timeout, int32 *uaddr2, int32 val2);
+TEXT runtimefutex(SB),NOSPLIT,$0
+	MOVW  addr+0(FP), R0
+	MOVW  op+4(FP), R1
+	MOVW  val+8(FP), R2
+	MOVW  ts+12(FP), R3
+	MOVW  addr2+16(FP), R4
+	MOVW  val3+20(FP), R5
+	MOVW  $SYS_futex, R7
+	SWI   $0
+	MOVW  R0, ret+24(FP)
+	RET   
+
+// int32 clone(int32 flags, void *stack, M *mp, G *gp, void (*fn)(void));
+TEXT runtimeclone(SB),NOSPLIT,$0
+	MOVW  flags+0(FP), R0
+	MOVW  stk+4(FP), R1
+	MOVW  $0, R2  // parent tid ptr
+	MOVW  $0, R3  // tls_val
+	MOVW  $0, R4  // child tid ptr
+	MOVW  $0, R5
+
+	// Copy mp, gp, fn off parent stack for use by child.
+	MOVW  $-16(R1), R1
+	MOVW  mp+8(FP), R6
+	MOVW  R6, 0(R1)
+	MOVW  gp+12(FP), R6
+	MOVW  R6, 4(R1)
+	MOVW  fn+16(FP), R6
+	MOVW  R6, 8(R1)
+	MOVW  $1234, R6
+	MOVW  R6, 12(R1)
+
+	MOVW  $SYS_clone, R7
+	SWI   $0
+
+	// In parent, return.
+	CMP   $0, R0
+	BEQ   3(PC)
+	MOVW  R0, ret+20(FP)
+	RET   
+
+	// Paranoia: check that SP is as we expect. Use R13 to avoid linker 'fixup'
+	NOP   R13  // tell vet SP/R13 changed - stop checking offsets
+	MOVW  12(R13), R0
+	MOVW  $1234, R1
+	CMP   R0, R1
+	BEQ   2(PC)
+	BL    runtimeabort(SB)
+
+	MOVW  0(R13), R8  // m
+	MOVW  4(R13), R0  // g
+
+	CMP  $0, R8
+	BEQ  nog
+	CMP  $0, R0
+	BEQ  nog
+
+	MOVW  R0, g
+	MOVW  R8, g_m(g)
+
+	// paranoia; check they are not nil
+	MOVW  0(R8), R0
+	MOVW  0(g), R0
+
+	BL  runtimeemptyfunc(SB)  // fault if stack check is wrong
+
+	// Initialize m->procid to Linux tid
+	MOVW  $SYS_gettid, R7
+	SWI   $0
+	MOVW  g_m(g), R8
+	MOVW  R0, m_procid(R8)
+
+nog:
+	// Call fn
+	MOVW  8(R13), R0
+	MOVW  $16(R13), R13
+	BL    (R0)
+
+	// It shouldn't return. If it does, exit that thread.
+	SUB   $16, R13  // restore the stack pointer to avoid memory corruption
+	MOVW  $0, R0
+	MOVW  R0, 4(R13)
+	BL    exit1<>(SB)
+
+	MOVW  $1234, R0
+	MOVW  $1005, R1
+	MOVW  R0, (R1)
+
+TEXT runtimesigaltstack(SB),NOSPLIT,$0
+	MOVW       new+0(FP), R0
+	MOVW       old+4(FP), R1
+	MOVW       $SYS_sigaltstack, R7
+	SWI        $0
+	MOVW       $0xfffff001, R6
+	CMP        R6, R0
+	MOVW.P.HI  $0, R8  // crash on syscall failure
+	MOVW.HI    R8, (R8)
+	RET        
+
+TEXT runtimesigfwd(SB),NOSPLIT,$0-16
+	MOVW  sig+4(FP), R0
+	MOVW  info+8(FP), R1
+	MOVW  ctx+12(FP), R2
+	MOVW  fn+0(FP), REGTMP
+	MOVW  R13, R4
+	SUB   $24, R13
+	BIC   $0x7, R13  // alignment for ELF ABI
+	BL    (REGTMP)
+	MOVW  R4, R13
+	RET   
+
+TEXT runtimesigtramp(SB),NOSPLIT,$12
+	// this might be called in external code context,
+	// where g is not set.
+	MOVW  R0, 4(R13)
+	MOVW  R1, 8(R13)
+	MOVW  R2, 12(R13)
+	MOVW  $runtimesigtrampgo(SB), REGTMP
+	BL    (REGTMP)
+	RET   
+
+TEXT runtimecgoSigtramp(SB),NOSPLIT,$0
+	MOVW  $runtimesigtramp(SB), REGTMP
+	B     (REGTMP)
+
+TEXT runtimertsigprocmask(SB),NOSPLIT,$0
+	MOVW  how+0(FP), R0
+	MOVW  new+4(FP), R1
+	MOVW  old+8(FP), R2
+	MOVW  size+12(FP), R3
+	MOVW  $SYS_rt_sigprocmask, R7
+	SWI   $0
+	RET   
+
+TEXT runtimert_sigaction(SB),NOSPLIT,$0
+	MOVW  sig+0(FP), R0
+	MOVW  new+4(FP), R1
+	MOVW  old+8(FP), R2
+	MOVW  size+12(FP), R3
+	MOVW  $SYS_rt_sigaction, R7
+	SWI   $0
+	MOVW  R0, ret+16(FP)
+	RET   
+
+TEXT runtimeusleep(SB),NOSPLIT,$12
+	MOVW  usec+0(FP), R0
+	CALL  runtimeusplitR0(SB)
+	MOVW  R0, 4(R13)
+	MOVW  $1000, R0  // usec to nsec
+	MUL   R0, R1
+	MOVW  R1, 8(R13)
+	MOVW  $4(R13), R0
+	MOVW  $0, R1
+	MOVW  $SYS_nanosleep, R7
+	SWI   $0
+	RET   
+
+TEXT publicationBarrier(SB),NOSPLIT,$0
+	DMB  MB_ST
+	RET
+
+TEXT runtimeosyield(SB),NOSPLIT,$0
+	MOVW  $SYS_sched_yield, R7
+	SWI   $0
+	RET   
+
+TEXT runtimesched_getaffinity(SB),NOSPLIT,$0
+	MOVW  pid+0(FP), R0
+	MOVW  len+4(FP), R1
+	MOVW  buf+8(FP), R2
+	MOVW  $SYS_sched_getaffinity, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET   
+
+// int32 runtimeepollcreate(int32 size)
+TEXT runtimeepollcreate(SB),NOSPLIT,$0
+	MOVW  size+0(FP), R0
+	MOVW  $SYS_epoll_create, R7
+	SWI   $0
+	MOVW  R0, ret+4(FP)
+	RET   
+
+// int32 runtimeepollcreate1(int32 flags)
+TEXT runtimeepollcreate1(SB),NOSPLIT,$0
+	MOVW  flags+0(FP), R0
+	MOVW  $SYS_epoll_create1, R7
+	SWI   $0
+	MOVW  R0, ret+4(FP)
+	RET   
+
+// func epollctl(epfd, op, fd int32, ev *epollEvent) int
+TEXT runtimeepollctl(SB),NOSPLIT,$0
+	MOVW  epfd+0(FP), R0
+	MOVW  op+4(FP), R1
+	MOVW  fd+8(FP), R2
+	MOVW  ev+12(FP), R3
+	MOVW  $SYS_epoll_ctl, R7
+	SWI   $0
+	MOVW  R0, ret+16(FP)
+	RET   
+
+// int32 runtimeepollwait(int32 epfd, EpollEvent *ev, int32 nev, int32 timeout)
+TEXT runtimeepollwait(SB),NOSPLIT,$0
+	MOVW  epfd+0(FP), R0
+	MOVW  ev+4(FP), R1
+	MOVW  nev+8(FP), R2
+	MOVW  timeout+12(FP), R3
+	MOVW  $SYS_epoll_wait, R7
+	SWI   $0
+	MOVW  R0, ret+16(FP)
+	RET   
+
+// void runtimecloseonexec(int32 fd)
+TEXT runtimecloseonexec(SB),NOSPLIT,$0
+	MOVW  fd+0(FP), R0  // fd
+	MOVW  $2, R1        // F_SETFD
+	MOVW  $1, R2        // FD_CLOEXEC
+	MOVW  $SYS_fcntl, R7
+	SWI   $0
+	RET   
+
+// b __kuser_get_tls @ 0xffff0fe0
+TEXT runtimeread_tls_fallback(SB),NOSPLIT|NOFRAME,$0
+	MOVW  $0xffff0fe0, R0
+	B     (R0)
+
+TEXT runtimeaccess(SB),NOSPLIT,$0
+	MOVW  name+0(FP), R0
+	MOVW  mode+4(FP), R1
+	MOVW  $SYS_access, R7
+	SWI   $0
+	MOVW  R0, ret+8(FP)
+	RET   
+
+TEXT runtimeconnect(SB),NOSPLIT,$0
+	MOVW  fd+0(FP), R0
+	MOVW  addr+4(FP), R1
+	MOVW  len+8(FP), R2
+	MOVW  $SYS_connect, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET   
+
+TEXT runtimesocket(SB),NOSPLIT,$0
+	MOVW  domain+0(FP), R0
+	MOVW  typ+4(FP), R1
+	MOVW  prot+8(FP), R2
+	MOVW  $SYS_socket, R7
+	SWI   $0
+	MOVW  R0, ret+12(FP)
+	RET   
+
+// func sbrk0() uintptr
+TEXT runtimesbrk0(SB),NOSPLIT,$0-4
+	// Implemented as brk(NULL).
+	MOVW  $0, R0
+	MOVW  $SYS_brk, R7
+	SWI   $0
+	MOVW  R0, ret+0(FP)
+	RET   
+
+TEXT runtimesigreturn(SB),NOSPLIT,$0-0
+	RET  
diff --git a/src/runtime/sys_noos_thumb.s b/src/runtime/sys_noos_thumb.s
new file mode 100644
index 0000000000..bd25fa07b4
--- /dev/null
+++ b/src/runtime/sys_noos_thumb.s
@@ -0,0 +1,148 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+#include "syscall_noos.h"
+
+// if you add new syscall you must check SYS_MAX_ARGS in tasker_noos_thumb.s
+
+// syscalls allowed for low priority interrupt handlers
+DATA runtimesyscalls+(SYS_nanotime*4)(SB)/4, $sysnanotime(SB)
+DATA runtimesyscalls+(SYS_walltime*4)(SB)/4, $syswalltime(SB)
+DATA runtimesyscalls+(SYS_setwalltime*4)(SB)/4, $syssetwalltime(SB)
+DATA runtimesyscalls+(SYS_irqctl*4)(SB)/4, $sysirqctl(SB)
+DATA runtimesyscalls+(SYS_setprivlevel*4)(SB)/4, $syssetprivlevel(SB)
+DATA runtimesyscalls+(SYS_write*4)(SB)/4, $syswrite(SB)
+
+// syscalls disallowed for low priority interrupt handlers
+DATA runtimesyscalls+(SYS_setsystim1*4)(SB)/4, $syssetsystim1(SB)
+DATA runtimesyscalls+(SYS_newosproc*4)(SB)/4, $sysnewosproc(SB)
+DATA runtimesyscalls+(SYS_exitThread*4)(SB)/4, $sysexitThread(SB)
+DATA runtimesyscalls+(SYS_futexsleep*4)(SB)/4, $sysfutexsleep(SB)
+DATA runtimesyscalls+(SYS_futexwakeup*4)(SB)/4, $sysfutexwakeup(SB)
+DATA runtimesyscalls+(SYS_osyield*4)(SB)/4, $curcpuSchedule(SB)
+DATA runtimesyscalls+(SYS_nanosleep*4)(SB)/4, $sysnanosleep(SB)
+
+GLOBL runtimesyscalls(SB), RODATA, $(SYS_NUM*4)
+
+// func nanotime() int64
+TEXT nanotime(SB),NOSPLIT|NOFRAME,$0-8
+	MOVW  $SYS_nanotime, R4
+	MOVW  $(0+4), R5
+	MOVW  $8, R6
+	SWI
+	RET
+
+// func walltime() (sec int64, nsec int32)
+TEXT walltime(SB),NOSPLIT|NOFRAME,$0-12
+	MOVW  $SYS_walltime, R4
+	MOVW  $(0+4), R5
+	MOVW  $12, R6
+	SWI
+	RET
+
+// func setwalltime(sec int64, nsec int32)
+TEXT setwalltime(SB),NOSPLIT|NOFRAME,$0-12
+	MOVW  $SYS_setwalltime, R4
+	MOVW  $(12+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func irqctl(irq, ctl int) (enabled, prio, errno int)
+TEXT irqctl(SB),NOSPLIT|NOFRAME,$0-20
+	MOVW  $SYS_irqctl, R4
+	MOVW  $(8+4), R5
+	MOVW  $12, R6
+	SWI
+	RET
+
+// func setprivlevel(newlevel int) (oldlevel, errno int)
+TEXT setprivlevel(SB),NOSPLIT|NOFRAME,$0-12
+	MOVW  $SYS_setprivlevel, R4
+	MOVW  $(4+4), R5
+	MOVW  $8, R6
+	SWI
+	RET
+
+// func write(fd uintptr, p unsafe.Pointer, n int32) int32
+TEXT write(SB),NOSPLIT|NOFRAME,$0-16
+	MOVW  $SYS_write, R4
+	MOVW  $(12+4), R5
+	MOVW  $4, R6
+	SWI
+	RET
+
+// func setsystim1()
+TEXT setsystim1(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $SYS_setsystim1, R4
+	MOVW  $(0+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+
+// func newosproc(mp *m)
+TEXT newosproc(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  $SYS_newosproc, R4
+	MOVW  $(4+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func exitThread(wait *uint32)
+TEXT exitThread(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  $SYS_exitThread, R4
+	MOVW  $(4+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func futexsleep(addr *uint32, val uint32, ns int64)
+TEXT futexsleep(SB),NOSPLIT|NOFRAME,$0-16
+	MOVW  $SYS_futexsleep, R4
+	MOVW  $(16+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func futexwakeup(addr *uint32, cnt uint32)
+TEXT futexwakeup(SB),NOSPLIT|NOFRAME,$0-8
+	MOVW  $SYS_futexwakeup, R4
+	MOVW  $(8+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func osyield()
+TEXT osyield(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  $SYS_osyield, R4
+	MOVW  $(0+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// func nanosleep(ns int64)
+TEXT nanosleep(SB),NOSPLIT|NOFRAME,$0-8
+	MOVW  $SYS_nanosleep, R4
+	MOVW  $(8+4), R5
+	MOVW  $0, R6
+	SWI
+	RET
+
+// unsupported syscalls
+
+// func exit(r int32)
+TEXT exit(SB),NOSPLIT|NOFRAME,$0-4
+	BKPT
+	B   -1(PC)
+
+// utils
+
+// func publicationBarrier()
+TEXT publicationBarrier(SB),NOSPLIT|NOFRAME,$0-0
+	DMB  MB_ST
+	RET
diff --git a/src/runtime/syscall_noos.h b/src/runtime/syscall_noos.h
new file mode 100644
index 0000000000..e314346b1f
--- /dev/null
+++ b/src/runtime/syscall_noos.h
@@ -0,0 +1,27 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+// Some syscalls are alowed to be called by low priority interrupt handlers.
+//
+// Other syscalls, that can't run concurently with the scheduler, are available
+// only to threads.
+
+#define SYS_nanotime      0
+#define SYS_walltime      1
+#define SYS_setwalltime   2
+#define SYS_irqctl        3
+#define SYS_setprivlevel  4
+#define SYS_write         5
+
+#define SYS_LAST_LOWISR   5 
+
+#define SYS_setsystim1    6
+#define SYS_newosproc     7
+#define SYS_exitThread    8
+#define SYS_futexsleep    9
+#define SYS_futexwakeup  10
+#define SYS_osyield      11
+#define SYS_nanosleep    12
+
+#define SYS_NUM          13
diff --git a/src/runtime/tasker_noos.go b/src/runtime/tasker_noos.go
new file mode 100644
index 0000000000..b87b2b4bc9
--- /dev/null
+++ b/src/runtime/tasker_noos.go
@@ -0,0 +1,519 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/atomic"
+	"unsafe"
+)
+
+// Tasker implements simple threads for noos target. It works directly on the m
+// type pointers obtained from the Go scheduler.
+//
+// Every CPU has two local lists of threads: the cpuctx.runnable queue and the
+// cpuctx.waitingt sorted list.
+//
+// The cpuctx.runnable queue contains runnable threads waiting for their
+// timeslots on the CPU.
+//
+// The cpuctx.waitingt list contains threads that sleep until some time in the
+// future, sorted by this time.
+//
+// Both lists are also accessed by other CPUs if they wakeup this CPU's thread
+// that sleeps on a futex. The other CPU can only remove from the
+// cpuctx.waitingt or add to the cpuctx.runnable.
+//
+// There is also tasker.waitingf, a global hash table of threads that sleep on
+// futexes.
+//
+// The sleeping thread can be found in the cpuctx.waitingt, in the
+// tasker.waitingf or in the both places.
+//
+// Tasker does not define new fields in m but reuses unused ones:
+//
+// - tls[0:4], ncgocall, thread: used by mq, msl, mcl types (see mq.go),
+// - tls[4:6] and the space from libcall to the end of m struct: used by
+//   architecture-specific code (see: tasker_GOARCH.go),
+// - cgoCallersUse, caughtsig : used by tasker.
+//
+// Tasker relies on the following architecture-specific functions:
+//
+// taskerpreinit, taskerinit
+//
+// These functions should initialize the tasker. Taskerpreinit is run before the
+// Go scheduler and memory allocator are ready. It should setup a minimal
+// enviroment for them.
+//
+// curcpuSleep, cpuctx.wakeup
+//
+// These functions are called to put the CPU to sleep and wake up it. If the
+// CPU is running the wakeup command must be registered so the subsequent sleep
+// will not happen (to avoid race condition). It is allowed that both of these
+// functions do nothing.
+//
+// curcpuSavectxCall, curcpuSavectxSched
+//
+// This function is called to save the remaining context, not saved at syscall
+// entry (eg. it can save FPU state).
+//
+// archnewm
+//
+// This function is called to create the inintial state of the new thread and
+// save it in provided m so the curcpuEnterScheduler can later switch the CPU to
+// it.
+//
+// curcpuSchedule
+//
+// Run scheduler immediately or at syscall exit.
+//
+// The actual context switch is performed by architecture specific code at
+// curcpuRunScheduler exit. It should check the cpuctx.newexe variable and if
+// it is not nil, switch the context to the new thread specified in cpuctx.exe.
+//
+// Tasker code does not use FPU so the architecture specific context switch
+// code can avoid saving/restoring FPU context if not need.
+
+func dummyNanotime() int64        { return 1 }
+func dummySetalarm(ns int64) bool { return false }
+
+var thetasker = tasker{nanotime: dummyNanotime, setalarm: dummySetalarm}
+
+const fbnum = 4 // number of futex hash table buckets, must be power of two
+
+type cpuctx struct {
+	gh       g               // for ISRs, must be the first field in this struct
+	t        *tasker         // points to thetasker
+	exe      muintptr        // m currently executed by CPU
+	newexe   bool            // for architecture-dependent code: exe changed
+	runnable mq              // threads in runnable state
+	waitingt msl             // threads waiting until some time elapses
+	wakerq   [fbnum]notelist // futex wakeup request from interrupt handlers
+	mh       m               // for ISRs, mostly not written so works as cache line pad
+}
+
+type tasker struct {
+	allcpu   []*cpuctx
+	waitingf [fbnum]mcl // threads waiting on futex
+	tidgen   uintptr
+
+	timestart struct {
+		sec  int64
+		nsec int32
+		mx   cpumtx
+	}
+
+	nanotime func() int64
+	setalarm func(ns int64) bool
+
+	newnanotime func() int64        // see embedded/rtos.SetSystemTimer
+	newsetalarm func(ns int64) bool // see embedded/rtos.SetSystemTimer
+}
+
+//go:nosplit
+func fhash(addr uintptr) int { return int(addr>>3) & (fbnum - 1) }
+
+//go:nosplit
+func (t *tasker) fbucketbyaddr(addr uintptr) *mcl {
+	return &t.waitingf[fhash(addr)]
+}
+
+// gh is the first field of the cpuctx so we can benefit from the getg which is
+// intrinsic function, often compiled to 0 or 1 instruction. Don't call in
+// thread mode (valid only in handler mode).
+//go:nosplit
+func getcpuctx() *cpuctx { return (*cpuctx)(unsafe.Pointer(getg())) }
+
+//go:nosplit
+func taskerSetrunnable(m *m) {
+	curcpu := getcpuctx()
+	allcpu := curcpu.t.allcpu
+	var (
+		bestcpu *cpuctx
+		bestn   int
+	)
+	p := m.nextp
+	if p != 0 {
+		goto byid
+	}
+	p = m.p
+	if p != 0 {
+		goto byid
+	}
+	p = m.oldp
+	if p != 0 {
+		goto byid
+	}
+	// naive search for the less loaded cpu
+	bestcpu = curcpu
+	bestn = bestcpu.runnable.atomicLen()
+	for _, cpu := range allcpu {
+		if n := cpu.runnable.atomicLen(); n < bestn {
+			bestcpu = cpu
+			bestn = n
+		}
+	}
+	goto end
+byid:
+	bestcpu = allcpu[int(p.ptr().id)%len(allcpu)]
+end:
+	bestcpu.runnable.lock()
+	bestcpu.runnable.push(m)
+	bestcpu.runnable.unlock()
+	if bestcpu != curcpu {
+		bestcpu.wakeup()
+	}
+}
+
+//go:nosplit
+func taskerFutexwakeup(fb *mcl, addr *uint32, cnt uint32) {
+	for ; cnt != 0; cnt-- {
+		fb.lock()
+		m := fb.find(uintptr(unsafe.Pointer(addr)))
+		if m == nil {
+			fb.unlock()
+			break
+		}
+		owned := true
+		wt := mwt(m)
+		if wt != nil {
+			// this thread sleeps also in the cpuctx.waitingt
+			owned = atomic.Cas(mownedptr(m), 0, 1)
+		}
+		if owned {
+			fb.remove(m)
+		}
+		fb.unlock()
+		if owned {
+			if wt != nil {
+				wt.lock()
+				wt.remove(m)
+				wt.unlock()
+			}
+			taskerSetrunnable(m)
+		}
+	}
+	return
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func curcpuRunScheduler() {
+	curcpu := getcpuctx()
+	exe := curcpu.exe.ptr()
+	for {
+		// handle the wakeup requests from interrupt handlers
+		for i := range curcpu.wakerq {
+			n := curcpu.wakerq[i].removeall()
+			for n != nil {
+				next := n.release()
+				taskerFutexwakeup(&curcpu.t.waitingf[i], key32(&n.key), 1)
+				n = next
+			}
+		}
+
+		var sleepuntil int64
+
+		// waking up the threads sleeping in the curcpu.waitingt
+		now := curcpu.t.nanotime()
+		wt := &curcpu.waitingt
+		for {
+			wt.lock()
+			m := wt.first()
+			if m == nil {
+				sleepuntil = -1
+				break
+			}
+			sleepuntil = mval(m)
+			if sleepuntil > now {
+				break
+			}
+			owned := true
+			addr := mkey(m)
+			if addr != 0 {
+				// this thread sleeps also in the tasker.waitingf
+				owned = atomic.Cas(mownedptr(m), 0, 1)
+			}
+			if owned {
+				wt.remove(m)
+			}
+			wt.unlock()
+			if owned {
+				if addr != 0 {
+					fb := curcpu.t.fbucketbyaddr(addr)
+					fb.lock()
+					fb.remove(m)
+					fb.unlock()
+				}
+				taskerSetrunnable(m)
+			}
+		}
+		wt.unlock()
+
+		// schedule the next thread from the curcpu.runnable
+		curcpu.runnable.lock()
+		next := curcpu.runnable.pop()
+		if next != nil && exe != nil {
+			curcpuSavectxSched()
+			curcpu.runnable.push(exe)
+		}
+		curcpu.runnable.unlock()
+		if next != nil {
+			curcpu.exe.set(next)
+			curcpu.newexe = true
+			return
+		}
+		if exe != nil {
+			return
+		}
+
+		// Nothing to execute. If this will be a work-stealing scheduler it will
+		// try to steal some work from the other CPU here.
+		if curcpu.t.setalarm(sleepuntil) {
+			curcpuSleep()
+		}
+	}
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func rtos_notewakeup(n *notel) {
+	if !atomic.Cas(key32(&n.key), 0, 1) {
+		return
+	}
+	if !isr() {
+		futexwakeup(key32(&n.key), 1)
+		return
+	}
+	if n.acquire() {
+		getcpuctx().wakerq[fhash(uintptr(unsafe.Pointer(&n.key)))].insert(n)
+		curcpuWakeup()
+	}
+}
+
+// notelist
+
+// notel is a note that contains a link field to construct linked lists of notes
+type notel struct {
+	key  uintptr // must be the first field
+	link notelptr
+}
+
+//go:nosplit
+func (n *notel) acquire() bool {
+	return (&n.link).atomicCAS(0, 1)
+}
+
+//go:nosplit
+func (n *notel) release() *notel {
+	next := n.link
+	atomic.Storeuintptr((*uintptr)(&n.link), 0)
+	return next.ptr()
+}
+
+//go:nosplit
+func (n *notel) notelptr() notelptr { return notelptr(unsafe.Pointer(n)) }
+
+type notelptr uintptr
+
+//go:nosplit
+func (n notelptr) ptr() *notel { return (*notel)(unsafe.Pointer(n)) }
+
+//go:nosplit
+func (p *notelptr) atomicLoad() notelptr {
+	return notelptr(atomic.Loaduintptr((*uintptr)(p)))
+}
+
+//go:nosplit
+func (p *notelptr) atomicCAS(old, new notelptr) bool {
+	return atomic.Casuintptr((*uintptr)(p), uintptr(old), uintptr(new))
+}
+
+type notelist struct {
+	head notelptr
+}
+
+// insert inserts n at the beginning of l. You must acquire n before insert it.
+//go:nosplit
+func (l *notelist) insert(n *notel) {
+	if n.link != 1 {
+		for {
+			breakpoint()
+		}
+	}
+	for {
+		head := (&l.head).atomicLoad()
+		n.link = head
+		if (&l.head).atomicCAS(head, n.notelptr()) {
+			return
+		}
+	}
+}
+
+// removeall removes and returns the whole content of l.
+//go:nosplit
+func (l *notelist) removeall() *notel {
+	for {
+		head := (&l.head).atomicLoad()
+		if (&l.head).atomicCAS(head, 0) {
+			return head.ptr()
+		}
+	}
+}
+
+// syscall handlers
+
+//go:nowritebarrierrec
+//go:nosplit
+func syssetsystim1() {
+	t := getcpuctx().t
+	const n = unsafe.Sizeof(t.nanotime) / unsafe.Sizeof(uintptr(0))
+	*(*[n]uintptr)(unsafe.Pointer(&t.nanotime)) = *(*[n]uintptr)(unsafe.Pointer(&t.newnanotime))
+	*(*[n]uintptr)(unsafe.Pointer(&t.setalarm)) = *(*[n]uintptr)(unsafe.Pointer(&t.newsetalarm))
+
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysnanotime() int64 {
+	return getcpuctx().t.nanotime()
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysnewosproc(m *m) {
+	curcpu := getcpuctx()
+	m.procid = uint64(atomic.Xadduintptr(&curcpu.t.tidgen, 1))
+	archnewm(m)
+	taskerSetrunnable(m)
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysexitThread(wait *uint32) {
+	curcpu := getcpuctx()
+	curcpu.exe = 0
+	*wait = 0
+	curcpuSchedule()
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysfutexsleep(addr *uint32, val uint32, ns int64) {
+	if uint64(ns) < 64 {
+		return // to short to sleep (64 ns selected arbitrary)
+	}
+	curcpu := getcpuctx()
+	m := curcpu.exe.ptr()
+	if ns >= 0 {
+		// pre-insert m into curcpu.waitingt, m is not visible for other CPUs
+		// until it is published in the thetasker.waitingf.
+		msetval(m, curcpu.t.nanotime()+ns)
+		msetowned(m, 0)
+		wt := &curcpu.waitingt
+		msetwt(m, wt)
+		wt.lock()
+		wt.insertbyval(m)
+		wt.unlock()
+	} else {
+		msetwt(m, nil)
+	}
+	fb := curcpu.t.fbucketbyaddr(uintptr(unsafe.Pointer(addr)))
+	fb.lock()
+	sleep := (*addr == val)
+	if sleep {
+		curcpuSavectxCall()
+		curcpu.exe = 0
+		msetkey(m, uintptr(unsafe.Pointer(addr)))
+		fb.push(m)
+	}
+	fb.unlock()
+	if sleep {
+		curcpuSchedule()
+	} else if ns >= 0 {
+		// revert the pre-insert
+		curcpu.waitingt.lock()
+		curcpu.waitingt.remove(m)
+		curcpu.waitingt.unlock()
+	}
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysfutexwakeup(addr *uint32, cnt uint32) {
+	fb := getcpuctx().t.fbucketbyaddr(uintptr(unsafe.Pointer(addr)))
+	taskerFutexwakeup(fb, addr, cnt)
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysnanosleep(ns int64) {
+	if uint64(ns) < 64 {
+		return // to short to sleep (64 ns selected arbitrary)
+	}
+	curcpuSavectxCall()
+	curcpu := getcpuctx()
+	m := curcpu.exe.ptr()
+	curcpu.exe = 0
+	msetkey(m, 0)
+	msetval(m, curcpu.t.nanotime()+ns)
+	wt := &curcpu.waitingt
+	wt.lock()
+	wt.insertbyval(m)
+	wt.unlock()
+	curcpuSchedule()
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func syswalltime() (sec int64, nsec int32) {
+	t := getcpuctx().t
+	t.timestart.mx.lock()
+	sec = t.timestart.sec
+	nsec = t.timestart.nsec
+	t.timestart.mx.unlock()
+	now := t.nanotime()
+	s := now / 1e9
+	ns := int32(now - s*1e9)
+	sec += s
+	nsec += ns
+	if nsec >= 1e9 {
+		sec++
+		nsec -= 1e9
+	}
+	return
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func syssetwalltime(sec int64, nsec int32) {
+	t := getcpuctx().t
+	now := t.nanotime()
+	s := now / 1e9
+	ns := int32(now - s*1e9)
+	sec -= s
+	nsec -= ns
+	if nsec < 0 {
+		sec--
+		nsec += 1e9
+	}
+	t.timestart.mx.lock()
+	t.timestart.sec = sec
+	t.timestart.nsec = nsec
+	t.timestart.mx.unlock()
+}
+
+// m fields used
+
+//go:nosplit
+func mownedptr(m *m) *uint32 { return &m.cgoCallersUse }
+
+//go:nosplit
+func msetowned(m *m, owned uint32) { m.cgoCallersUse = owned }
+
+//go:nosplit
+func mwt(m *m) *msl { return (*msl)(unsafe.Pointer(m.caughtsig)) }
+
+//go:nosplit
+func msetwt(m *m, wt *msl) { m.caughtsig = guintptr(unsafe.Pointer(wt)) }
diff --git a/src/runtime/tasker_noos_thumb.go b/src/runtime/tasker_noos_thumb.go
new file mode 100644
index 0000000000..010d27299f
--- /dev/null
+++ b/src/runtime/tasker_noos_thumb.go
@@ -0,0 +1,276 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"embedded/mmio"
+	"internal/cpu/cortexm"
+	"internal/cpu/cortexm/debug/itm"
+	"internal/cpu/cortexm/mpu"
+	"internal/cpu/cortexm/nvic"
+	"internal/cpu/cortexm/scb"
+	"internal/cpu/cortexm/scid"
+	"unsafe"
+)
+
+// for now noos/thumb supports only single CPU
+
+func sev()
+func curcpuSleep()
+func curcpuSavectxSched()
+func curcpuSavectxCall() {} // all registars saved on caller's stack
+
+//go:nosplit
+func (cpu *cpuctx) wakeup() { sev() }
+
+//go:nosplit
+func curcpuWakeup() { sev() } // see ARM Errata 563915, STM32F10xx Errata 1.1.2
+
+//go:nosplit
+func curcpuSchedule() {
+	// Can't run the scheduler directly in the system call because the SVCall
+	// has higher priority than some interrupts. If there is no any thread to
+	// run the scheduler sleeps in a loop (there is no idle threads) and if run
+	// in SVCall the CPU can't handle lower priority interrupts.
+	//
+	// Instead, we trigger PendSV (which has priority lower that any other
+	// interrupt) to run the scheduler and rely on the exception tail-chaining
+	// to don't execute any instruction after SWI until the scheduler does its
+	// job.
+	//
+	// Caution! You can't rely on tail-chaining in case of debuging.
+	scb.SCB().ICSR.Store(scb.PENDSVSET)
+}
+
+// ARMv7-M requires at least 4 byte stack alignment so there are two bits
+// in saved stack pointer that can be used by tasker. It uses it for:
+const (
+	thrPrivLevel = 1 << 0 // thread privilege level
+	thrSmallCtx  = 1 << 1 // context saved in m contains only g (R10) register
+)
+
+// archnewm setups m's stack
+//go:nosplit
+func archnewm(m *m) {
+	sp := m.g0.stack.hi - unsafe.Sizeof(cortexm.StackFrame{})
+	sf := (*cortexm.StackFrame)(unsafe.Pointer(sp))
+	sf.PSR = cortexm.T
+	sf.PC = funcPC(mstart)
+	m.tls[msp] = sp | thrSmallCtx // small ctx
+	m.tls[mer] = cortexm.ExcReturnBase | cortexm.ExcReturnNoFPU |
+		cortexm.ExcReturnPSP
+	m.libcall.fn = uintptr(unsafe.Pointer(m.g0))
+}
+
+var (
+	cpu0  cpuctx
+	pcpu0 = &cpu0
+)
+
+//go:nowritebarrierrec
+//go:nosplit
+func taskerpreinit() {
+	*(*uintptr)(unsafe.Pointer(&cpu0.t)) = uintptr(unsafe.Pointer(&thetasker))
+	cpu0.exe.set(getg().m)
+	allcpu := (*slice)(unsafe.Pointer(&thetasker.allcpu))
+	*(*uintptr)(unsafe.Pointer(&allcpu.array)) = uintptr(unsafe.Pointer(&pcpu0))
+	allcpu.len = 1
+	allcpu.cap = 1
+
+	// setup exception priority levels
+
+	sc := scb.SCB()
+
+	// enable fault handlers
+	sc.SHCSR.SetBits(scb.MEMFAULTENA | scb.BUSFAULTENA | scb.USGFAULTENA)
+
+	// division by zero will cause the UsageFault
+	sc.DIV_0_TRP().Set()
+
+	// set PendSV and SVCall priorities according to description in rtos package
+	sc.PRI_SVCall().Store((4 << 5) << scb.PRI_SVCalln)
+	sc.PRI_PendSV().Store(255 << scb.PRI_PendSVn)
+
+	// All other exceptions/interrupts by default have the highest priority.
+
+	// use MPU if available to catch nil pointer dereferences (need 4 regions)
+	if _, d, _ := mpu.Type(); d >= 4 {
+		// Bellow there is the MPU configuration that more or less corresponds
+		// to the default CPU behavior, without MPU enabled.
+		//
+		// All regions starts at address 0x00000000. The SIZE and SRD
+		// (sub-region disabled) fields are used to set the region address
+		// ranges.
+		//
+		// The first 64 bytes of the code region that corresponds to the first
+		// 16 exception vectors are set inaccessible to catch nil pointer
+		// dereferences. The code region is declared read/write because some
+		// MCUs use normal memory access to program Flash.
+		//
+		// Tha RAM region is configured as shareable (usually shared with DMA).
+		// Shareable regions are by default not cacheable. If you enable L1
+		// cache in Cortex-M7 set the acc.SIWT bit so the RAM will be cacheable
+		// in write-through mode. WT mode degrades performance (not as much as
+		// you may think) but allows to avoid cache maintenance operations which
+		// are problematic in case of Cortex-M7.
+		var (
+			noacc  = mpu.A____
+			code   = mpu.Arwrw | mpu.C
+			ram    = mpu.Arwrw | mpu.TEX(1) | mpu.C | mpu.B | mpu.S
+			periph = mpu.Arwrw | mpu.B | mpu.S | mpu.XN
+		)
+		mpu.SetRegion(0x00000000|mpu.VALID|0, mpu.ENA|mpu.SIZE(29)|code)
+		mpu.SetRegion(0x00000000|mpu.VALID|1, mpu.ENA|mpu.SIZE(6)|noacc)
+		mpu.SetRegion(
+			0x00000000|mpu.VALID|2,
+			mpu.ENA|mpu.SIZE(32)|mpu.SRD(0b10011011)|periph,
+		)
+		mpu.SetRegion(
+			0x00000000|mpu.VALID|3,
+			mpu.ENA|mpu.SIZE(32)|mpu.SRD(0b11100101)|ram,
+		)
+		mmio.MB() // ensure any previous memory access is done before enable MPU
+		mpu.Set(mpu.ENABLE | mpu.PRIVDEFENA)
+	}
+
+	// ensure everything is set before any subsequent memory access
+	mmio.MB()
+}
+
+func taskerinit() {}
+
+// syscall handlers (architecture specific code)
+
+//go:nowritebarrierrec
+//go:nosplit
+func syswrite(fd uintptr, p unsafe.Pointer, n int32) int32 {
+	ITM := itm.ITM()
+	itmena := ITM.ITMENA()
+	port := &ITM.STIM[fd]
+	portena := mmio.UM32{&ITM.TER[fd>>5].U32, 1 << (fd & 31)}
+	s := (*[1 << 30]byte)(p)
+	for i := 0; i < int(n); {
+		for port.LoadBit(0) == 0 {
+			if portena.Load() == 0 || itmena.Load() == 0 {
+				return n // do not block on disabled port/ITM
+			}
+		}
+		switch m := int(n) - i; {
+		case m >= 4:
+			port.U32.Store(uint32(s[i]) + uint32(s[i+1])<<8 |
+				uint32(s[i+2])<<16 | uint32(s[i+3])<<24)
+			i += 4
+		case m >= 2:
+			p := (*mmio.U16)(unsafe.Pointer(&port.U32))
+			p.Store(uint16(s[i]) | uint16(s[i+1])<<8)
+			i += 2
+		default:
+			p := (*mmio.U8)(unsafe.Pointer(&port.U32))
+			p.Store(s[i])
+			i++
+		}
+	}
+	return n
+}
+
+// syscalls not used by runtime
+
+//go:nowritebarrierrec
+//go:nosplit
+func syssetprivlevel(newlevel int) (oldlevel, errno int) {
+	// this code requires thrPrivLevel == 1
+	const check byte = (thrPrivLevel - 1) * 256
+
+	ctrl := cpucontrol()
+	oldlevel = int(ctrl | 1)
+	if uint(newlevel) <= 1 {
+		setcpucontrol(ctrl&^1 | uint32(newlevel))
+	} else {
+		errno = 2 // rtos.ErrBadPrivLevel
+	}
+	return
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysirqenabled(irq int) (enabled, errno int) {
+	if uint(irq) >= irqNum() {
+		return 0, 4 // rtos.ErrBadIRQNumber
+	}
+	return int(nvic.NVIC().ISER[irq>>5].Load()) >> uint(irq&31), 0
+}
+
+//go:nowritebarrierrec
+//go:nosplit
+func sysirqctl(irq, ctl int) (enabled, prio, errno int) {
+	if uint(irq) >= irqNum() {
+		errno = 4 // rtos.ErrBadIntNumber
+		return
+	}
+	NVIC := nvic.NVIC()
+	// rtos package ensures valid ctl
+	if ctl >= 0 {
+		NVIC.IPR[irq].Store(nvic.IPR(255 - ctl))
+	}
+	rn, bo := irq>>5, uint(irq&31)
+	switch {
+	case ctl >= -1:
+		NVIC.ISER[rn].Store(1 << bo)
+	case ctl == -2:
+		NVIC.ICER[rn].Store(1 << bo)
+	default:
+		enabled = int(NVIC.ISER[irq>>5].Load()) >> bo
+	}
+	return
+}
+
+// utils
+
+func cpucontrol() uint32
+func setcpucontrol(ctrl uint32)
+
+//go:nosplit
+func irqNum() uint {
+	n := uint(scid.SCID().INTLINESNUM().Load()+1) * 32
+	if n > 496 {
+		n = 496
+	}
+	return n
+}
+
+// m.tls fields
+
+const msp = 4
+const mer = 5
+
+// Use libcall, libcallpc, libcallsp, libcallg, syscall, vdsoSP, vdsoPC and mOS
+// to save the second part of thread context. We do not save it on the gorutine
+// stack to avoid waste of memory (need to increasing stack guard for any
+// gorutine stack and there are much more gorutines than threads).
+//
+// We realy do not want to add another 96 bytes to the m so we take the trouble
+// to use these unused fields for our needs. The following constant declarations
+// are compile-time tests to ensure the fields were not changed or splitted.
+const (
+	_mLibcallAlign       = -(unsafe.Offsetof(m{}.libcall) & 3)
+	_mLibcallSize        = int8((unsafe.Sizeof(m{}.libcall) - 6*4) * 129)
+	_mLibcallFn          = int8(unsafe.Offsetof(m{}.libcall.fn) * 129)
+	_mLibcallLibcallpc   = int8((unsafe.Offsetof(m{}.libcallpc) - unsafe.Offsetof(m{}.libcall) - 6*4) * 129)
+	_mLibcallpcSize      = int8((unsafe.Sizeof(m{}.libcallpc) - 4) * 129)
+	_mLibcallpcLibcallsp = int8((unsafe.Offsetof(m{}.libcallsp) - unsafe.Offsetof(m{}.libcallpc) - 4) * 129)
+	_mLibcallspSize      = int8((unsafe.Sizeof(m{}.libcallsp) - 4) * 129)
+	_mLibcallspLibcallg  = int8((unsafe.Offsetof(m{}.libcallg) - unsafe.Offsetof(m{}.libcallsp) - 4) * 129)
+	_mLibcallgSize       = int8((unsafe.Sizeof(m{}.libcallg) - 4) * 129)
+	_mLibcallgSyscall    = int8((unsafe.Offsetof(m{}.syscall) - unsafe.Offsetof(m{}.libcallg) - 4) * 129)
+	_mSyscallSize        = int8((unsafe.Sizeof(m{}.syscall) - 6*4) * 129)
+	_mSyscallVdsosp      = int8((unsafe.Offsetof(m{}.vdsoSP) - unsafe.Offsetof(m{}.syscall) - 6*4) * 129)
+	_mVdsospSize         = int8((unsafe.Sizeof(m{}.vdsoSP) - 4) * 129)
+	_mVdsospVdsopc       = int8((unsafe.Offsetof(m{}.vdsoPC) - unsafe.Offsetof(m{}.vdsoSP) - 4) * 129)
+	_mVdsopcSize         = int8((unsafe.Sizeof(m{}.vdsoPC) - 4) * 129)
+	_mVdsopcMos          = int8((unsafe.Offsetof(m{}.mOS) - unsafe.Offsetof(m{}.vdsoPC) - 4) * 129)
+	_mSize               = int8((unsafe.Offsetof(m{}.mOS) - unsafe.Offsetof(m{}.libcall) + unsafe.Sizeof(m{}.mOS) - 24*4) * 129)
+)
+
+type mOS [7]uint32
diff --git a/src/runtime/tasker_noos_thumb.s b/src/runtime/tasker_noos_thumb.s
new file mode 100644
index 0000000000..fed9c3367b
--- /dev/null
+++ b/src/runtime/tasker_noos_thumb.s
@@ -0,0 +1,322 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "go_asm.h"
+#include "go_tls.h"
+#include "textflag.h"
+#include "syscall_noos.h"
+
+// This code uses ADD and ORR instructions when wants to set bits from 0 to 1.  // Mixing these two ways to do the same thing may seem seemingly inconsistent
+// but it is not. The shorter encoding is prefered. If both gives the same
+// length the ORR instruction is used because of its less energy per instruction
+// factor (see: https://www.ics.forth.gr/carv/greenvm/files/tr450.pdf).
+
+// TODO: Use ICSR.RETTOBASE to avoid manual stacking of previously stacked
+// registers.
+
+#define ICSR_ADDR 0xE000ED04
+#define ICSR_PENDSVCLR (1<<27)
+
+// identcurcpu indetifies the current CPU and returns a pointer to its cpuctx in
+// R0. It can clobber R0-R4,LR registers (other registers must be preserved).
+TEXT runtimeidentcurcpu(SB),NOSPLIT|NOFRAME,$0-0
+	// for now only single CPU is supported (see also cpuid, osinit)
+	MOVW  $cpu0(SB), R0
+	RET
+
+// func sev()
+TEXT sev(SB),NOSPLIT|NOFRAME,$0-0
+	SEV
+	RET
+
+// func cpucontrol() uint32
+TEXT cpucontrol(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  CONTROL, R0
+	MOVW  R0, ret+0(FP)
+	RET
+
+// func setcpucontrol(ctrl uint32)
+TEXT setcpucontrol(SB),NOSPLIT|NOFRAME,$0-4
+	MOVW  ctrl+0(FP), R0
+	MOVW  R0, CONTROL
+	RET
+
+// func curcpuSleep()
+TEXT curcpuSleep(SB),NOSPLIT|NOFRAME,$0-0
+	DSB  // flush CPU write buffers before sleep
+	WFE
+	// still in pendsvHandler so clear PendSV to avoid unnecessary reentry
+	MOVW  $ICSR_ADDR, R0
+	MOVW  $ICSR_PENDSVCLR, R1
+	MOVW  R1, (R0)
+	DMB   // ensure clearing happens before reading something for what we woke
+	RET
+
+// Exception handlers
+
+TEXT runtimefaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	// At this point a lot of things can be broken so don't touch
+	// stack nor memory. Do only few things that helps debuging.
+	TST   $4, LR
+	BNE   3(PC)
+	MOVW  MSP, R1
+	B     2(PC)
+	MOVW  PSP, R1
+	MOVW  IPSR, R0
+
+	// Now R0 and R1 contain useful information.
+
+	// R0 contains exception number:
+	// 3: HardFault  - see HFSR: x/xw 0xE000ED2C
+	// 4: MemManage  - see MMSR: x/xb 0xE000ED28, MMAR: x/xw 0xE000ED34
+	// 5: BusFault   - see BFSR: x/xb 0xE000ED29, BFAR: x/xw 0xE000ED38
+	// 6: UsageFault - see UFSR: x/xh 0xE000ED2A
+
+	// R1 should contain pointer to the exception stack frame:
+	// (R1) -> [R0, R1, R2, R3, IP, LR, PC, PSR]
+	// If R1 points to the valid memory examine:
+	// 1. Where PC points.
+	// 2. Thumb bit in PSR
+	// 3. IPSR in PSR
+
+	// To print stack frame in gdb use:
+	//   x/8xw $r1
+
+	BKPT
+	B   -1(PC)
+
+TEXT runtimereservedHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   faultHandler(SB)
+
+TEXT runtimenmiHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   faultHandler(SB)
+
+TEXT runtimehardfaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   faultHandler(SB)
+
+TEXT runtimememmanageHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   faultHandler(SB)
+
+TEXT runtimebusfaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   faultHandler(SB)
+
+TEXT runtimeusagefaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   faultHandler(SB)
+
+TEXT runtimesecurefaultHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   faultHandler(SB)
+
+TEXT runtimedebugmonHandler(SB),NOSPLIT|NOFRAME,$0-0
+	B   faultHandler(SB)
+
+
+#define SYS_MAX_ARGS (20+4) // max. size of argumants and return values + LR
+
+
+// svcallHandler handles synhronous SVCall exception generated by SWI (SVC)
+// instruction. The tiny wrappers over SWI instruction add three additional
+// parameters in registers:
+//
+// R4 - syscall number,
+// R5 - argument data size on the stack (+4 for frame-pointer),
+// R6 - return data size on the stack.
+//
+// The R0-R3,R12,LR,PC,PSR registers have been pushed on the stack at exception
+// entry. If FP registers have been used the place for D0-D7,FPSCR have been
+// also reserved on the stack (lazy stacking). The content of R0-R3,R12
+// registers can be broken by late-arriving higher-priority exception so using
+// R4-R6 avoids reading from the stack.
+//
+// The ABI0 does not follow the AAPCS: almost all GP registers and all FP
+// registers are caller saved. Therefore other (asynchronous) excepion handlers
+// need to save and restore not stacked GP registers and FP ones (if extended
+// frame was used). The SVCall is synchronous exception so this handler is
+// called almost like normal function and does not have to save any registers
+// except g (R10) and LR (EXC_RETURN).
+TEXT runtimesvcallHandler(SB),NOSPLIT|NOFRAME,$0-0
+	// check the syscall number
+	CMP  $SYS_NUM, R4
+	BGE  badSyscall
+
+	// stacked SP to R7
+	TST      $(1<<2), LR
+	MOVW.EQ  MSP, R7
+	MOVW.NE  PSP, R7
+	MOVW     R7, R1
+
+	// check does paddnig was added to the frame (stacked xPSR bit 9)
+	MOVW    (7*4)(R7), R0
+	TST     $(1<<9), R0
+	ADD.NE  $4, R1
+
+	// check does the extended frame is used
+	TST     $(1<<4), LR
+	ADD.NE  $(8*4), R1
+	ADD.EQ  $(26*4), R1
+
+	// push g, LR on the stack
+	MOVM.DB.W  [g,LR], (R13)
+
+	// make space on the stack for arguments + 3 registers
+	SUB  $(SYS_MAX_ARGS+3*4), R13
+
+	// copy arguments from the caller's stack
+	MOVW  $duffcopy+1024(SB), R0
+	MOVW  R13, R2
+	SUB   R5<<1, R0
+	MOVW  LR, R5  // save EXC_RETURN before call
+	BL    (R0)
+
+	// save data needed to copy the return values back to the caller's stack
+	ADD        $SYS_MAX_ARGS, R13, R0
+	MOVM.IA.W  [R1,R2,R6], (R0)
+
+	// current CPU context to R0
+	BL  identcurcpu(SB)
+
+	// check for fast syscall (unfortunately it lets through the calls by
+	// higher priority exceptions that are disallowed to use syscalls at all)
+	CMP  $SYS_LAST_LOWISR, R4
+	BLS  fast
+
+	// check for syscall from interrupt handler
+	CMP  R0, g
+	BEQ  badHandlerCall  // syscall not allowed in handler mode
+
+	// save thread context (small): SP+CONTROL[nPRIV], EXC_RETURN, g
+	MOVW  (cpuctx_exe)(R0), R3
+	MOVW  CONTROL, R2
+	AND   $const_thrPrivLevel, R2
+	ORR   R2, R7
+	ADD   $const_thrSmallCtx, R7  // set thrSmallCtx (only g saved)
+	MOVW  R7, (m_tls+const_msp*4)(R3)
+	MOVW  R5, (m_tls+const_mer*4)(R3)
+	MOVW  g, (m_libcall)(R3)
+
+fast:
+	// call the service routine
+	MOVW  R0, g
+	MOVW  $syscalls(SB), R0
+	MOVW  (R0)(R4*4), R0
+	BL    (R0)
+
+	// copy the return values back to the caller's stack
+	MOVW  (SYS_MAX_ARGS+2*4)(R13), R5
+	CBZ   R5, 6(PC)  // check if is something to copy
+	MOVW  (SYS_MAX_ARGS+0*4)(R13), R2
+	MOVW  (SYS_MAX_ARGS+1*4)(R13), R1
+	MOVW  $duffcopy+1024(SB), R0
+	SUB   R5<<1, R0
+	BL    (R0)
+
+	// wind up the stack and return from syscall
+	ADD        $(SYS_MAX_ARGS+3*4), R13
+	MOVM.IA.W  (R13), [g,R15]
+
+badSyscall:
+	BKPT
+	B   -1(PC)
+
+badHandlerCall:
+	BKPT
+	B   -1(PC)
+
+
+// pendsvHandler handles asynhronous PendSV exceptions generated by the system
+// timer routine or system calls, to schedule/wakeup next thread.
+TEXT runtimependsvHandler(SB),NOSPLIT|NOFRAME,$0-0
+	// load cpuctx
+	MOVW  LR, R12
+	BL    identcurcpu(SB)  // current CPU context to R0
+
+	// load cpuctx.exe
+	MOVW  (cpuctx_exe)(R0), R3
+
+	// if cpuctx.exe == nil then context saved by syscall
+	CBZ  R3, contextSaved
+
+	// save not stacked registers (R4-R11), SP, CONTROL[nPRIV], EXC_RETURN
+	TST      $(1<<2), R12
+	MOVW.EQ  MSP, R1
+	MOVW.NE  PSP, R1
+	MOVW     CONTROL, R2
+	AND      $const_thrPrivLevel, R2
+	ORR      R2, R1
+	MOVW     R1, (m_tls+const_msp*4)(R3)
+	MOVW     R12, (m_tls+const_mer*4)(R3)
+	ADD      $m_libcall, R3
+	MOVM.IA  [R4-R11], (R3)
+
+contextSaved:
+	// clear PendSV if set again to avoid unnecessary reentry to this handler
+	MOVW  $ICSR_ADDR, R1
+	MOVW  $ICSR_PENDSVCLR, R2
+	MOVW  R2, (R1)
+	DMB   // ensure clearing happens before checking nanotime and futexes
+
+	// enter scheduler
+	MOVW  R0, g
+	BL    curcpuRunScheduler(SB)
+
+	// load cpuctx.exe and cpuctx.newexe
+	MOVW   (cpuctx_exe)(g), R3
+	MOVBU  (cpuctx_newexe)(g), R0
+
+	// fast path if exe did not changed (cpuctx.newexe == false)
+	CBNZ     R0, newexe
+	ADD      $m_libcall, R3, R1
+	MOVM.IA  (R1), [R4-R11]
+	MOVW     (m_tls+const_mer*4)(R3), R15
+
+newexe:
+	// clear cpuctx.newexe
+	MOVW  $0, R0
+	MOVB  R0, (cpuctx_newexe)(g)
+
+	// load SP+CONTROL[nPRIV] and EXC_RETURN from new context
+	MOVW  (m_tls+const_msp*4)(R3), R0
+	MOVW  (m_tls+const_mer*4)(R3), R1
+
+	// restore privilege level
+	MOVW  CONTROL, R2
+	BIC   $const_thrPrivLevel, R2
+	AND   $const_thrPrivLevel, R0, R4
+	ORR   R4, R2
+	MOVW  R2, CONTROL
+
+	// restore PSP or MSP
+	BIC      $(const_thrPrivLevel+const_thrSmallCtx), R0, R2
+	TST      $(1<<2), R1
+	MOVW.EQ  R2, MSP
+	MOVW.NE  R2, PSP
+
+	// fast path in case of small context (only g saved in libcall)
+	TST      $const_thrSmallCtx, R0
+	MOVW.NE  (m_libcall)(R3), g
+	B.NE     (R1)
+
+	// restore registers saved in m.libcall
+	ADD        $m_libcall, R3
+	MOVM.IA.W  (R3), [R4-R11]
+	TST        $0x10, R1
+	BNE        3(PC)
+	HWORD      $0xEC93  // VLDM R3
+	HWORD      $0x8B10  // [D8-D15]
+	B          (R1)
+
+TEXT runtimecurcpuSavectxSched(SB),NOSPLIT|NOFRAME,$0-0
+	MOVW  (cpuctx_exe)(g), R0
+
+	MOVW  (m_tls+const_mer*4)(R0), R1
+	TST   $0x10, R1
+	RET.NE
+
+	ADD   $(m_libcall+8*4), R0
+	MOVW  CONTROL, R1
+	CPSID
+	HWORD  $0xEC80      // VSTM R0
+	HWORD  $0x8B10      // [D8-D15]
+	MOVW   R1, CONTROL  // to avoid stacking again by higher priority handler
+	CPSIE
+	RET
diff --git a/src/runtime/tasker_test.go b/src/runtime/tasker_test.go
new file mode 100644
index 0000000000..71d237e4cb
--- /dev/null
+++ b/src/runtime/tasker_test.go
@@ -0,0 +1,18 @@
+// Copyright 2019 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime_test
+
+import (
+	"runtime"
+	"testing"
+)
+
+// Unity tests that can be run on the host machine (not on the target MCU)
+
+func TestMQ(t *testing.T) {
+	if s := runtime.MQTest(); s != "" {
+		t.Error(s)
+	}
+}
diff --git a/src/runtime/textflag.h b/src/runtime/textflag.h
index daca36d948..512eac25ac 100644
--- a/src/runtime/textflag.h
+++ b/src/runtime/textflag.h
@@ -35,3 +35,5 @@
 // Function is the top of the call stack. Call stack unwinders should stop
 // at this function.
 #define TOPFRAME 2048
+// Generate interrupt handler prologue / epilogue.
+#define ISR 4096
diff --git a/src/runtime/time.go b/src/runtime/time.go
index 28a4722866..1dd36c3017 100644
--- a/src/runtime/time.go
+++ b/src/runtime/time.go
@@ -36,7 +36,7 @@ type timer struct {
 //
 // The current value is a compromise between memory usage and performance
 // that should cover the majority of GOMAXPROCS values used in the wild.
-const timersLen = 64
+const timersLen = 64*(1-_MCU) + 8*_MCU
 
 // timers contains "per-P" timer heaps.
 //
diff --git a/src/runtime/trace.go b/src/runtime/trace.go
index 08e92d2efe..1b0ea62c04 100644
--- a/src/runtime/trace.go
+++ b/src/runtime/trace.go
@@ -10,6 +10,8 @@
 // trace is captured for most events.
 // See https://golang.org/s/go15trace for more info.
 
+// +build !noos
+
 package runtime
 
 import (
diff --git a/src/runtime/trace_noos.go b/src/runtime/trace_noos.go
new file mode 100644
index 0000000000..867ba80768
--- /dev/null
+++ b/src/runtime/trace_noos.go
@@ -0,0 +1,92 @@
+package runtime
+
+// Event types in the trace, args are given in square brackets.
+const (
+	traceEvNone              = 0  // unused
+	traceEvBatch             = 1  // start of per-P batch of events [pid, timestamp]
+	traceEvFrequency         = 2  // contains tracer timer frequency [frequency (ticks per second)]
+	traceEvStack             = 3  // stack [stack id, number of PCs, array of {PC, func string ID, file string ID, line}]
+	traceEvGomaxprocs        = 4  // current value of GOMAXPROCS [timestamp, GOMAXPROCS, stack id]
+	traceEvProcStart         = 5  // start of P [timestamp, thread id]
+	traceEvProcStop          = 6  // stop of P [timestamp]
+	traceEvGCStart           = 7  // GC start [timestamp, seq, stack id]
+	traceEvGCDone            = 8  // GC done [timestamp]
+	traceEvGCSTWStart        = 9  // GC STW start [timestamp, kind]
+	traceEvGCSTWDone         = 10 // GC STW done [timestamp]
+	traceEvGCSweepStart      = 11 // GC sweep start [timestamp, stack id]
+	traceEvGCSweepDone       = 12 // GC sweep done [timestamp, swept, reclaimed]
+	traceEvGoCreate          = 13 // goroutine creation [timestamp, new goroutine id, new stack id, stack id]
+	traceEvGoStart           = 14 // goroutine starts running [timestamp, goroutine id, seq]
+	traceEvGoEnd             = 15 // goroutine ends [timestamp]
+	traceEvGoStop            = 16 // goroutine stops (like in select{}) [timestamp, stack]
+	traceEvGoSched           = 17 // goroutine calls Gosched [timestamp, stack]
+	traceEvGoPreempt         = 18 // goroutine is preempted [timestamp, stack]
+	traceEvGoSleep           = 19 // goroutine calls Sleep [timestamp, stack]
+	traceEvGoBlock           = 20 // goroutine blocks [timestamp, stack]
+	traceEvGoUnblock         = 21 // goroutine is unblocked [timestamp, goroutine id, seq, stack]
+	traceEvGoBlockSend       = 22 // goroutine blocks on chan send [timestamp, stack]
+	traceEvGoBlockRecv       = 23 // goroutine blocks on chan recv [timestamp, stack]
+	traceEvGoBlockSelect     = 24 // goroutine blocks on select [timestamp, stack]
+	traceEvGoBlockSync       = 25 // goroutine blocks on Mutex/RWMutex [timestamp, stack]
+	traceEvGoBlockCond       = 26 // goroutine blocks on Cond [timestamp, stack]
+	traceEvGoBlockNet        = 27 // goroutine blocks on network [timestamp, stack]
+	traceEvGoSysCall         = 28 // syscall enter [timestamp, stack]
+	traceEvGoSysExit         = 29 // syscall exit [timestamp, goroutine id, seq, real timestamp]
+	traceEvGoSysBlock        = 30 // syscall blocks [timestamp]
+	traceEvGoWaiting         = 31 // denotes that goroutine is blocked when tracing starts [timestamp, goroutine id]
+	traceEvGoInSyscall       = 32 // denotes that goroutine is in syscall when tracing starts [timestamp, goroutine id]
+	traceEvHeapAlloc         = 33 // memstats.heap_live change [timestamp, heap_alloc]
+	traceEvNextGC            = 34 // memstats.next_gc change [timestamp, next_gc]
+	traceEvTimerGoroutine    = 35 // denotes timer goroutine [timer goroutine id]
+	traceEvFutileWakeup      = 36 // denotes that the previous wakeup of this goroutine was futile [timestamp]
+	traceEvString            = 37 // string dictionary entry [ID, length, string]
+	traceEvGoStartLocal      = 38 // goroutine starts running on the same P as the last event [timestamp, goroutine id]
+	traceEvGoUnblockLocal    = 39 // goroutine is unblocked on the same P as the last event [timestamp, goroutine id, stack]
+	traceEvGoSysExitLocal    = 40 // syscall exit on the same P as the last event [timestamp, goroutine id, real timestamp]
+	traceEvGoStartLabel      = 41 // goroutine starts running with label [timestamp, goroutine id, seq, label string id]
+	traceEvGoBlockGC         = 42 // goroutine blocks on GC assist [timestamp, stack]
+	traceEvGCMarkAssistStart = 43 // GC mark assist start [timestamp, stack]
+	traceEvGCMarkAssistDone  = 44 // GC mark assist done [timestamp]
+	traceEvUserTaskCreate    = 45 // trace.NewContext [timestamp, internal task id, internal parent task id, stack, name string]
+	traceEvUserTaskEnd       = 46 // end of a task [timestamp, internal task id, stack]
+	traceEvUserRegion        = 47 // trace.WithRegion [timestamp, internal task id, mode(0:start, 1:end), stack, name string]
+	traceEvUserLog           = 48 // trace.Log [timestamp, internal task id, key string id, stack, value string]
+	traceEvCount             = 49
+	// Byte is used but only 6 bits are available for event type.
+	// The remaining 2 bits are used to specify the number of arguments.
+	// That means, the max event type value is 63.
+)
+
+type traceBufPtr uintptr
+
+var trace struct {
+	enabled  bool
+	shutdown bool
+}
+
+func traceGCSweepStart()                  {}
+func traceGCSweepDone()                   {}
+func traceGCMarkAssistStart()             {}
+func traceGCMarkAssistDone()              {}
+func traceHeapAlloc()                     {}
+func traceGoUnpark(gp *g, skip int)       {}
+func traceNextGC()                        {}
+func traceGCStart()                       {}
+func traceGCDone()                        {}
+func traceGCSTWStart(kind int)            {}
+func traceGCSTWDone()                     {}
+func traceGCSweepSpan(bytesSwept uintptr) {}
+func traceProcStart()                     {}
+func traceProcStop(pp *p)                 {}
+func traceProcFree(pp *p)                 {}
+func traceGoCreate(newg *g, pc uintptr)   {}
+func traceGoStart()                       {}
+func traceGoSched()                       {}
+func traceGoPreempt()                     {}
+func traceGoEnd()                         {}
+func traceGoPark(traceEv byte, skip int)  {}
+func traceGomaxprocs(procs int32)         {}
+func traceGoSysCall()                     {}
+func traceGoSysBlock(pp *p)               {}
+func traceGoSysExit(ts int64)             {}
+func traceReader() *g                     { return nil }
diff --git a/src/runtime/vdso_elf32.go b/src/runtime/vdso_elf32.go
index 2720f33eed..49fa841549 100644
--- a/src/runtime/vdso_elf32.go
+++ b/src/runtime/vdso_elf32.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build 386 arm
+// +build 386 arm thumb
 
 package runtime
 
diff --git a/src/runtime/vdso_in_none.go b/src/runtime/vdso_in_none.go
index f2d6bb55d9..c6ea66f0a9 100644
--- a/src/runtime/vdso_in_none.go
+++ b/src/runtime/vdso_in_none.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build linux,!386,!amd64,!arm,!arm64,!ppc64,!ppc64le !linux
+// +build linux,!386,!amd64,!arm,!thumb,!arm64,!ppc64,!ppc64le !linux
 
 package runtime
 
diff --git a/src/runtime/vdso_linux.go b/src/runtime/vdso_linux.go
index 71ba4ce416..df2dec0cd5 100644
--- a/src/runtime/vdso_linux.go
+++ b/src/runtime/vdso_linux.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build 386 amd64 arm arm64 ppc64 ppc64le
+// +build 386 amd64 arm thumb arm64 ppc64 ppc64le
 
 package runtime
 
diff --git a/src/runtime/vdso_linux_arm.go b/src/runtime/vdso_linux_armt.go
similarity index 94%
rename from src/runtime/vdso_linux_arm.go
rename to src/runtime/vdso_linux_armt.go
index ac3bdcf043..99ae532e15 100644
--- a/src/runtime/vdso_linux_arm.go
+++ b/src/runtime/vdso_linux_armt.go
@@ -2,6 +2,9 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build linux
+// +build arm thumb
+
 package runtime
 
 const (
diff --git a/src/runtime/vlop_thumb.s b/src/runtime/vlop_thumb.s
new file mode 100644
index 0000000000..d8f4836fe0
--- /dev/null
+++ b/src/runtime/vlop_thumb.s
@@ -0,0 +1,35 @@
+// Inferno's libkern/vlop-arm.s
+// https://bitbucket.org/inferno-os/inferno-os/src/default/libkern/vlop-arm.s
+//
+//         Copyright  1994-1999 Lucent Technologies Inc. All rights reserved.
+//         Revisions Copyright  2000-2007 Vita Nuova Holdings Limited (www.vitanuova.com).  All rights reserved.
+//         Portions Copyright 2009 The Go Authors. All rights reserved.
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+// THE SOFTWARE.
+
+#include "textflag.h"
+
+// _mul64by32 and _div64by32 not implemented on thumb
+TEXT runtime_mul64by32(SB), NOSPLIT, $0
+	MOVW	$-1, R0
+	MOVW	(R0), R1 // crash
+
+TEXT runtime_div64by32(SB), NOSPLIT, $0
+	MOVW	$-1, R0
+	MOVW	(R0), R1 // crash
diff --git a/src/runtime/vlop_thumb_test.go b/src/runtime/vlop_thumb_test.go
new file mode 100644
index 0000000000..015126adb5
--- /dev/null
+++ b/src/runtime/vlop_thumb_test.go
@@ -0,0 +1,128 @@
+// Copyright 2012 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime_test
+
+import (
+	"runtime"
+	"testing"
+)
+
+// arm soft division benchmarks adapted from
+// https://ridiculousfish.com/files/division_benchmarks.tar.gz
+
+const numeratorsSize = 1 << 21
+
+var numerators = randomNumerators()
+
+type randstate struct {
+	hi, lo uint32
+}
+
+func (r *randstate) rand() uint32 {
+	r.hi = r.hi<<16 + r.hi>>16
+	r.hi += r.lo
+	r.lo += r.hi
+	return r.hi
+}
+
+func randomNumerators() []uint32 {
+	numerators := make([]uint32, numeratorsSize)
+	random := &randstate{2147483563, 2147483563 ^ 0x49616E42}
+	for i := range numerators {
+		numerators[i] = random.rand()
+	}
+	return numerators
+}
+
+func bmUint32Div(divisor uint32, b *testing.B) {
+	var sum uint32
+	for i := 0; i < b.N; i++ {
+		sum += numerators[i&(numeratorsSize-1)] / divisor
+	}
+}
+
+func BenchmarkUint32Div7(b *testing.B)         { bmUint32Div(7, b) }
+func BenchmarkUint32Div37(b *testing.B)        { bmUint32Div(37, b) }
+func BenchmarkUint32Div123(b *testing.B)       { bmUint32Div(123, b) }
+func BenchmarkUint32Div763(b *testing.B)       { bmUint32Div(763, b) }
+func BenchmarkUint32Div1247(b *testing.B)      { bmUint32Div(1247, b) }
+func BenchmarkUint32Div9305(b *testing.B)      { bmUint32Div(9305, b) }
+func BenchmarkUint32Div13307(b *testing.B)     { bmUint32Div(13307, b) }
+func BenchmarkUint32Div52513(b *testing.B)     { bmUint32Div(52513, b) }
+func BenchmarkUint32Div60978747(b *testing.B)  { bmUint32Div(60978747, b) }
+func BenchmarkUint32Div106956295(b *testing.B) { bmUint32Div(106956295, b) }
+
+func bmUint32Mod(divisor uint32, b *testing.B) {
+	var sum uint32
+	for i := 0; i < b.N; i++ {
+		sum += numerators[i&(numeratorsSize-1)] % divisor
+	}
+}
+
+func BenchmarkUint32Mod7(b *testing.B)         { bmUint32Mod(7, b) }
+func BenchmarkUint32Mod37(b *testing.B)        { bmUint32Mod(37, b) }
+func BenchmarkUint32Mod123(b *testing.B)       { bmUint32Mod(123, b) }
+func BenchmarkUint32Mod763(b *testing.B)       { bmUint32Mod(763, b) }
+func BenchmarkUint32Mod1247(b *testing.B)      { bmUint32Mod(1247, b) }
+func BenchmarkUint32Mod9305(b *testing.B)      { bmUint32Mod(9305, b) }
+func BenchmarkUint32Mod13307(b *testing.B)     { bmUint32Mod(13307, b) }
+func BenchmarkUint32Mod52513(b *testing.B)     { bmUint32Mod(52513, b) }
+func BenchmarkUint32Mod60978747(b *testing.B)  { bmUint32Mod(60978747, b) }
+func BenchmarkUint32Mod106956295(b *testing.B) { bmUint32Mod(106956295, b) }
+
+func TestUsplit(t *testing.T) {
+	var den uint32 = 1000000
+	for _, x := range []uint32{0, 1, 999999, 1000000, 1010101, 0xFFFFFFFF} {
+		q1, r1 := runtime.Usplit(x)
+		q2, r2 := x/den, x%den
+		if q1 != q2 || r1 != r2 {
+			t.Errorf("%d/1e6, %d%%1e6 = %d, %d, want %d, %d", x, x, q1, r1, q2, r2)
+		}
+	}
+}
+
+//go:noinline
+func armFloatWrite(a *[129]float64) {
+	// This used to miscompile on arm5.
+	// The offset is too big to fit in a load.
+	// So the code does:
+	//   ldr     r0, [sp, #8]
+	//   bl      6f690 <_sfloat>
+	//   ldr     fp, [pc, #32]   ; (address of 128.0)
+	//   vldr    d0, [fp]
+	//   ldr     fp, [pc, #28]   ; (1024)
+	//   add     fp, fp, r0
+	//   vstr    d0, [fp]
+	// The software floating-point emulator gives up on the add.
+	// This causes the store to not work.
+	// See issue 15440.
+	a[128] = 128.0
+}
+func TestArmFloatBigOffsetWrite(t *testing.T) {
+	var a [129]float64
+	for i := 0; i < 128; i++ {
+		a[i] = float64(i)
+	}
+	armFloatWrite(&a)
+	for i, x := range a {
+		if x != float64(i) {
+			t.Errorf("bad entry %d:%f\n", i, x)
+		}
+	}
+}
+
+//go:noinline
+func armFloatRead(a *[129]float64) float64 {
+	return a[128]
+}
+func TestArmFloatBigOffsetRead(t *testing.T) {
+	var a [129]float64
+	for i := 0; i < 129; i++ {
+		a[i] = float64(i)
+	}
+	if x := armFloatRead(&a); x != 128.0 {
+		t.Errorf("bad value %f\n", x)
+	}
+}
diff --git a/src/runtime/vlrt.go b/src/runtime/vlrt.go
index f790d3b17f..b6929f4e7b 100644
--- a/src/runtime/vlrt.go
+++ b/src/runtime/vlrt.go
@@ -23,7 +23,7 @@
 // OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 // THE SOFTWARE.
 
-// +build arm 386 mips mipsle
+// +build arm thumb 386 mips mipsle
 
 package runtime
 
@@ -196,7 +196,7 @@ func _div64by32(a uint64, b uint32, r *uint32) (q uint32)
 
 //go:nosplit
 func dodiv(n, d uint64) (q, r uint64) {
-	if GOARCH == "arm" {
+	if GOARCH == "arm" || GOARCH == "thumb" {
 		// arm doesn't have a division instruction, so
 		// slowdodiv is the best that we can do.
 		return slowdodiv(n, d)
diff --git a/src/strconv/decimal.go b/src/strconv/decimal.go
index b58001888e..aa2ac5e960 100644
--- a/src/strconv/decimal.go
+++ b/src/strconv/decimal.go
@@ -175,7 +175,7 @@ type leftCheat struct {
 	cutoff string // minus one digit if original < a.
 }
 
-var leftcheats = []leftCheat{
+var leftcheats = [...]leftCheat{
 	// Leading digits of 1/2^i = 5^i.
 	// 5^23 is not an exact 64-bit floating point number,
 	// so have to use bc for the math.
diff --git a/src/strconv/isprint.go b/src/strconv/isprint.go
index a8dfdb2d9b..35e9eaf38a 100644
--- a/src/strconv/isprint.go
+++ b/src/strconv/isprint.go
@@ -8,7 +8,7 @@ package strconv
 
 // (448+137+90)*2 + (418)*4 = 3022 bytes
 
-var isPrint16 = []uint16{
+var isPrint16 = [...]uint16{
 	0x0020, 0x007e,
 	0x00a1, 0x0377,
 	0x037a, 0x037f,
@@ -235,7 +235,7 @@ var isPrint16 = []uint16{
 	0xfffc, 0xfffd,
 }
 
-var isNotPrint16 = []uint16{
+var isNotPrint16 = [...]uint16{
 	0x00ad,
 	0x038b,
 	0x038d,
@@ -375,7 +375,7 @@ var isNotPrint16 = []uint16{
 	0xffe7,
 }
 
-var isPrint32 = []uint32{
+var isPrint32 = [...]uint32{
 	0x010000, 0x01004d,
 	0x010050, 0x01005d,
 	0x010080, 0x0100fa,
@@ -587,7 +587,7 @@ var isPrint32 = []uint32{
 	0x0e0100, 0x0e01ef,
 }
 
-var isNotPrint32 = []uint16{ // add 0x10000 to each entry
+var isNotPrint32 = [...]uint16{ // add 0x10000 to each entry
 	0x000c,
 	0x0027,
 	0x003b,
@@ -681,7 +681,7 @@ var isNotPrint32 = []uint16{ // add 0x10000 to each entry
 }
 
 // isGraphic lists the graphic runes not matched by IsPrint.
-var isGraphic = []uint16{
+var isGraphic = [...]uint16{
 	0x00a0,
 	0x1680,
 	0x2000,
diff --git a/src/strconv/makeisprint.go b/src/strconv/makeisprint.go
index 1a3248f308..45d8a2c23b 100644
--- a/src/strconv/makeisprint.go
+++ b/src/strconv/makeisprint.go
@@ -148,25 +148,25 @@ func main() {
 		(len(range16)+len(except16)+len(except32))*2+
 			(len(range32))*4)
 
-	fmt.Fprintf(&buf, "var isPrint16 = []uint16{\n")
+	fmt.Fprintf(&buf, "var isPrint16 = [...]uint16{\n")
 	for i := 0; i < len(range16); i += 2 {
 		fmt.Fprintf(&buf, "\t%#04x, %#04x,\n", range16[i], range16[i+1])
 	}
 	fmt.Fprintf(&buf, "}\n\n")
 
-	fmt.Fprintf(&buf, "var isNotPrint16 = []uint16{\n")
+	fmt.Fprintf(&buf, "var isNotPrint16 = [...]uint16{\n")
 	for _, r := range except16 {
 		fmt.Fprintf(&buf, "\t%#04x,\n", r)
 	}
 	fmt.Fprintf(&buf, "}\n\n")
 
-	fmt.Fprintf(&buf, "var isPrint32 = []uint32{\n")
+	fmt.Fprintf(&buf, "var isPrint32 = [...]uint32{\n")
 	for i := 0; i < len(range32); i += 2 {
 		fmt.Fprintf(&buf, "\t%#06x, %#06x,\n", range32[i], range32[i+1])
 	}
 	fmt.Fprintf(&buf, "}\n\n")
 
-	fmt.Fprintf(&buf, "var isNotPrint32 = []uint16{ // add 0x10000 to each entry\n")
+	fmt.Fprintf(&buf, "var isNotPrint32 = [...]uint16{ // add 0x10000 to each entry\n")
 	for _, r := range except32 {
 		if r >= 0x20000 {
 			log.Fatalf("%U too big for isNotPrint32\n", r)
@@ -177,7 +177,7 @@ func main() {
 
 	// The list of graphic but not "printable" runes is short. Just make one easy table.
 	fmt.Fprintf(&buf, "// isGraphic lists the graphic runes not matched by IsPrint.\n")
-	fmt.Fprintf(&buf, "var isGraphic = []uint16{\n")
+	fmt.Fprintf(&buf, "var isGraphic = [...]uint16{\n")
 	for r := rune(0); r <= unicode.MaxRune; r++ {
 		if unicode.IsPrint(r) != unicode.IsGraphic(r) {
 			// Sanity check.
diff --git a/src/strconv/quote.go b/src/strconv/quote.go
index b50496a0ff..e8b8d110d3 100644
--- a/src/strconv/quote.go
+++ b/src/strconv/quote.go
@@ -500,7 +500,7 @@ func IsPrint(r rune) bool {
 	// If we find x in a range, make sure x is not in isNotPrint list.
 
 	if 0 <= r && r < 1<<16 {
-		rr, isPrint, isNotPrint := uint16(r), isPrint16, isNotPrint16
+		rr, isPrint, isNotPrint := uint16(r), isPrint16[:], isNotPrint16[:]
 		i := bsearch16(isPrint, rr)
 		if i >= len(isPrint) || rr < isPrint[i&^1] || isPrint[i|1] < rr {
 			return false
@@ -509,7 +509,7 @@ func IsPrint(r rune) bool {
 		return j >= len(isNotPrint) || isNotPrint[j] != rr
 	}
 
-	rr, isPrint, isNotPrint := uint32(r), isPrint32, isNotPrint32
+	rr, isPrint, isNotPrint := uint32(r), isPrint32[:], isNotPrint32[:]
 	i := bsearch32(isPrint, rr)
 	if i >= len(isPrint) || rr < isPrint[i&^1] || isPrint[i|1] < rr {
 		return false
@@ -541,6 +541,6 @@ func isInGraphicList(r rune) bool {
 		return false
 	}
 	rr := uint16(r)
-	i := bsearch16(isGraphic, rr)
+	i := bsearch16(isGraphic[:], rr)
 	return i < len(isGraphic) && rr == isGraphic[i]
 }
diff --git a/src/syscall/asm_linux_thumb.s b/src/syscall/asm_linux_thumb.s
new file mode 100644
index 0000000000..458e9cce79
--- /dev/null
+++ b/src/syscall/asm_linux_thumb.s
@@ -0,0 +1,167 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "textflag.h"
+#include "funcdata.h"
+
+//
+// System calls for arm, Linux
+//
+
+// func Syscall(syscall uintptr, a1, a2, a3 uintptr) (r1, r2, err uintptr);
+TEXT Syscall(SB),NOSPLIT,$0-28
+	BL	runtimeentersyscall(SB)
+	MOVW	trap+0(FP), R7
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	MOVW	$0, R3
+	MOVW	$0, R4
+	MOVW	$0, R5
+	SWI	$0
+	MOVW	$0xfffff001, R1
+	CMP	R1, R0
+	BLS	ok
+	MOVW	$-1, R1
+	MOVW	R1, r1+16(FP)
+	MOVW	$0, R2
+	MOVW	R2, r2+20(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+24(FP)
+	BL	runtimeexitsyscall(SB)
+	RET
+ok:
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	MOVW	R0, err+24(FP)
+	BL	runtimeexitsyscall(SB)
+	RET
+
+// func Syscall6(trap uintptr, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr);
+// Actually Syscall5 but the rest of the code expects it to be named Syscall6.
+TEXT Syscall6(SB),NOSPLIT,$0-40
+	BL	runtimeentersyscall(SB)
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	MOVW	a4+16(FP), R3
+	MOVW	a5+20(FP), R4
+	MOVW	a6+24(FP), R5
+	SWI	$0
+	MOVW	$0xfffff001, R6
+	CMP	R6, R0
+	BLS	ok6
+	MOVW	$-1, R1
+	MOVW	R1, r1+28(FP)
+	MOVW	$0, R2
+	MOVW	R2, r2+32(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+36(FP)
+	BL	runtimeexitsyscall(SB)
+	RET
+ok6:
+	MOVW	R0, r1+28(FP)
+	MOVW	R1, r2+32(FP)
+	MOVW	$0, R0
+	MOVW	R0, err+36(FP)
+	BL	runtimeexitsyscall(SB)
+	RET
+
+// func RawSyscall6(trap uintptr, a1, a2, a3, a4, a5, a6 uintptr) (r1, r2, err uintptr);
+// Actually RawSyscall5 but the rest of the code expects it to be named RawSyscall6.
+TEXT	RawSyscall6(SB),NOSPLIT,$0-40
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	MOVW	a4+16(FP), R3
+	MOVW	a5+20(FP), R4
+	MOVW	a6+24(FP), R5
+	SWI	$0
+	MOVW	$0xfffff001, R6
+	CMP	R6, R0
+	BLS	ok2
+	MOVW	$-1, R1
+	MOVW	R1, r1+28(FP)
+	MOVW	$0, R2
+	MOVW	R2, r2+32(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+36(FP)
+	RET
+ok2:
+	MOVW	R0, r1+28(FP)
+	MOVW	R1, r2+32(FP)
+	MOVW	$0, R0
+	MOVW	R0, err+36(FP)
+	RET
+
+#define SYS__LLSEEK 140  /* from zsysnum_linux_arm.go */
+// func seek(fd int, offset int64, whence int) (newoffset int64, errno int)
+// Implemented in assembly to avoid allocation when
+// taking the address of the return value newoffset.
+// Underlying system call is
+//	llseek(int fd, int offhi, int offlo, int64 *result, int whence)
+TEXT seek(SB),NOSPLIT,$0-28
+	BL	runtimeentersyscall(SB)
+	MOVW	$SYS__LLSEEK, R7	// syscall entry
+	MOVW	fd+0(FP), R0
+	MOVW	offset_hi+8(FP), R1
+	MOVW	offset_lo+4(FP), R2
+	MOVW	$newoffset_lo+16(FP), R3
+	MOVW	whence+12(FP), R4
+	SWI	$0
+	MOVW	$0xfffff001, R6
+	CMP	R6, R0
+	BLS	okseek
+	MOVW	$0, R1
+	MOVW	R1, newoffset_lo+16(FP)
+	MOVW	R1, newoffset_hi+20(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+24(FP)
+	BL	runtimeexitsyscall(SB)
+	RET
+okseek:
+	// system call filled in newoffset already
+	MOVW	$0, R0
+	MOVW	R0, err+24(FP)
+	BL	runtimeexitsyscall(SB)
+	RET
+
+// func RawSyscall(trap uintptr, a1, a2, a3 uintptr) (r1, r2, err uintptr);
+TEXT RawSyscall(SB),NOSPLIT,$0-28
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	SWI	$0
+	MOVW	$0xfffff001, R1
+	CMP	R1, R0
+	BLS	ok1
+	MOVW	$-1, R1
+	MOVW	R1, r1+16(FP)
+	MOVW	$0, R2
+	MOVW	R2, r2+20(FP)
+	RSB	$0, R0, R0
+	MOVW	R0, err+24(FP)
+	RET
+ok1:
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	MOVW	R0, err+24(FP)
+	RET
+
+// func rawSyscallNoError(trap uintptr, a1, a2, a3 uintptr) (r1, r2 uintptr);
+TEXT rawSyscallNoError(SB),NOSPLIT,$0-24
+	MOVW	trap+0(FP), R7	// syscall entry
+	MOVW	a1+4(FP), R0
+	MOVW	a2+8(FP), R1
+	MOVW	a3+12(FP), R2
+	SWI	$0
+	MOVW	R0, r1+16(FP)
+	MOVW	$0, R0
+	MOVW	R0, r2+20(FP)
+	RET
diff --git a/src/syscall/endian_little.go b/src/syscall/endian_little.go
index 013d878b8d..bce07632c9 100644
--- a/src/syscall/endian_little.go
+++ b/src/syscall/endian_little.go
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 //
-// +build 386 amd64 amd64p32 arm arm64 ppc64le mips64le mipsle wasm
+// +build 386 amd64 amd64p32 arm thumb arm64 ppc64le mips64le mipsle wasm
 
 package syscall
 
diff --git a/src/syscall/flock_linux_32bit.go b/src/syscall/flock_linux_32bit.go
index e1548995b2..0221790a28 100644
--- a/src/syscall/flock_linux_32bit.go
+++ b/src/syscall/flock_linux_32bit.go
@@ -1,4 +1,4 @@
-// +build linux,386 linux,arm linux,mips linux,mipsle
+// +build linux,386 linux,arm linux,thumb linux,mips linux,mipsle
 
 // Copyright 2014 The Go Authors. All rights reserved.
 // Use of this source code is governed by a BSD-style
diff --git a/src/syscall/mkall.sh b/src/syscall/mkall.sh
index cbf5540e04..f0085da8d5 100755
--- a/src/syscall/mkall.sh
+++ b/src/syscall/mkall.sh
@@ -190,7 +190,7 @@ linux_amd64)
 	mksysnum="./mksysnum_linux.pl $unistd_h"
 	mktypes="GOARCH=$GOARCH go tool cgo -godefs"
 	;;
-linux_arm)
+linux_arm|linux_thumb)
 	mkerrors="$mkerrors"
 	mksyscall="./mksyscall.pl -l32 -arm"
 	mksysnum="curl -s 'http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/plain/arch/arm/include/uapi/asm/unistd.h' | ./mksysnum_linux.pl -"
diff --git a/src/syscall/setuidgid_32_linux.go b/src/syscall/setuidgid_32_linux.go
index 1fe7120d1c..a39a17cb33 100644
--- a/src/syscall/setuidgid_32_linux.go
+++ b/src/syscall/setuidgid_32_linux.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build 386 arm
+// +build 386 arm thumb
 
 package syscall
 
diff --git a/src/syscall/setuidgid_linux.go b/src/syscall/setuidgid_linux.go
index 22fa334bfa..01f09ff6de 100644
--- a/src/syscall/setuidgid_linux.go
+++ b/src/syscall/setuidgid_linux.go
@@ -3,7 +3,7 @@
 // license that can be found in the LICENSE file.
 
 // +build linux
-// +build !386,!arm
+// +build !386,!arm,!thumb
 
 package syscall
 
diff --git a/src/syscall/syscall_linux_thumb.go b/src/syscall/syscall_linux_thumb.go
new file mode 100644
index 0000000000..65543193e1
--- /dev/null
+++ b/src/syscall/syscall_linux_thumb.go
@@ -0,0 +1,231 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package syscall
+
+import "unsafe"
+
+const (
+	_SYS_dup       = SYS_DUP2
+	_SYS_setgroups = SYS_SETGROUPS32
+)
+
+func setTimespec(sec, nsec int64) Timespec {
+	return Timespec{Sec: int32(sec), Nsec: int32(nsec)}
+}
+
+func setTimeval(sec, usec int64) Timeval {
+	return Timeval{Sec: int32(sec), Usec: int32(usec)}
+}
+
+func Pipe(p []int) (err error) {
+	if len(p) != 2 {
+		return EINVAL
+	}
+	var pp [2]_C_int
+	err = pipe2(&pp, 0)
+	p[0] = int(pp[0])
+	p[1] = int(pp[1])
+	return
+}
+
+//sysnb pipe2(p *[2]_C_int, flags int) (err error)
+
+func Pipe2(p []int, flags int) (err error) {
+	if len(p) != 2 {
+		return EINVAL
+	}
+	var pp [2]_C_int
+	err = pipe2(&pp, flags)
+	p[0] = int(pp[0])
+	p[1] = int(pp[1])
+	return
+}
+
+// Underlying system call writes to newoffset via pointer.
+// Implemented in assembly to avoid allocation.
+func seek(fd int, offset int64, whence int) (newoffset int64, err Errno)
+
+func Seek(fd int, offset int64, whence int) (newoffset int64, err error) {
+	newoffset, errno := seek(fd, offset, whence)
+	if errno != 0 {
+		return 0, errno
+	}
+	return newoffset, nil
+}
+
+//sys	accept(s int, rsa *RawSockaddrAny, addrlen *_Socklen) (fd int, err error)
+//sys	accept4(s int, rsa *RawSockaddrAny, addrlen *_Socklen, flags int) (fd int, err error)
+//sys	bind(s int, addr unsafe.Pointer, addrlen _Socklen) (err error)
+//sys	connect(s int, addr unsafe.Pointer, addrlen _Socklen) (err error)
+//sysnb	getgroups(n int, list *_Gid_t) (nn int, err error) = SYS_GETGROUPS32
+//sysnb	setgroups(n int, list *_Gid_t) (err error) = SYS_SETGROUPS32
+//sys	getsockopt(s int, level int, name int, val unsafe.Pointer, vallen *_Socklen) (err error)
+//sys	setsockopt(s int, level int, name int, val unsafe.Pointer, vallen uintptr) (err error)
+//sysnb	socket(domain int, typ int, proto int) (fd int, err error)
+//sysnb	getpeername(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error)
+//sysnb	getsockname(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error)
+//sys	recvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error)
+//sys	sendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error)
+//sysnb	socketpair(domain int, typ int, flags int, fd *[2]int32) (err error)
+//sys	recvmsg(s int, msg *Msghdr, flags int) (n int, err error)
+//sys	sendmsg(s int, msg *Msghdr, flags int) (n int, err error)
+
+// 64-bit file system and 32-bit uid calls
+// (16-bit uid calls are not always supported in newer kernels)
+//sys	Dup2(oldfd int, newfd int) (err error)
+//sys	Fchown(fd int, uid int, gid int) (err error) = SYS_FCHOWN32
+//sys	Fstat(fd int, stat *Stat_t) (err error) = SYS_FSTAT64
+//sys	fstatat(dirfd int, path string, stat *Stat_t, flags int) (err error) = SYS_FSTATAT64
+//sysnb	Getegid() (egid int) = SYS_GETEGID32
+//sysnb	Geteuid() (euid int) = SYS_GETEUID32
+//sysnb	Getgid() (gid int) = SYS_GETGID32
+//sysnb	Getuid() (uid int) = SYS_GETUID32
+//sysnb	InotifyInit() (fd int, err error)
+//sys	Listen(s int, n int) (err error)
+//sys	sendfile(outfd int, infd int, offset *int64, count int) (written int, err error) = SYS_SENDFILE64
+//sys	Select(nfd int, r *FdSet, w *FdSet, e *FdSet, timeout *Timeval) (n int, err error) = SYS__NEWSELECT
+//sys	Setfsgid(gid int) (err error) = SYS_SETFSGID32
+//sys	Setfsuid(uid int) (err error) = SYS_SETFSUID32
+//sysnb	Setregid(rgid int, egid int) (err error) = SYS_SETREGID32
+//sysnb	Setresgid(rgid int, egid int, sgid int) (err error) = SYS_SETRESGID32
+//sysnb	Setresuid(ruid int, euid int, suid int) (err error) = SYS_SETRESUID32
+//sysnb	Setreuid(ruid int, euid int) (err error) = SYS_SETREUID32
+//sys	Shutdown(fd int, how int) (err error)
+//sys	Splice(rfd int, roff *int64, wfd int, woff *int64, len int, flags int) (n int, err error)
+
+// Vsyscalls on amd64.
+//sysnb	Gettimeofday(tv *Timeval) (err error)
+//sysnb	Time(t *Time_t) (tt Time_t, err error)
+
+//sys   Pread(fd int, p []byte, offset int64) (n int, err error) = SYS_PREAD64
+//sys   Pwrite(fd int, p []byte, offset int64) (n int, err error) = SYS_PWRITE64
+//sys	Truncate(path string, length int64) (err error) = SYS_TRUNCATE64
+//sys	Ftruncate(fd int, length int64) (err error) = SYS_FTRUNCATE64
+
+//sys	mmap2(addr uintptr, length uintptr, prot int, flags int, fd int, pageOffset uintptr) (xaddr uintptr, err error)
+//sys	EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error)
+
+func Stat(path string, stat *Stat_t) (err error) {
+	return fstatat(_AT_FDCWD, path, stat, 0)
+}
+
+func Lchown(path string, uid int, gid int) (err error) {
+	return Fchownat(_AT_FDCWD, path, uid, gid, _AT_SYMLINK_NOFOLLOW)
+}
+
+func Lstat(path string, stat *Stat_t) (err error) {
+	return fstatat(_AT_FDCWD, path, stat, _AT_SYMLINK_NOFOLLOW)
+}
+
+func Fstatfs(fd int, buf *Statfs_t) (err error) {
+	_, _, e := Syscall(SYS_FSTATFS64, uintptr(fd), unsafe.Sizeof(*buf), uintptr(unsafe.Pointer(buf)))
+	if e != 0 {
+		err = e
+	}
+	return
+}
+
+func Statfs(path string, buf *Statfs_t) (err error) {
+	pathp, err := BytePtrFromString(path)
+	if err != nil {
+		return err
+	}
+	_, _, e := Syscall(SYS_STATFS64, uintptr(unsafe.Pointer(pathp)), unsafe.Sizeof(*buf), uintptr(unsafe.Pointer(buf)))
+	if e != 0 {
+		err = e
+	}
+	return
+}
+
+func mmap(addr uintptr, length uintptr, prot int, flags int, fd int, offset int64) (xaddr uintptr, err error) {
+	page := uintptr(offset / 4096)
+	if offset != int64(page)*4096 {
+		return 0, EINVAL
+	}
+	return mmap2(addr, length, prot, flags, fd, page)
+}
+
+type rlimit32 struct {
+	Cur uint32
+	Max uint32
+}
+
+//sysnb getrlimit(resource int, rlim *rlimit32) (err error) = SYS_GETRLIMIT
+
+const rlimInf32 = ^uint32(0)
+const rlimInf64 = ^uint64(0)
+
+func Getrlimit(resource int, rlim *Rlimit) (err error) {
+	err = prlimit(0, resource, nil, rlim)
+	if err != ENOSYS {
+		return err
+	}
+
+	rl := rlimit32{}
+	err = getrlimit(resource, &rl)
+	if err != nil {
+		return
+	}
+
+	if rl.Cur == rlimInf32 {
+		rlim.Cur = rlimInf64
+	} else {
+		rlim.Cur = uint64(rl.Cur)
+	}
+
+	if rl.Max == rlimInf32 {
+		rlim.Max = rlimInf64
+	} else {
+		rlim.Max = uint64(rl.Max)
+	}
+	return
+}
+
+//sysnb setrlimit(resource int, rlim *rlimit32) (err error) = SYS_SETRLIMIT
+
+func Setrlimit(resource int, rlim *Rlimit) (err error) {
+	err = prlimit(0, resource, rlim, nil)
+	if err != ENOSYS {
+		return err
+	}
+
+	rl := rlimit32{}
+	if rlim.Cur == rlimInf64 {
+		rl.Cur = rlimInf32
+	} else if rlim.Cur < uint64(rlimInf32) {
+		rl.Cur = uint32(rlim.Cur)
+	} else {
+		return EINVAL
+	}
+	if rlim.Max == rlimInf64 {
+		rl.Max = rlimInf32
+	} else if rlim.Max < uint64(rlimInf32) {
+		rl.Max = uint32(rlim.Max)
+	} else {
+		return EINVAL
+	}
+
+	return setrlimit(resource, &rl)
+}
+
+func (r *PtraceRegs) PC() uint64 { return uint64(r.Uregs[15]) }
+
+func (r *PtraceRegs) SetPC(pc uint64) { r.Uregs[15] = uint32(pc) }
+
+func (iov *Iovec) SetLen(length int) {
+	iov.Len = uint32(length)
+}
+
+func (msghdr *Msghdr) SetControllen(length int) {
+	msghdr.Controllen = uint32(length)
+}
+
+func (cmsg *Cmsghdr) SetLen(length int) {
+	cmsg.Len = uint32(length)
+}
+
+func rawVforkSyscall(trap, a1 uintptr) (r1 uintptr, err Errno) {
+	panic("not implemented")
+}
diff --git a/src/syscall/syscall_noos.go b/src/syscall/syscall_noos.go
new file mode 100644
index 0000000000..3d27ab557a
--- /dev/null
+++ b/src/syscall/syscall_noos.go
@@ -0,0 +1,192 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package syscall
+
+import (
+	"internal/oserror"
+	"sync"
+)
+
+var (
+	Stdin  = 0
+	Stdout = 1
+	Stderr = 2
+)
+
+type Errno uintptr
+
+const (
+	EAGAIN       Errno = 1
+	ENOTSUP      Errno = 2
+	EINVAL       Errno = 3
+	ENOENT       Errno = 4
+	EACCES       Errno = 5
+	EPERM        Errno = 6
+	EEXIST       Errno = 7
+	ENOTEMPTY    Errno = 8
+	EISDIR       Errno = 9
+	ENOTDIR      Errno = 10
+	ENAMETOOLONG Errno = 11
+)
+
+var errors = [...]string{
+	EAGAIN:       "resource temporarily unavailable",
+	ENOTSUP:      "operation not supported",
+	EINVAL:       "invalid argument",
+	ENOENT:       "no such file or directory",
+	EACCES:       "permission denied",
+	EPERM:        "operation not permitted",
+	EEXIST:       "file exists",
+	ENOTEMPTY:    "directory not empty",
+	EISDIR:       "is a directory",
+	ENOTDIR:      "not a directory",
+	ENAMETOOLONG: "file name too long",
+}
+
+func (e Errno) Is(target error) bool {
+	switch target {
+	case oserror.ErrPermission:
+		return e == EACCES || e == EPERM
+	case oserror.ErrExist:
+		return e == EEXIST || e == ENOTEMPTY
+	case oserror.ErrNotExist:
+		return e == ENOENT
+	}
+	return false
+}
+
+func (e Errno) Error() string {
+	if 0 <= int(e) && int(e) < len(errors) {
+		s := errors[e]
+		if s != "" {
+			return s
+		}
+	}
+	return "errno " + itoa(int(e))
+}
+
+type Timespec struct {
+	Sec  int64
+	Nsec int64
+}
+
+type Timeval struct {
+	Sec  int32
+	Usec int32
+}
+
+var env = struct {
+	mx   sync.RWMutex
+	dict map[string]string
+}{dict: map[string]string{
+	"HOME": "/tmp",
+}}
+
+func Setenv(key, value string) error {
+	env.mx.Lock()
+	env.dict[key] = value
+	env.mx.Unlock()
+	return nil
+}
+
+func Getenv(key string) (value string, found bool) {
+	env.mx.RLock()
+	val, ok := env.dict[key]
+	env.mx.RUnlock()
+	return val, ok
+}
+
+func Unsetenv(key string) error {
+	env.mx.Lock()
+	delete(env.dict, key)
+	env.mx.Unlock()
+	return nil
+}
+
+func Clearenv() {
+	env.mx.Lock()
+	for key := range env.dict {
+		delete(env.dict, key)
+	}
+	env.mx.Unlock()
+}
+
+func Environ() []string {
+	env.mx.RLock()
+	a := make([]string, 0, len(env.dict))
+	for key, val := range env.dict {
+		a = append(a, key+"="+val)
+	}
+	env.mx.RUnlock()
+	return a
+}
+
+type SysProcAttr struct{}
+
+const (
+	O_RDONLY = 0x0
+	O_WRONLY = 0x1
+	O_RDWR   = 0x2
+	O_CREAT  = 0x40
+	O_EXCL   = 0x80
+	O_TRUNC  = 0x200
+	O_APPEND = 0x400
+	O_SYNC   = 0x101000
+)
+
+type Qid struct {
+	Path uint64 // the file server's unique identification for the file
+}
+
+type Dir struct {
+	// system-modified data
+	Type uint16 // server type
+	Dev  uint32 // server subtype
+
+	// file data
+	Qid Qid // unique id from server
+}
+
+func Getpid() int {
+	return 1
+}
+
+func Getppid() int {
+	return 1
+}
+
+func Mkdir(path string, mode uint32) (err error) {
+	return ENOTSUP
+}
+
+func Chdir(path string) error {
+	return ENOTSUP
+}
+
+const ImplementsGetwd = true
+
+func Getwd() (wd string, err error) {
+	return "", ENOTSUP
+}
+
+func Getuid() int {
+	return -1
+}
+
+func Geteuid() int {
+	return -1
+}
+
+func Getgid() int {
+	return -1
+}
+
+func Getegid() int {
+	return -1
+}
+
+func Getgroups() (gids []int, err error) {
+	return nil, ENOTSUP
+}
diff --git a/src/syscall/zerrors_linux_thumb.go b/src/syscall/zerrors_linux_thumb.go
new file mode 100644
index 0000000000..e7fddf95d1
--- /dev/null
+++ b/src/syscall/zerrors_linux_thumb.go
@@ -0,0 +1,1560 @@
+// mkerrors.sh
+// Code generated by the command above; DO NOT EDIT.
+
+// Created by cgo -godefs - DO NOT EDIT
+// cgo -godefs -- _const.go
+
+// +build thumb,linux
+
+package syscall
+
+const (
+	AF_ALG                           = 0x26
+	AF_APPLETALK                     = 0x5
+	AF_ASH                           = 0x12
+	AF_ATMPVC                        = 0x8
+	AF_ATMSVC                        = 0x14
+	AF_AX25                          = 0x3
+	AF_BLUETOOTH                     = 0x1f
+	AF_BRIDGE                        = 0x7
+	AF_CAIF                          = 0x25
+	AF_CAN                           = 0x1d
+	AF_DECnet                        = 0xc
+	AF_ECONET                        = 0x13
+	AF_FILE                          = 0x1
+	AF_IEEE802154                    = 0x24
+	AF_INET                          = 0x2
+	AF_INET6                         = 0xa
+	AF_IPX                           = 0x4
+	AF_IRDA                          = 0x17
+	AF_ISDN                          = 0x22
+	AF_IUCV                          = 0x20
+	AF_KEY                           = 0xf
+	AF_LLC                           = 0x1a
+	AF_LOCAL                         = 0x1
+	AF_MAX                           = 0x27
+	AF_NETBEUI                       = 0xd
+	AF_NETLINK                       = 0x10
+	AF_NETROM                        = 0x6
+	AF_PACKET                        = 0x11
+	AF_PHONET                        = 0x23
+	AF_PPPOX                         = 0x18
+	AF_RDS                           = 0x15
+	AF_ROSE                          = 0xb
+	AF_ROUTE                         = 0x10
+	AF_RXRPC                         = 0x21
+	AF_SECURITY                      = 0xe
+	AF_SNA                           = 0x16
+	AF_TIPC                          = 0x1e
+	AF_UNIX                          = 0x1
+	AF_UNSPEC                        = 0x0
+	AF_WANPIPE                       = 0x19
+	AF_X25                           = 0x9
+	ARPHRD_ADAPT                     = 0x108
+	ARPHRD_APPLETLK                  = 0x8
+	ARPHRD_ARCNET                    = 0x7
+	ARPHRD_ASH                       = 0x30d
+	ARPHRD_ATM                       = 0x13
+	ARPHRD_AX25                      = 0x3
+	ARPHRD_BIF                       = 0x307
+	ARPHRD_CHAOS                     = 0x5
+	ARPHRD_CISCO                     = 0x201
+	ARPHRD_CSLIP                     = 0x101
+	ARPHRD_CSLIP6                    = 0x103
+	ARPHRD_DDCMP                     = 0x205
+	ARPHRD_DLCI                      = 0xf
+	ARPHRD_ECONET                    = 0x30e
+	ARPHRD_EETHER                    = 0x2
+	ARPHRD_ETHER                     = 0x1
+	ARPHRD_EUI64                     = 0x1b
+	ARPHRD_FCAL                      = 0x311
+	ARPHRD_FCFABRIC                  = 0x313
+	ARPHRD_FCPL                      = 0x312
+	ARPHRD_FCPP                      = 0x310
+	ARPHRD_FDDI                      = 0x306
+	ARPHRD_FRAD                      = 0x302
+	ARPHRD_HDLC                      = 0x201
+	ARPHRD_HIPPI                     = 0x30c
+	ARPHRD_HWX25                     = 0x110
+	ARPHRD_IEEE1394                  = 0x18
+	ARPHRD_IEEE802                   = 0x6
+	ARPHRD_IEEE80211                 = 0x321
+	ARPHRD_IEEE80211_PRISM           = 0x322
+	ARPHRD_IEEE80211_RADIOTAP        = 0x323
+	ARPHRD_IEEE802154                = 0x324
+	ARPHRD_IEEE802154_PHY            = 0x325
+	ARPHRD_IEEE802_TR                = 0x320
+	ARPHRD_INFINIBAND                = 0x20
+	ARPHRD_IPDDP                     = 0x309
+	ARPHRD_IPGRE                     = 0x30a
+	ARPHRD_IRDA                      = 0x30f
+	ARPHRD_LAPB                      = 0x204
+	ARPHRD_LOCALTLK                  = 0x305
+	ARPHRD_LOOPBACK                  = 0x304
+	ARPHRD_METRICOM                  = 0x17
+	ARPHRD_NETROM                    = 0x0
+	ARPHRD_NONE                      = 0xfffe
+	ARPHRD_PIMREG                    = 0x30b
+	ARPHRD_PPP                       = 0x200
+	ARPHRD_PRONET                    = 0x4
+	ARPHRD_RAWHDLC                   = 0x206
+	ARPHRD_ROSE                      = 0x10e
+	ARPHRD_RSRVD                     = 0x104
+	ARPHRD_SIT                       = 0x308
+	ARPHRD_SKIP                      = 0x303
+	ARPHRD_SLIP                      = 0x100
+	ARPHRD_SLIP6                     = 0x102
+	ARPHRD_TUNNEL                    = 0x300
+	ARPHRD_TUNNEL6                   = 0x301
+	ARPHRD_VOID                      = 0xffff
+	ARPHRD_X25                       = 0x10f
+	BPF_A                            = 0x10
+	BPF_ABS                          = 0x20
+	BPF_ADD                          = 0x0
+	BPF_ALU                          = 0x4
+	BPF_AND                          = 0x50
+	BPF_B                            = 0x10
+	BPF_DIV                          = 0x30
+	BPF_H                            = 0x8
+	BPF_IMM                          = 0x0
+	BPF_IND                          = 0x40
+	BPF_JA                           = 0x0
+	BPF_JEQ                          = 0x10
+	BPF_JGE                          = 0x30
+	BPF_JGT                          = 0x20
+	BPF_JMP                          = 0x5
+	BPF_JSET                         = 0x40
+	BPF_K                            = 0x0
+	BPF_LD                           = 0x0
+	BPF_LDX                          = 0x1
+	BPF_LEN                          = 0x80
+	BPF_LSH                          = 0x60
+	BPF_MAJOR_VERSION                = 0x1
+	BPF_MAXINSNS                     = 0x1000
+	BPF_MEM                          = 0x60
+	BPF_MEMWORDS                     = 0x10
+	BPF_MINOR_VERSION                = 0x1
+	BPF_MISC                         = 0x7
+	BPF_MSH                          = 0xa0
+	BPF_MUL                          = 0x20
+	BPF_NEG                          = 0x80
+	BPF_OR                           = 0x40
+	BPF_RET                          = 0x6
+	BPF_RSH                          = 0x70
+	BPF_ST                           = 0x2
+	BPF_STX                          = 0x3
+	BPF_SUB                          = 0x10
+	BPF_TAX                          = 0x0
+	BPF_TXA                          = 0x80
+	BPF_W                            = 0x0
+	BPF_X                            = 0x8
+	CLONE_CHILD_CLEARTID             = 0x200000
+	CLONE_CHILD_SETTID               = 0x1000000
+	CLONE_DETACHED                   = 0x400000
+	CLONE_FILES                      = 0x400
+	CLONE_FS                         = 0x200
+	CLONE_IO                         = 0x80000000
+	CLONE_NEWIPC                     = 0x8000000
+	CLONE_NEWNET                     = 0x40000000
+	CLONE_NEWNS                      = 0x20000
+	CLONE_NEWPID                     = 0x20000000
+	CLONE_NEWUSER                    = 0x10000000
+	CLONE_NEWUTS                     = 0x4000000
+	CLONE_PARENT                     = 0x8000
+	CLONE_PARENT_SETTID              = 0x100000
+	CLONE_PTRACE                     = 0x2000
+	CLONE_SETTLS                     = 0x80000
+	CLONE_SIGHAND                    = 0x800
+	CLONE_SYSVSEM                    = 0x40000
+	CLONE_THREAD                     = 0x10000
+	CLONE_UNTRACED                   = 0x800000
+	CLONE_VFORK                      = 0x4000
+	CLONE_VM                         = 0x100
+	DT_BLK                           = 0x6
+	DT_CHR                           = 0x2
+	DT_DIR                           = 0x4
+	DT_FIFO                          = 0x1
+	DT_LNK                           = 0xa
+	DT_REG                           = 0x8
+	DT_SOCK                          = 0xc
+	DT_UNKNOWN                       = 0x0
+	DT_WHT                           = 0xe
+	ELF_NGREG                        = 0x12
+	ELF_PRARGSZ                      = 0x50
+	EPOLLERR                         = 0x8
+	EPOLLET                          = -0x80000000
+	EPOLLHUP                         = 0x10
+	EPOLLIN                          = 0x1
+	EPOLLMSG                         = 0x400
+	EPOLLONESHOT                     = 0x40000000
+	EPOLLOUT                         = 0x4
+	EPOLLPRI                         = 0x2
+	EPOLLRDBAND                      = 0x80
+	EPOLLRDHUP                       = 0x2000
+	EPOLLRDNORM                      = 0x40
+	EPOLLWRBAND                      = 0x200
+	EPOLLWRNORM                      = 0x100
+	EPOLL_CLOEXEC                    = 0x80000
+	EPOLL_CTL_ADD                    = 0x1
+	EPOLL_CTL_DEL                    = 0x2
+	EPOLL_CTL_MOD                    = 0x3
+	EPOLL_NONBLOCK                   = 0x800
+	ETH_P_1588                       = 0x88f7
+	ETH_P_8021Q                      = 0x8100
+	ETH_P_802_2                      = 0x4
+	ETH_P_802_3                      = 0x1
+	ETH_P_AARP                       = 0x80f3
+	ETH_P_ALL                        = 0x3
+	ETH_P_AOE                        = 0x88a2
+	ETH_P_ARCNET                     = 0x1a
+	ETH_P_ARP                        = 0x806
+	ETH_P_ATALK                      = 0x809b
+	ETH_P_ATMFATE                    = 0x8884
+	ETH_P_ATMMPOA                    = 0x884c
+	ETH_P_AX25                       = 0x2
+	ETH_P_BPQ                        = 0x8ff
+	ETH_P_CAIF                       = 0xf7
+	ETH_P_CAN                        = 0xc
+	ETH_P_CONTROL                    = 0x16
+	ETH_P_CUST                       = 0x6006
+	ETH_P_DDCMP                      = 0x6
+	ETH_P_DEC                        = 0x6000
+	ETH_P_DIAG                       = 0x6005
+	ETH_P_DNA_DL                     = 0x6001
+	ETH_P_DNA_RC                     = 0x6002
+	ETH_P_DNA_RT                     = 0x6003
+	ETH_P_DSA                        = 0x1b
+	ETH_P_ECONET                     = 0x18
+	ETH_P_EDSA                       = 0xdada
+	ETH_P_FCOE                       = 0x8906
+	ETH_P_FIP                        = 0x8914
+	ETH_P_HDLC                       = 0x19
+	ETH_P_IEEE802154                 = 0xf6
+	ETH_P_IEEEPUP                    = 0xa00
+	ETH_P_IEEEPUPAT                  = 0xa01
+	ETH_P_IP                         = 0x800
+	ETH_P_IPV6                       = 0x86dd
+	ETH_P_IPX                        = 0x8137
+	ETH_P_IRDA                       = 0x17
+	ETH_P_LAT                        = 0x6004
+	ETH_P_LINK_CTL                   = 0x886c
+	ETH_P_LOCALTALK                  = 0x9
+	ETH_P_LOOP                       = 0x60
+	ETH_P_MOBITEX                    = 0x15
+	ETH_P_MPLS_MC                    = 0x8848
+	ETH_P_MPLS_UC                    = 0x8847
+	ETH_P_PAE                        = 0x888e
+	ETH_P_PAUSE                      = 0x8808
+	ETH_P_PHONET                     = 0xf5
+	ETH_P_PPPTALK                    = 0x10
+	ETH_P_PPP_DISC                   = 0x8863
+	ETH_P_PPP_MP                     = 0x8
+	ETH_P_PPP_SES                    = 0x8864
+	ETH_P_PUP                        = 0x200
+	ETH_P_PUPAT                      = 0x201
+	ETH_P_RARP                       = 0x8035
+	ETH_P_SCA                        = 0x6007
+	ETH_P_SLOW                       = 0x8809
+	ETH_P_SNAP                       = 0x5
+	ETH_P_TEB                        = 0x6558
+	ETH_P_TIPC                       = 0x88ca
+	ETH_P_TRAILER                    = 0x1c
+	ETH_P_TR_802_2                   = 0x11
+	ETH_P_WAN_PPP                    = 0x7
+	ETH_P_WCCP                       = 0x883e
+	ETH_P_X25                        = 0x805
+	FD_CLOEXEC                       = 0x1
+	FD_SETSIZE                       = 0x400
+	F_DUPFD                          = 0x0
+	F_DUPFD_CLOEXEC                  = 0x406
+	F_EXLCK                          = 0x4
+	F_GETFD                          = 0x1
+	F_GETFL                          = 0x3
+	F_GETLEASE                       = 0x401
+	F_GETLK                          = 0xc
+	F_GETLK64                        = 0xc
+	F_GETOWN                         = 0x9
+	F_GETOWN_EX                      = 0x10
+	F_GETPIPE_SZ                     = 0x408
+	F_GETSIG                         = 0xb
+	F_LOCK                           = 0x1
+	F_NOTIFY                         = 0x402
+	F_OK                             = 0x0
+	F_RDLCK                          = 0x0
+	F_SETFD                          = 0x2
+	F_SETFL                          = 0x4
+	F_SETLEASE                       = 0x400
+	F_SETLK                          = 0xd
+	F_SETLK64                        = 0xd
+	F_SETLKW                         = 0xe
+	F_SETLKW64                       = 0xe
+	F_SETOWN                         = 0x8
+	F_SETOWN_EX                      = 0xf
+	F_SETPIPE_SZ                     = 0x407
+	F_SETSIG                         = 0xa
+	F_SHLCK                          = 0x8
+	F_TEST                           = 0x3
+	F_TLOCK                          = 0x2
+	F_ULOCK                          = 0x0
+	F_UNLCK                          = 0x2
+	F_WRLCK                          = 0x1
+	ICMPV6_FILTER                    = 0x1
+	IFA_F_DADFAILED                  = 0x8
+	IFA_F_DEPRECATED                 = 0x20
+	IFA_F_HOMEADDRESS                = 0x10
+	IFA_F_NODAD                      = 0x2
+	IFA_F_OPTIMISTIC                 = 0x4
+	IFA_F_PERMANENT                  = 0x80
+	IFA_F_SECONDARY                  = 0x1
+	IFA_F_TEMPORARY                  = 0x1
+	IFA_F_TENTATIVE                  = 0x40
+	IFA_MAX                          = 0x7
+	IFF_ALLMULTI                     = 0x200
+	IFF_AUTOMEDIA                    = 0x4000
+	IFF_BROADCAST                    = 0x2
+	IFF_DEBUG                        = 0x4
+	IFF_DYNAMIC                      = 0x8000
+	IFF_LOOPBACK                     = 0x8
+	IFF_MASTER                       = 0x400
+	IFF_MULTICAST                    = 0x1000
+	IFF_NOARP                        = 0x80
+	IFF_NOTRAILERS                   = 0x20
+	IFF_NO_PI                        = 0x1000
+	IFF_ONE_QUEUE                    = 0x2000
+	IFF_POINTOPOINT                  = 0x10
+	IFF_PORTSEL                      = 0x2000
+	IFF_PROMISC                      = 0x100
+	IFF_RUNNING                      = 0x40
+	IFF_SLAVE                        = 0x800
+	IFF_TAP                          = 0x2
+	IFF_TUN                          = 0x1
+	IFF_TUN_EXCL                     = 0x8000
+	IFF_UP                           = 0x1
+	IFF_VNET_HDR                     = 0x4000
+	IFNAMSIZ                         = 0x10
+	IN_ACCESS                        = 0x1
+	IN_ALL_EVENTS                    = 0xfff
+	IN_ATTRIB                        = 0x4
+	IN_CLASSA_HOST                   = 0xffffff
+	IN_CLASSA_MAX                    = 0x80
+	IN_CLASSA_NET                    = 0xff000000
+	IN_CLASSA_NSHIFT                 = 0x18
+	IN_CLASSB_HOST                   = 0xffff
+	IN_CLASSB_MAX                    = 0x10000
+	IN_CLASSB_NET                    = 0xffff0000
+	IN_CLASSB_NSHIFT                 = 0x10
+	IN_CLASSC_HOST                   = 0xff
+	IN_CLASSC_NET                    = 0xffffff00
+	IN_CLASSC_NSHIFT                 = 0x8
+	IN_CLOEXEC                       = 0x80000
+	IN_CLOSE                         = 0x18
+	IN_CLOSE_NOWRITE                 = 0x10
+	IN_CLOSE_WRITE                   = 0x8
+	IN_CREATE                        = 0x100
+	IN_DELETE                        = 0x200
+	IN_DELETE_SELF                   = 0x400
+	IN_DONT_FOLLOW                   = 0x2000000
+	IN_EXCL_UNLINK                   = 0x4000000
+	IN_IGNORED                       = 0x8000
+	IN_ISDIR                         = 0x40000000
+	IN_LOOPBACKNET                   = 0x7f
+	IN_MASK_ADD                      = 0x20000000
+	IN_MODIFY                        = 0x2
+	IN_MOVE                          = 0xc0
+	IN_MOVED_FROM                    = 0x40
+	IN_MOVED_TO                      = 0x80
+	IN_MOVE_SELF                     = 0x800
+	IN_NONBLOCK                      = 0x800
+	IN_ONESHOT                       = 0x80000000
+	IN_ONLYDIR                       = 0x1000000
+	IN_OPEN                          = 0x20
+	IN_Q_OVERFLOW                    = 0x4000
+	IN_UNMOUNT                       = 0x2000
+	IPPROTO_AH                       = 0x33
+	IPPROTO_COMP                     = 0x6c
+	IPPROTO_DCCP                     = 0x21
+	IPPROTO_DSTOPTS                  = 0x3c
+	IPPROTO_EGP                      = 0x8
+	IPPROTO_ENCAP                    = 0x62
+	IPPROTO_ESP                      = 0x32
+	IPPROTO_FRAGMENT                 = 0x2c
+	IPPROTO_GRE                      = 0x2f
+	IPPROTO_HOPOPTS                  = 0x0
+	IPPROTO_ICMP                     = 0x1
+	IPPROTO_ICMPV6                   = 0x3a
+	IPPROTO_IDP                      = 0x16
+	IPPROTO_IGMP                     = 0x2
+	IPPROTO_IP                       = 0x0
+	IPPROTO_IPIP                     = 0x4
+	IPPROTO_IPV6                     = 0x29
+	IPPROTO_MTP                      = 0x5c
+	IPPROTO_NONE                     = 0x3b
+	IPPROTO_PIM                      = 0x67
+	IPPROTO_PUP                      = 0xc
+	IPPROTO_RAW                      = 0xff
+	IPPROTO_ROUTING                  = 0x2b
+	IPPROTO_RSVP                     = 0x2e
+	IPPROTO_SCTP                     = 0x84
+	IPPROTO_TCP                      = 0x6
+	IPPROTO_TP                       = 0x1d
+	IPPROTO_UDP                      = 0x11
+	IPPROTO_UDPLITE                  = 0x88
+	IPV6_2292DSTOPTS                 = 0x4
+	IPV6_2292HOPLIMIT                = 0x8
+	IPV6_2292HOPOPTS                 = 0x3
+	IPV6_2292PKTINFO                 = 0x2
+	IPV6_2292PKTOPTIONS              = 0x6
+	IPV6_2292RTHDR                   = 0x5
+	IPV6_ADDRFORM                    = 0x1
+	IPV6_ADD_MEMBERSHIP              = 0x14
+	IPV6_AUTHHDR                     = 0xa
+	IPV6_CHECKSUM                    = 0x7
+	IPV6_DROP_MEMBERSHIP             = 0x15
+	IPV6_DSTOPTS                     = 0x3b
+	IPV6_HOPLIMIT                    = 0x34
+	IPV6_HOPOPTS                     = 0x36
+	IPV6_IPSEC_POLICY                = 0x22
+	IPV6_JOIN_ANYCAST                = 0x1b
+	IPV6_JOIN_GROUP                  = 0x14
+	IPV6_LEAVE_ANYCAST               = 0x1c
+	IPV6_LEAVE_GROUP                 = 0x15
+	IPV6_MTU                         = 0x18
+	IPV6_MTU_DISCOVER                = 0x17
+	IPV6_MULTICAST_HOPS              = 0x12
+	IPV6_MULTICAST_IF                = 0x11
+	IPV6_MULTICAST_LOOP              = 0x13
+	IPV6_NEXTHOP                     = 0x9
+	IPV6_PKTINFO                     = 0x32
+	IPV6_PMTUDISC_DO                 = 0x2
+	IPV6_PMTUDISC_DONT               = 0x0
+	IPV6_PMTUDISC_PROBE              = 0x3
+	IPV6_PMTUDISC_WANT               = 0x1
+	IPV6_RECVDSTOPTS                 = 0x3a
+	IPV6_RECVERR                     = 0x19
+	IPV6_RECVHOPLIMIT                = 0x33
+	IPV6_RECVHOPOPTS                 = 0x35
+	IPV6_RECVPKTINFO                 = 0x31
+	IPV6_RECVRTHDR                   = 0x38
+	IPV6_RECVTCLASS                  = 0x42
+	IPV6_ROUTER_ALERT                = 0x16
+	IPV6_RTHDR                       = 0x39
+	IPV6_RTHDRDSTOPTS                = 0x37
+	IPV6_RTHDR_LOOSE                 = 0x0
+	IPV6_RTHDR_STRICT                = 0x1
+	IPV6_RTHDR_TYPE_0                = 0x0
+	IPV6_RXDSTOPTS                   = 0x3b
+	IPV6_RXHOPOPTS                   = 0x36
+	IPV6_TCLASS                      = 0x43
+	IPV6_UNICAST_HOPS                = 0x10
+	IPV6_V6ONLY                      = 0x1a
+	IPV6_XFRM_POLICY                 = 0x23
+	IP_ADD_MEMBERSHIP                = 0x23
+	IP_ADD_SOURCE_MEMBERSHIP         = 0x27
+	IP_BLOCK_SOURCE                  = 0x26
+	IP_DEFAULT_MULTICAST_LOOP        = 0x1
+	IP_DEFAULT_MULTICAST_TTL         = 0x1
+	IP_DF                            = 0x4000
+	IP_DROP_MEMBERSHIP               = 0x24
+	IP_DROP_SOURCE_MEMBERSHIP        = 0x28
+	IP_FREEBIND                      = 0xf
+	IP_HDRINCL                       = 0x3
+	IP_IPSEC_POLICY                  = 0x10
+	IP_MAXPACKET                     = 0xffff
+	IP_MAX_MEMBERSHIPS               = 0x14
+	IP_MF                            = 0x2000
+	IP_MINTTL                        = 0x15
+	IP_MSFILTER                      = 0x29
+	IP_MSS                           = 0x240
+	IP_MTU                           = 0xe
+	IP_MTU_DISCOVER                  = 0xa
+	IP_MULTICAST_IF                  = 0x20
+	IP_MULTICAST_LOOP                = 0x22
+	IP_MULTICAST_TTL                 = 0x21
+	IP_OFFMASK                       = 0x1fff
+	IP_OPTIONS                       = 0x4
+	IP_ORIGDSTADDR                   = 0x14
+	IP_PASSSEC                       = 0x12
+	IP_PKTINFO                       = 0x8
+	IP_PKTOPTIONS                    = 0x9
+	IP_PMTUDISC                      = 0xa
+	IP_PMTUDISC_DO                   = 0x2
+	IP_PMTUDISC_DONT                 = 0x0
+	IP_PMTUDISC_PROBE                = 0x3
+	IP_PMTUDISC_WANT                 = 0x1
+	IP_RECVERR                       = 0xb
+	IP_RECVOPTS                      = 0x6
+	IP_RECVORIGDSTADDR               = 0x14
+	IP_RECVRETOPTS                   = 0x7
+	IP_RECVTOS                       = 0xd
+	IP_RECVTTL                       = 0xc
+	IP_RETOPTS                       = 0x7
+	IP_RF                            = 0x8000
+	IP_ROUTER_ALERT                  = 0x5
+	IP_TOS                           = 0x1
+	IP_TRANSPARENT                   = 0x13
+	IP_TTL                           = 0x2
+	IP_UNBLOCK_SOURCE                = 0x25
+	IP_XFRM_POLICY                   = 0x11
+	LINUX_REBOOT_CMD_CAD_OFF         = 0x0
+	LINUX_REBOOT_CMD_CAD_ON          = 0x89abcdef
+	LINUX_REBOOT_CMD_HALT            = 0xcdef0123
+	LINUX_REBOOT_CMD_KEXEC           = 0x45584543
+	LINUX_REBOOT_CMD_POWER_OFF       = 0x4321fedc
+	LINUX_REBOOT_CMD_RESTART         = 0x1234567
+	LINUX_REBOOT_CMD_RESTART2        = 0xa1b2c3d4
+	LINUX_REBOOT_CMD_SW_SUSPEND      = 0xd000fce2
+	LINUX_REBOOT_MAGIC1              = 0xfee1dead
+	LINUX_REBOOT_MAGIC2              = 0x28121969
+	LOCK_EX                          = 0x2
+	LOCK_NB                          = 0x4
+	LOCK_SH                          = 0x1
+	LOCK_UN                          = 0x8
+	MADV_DOFORK                      = 0xb
+	MADV_DONTFORK                    = 0xa
+	MADV_DONTNEED                    = 0x4
+	MADV_HUGEPAGE                    = 0xe
+	MADV_HWPOISON                    = 0x64
+	MADV_MERGEABLE                   = 0xc
+	MADV_NOHUGEPAGE                  = 0xf
+	MADV_NORMAL                      = 0x0
+	MADV_RANDOM                      = 0x1
+	MADV_REMOVE                      = 0x9
+	MADV_SEQUENTIAL                  = 0x2
+	MADV_UNMERGEABLE                 = 0xd
+	MADV_WILLNEED                    = 0x3
+	MAP_ANON                         = 0x20
+	MAP_ANONYMOUS                    = 0x20
+	MAP_DENYWRITE                    = 0x800
+	MAP_EXECUTABLE                   = 0x1000
+	MAP_FILE                         = 0x0
+	MAP_FIXED                        = 0x10
+	MAP_GROWSDOWN                    = 0x100
+	MAP_LOCKED                       = 0x2000
+	MAP_NONBLOCK                     = 0x10000
+	MAP_NORESERVE                    = 0x4000
+	MAP_POPULATE                     = 0x8000
+	MAP_PRIVATE                      = 0x2
+	MAP_SHARED                       = 0x1
+	MAP_TYPE                         = 0xf
+	MCL_CURRENT                      = 0x1
+	MCL_FUTURE                       = 0x2
+	MNT_DETACH                       = 0x2
+	MNT_EXPIRE                       = 0x4
+	MNT_FORCE                        = 0x1
+	MSG_CMSG_CLOEXEC                 = 0x40000000
+	MSG_CONFIRM                      = 0x800
+	MSG_CTRUNC                       = 0x8
+	MSG_DONTROUTE                    = 0x4
+	MSG_DONTWAIT                     = 0x40
+	MSG_EOR                          = 0x80
+	MSG_ERRQUEUE                     = 0x2000
+	MSG_FASTOPEN                     = 0x20000000
+	MSG_FIN                          = 0x200
+	MSG_MORE                         = 0x8000
+	MSG_NOSIGNAL                     = 0x4000
+	MSG_OOB                          = 0x1
+	MSG_PEEK                         = 0x2
+	MSG_PROXY                        = 0x10
+	MSG_RST                          = 0x1000
+	MSG_SYN                          = 0x400
+	MSG_TRUNC                        = 0x20
+	MSG_TRYHARD                      = 0x4
+	MSG_WAITALL                      = 0x100
+	MSG_WAITFORONE                   = 0x10000
+	MS_ACTIVE                        = 0x40000000
+	MS_ASYNC                         = 0x1
+	MS_BIND                          = 0x1000
+	MS_DIRSYNC                       = 0x80
+	MS_INVALIDATE                    = 0x2
+	MS_I_VERSION                     = 0x800000
+	MS_KERNMOUNT                     = 0x400000
+	MS_MANDLOCK                      = 0x40
+	MS_MGC_MSK                       = 0xffff0000
+	MS_MGC_VAL                       = 0xc0ed0000
+	MS_MOVE                          = 0x2000
+	MS_NOATIME                       = 0x400
+	MS_NODEV                         = 0x4
+	MS_NODIRATIME                    = 0x800
+	MS_NOEXEC                        = 0x8
+	MS_NOSUID                        = 0x2
+	MS_NOUSER                        = -0x80000000
+	MS_POSIXACL                      = 0x10000
+	MS_PRIVATE                       = 0x40000
+	MS_RDONLY                        = 0x1
+	MS_REC                           = 0x4000
+	MS_RELATIME                      = 0x200000
+	MS_REMOUNT                       = 0x20
+	MS_RMT_MASK                      = 0x800051
+	MS_SHARED                        = 0x100000
+	MS_SILENT                        = 0x8000
+	MS_SLAVE                         = 0x80000
+	MS_STRICTATIME                   = 0x1000000
+	MS_SYNC                          = 0x4
+	MS_SYNCHRONOUS                   = 0x10
+	MS_UNBINDABLE                    = 0x20000
+	NAME_MAX                         = 0xff
+	NETLINK_ADD_MEMBERSHIP           = 0x1
+	NETLINK_AUDIT                    = 0x9
+	NETLINK_BROADCAST_ERROR          = 0x4
+	NETLINK_CONNECTOR                = 0xb
+	NETLINK_DNRTMSG                  = 0xe
+	NETLINK_DROP_MEMBERSHIP          = 0x2
+	NETLINK_ECRYPTFS                 = 0x13
+	NETLINK_FIB_LOOKUP               = 0xa
+	NETLINK_FIREWALL                 = 0x3
+	NETLINK_GENERIC                  = 0x10
+	NETLINK_INET_DIAG                = 0x4
+	NETLINK_IP6_FW                   = 0xd
+	NETLINK_ISCSI                    = 0x8
+	NETLINK_KOBJECT_UEVENT           = 0xf
+	NETLINK_NETFILTER                = 0xc
+	NETLINK_NFLOG                    = 0x5
+	NETLINK_NO_ENOBUFS               = 0x5
+	NETLINK_PKTINFO                  = 0x3
+	NETLINK_RDMA                     = 0x14
+	NETLINK_ROUTE                    = 0x0
+	NETLINK_SCSITRANSPORT            = 0x12
+	NETLINK_SELINUX                  = 0x7
+	NETLINK_UNUSED                   = 0x1
+	NETLINK_USERSOCK                 = 0x2
+	NETLINK_XFRM                     = 0x6
+	NLA_ALIGNTO                      = 0x4
+	NLA_F_NESTED                     = 0x8000
+	NLA_F_NET_BYTEORDER              = 0x4000
+	NLA_HDRLEN                       = 0x4
+	NLMSG_ALIGNTO                    = 0x4
+	NLMSG_DONE                       = 0x3
+	NLMSG_ERROR                      = 0x2
+	NLMSG_HDRLEN                     = 0x10
+	NLMSG_MIN_TYPE                   = 0x10
+	NLMSG_NOOP                       = 0x1
+	NLMSG_OVERRUN                    = 0x4
+	NLM_F_ACK                        = 0x4
+	NLM_F_APPEND                     = 0x800
+	NLM_F_ATOMIC                     = 0x400
+	NLM_F_CREATE                     = 0x400
+	NLM_F_DUMP                       = 0x300
+	NLM_F_ECHO                       = 0x8
+	NLM_F_EXCL                       = 0x200
+	NLM_F_MATCH                      = 0x200
+	NLM_F_MULTI                      = 0x2
+	NLM_F_REPLACE                    = 0x100
+	NLM_F_REQUEST                    = 0x1
+	NLM_F_ROOT                       = 0x100
+	O_ACCMODE                        = 0x3
+	O_APPEND                         = 0x400
+	O_ASYNC                          = 0x2000
+	O_CLOEXEC                        = 0x80000
+	O_CREAT                          = 0x40
+	O_DIRECT                         = 0x10000
+	O_DIRECTORY                      = 0x4000
+	O_DSYNC                          = 0x1000
+	O_EXCL                           = 0x80
+	O_FSYNC                          = 0x1000
+	O_LARGEFILE                      = 0x20000
+	O_NDELAY                         = 0x800
+	O_NOATIME                        = 0x40000
+	O_NOCTTY                         = 0x100
+	O_NOFOLLOW                       = 0x8000
+	O_NONBLOCK                       = 0x800
+	O_RDONLY                         = 0x0
+	O_RDWR                           = 0x2
+	O_RSYNC                          = 0x1000
+	O_SYNC                           = 0x1000
+	O_TRUNC                          = 0x200
+	O_WRONLY                         = 0x1
+	PACKET_ADD_MEMBERSHIP            = 0x1
+	PACKET_BROADCAST                 = 0x1
+	PACKET_DROP_MEMBERSHIP           = 0x2
+	PACKET_FASTROUTE                 = 0x6
+	PACKET_HOST                      = 0x0
+	PACKET_LOOPBACK                  = 0x5
+	PACKET_MR_ALLMULTI               = 0x2
+	PACKET_MR_MULTICAST              = 0x0
+	PACKET_MR_PROMISC                = 0x1
+	PACKET_MULTICAST                 = 0x2
+	PACKET_OTHERHOST                 = 0x3
+	PACKET_OUTGOING                  = 0x4
+	PACKET_RECV_OUTPUT               = 0x3
+	PACKET_RX_RING                   = 0x5
+	PACKET_STATISTICS                = 0x6
+	PRIO_PGRP                        = 0x1
+	PRIO_PROCESS                     = 0x0
+	PRIO_USER                        = 0x2
+	PROT_EXEC                        = 0x4
+	PROT_GROWSDOWN                   = 0x1000000
+	PROT_GROWSUP                     = 0x2000000
+	PROT_NONE                        = 0x0
+	PROT_READ                        = 0x1
+	PROT_WRITE                       = 0x2
+	PR_CAPBSET_DROP                  = 0x18
+	PR_CAPBSET_READ                  = 0x17
+	PR_CLEAR_SECCOMP_FILTER          = 0x25
+	PR_ENDIAN_BIG                    = 0x0
+	PR_ENDIAN_LITTLE                 = 0x1
+	PR_ENDIAN_PPC_LITTLE             = 0x2
+	PR_FPEMU_NOPRINT                 = 0x1
+	PR_FPEMU_SIGFPE                  = 0x2
+	PR_FP_EXC_ASYNC                  = 0x2
+	PR_FP_EXC_DISABLED               = 0x0
+	PR_FP_EXC_DIV                    = 0x10000
+	PR_FP_EXC_INV                    = 0x100000
+	PR_FP_EXC_NONRECOV               = 0x1
+	PR_FP_EXC_OVF                    = 0x20000
+	PR_FP_EXC_PRECISE                = 0x3
+	PR_FP_EXC_RES                    = 0x80000
+	PR_FP_EXC_SW_ENABLE              = 0x80
+	PR_FP_EXC_UND                    = 0x40000
+	PR_GET_DUMPABLE                  = 0x3
+	PR_GET_ENDIAN                    = 0x13
+	PR_GET_FPEMU                     = 0x9
+	PR_GET_FPEXC                     = 0xb
+	PR_GET_KEEPCAPS                  = 0x7
+	PR_GET_NAME                      = 0x10
+	PR_GET_PDEATHSIG                 = 0x2
+	PR_GET_SECCOMP                   = 0x15
+	PR_GET_SECCOMP_FILTER            = 0x23
+	PR_GET_SECUREBITS                = 0x1b
+	PR_GET_TIMERSLACK                = 0x1e
+	PR_GET_TIMING                    = 0xd
+	PR_GET_TSC                       = 0x19
+	PR_GET_UNALIGN                   = 0x5
+	PR_MCE_KILL                      = 0x21
+	PR_MCE_KILL_CLEAR                = 0x0
+	PR_MCE_KILL_DEFAULT              = 0x2
+	PR_MCE_KILL_EARLY                = 0x1
+	PR_MCE_KILL_GET                  = 0x22
+	PR_MCE_KILL_LATE                 = 0x0
+	PR_MCE_KILL_SET                  = 0x1
+	PR_SECCOMP_FILTER_EVENT          = 0x1
+	PR_SECCOMP_FILTER_SYSCALL        = 0x0
+	PR_SET_DUMPABLE                  = 0x4
+	PR_SET_ENDIAN                    = 0x14
+	PR_SET_FPEMU                     = 0xa
+	PR_SET_FPEXC                     = 0xc
+	PR_SET_KEEPCAPS                  = 0x8
+	PR_SET_NAME                      = 0xf
+	PR_SET_PDEATHSIG                 = 0x1
+	PR_SET_PTRACER                   = 0x59616d61
+	PR_SET_SECCOMP                   = 0x16
+	PR_SET_SECCOMP_FILTER            = 0x24
+	PR_SET_SECUREBITS                = 0x1c
+	PR_SET_TIMERSLACK                = 0x1d
+	PR_SET_TIMING                    = 0xe
+	PR_SET_TSC                       = 0x1a
+	PR_SET_UNALIGN                   = 0x6
+	PR_TASK_PERF_EVENTS_DISABLE      = 0x1f
+	PR_TASK_PERF_EVENTS_ENABLE       = 0x20
+	PR_TIMING_STATISTICAL            = 0x0
+	PR_TIMING_TIMESTAMP              = 0x1
+	PR_TSC_ENABLE                    = 0x1
+	PR_TSC_SIGSEGV                   = 0x2
+	PR_UNALIGN_NOPRINT               = 0x1
+	PR_UNALIGN_SIGBUS                = 0x2
+	PTRACE_ATTACH                    = 0x10
+	PTRACE_CONT                      = 0x7
+	PTRACE_DETACH                    = 0x11
+	PTRACE_EVENT_CLONE               = 0x3
+	PTRACE_EVENT_EXEC                = 0x4
+	PTRACE_EVENT_EXIT                = 0x6
+	PTRACE_EVENT_FORK                = 0x1
+	PTRACE_EVENT_VFORK               = 0x2
+	PTRACE_EVENT_VFORK_DONE          = 0x5
+	PTRACE_GETCRUNCHREGS             = 0x19
+	PTRACE_GETEVENTMSG               = 0x4201
+	PTRACE_GETFPREGS                 = 0xe
+	PTRACE_GETHBPREGS                = 0x1d
+	PTRACE_GETREGS                   = 0xc
+	PTRACE_GETREGSET                 = 0x4204
+	PTRACE_GETSIGINFO                = 0x4202
+	PTRACE_GETVFPREGS                = 0x1b
+	PTRACE_GETWMMXREGS               = 0x12
+	PTRACE_GET_THREAD_AREA           = 0x16
+	PTRACE_KILL                      = 0x8
+	PTRACE_OLDSETOPTIONS             = 0x15
+	PTRACE_O_MASK                    = 0x7f
+	PTRACE_O_TRACECLONE              = 0x8
+	PTRACE_O_TRACEEXEC               = 0x10
+	PTRACE_O_TRACEEXIT               = 0x40
+	PTRACE_O_TRACEFORK               = 0x2
+	PTRACE_O_TRACESYSGOOD            = 0x1
+	PTRACE_O_TRACEVFORK              = 0x4
+	PTRACE_O_TRACEVFORKDONE          = 0x20
+	PTRACE_PEEKDATA                  = 0x2
+	PTRACE_PEEKTEXT                  = 0x1
+	PTRACE_PEEKUSR                   = 0x3
+	PTRACE_POKEDATA                  = 0x5
+	PTRACE_POKETEXT                  = 0x4
+	PTRACE_POKEUSR                   = 0x6
+	PTRACE_SETCRUNCHREGS             = 0x1a
+	PTRACE_SETFPREGS                 = 0xf
+	PTRACE_SETHBPREGS                = 0x1e
+	PTRACE_SETOPTIONS                = 0x4200
+	PTRACE_SETREGS                   = 0xd
+	PTRACE_SETREGSET                 = 0x4205
+	PTRACE_SETSIGINFO                = 0x4203
+	PTRACE_SETVFPREGS                = 0x1c
+	PTRACE_SETWMMXREGS               = 0x13
+	PTRACE_SET_SYSCALL               = 0x17
+	PTRACE_SINGLESTEP                = 0x9
+	PTRACE_SYSCALL                   = 0x18
+	PTRACE_TRACEME                   = 0x0
+	PT_DATA_ADDR                     = 0x10004
+	PT_TEXT_ADDR                     = 0x10000
+	PT_TEXT_END_ADDR                 = 0x10008
+	RLIMIT_AS                        = 0x9
+	RLIMIT_CORE                      = 0x4
+	RLIMIT_CPU                       = 0x0
+	RLIMIT_DATA                      = 0x2
+	RLIMIT_FSIZE                     = 0x1
+	RLIMIT_NOFILE                    = 0x7
+	RLIMIT_STACK                     = 0x3
+	RLIM_INFINITY                    = -0x1
+	RTAX_ADVMSS                      = 0x8
+	RTAX_CWND                        = 0x7
+	RTAX_FEATURES                    = 0xc
+	RTAX_FEATURE_ALLFRAG             = 0x8
+	RTAX_FEATURE_ECN                 = 0x1
+	RTAX_FEATURE_SACK                = 0x2
+	RTAX_FEATURE_TIMESTAMP           = 0x4
+	RTAX_HOPLIMIT                    = 0xa
+	RTAX_INITCWND                    = 0xb
+	RTAX_INITRWND                    = 0xe
+	RTAX_LOCK                        = 0x1
+	RTAX_MAX                         = 0xe
+	RTAX_MTU                         = 0x2
+	RTAX_REORDERING                  = 0x9
+	RTAX_RTO_MIN                     = 0xd
+	RTAX_RTT                         = 0x4
+	RTAX_RTTVAR                      = 0x5
+	RTAX_SSTHRESH                    = 0x6
+	RTAX_UNSPEC                      = 0x0
+	RTAX_WINDOW                      = 0x3
+	RTA_ALIGNTO                      = 0x4
+	RTA_MAX                          = 0x10
+	RTCF_DIRECTSRC                   = 0x4000000
+	RTCF_DOREDIRECT                  = 0x1000000
+	RTCF_LOG                         = 0x2000000
+	RTCF_MASQ                        = 0x400000
+	RTCF_NAT                         = 0x800000
+	RTCF_VALVE                       = 0x200000
+	RTF_ADDRCLASSMASK                = 0xf8000000
+	RTF_ADDRCONF                     = 0x40000
+	RTF_ALLONLINK                    = 0x20000
+	RTF_BROADCAST                    = 0x10000000
+	RTF_CACHE                        = 0x1000000
+	RTF_DEFAULT                      = 0x10000
+	RTF_DYNAMIC                      = 0x10
+	RTF_FLOW                         = 0x2000000
+	RTF_GATEWAY                      = 0x2
+	RTF_HOST                         = 0x4
+	RTF_INTERFACE                    = 0x40000000
+	RTF_IRTT                         = 0x100
+	RTF_LINKRT                       = 0x100000
+	RTF_LOCAL                        = 0x80000000
+	RTF_MODIFIED                     = 0x20
+	RTF_MSS                          = 0x40
+	RTF_MTU                          = 0x40
+	RTF_MULTICAST                    = 0x20000000
+	RTF_NAT                          = 0x8000000
+	RTF_NOFORWARD                    = 0x1000
+	RTF_NONEXTHOP                    = 0x200000
+	RTF_NOPMTUDISC                   = 0x4000
+	RTF_POLICY                       = 0x4000000
+	RTF_REINSTATE                    = 0x8
+	RTF_REJECT                       = 0x200
+	RTF_STATIC                       = 0x400
+	RTF_THROW                        = 0x2000
+	RTF_UP                           = 0x1
+	RTF_WINDOW                       = 0x80
+	RTF_XRESOLVE                     = 0x800
+	RTM_BASE                         = 0x10
+	RTM_DELACTION                    = 0x31
+	RTM_DELADDR                      = 0x15
+	RTM_DELADDRLABEL                 = 0x49
+	RTM_DELLINK                      = 0x11
+	RTM_DELNEIGH                     = 0x1d
+	RTM_DELQDISC                     = 0x25
+	RTM_DELROUTE                     = 0x19
+	RTM_DELRULE                      = 0x21
+	RTM_DELTCLASS                    = 0x29
+	RTM_DELTFILTER                   = 0x2d
+	RTM_F_CLONED                     = 0x200
+	RTM_F_EQUALIZE                   = 0x400
+	RTM_F_NOTIFY                     = 0x100
+	RTM_F_PREFIX                     = 0x800
+	RTM_GETACTION                    = 0x32
+	RTM_GETADDR                      = 0x16
+	RTM_GETADDRLABEL                 = 0x4a
+	RTM_GETANYCAST                   = 0x3e
+	RTM_GETDCB                       = 0x4e
+	RTM_GETLINK                      = 0x12
+	RTM_GETMULTICAST                 = 0x3a
+	RTM_GETNEIGH                     = 0x1e
+	RTM_GETNEIGHTBL                  = 0x42
+	RTM_GETQDISC                     = 0x26
+	RTM_GETROUTE                     = 0x1a
+	RTM_GETRULE                      = 0x22
+	RTM_GETTCLASS                    = 0x2a
+	RTM_GETTFILTER                   = 0x2e
+	RTM_MAX                          = 0x4f
+	RTM_NEWACTION                    = 0x30
+	RTM_NEWADDR                      = 0x14
+	RTM_NEWADDRLABEL                 = 0x48
+	RTM_NEWLINK                      = 0x10
+	RTM_NEWNDUSEROPT                 = 0x44
+	RTM_NEWNEIGH                     = 0x1c
+	RTM_NEWNEIGHTBL                  = 0x40
+	RTM_NEWPREFIX                    = 0x34
+	RTM_NEWQDISC                     = 0x24
+	RTM_NEWROUTE                     = 0x18
+	RTM_NEWRULE                      = 0x20
+	RTM_NEWTCLASS                    = 0x28
+	RTM_NEWTFILTER                   = 0x2c
+	RTM_NR_FAMILIES                  = 0x10
+	RTM_NR_MSGTYPES                  = 0x40
+	RTM_SETDCB                       = 0x4f
+	RTM_SETLINK                      = 0x13
+	RTM_SETNEIGHTBL                  = 0x43
+	RTNH_ALIGNTO                     = 0x4
+	RTNH_F_DEAD                      = 0x1
+	RTNH_F_ONLINK                    = 0x4
+	RTNH_F_PERVASIVE                 = 0x2
+	RTN_MAX                          = 0xb
+	RTPROT_BIRD                      = 0xc
+	RTPROT_BOOT                      = 0x3
+	RTPROT_DHCP                      = 0x10
+	RTPROT_DNROUTED                  = 0xd
+	RTPROT_GATED                     = 0x8
+	RTPROT_KERNEL                    = 0x2
+	RTPROT_MRT                       = 0xa
+	RTPROT_NTK                       = 0xf
+	RTPROT_RA                        = 0x9
+	RTPROT_REDIRECT                  = 0x1
+	RTPROT_STATIC                    = 0x4
+	RTPROT_UNSPEC                    = 0x0
+	RTPROT_XORP                      = 0xe
+	RTPROT_ZEBRA                     = 0xb
+	RT_CLASS_DEFAULT                 = 0xfd
+	RT_CLASS_LOCAL                   = 0xff
+	RT_CLASS_MAIN                    = 0xfe
+	RT_CLASS_MAX                     = 0xff
+	RT_CLASS_UNSPEC                  = 0x0
+	RUSAGE_CHILDREN                  = -0x1
+	RUSAGE_SELF                      = 0x0
+	RUSAGE_THREAD                    = 0x1
+	SCM_CREDENTIALS                  = 0x2
+	SCM_RIGHTS                       = 0x1
+	SCM_TIMESTAMP                    = 0x1d
+	SCM_TIMESTAMPING                 = 0x25
+	SCM_TIMESTAMPNS                  = 0x23
+	SHUT_RD                          = 0x0
+	SHUT_RDWR                        = 0x2
+	SHUT_WR                          = 0x1
+	SIOCADDDLCI                      = 0x8980
+	SIOCADDMULTI                     = 0x8931
+	SIOCADDRT                        = 0x890b
+	SIOCATMARK                       = 0x8905
+	SIOCDARP                         = 0x8953
+	SIOCDELDLCI                      = 0x8981
+	SIOCDELMULTI                     = 0x8932
+	SIOCDELRT                        = 0x890c
+	SIOCDEVPRIVATE                   = 0x89f0
+	SIOCDIFADDR                      = 0x8936
+	SIOCDRARP                        = 0x8960
+	SIOCGARP                         = 0x8954
+	SIOCGIFADDR                      = 0x8915
+	SIOCGIFBR                        = 0x8940
+	SIOCGIFBRDADDR                   = 0x8919
+	SIOCGIFCONF                      = 0x8912
+	SIOCGIFCOUNT                     = 0x8938
+	SIOCGIFDSTADDR                   = 0x8917
+	SIOCGIFENCAP                     = 0x8925
+	SIOCGIFFLAGS                     = 0x8913
+	SIOCGIFHWADDR                    = 0x8927
+	SIOCGIFINDEX                     = 0x8933
+	SIOCGIFMAP                       = 0x8970
+	SIOCGIFMEM                       = 0x891f
+	SIOCGIFMETRIC                    = 0x891d
+	SIOCGIFMTU                       = 0x8921
+	SIOCGIFNAME                      = 0x8910
+	SIOCGIFNETMASK                   = 0x891b
+	SIOCGIFPFLAGS                    = 0x8935
+	SIOCGIFSLAVE                     = 0x8929
+	SIOCGIFTXQLEN                    = 0x8942
+	SIOCGPGRP                        = 0x8904
+	SIOCGRARP                        = 0x8961
+	SIOCGSTAMP                       = 0x8906
+	SIOCGSTAMPNS                     = 0x8907
+	SIOCPROTOPRIVATE                 = 0x89e0
+	SIOCRTMSG                        = 0x890d
+	SIOCSARP                         = 0x8955
+	SIOCSIFADDR                      = 0x8916
+	SIOCSIFBR                        = 0x8941
+	SIOCSIFBRDADDR                   = 0x891a
+	SIOCSIFDSTADDR                   = 0x8918
+	SIOCSIFENCAP                     = 0x8926
+	SIOCSIFFLAGS                     = 0x8914
+	SIOCSIFHWADDR                    = 0x8924
+	SIOCSIFHWBROADCAST               = 0x8937
+	SIOCSIFLINK                      = 0x8911
+	SIOCSIFMAP                       = 0x8971
+	SIOCSIFMEM                       = 0x8920
+	SIOCSIFMETRIC                    = 0x891e
+	SIOCSIFMTU                       = 0x8922
+	SIOCSIFNAME                      = 0x8923
+	SIOCSIFNETMASK                   = 0x891c
+	SIOCSIFPFLAGS                    = 0x8934
+	SIOCSIFSLAVE                     = 0x8930
+	SIOCSIFTXQLEN                    = 0x8943
+	SIOCSPGRP                        = 0x8902
+	SIOCSRARP                        = 0x8962
+	SOCK_CLOEXEC                     = 0x80000
+	SOCK_DCCP                        = 0x6
+	SOCK_DGRAM                       = 0x2
+	SOCK_NONBLOCK                    = 0x800
+	SOCK_PACKET                      = 0xa
+	SOCK_RAW                         = 0x3
+	SOCK_RDM                         = 0x4
+	SOCK_SEQPACKET                   = 0x5
+	SOCK_STREAM                      = 0x1
+	SOL_AAL                          = 0x109
+	SOL_ATM                          = 0x108
+	SOL_DECNET                       = 0x105
+	SOL_ICMPV6                       = 0x3a
+	SOL_IP                           = 0x0
+	SOL_IPV6                         = 0x29
+	SOL_IRDA                         = 0x10a
+	SOL_PACKET                       = 0x107
+	SOL_RAW                          = 0xff
+	SOL_SOCKET                       = 0x1
+	SOL_TCP                          = 0x6
+	SOL_X25                          = 0x106
+	SOMAXCONN                        = 0x80
+	SO_ACCEPTCONN                    = 0x1e
+	SO_ATTACH_FILTER                 = 0x1a
+	SO_BINDTODEVICE                  = 0x19
+	SO_BROADCAST                     = 0x6
+	SO_BSDCOMPAT                     = 0xe
+	SO_DEBUG                         = 0x1
+	SO_DETACH_FILTER                 = 0x1b
+	SO_DOMAIN                        = 0x27
+	SO_DONTROUTE                     = 0x5
+	SO_ERROR                         = 0x4
+	SO_KEEPALIVE                     = 0x9
+	SO_LINGER                        = 0xd
+	SO_MARK                          = 0x24
+	SO_NO_CHECK                      = 0xb
+	SO_OOBINLINE                     = 0xa
+	SO_PASSCRED                      = 0x10
+	SO_PASSSEC                       = 0x22
+	SO_PEERCRED                      = 0x11
+	SO_PEERNAME                      = 0x1c
+	SO_PEERSEC                       = 0x1f
+	SO_PRIORITY                      = 0xc
+	SO_PROTOCOL                      = 0x26
+	SO_RCVBUF                        = 0x8
+	SO_RCVBUFFORCE                   = 0x21
+	SO_RCVLOWAT                      = 0x12
+	SO_RCVTIMEO                      = 0x14
+	SO_REUSEADDR                     = 0x2
+	SO_RXQ_OVFL                      = 0x28
+	SO_SECURITY_AUTHENTICATION       = 0x16
+	SO_SECURITY_ENCRYPTION_NETWORK   = 0x18
+	SO_SECURITY_ENCRYPTION_TRANSPORT = 0x17
+	SO_SNDBUF                        = 0x7
+	SO_SNDBUFFORCE                   = 0x20
+	SO_SNDLOWAT                      = 0x13
+	SO_SNDTIMEO                      = 0x15
+	SO_TIMESTAMP                     = 0x1d
+	SO_TIMESTAMPING                  = 0x25
+	SO_TIMESTAMPNS                   = 0x23
+	SO_TYPE                          = 0x3
+	S_BLKSIZE                        = 0x200
+	S_IEXEC                          = 0x40
+	S_IFBLK                          = 0x6000
+	S_IFCHR                          = 0x2000
+	S_IFDIR                          = 0x4000
+	S_IFIFO                          = 0x1000
+	S_IFLNK                          = 0xa000
+	S_IFMT                           = 0xf000
+	S_IFREG                          = 0x8000
+	S_IFSOCK                         = 0xc000
+	S_IREAD                          = 0x100
+	S_IRGRP                          = 0x20
+	S_IROTH                          = 0x4
+	S_IRUSR                          = 0x100
+	S_IRWXG                          = 0x38
+	S_IRWXO                          = 0x7
+	S_IRWXU                          = 0x1c0
+	S_ISGID                          = 0x400
+	S_ISUID                          = 0x800
+	S_ISVTX                          = 0x200
+	S_IWGRP                          = 0x10
+	S_IWOTH                          = 0x2
+	S_IWRITE                         = 0x80
+	S_IWUSR                          = 0x80
+	S_IXGRP                          = 0x8
+	S_IXOTH                          = 0x1
+	S_IXUSR                          = 0x40
+	TCIFLUSH                         = 0x0
+	TCIOFLUSH                        = 0x2
+	TCOFLUSH                         = 0x1
+	TCP_CONGESTION                   = 0xd
+	TCP_CORK                         = 0x3
+	TCP_DEFER_ACCEPT                 = 0x9
+	TCP_INFO                         = 0xb
+	TCP_KEEPCNT                      = 0x6
+	TCP_KEEPIDLE                     = 0x4
+	TCP_KEEPINTVL                    = 0x5
+	TCP_LINGER2                      = 0x8
+	TCP_MAXSEG                       = 0x2
+	TCP_MAXWIN                       = 0xffff
+	TCP_MAX_WINSHIFT                 = 0xe
+	TCP_MD5SIG                       = 0xe
+	TCP_MD5SIG_MAXKEYLEN             = 0x50
+	TCP_MSS                          = 0x200
+	TCP_NODELAY                      = 0x1
+	TCP_QUICKACK                     = 0xc
+	TCP_SYNCNT                       = 0x7
+	TCP_WINDOW_CLAMP                 = 0xa
+	TIOCCBRK                         = 0x5428
+	TIOCCONS                         = 0x541d
+	TIOCEXCL                         = 0x540c
+	TIOCGDEV                         = 0x80045432
+	TIOCGETD                         = 0x5424
+	TIOCGICOUNT                      = 0x545d
+	TIOCGLCKTRMIOS                   = 0x5456
+	TIOCGPGRP                        = 0x540f
+	TIOCGPTN                         = 0x80045430
+	TIOCGRS485                       = 0x542e
+	TIOCGSERIAL                      = 0x541e
+	TIOCGSID                         = 0x5429
+	TIOCGSOFTCAR                     = 0x5419
+	TIOCGWINSZ                       = 0x5413
+	TIOCINQ                          = 0x541b
+	TIOCLINUX                        = 0x541c
+	TIOCMBIC                         = 0x5417
+	TIOCMBIS                         = 0x5416
+	TIOCMGET                         = 0x5415
+	TIOCMIWAIT                       = 0x545c
+	TIOCMSET                         = 0x5418
+	TIOCM_CAR                        = 0x40
+	TIOCM_CD                         = 0x40
+	TIOCM_CTS                        = 0x20
+	TIOCM_DSR                        = 0x100
+	TIOCM_DTR                        = 0x2
+	TIOCM_LE                         = 0x1
+	TIOCM_RI                         = 0x80
+	TIOCM_RNG                        = 0x80
+	TIOCM_RTS                        = 0x4
+	TIOCM_SR                         = 0x10
+	TIOCM_ST                         = 0x8
+	TIOCNOTTY                        = 0x5422
+	TIOCNXCL                         = 0x540d
+	TIOCOUTQ                         = 0x5411
+	TIOCPKT                          = 0x5420
+	TIOCPKT_DATA                     = 0x0
+	TIOCPKT_DOSTOP                   = 0x20
+	TIOCPKT_FLUSHREAD                = 0x1
+	TIOCPKT_FLUSHWRITE               = 0x2
+	TIOCPKT_IOCTL                    = 0x40
+	TIOCPKT_NOSTOP                   = 0x10
+	TIOCPKT_START                    = 0x8
+	TIOCPKT_STOP                     = 0x4
+	TIOCSBRK                         = 0x5427
+	TIOCSCTTY                        = 0x540e
+	TIOCSERCONFIG                    = 0x5453
+	TIOCSERGETLSR                    = 0x5459
+	TIOCSERGETMULTI                  = 0x545a
+	TIOCSERGSTRUCT                   = 0x5458
+	TIOCSERGWILD                     = 0x5454
+	TIOCSERSETMULTI                  = 0x545b
+	TIOCSERSWILD                     = 0x5455
+	TIOCSER_TEMT                     = 0x1
+	TIOCSETD                         = 0x5423
+	TIOCSIG                          = 0x40045436
+	TIOCSLCKTRMIOS                   = 0x5457
+	TIOCSPGRP                        = 0x5410
+	TIOCSPTLCK                       = 0x40045431
+	TIOCSRS485                       = 0x542f
+	TIOCSSERIAL                      = 0x541f
+	TIOCSSOFTCAR                     = 0x541a
+	TIOCSTI                          = 0x5412
+	TIOCSWINSZ                       = 0x5414
+	TIOCVHANGUP                      = 0x5437
+	TUNATTACHFILTER                  = 0x400854d5
+	TUNDETACHFILTER                  = 0x400854d6
+	TUNGETFEATURES                   = 0x800454cf
+	TUNGETIFF                        = 0x800454d2
+	TUNGETSNDBUF                     = 0x800454d3
+	TUNGETVNETHDRSZ                  = 0x800454d7
+	TUNSETDEBUG                      = 0x400454c9
+	TUNSETGROUP                      = 0x400454ce
+	TUNSETIFF                        = 0x400454ca
+	TUNSETLINK                       = 0x400454cd
+	TUNSETNOCSUM                     = 0x400454c8
+	TUNSETOFFLOAD                    = 0x400454d0
+	TUNSETOWNER                      = 0x400454cc
+	TUNSETPERSIST                    = 0x400454cb
+	TUNSETSNDBUF                     = 0x400454d4
+	TUNSETTXFILTER                   = 0x400454d1
+	TUNSETVNETHDRSZ                  = 0x400454d8
+	WALL                             = 0x40000000
+	WCLONE                           = 0x80000000
+	WCONTINUED                       = 0x8
+	WEXITED                          = 0x4
+	WNOHANG                          = 0x1
+	WNOTHREAD                        = 0x20000000
+	WNOWAIT                          = 0x1000000
+	WORDSIZE                         = 0x20
+	WSTOPPED                         = 0x2
+	WUNTRACED                        = 0x2
+)
+
+// Errors
+const (
+	E2BIG           = Errno(0x7)
+	EACCES          = Errno(0xd)
+	EADDRINUSE      = Errno(0x62)
+	EADDRNOTAVAIL   = Errno(0x63)
+	EADV            = Errno(0x44)
+	EAFNOSUPPORT    = Errno(0x61)
+	EAGAIN          = Errno(0xb)
+	EALREADY        = Errno(0x72)
+	EBADE           = Errno(0x34)
+	EBADF           = Errno(0x9)
+	EBADFD          = Errno(0x4d)
+	EBADMSG         = Errno(0x4a)
+	EBADR           = Errno(0x35)
+	EBADRQC         = Errno(0x38)
+	EBADSLT         = Errno(0x39)
+	EBFONT          = Errno(0x3b)
+	EBUSY           = Errno(0x10)
+	ECANCELED       = Errno(0x7d)
+	ECHILD          = Errno(0xa)
+	ECHRNG          = Errno(0x2c)
+	ECOMM           = Errno(0x46)
+	ECONNABORTED    = Errno(0x67)
+	ECONNREFUSED    = Errno(0x6f)
+	ECONNRESET      = Errno(0x68)
+	EDEADLK         = Errno(0x23)
+	EDEADLOCK       = Errno(0x23)
+	EDESTADDRREQ    = Errno(0x59)
+	EDOM            = Errno(0x21)
+	EDOTDOT         = Errno(0x49)
+	EDQUOT          = Errno(0x7a)
+	EEXIST          = Errno(0x11)
+	EFAULT          = Errno(0xe)
+	EFBIG           = Errno(0x1b)
+	EHOSTDOWN       = Errno(0x70)
+	EHOSTUNREACH    = Errno(0x71)
+	EHWPOISON       = Errno(0x85)
+	EIDRM           = Errno(0x2b)
+	EILSEQ          = Errno(0x54)
+	EINPROGRESS     = Errno(0x73)
+	EINTR           = Errno(0x4)
+	EINVAL          = Errno(0x16)
+	EIO             = Errno(0x5)
+	EISCONN         = Errno(0x6a)
+	EISDIR          = Errno(0x15)
+	EISNAM          = Errno(0x78)
+	EKEYEXPIRED     = Errno(0x7f)
+	EKEYREJECTED    = Errno(0x81)
+	EKEYREVOKED     = Errno(0x80)
+	EL2HLT          = Errno(0x33)
+	EL2NSYNC        = Errno(0x2d)
+	EL3HLT          = Errno(0x2e)
+	EL3RST          = Errno(0x2f)
+	ELIBACC         = Errno(0x4f)
+	ELIBBAD         = Errno(0x50)
+	ELIBEXEC        = Errno(0x53)
+	ELIBMAX         = Errno(0x52)
+	ELIBSCN         = Errno(0x51)
+	ELNRNG          = Errno(0x30)
+	ELOOP           = Errno(0x28)
+	EMEDIUMTYPE     = Errno(0x7c)
+	EMFILE          = Errno(0x18)
+	EMLINK          = Errno(0x1f)
+	EMSGSIZE        = Errno(0x5a)
+	EMULTIHOP       = Errno(0x48)
+	ENAMETOOLONG    = Errno(0x24)
+	ENAVAIL         = Errno(0x77)
+	ENETDOWN        = Errno(0x64)
+	ENETRESET       = Errno(0x66)
+	ENETUNREACH     = Errno(0x65)
+	ENFILE          = Errno(0x17)
+	ENOANO          = Errno(0x37)
+	ENOBUFS         = Errno(0x69)
+	ENOCSI          = Errno(0x32)
+	ENODATA         = Errno(0x3d)
+	ENODEV          = Errno(0x13)
+	ENOENT          = Errno(0x2)
+	ENOEXEC         = Errno(0x8)
+	ENOKEY          = Errno(0x7e)
+	ENOLCK          = Errno(0x25)
+	ENOLINK         = Errno(0x43)
+	ENOMEDIUM       = Errno(0x7b)
+	ENOMEM          = Errno(0xc)
+	ENOMSG          = Errno(0x2a)
+	ENONET          = Errno(0x40)
+	ENOPKG          = Errno(0x41)
+	ENOPROTOOPT     = Errno(0x5c)
+	ENOSPC          = Errno(0x1c)
+	ENOSR           = Errno(0x3f)
+	ENOSTR          = Errno(0x3c)
+	ENOSYS          = Errno(0x26)
+	ENOTBLK         = Errno(0xf)
+	ENOTCONN        = Errno(0x6b)
+	ENOTDIR         = Errno(0x14)
+	ENOTEMPTY       = Errno(0x27)
+	ENOTNAM         = Errno(0x76)
+	ENOTRECOVERABLE = Errno(0x83)
+	ENOTSOCK        = Errno(0x58)
+	ENOTSUP         = Errno(0x5f)
+	ENOTTY          = Errno(0x19)
+	ENOTUNIQ        = Errno(0x4c)
+	ENXIO           = Errno(0x6)
+	EOPNOTSUPP      = Errno(0x5f)
+	EOVERFLOW       = Errno(0x4b)
+	EOWNERDEAD      = Errno(0x82)
+	EPERM           = Errno(0x1)
+	EPFNOSUPPORT    = Errno(0x60)
+	EPIPE           = Errno(0x20)
+	EPROTO          = Errno(0x47)
+	EPROTONOSUPPORT = Errno(0x5d)
+	EPROTOTYPE      = Errno(0x5b)
+	ERANGE          = Errno(0x22)
+	EREMCHG         = Errno(0x4e)
+	EREMOTE         = Errno(0x42)
+	EREMOTEIO       = Errno(0x79)
+	ERESTART        = Errno(0x55)
+	ERFKILL         = Errno(0x84)
+	EROFS           = Errno(0x1e)
+	ESHUTDOWN       = Errno(0x6c)
+	ESOCKTNOSUPPORT = Errno(0x5e)
+	ESPIPE          = Errno(0x1d)
+	ESRCH           = Errno(0x3)
+	ESRMNT          = Errno(0x45)
+	ESTALE          = Errno(0x74)
+	ESTRPIPE        = Errno(0x56)
+	ETIME           = Errno(0x3e)
+	ETIMEDOUT       = Errno(0x6e)
+	ETOOMANYREFS    = Errno(0x6d)
+	ETXTBSY         = Errno(0x1a)
+	EUCLEAN         = Errno(0x75)
+	EUNATCH         = Errno(0x31)
+	EUSERS          = Errno(0x57)
+	EWOULDBLOCK     = Errno(0xb)
+	EXDEV           = Errno(0x12)
+	EXFULL          = Errno(0x36)
+)
+
+// Signals
+const (
+	SIGABRT   = Signal(0x6)
+	SIGALRM   = Signal(0xe)
+	SIGBUS    = Signal(0x7)
+	SIGCHLD   = Signal(0x11)
+	SIGCLD    = Signal(0x11)
+	SIGCONT   = Signal(0x12)
+	SIGFPE    = Signal(0x8)
+	SIGHUP    = Signal(0x1)
+	SIGILL    = Signal(0x4)
+	SIGINT    = Signal(0x2)
+	SIGIO     = Signal(0x1d)
+	SIGIOT    = Signal(0x6)
+	SIGKILL   = Signal(0x9)
+	SIGPIPE   = Signal(0xd)
+	SIGPOLL   = Signal(0x1d)
+	SIGPROF   = Signal(0x1b)
+	SIGPWR    = Signal(0x1e)
+	SIGQUIT   = Signal(0x3)
+	SIGSEGV   = Signal(0xb)
+	SIGSTKFLT = Signal(0x10)
+	SIGSTOP   = Signal(0x13)
+	SIGSYS    = Signal(0x1f)
+	SIGTERM   = Signal(0xf)
+	SIGTRAP   = Signal(0x5)
+	SIGTSTP   = Signal(0x14)
+	SIGTTIN   = Signal(0x15)
+	SIGTTOU   = Signal(0x16)
+	SIGUNUSED = Signal(0x1f)
+	SIGURG    = Signal(0x17)
+	SIGUSR1   = Signal(0xa)
+	SIGUSR2   = Signal(0xc)
+	SIGVTALRM = Signal(0x1a)
+	SIGWINCH  = Signal(0x1c)
+	SIGXCPU   = Signal(0x18)
+	SIGXFSZ   = Signal(0x19)
+)
+
+// Error table
+var errors = [...]string{
+	1:   "operation not permitted",
+	2:   "no such file or directory",
+	3:   "no such process",
+	4:   "interrupted system call",
+	5:   "input/output error",
+	6:   "no such device or address",
+	7:   "argument list too long",
+	8:   "exec format error",
+	9:   "bad file descriptor",
+	10:  "no child processes",
+	11:  "resource temporarily unavailable",
+	12:  "cannot allocate memory",
+	13:  "permission denied",
+	14:  "bad address",
+	15:  "block device required",
+	16:  "device or resource busy",
+	17:  "file exists",
+	18:  "invalid cross-device link",
+	19:  "no such device",
+	20:  "not a directory",
+	21:  "is a directory",
+	22:  "invalid argument",
+	23:  "too many open files in system",
+	24:  "too many open files",
+	25:  "inappropriate ioctl for device",
+	26:  "text file busy",
+	27:  "file too large",
+	28:  "no space left on device",
+	29:  "illegal seek",
+	30:  "read-only file system",
+	31:  "too many links",
+	32:  "broken pipe",
+	33:  "numerical argument out of domain",
+	34:  "numerical result out of range",
+	35:  "resource deadlock avoided",
+	36:  "file name too long",
+	37:  "no locks available",
+	38:  "function not implemented",
+	39:  "directory not empty",
+	40:  "too many levels of symbolic links",
+	42:  "no message of desired type",
+	43:  "identifier removed",
+	44:  "channel number out of range",
+	45:  "level 2 not synchronized",
+	46:  "level 3 halted",
+	47:  "level 3 reset",
+	48:  "link number out of range",
+	49:  "protocol driver not attached",
+	50:  "no CSI structure available",
+	51:  "level 2 halted",
+	52:  "invalid exchange",
+	53:  "invalid request descriptor",
+	54:  "exchange full",
+	55:  "no anode",
+	56:  "invalid request code",
+	57:  "invalid slot",
+	59:  "bad font file format",
+	60:  "device not a stream",
+	61:  "no data available",
+	62:  "timer expired",
+	63:  "out of streams resources",
+	64:  "machine is not on the network",
+	65:  "package not installed",
+	66:  "object is remote",
+	67:  "link has been severed",
+	68:  "advertise error",
+	69:  "srmount error",
+	70:  "communication error on send",
+	71:  "protocol error",
+	72:  "multihop attempted",
+	73:  "RFS specific error",
+	74:  "bad message",
+	75:  "value too large for defined data type",
+	76:  "name not unique on network",
+	77:  "file descriptor in bad state",
+	78:  "remote address changed",
+	79:  "can not access a needed shared library",
+	80:  "accessing a corrupted shared library",
+	81:  ".lib section in a.out corrupted",
+	82:  "attempting to link in too many shared libraries",
+	83:  "cannot exec a shared library directly",
+	84:  "invalid or incomplete multibyte or wide character",
+	85:  "interrupted system call should be restarted",
+	86:  "streams pipe error",
+	87:  "too many users",
+	88:  "socket operation on non-socket",
+	89:  "destination address required",
+	90:  "message too long",
+	91:  "protocol wrong type for socket",
+	92:  "protocol not available",
+	93:  "protocol not supported",
+	94:  "socket type not supported",
+	95:  "operation not supported",
+	96:  "protocol family not supported",
+	97:  "address family not supported by protocol",
+	98:  "address already in use",
+	99:  "cannot assign requested address",
+	100: "network is down",
+	101: "network is unreachable",
+	102: "network dropped connection on reset",
+	103: "software caused connection abort",
+	104: "connection reset by peer",
+	105: "no buffer space available",
+	106: "transport endpoint is already connected",
+	107: "transport endpoint is not connected",
+	108: "cannot send after transport endpoint shutdown",
+	109: "too many references: cannot splice",
+	110: "connection timed out",
+	111: "connection refused",
+	112: "host is down",
+	113: "no route to host",
+	114: "operation already in progress",
+	115: "operation now in progress",
+	116: "stale NFS file handle",
+	117: "structure needs cleaning",
+	118: "not a XENIX named type file",
+	119: "no XENIX semaphores available",
+	120: "is a named type file",
+	121: "remote I/O error",
+	122: "disk quota exceeded",
+	123: "no medium found",
+	124: "wrong medium type",
+	125: "operation canceled",
+	126: "required key not available",
+	127: "key has expired",
+	128: "key has been revoked",
+	129: "key was rejected by service",
+	130: "owner died",
+	131: "state not recoverable",
+	132: "operation not possible due to RF-kill",
+	133: "unknown error 133",
+}
+
+// Signal table
+var signals = [...]string{
+	1:  "hangup",
+	2:  "interrupt",
+	3:  "quit",
+	4:  "illegal instruction",
+	5:  "trace/breakpoint trap",
+	6:  "aborted",
+	7:  "bus error",
+	8:  "floating point exception",
+	9:  "killed",
+	10: "user defined signal 1",
+	11: "segmentation fault",
+	12: "user defined signal 2",
+	13: "broken pipe",
+	14: "alarm clock",
+	15: "terminated",
+	16: "stack fault",
+	17: "child exited",
+	18: "continued",
+	19: "stopped (signal)",
+	20: "stopped",
+	21: "stopped (tty input)",
+	22: "stopped (tty output)",
+	23: "urgent I/O condition",
+	24: "CPU time limit exceeded",
+	25: "file size limit exceeded",
+	26: "virtual timer expired",
+	27: "profiling timer expired",
+	28: "window changed",
+	29: "I/O possible",
+	30: "power failure",
+	31: "bad system call",
+}
diff --git a/src/syscall/zsyscall_linux_thumb.go b/src/syscall/zsyscall_linux_thumb.go
new file mode 100644
index 0000000000..a1c6462dfe
--- /dev/null
+++ b/src/syscall/zsyscall_linux_thumb.go
@@ -0,0 +1,1655 @@
+// mksyscall.pl -l32 -arm -tags linux,arm syscall_linux.go syscall_linux_arm.go
+// Code generated by the command above; DO NOT EDIT.
+
+// +build linux,thumb
+
+package syscall
+
+import "unsafe"
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func faccessat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FACCESSAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fchmodat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_FCHMODAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func linkat(olddirfd int, oldpath string, newdirfd int, newpath string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_LINKAT, uintptr(olddirfd), uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func openat(dirfd int, path string, flags int, mode uint32) (fd int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall6(SYS_OPENAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(flags), uintptr(mode), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func readlinkat(dirfd int, path string, buf []byte) (n int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(buf) > 0 {
+		_p1 = unsafe.Pointer(&buf[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_READLINKAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(buf)), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func symlinkat(oldpath string, newdirfd int, newpath string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_SYMLINKAT, uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func unlinkat(dirfd int, path string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UNLINKAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(flags))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func utimes(path string, times *[2]Timeval) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UTIMES, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func utimensat(dirfd int, path string, times *[2]Timespec, flag int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_UTIMENSAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(times)), uintptr(flag), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func futimesat(dirfd int, path *byte, times *[2]Timeval) (err error) {
+	_, _, e1 := Syscall(SYS_FUTIMESAT, uintptr(dirfd), uintptr(unsafe.Pointer(path)), uintptr(unsafe.Pointer(times)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getcwd(buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_GETCWD, uintptr(_p0), uintptr(len(buf)), 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func wait4(pid int, wstatus *_C_int, options int, rusage *Rusage) (wpid int, err error) {
+	r0, _, e1 := Syscall6(SYS_WAIT4, uintptr(pid), uintptr(unsafe.Pointer(wstatus)), uintptr(options), uintptr(unsafe.Pointer(rusage)), 0, 0)
+	wpid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func ptrace(request int, pid int, addr uintptr, data uintptr) (err error) {
+	_, _, e1 := Syscall6(SYS_PTRACE, uintptr(request), uintptr(pid), uintptr(addr), uintptr(data), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func reboot(magic1 uint, magic2 uint, cmd int, arg string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(arg)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_REBOOT, uintptr(magic1), uintptr(magic2), uintptr(cmd), uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func mount(source string, target string, fstype string, flags uintptr, data *byte) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(source)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(target)
+	if err != nil {
+		return
+	}
+	var _p2 *byte
+	_p2, err = BytePtrFromString(fstype)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_MOUNT, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(unsafe.Pointer(_p2)), uintptr(flags), uintptr(unsafe.Pointer(data)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Acct(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_ACCT, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Adjtimex(buf *Timex) (state int, err error) {
+	r0, _, e1 := Syscall(SYS_ADJTIMEX, uintptr(unsafe.Pointer(buf)), 0, 0)
+	state = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Chdir(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_CHDIR, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Chroot(path string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_CHROOT, uintptr(unsafe.Pointer(_p0)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Close(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_CLOSE, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup(oldfd int) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_DUP, uintptr(oldfd), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup3(oldfd int, newfd int, flags int) (err error) {
+	_, _, e1 := Syscall(SYS_DUP3, uintptr(oldfd), uintptr(newfd), uintptr(flags))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCreate(size int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_EPOLL_CREATE, uintptr(size), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCreate1(flag int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_EPOLL_CREATE1, uintptr(flag), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollCtl(epfd int, op int, fd int, event *EpollEvent) (err error) {
+	_, _, e1 := RawSyscall6(SYS_EPOLL_CTL, uintptr(epfd), uintptr(op), uintptr(fd), uintptr(unsafe.Pointer(event)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fallocate(fd int, mode uint32, off int64, len int64) (err error) {
+	_, _, e1 := Syscall6(SYS_FALLOCATE, uintptr(fd), uintptr(mode), uintptr(off), uintptr(off>>32), uintptr(len), uintptr(len>>32))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchdir(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FCHDIR, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchmod(fd int, mode uint32) (err error) {
+	_, _, e1 := Syscall(SYS_FCHMOD, uintptr(fd), uintptr(mode), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchownat(dirfd int, path string, uid int, gid int, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_FCHOWNAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(uid), uintptr(gid), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fcntl(fd int, cmd int, arg int) (val int, err error) {
+	r0, _, e1 := Syscall(SYS_FCNTL, uintptr(fd), uintptr(cmd), uintptr(arg))
+	val = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fdatasync(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FDATASYNC, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Flock(fd int, how int) (err error) {
+	_, _, e1 := Syscall(SYS_FLOCK, uintptr(fd), uintptr(how), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fsync(fd int) (err error) {
+	_, _, e1 := Syscall(SYS_FSYNC, uintptr(fd), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getdents(fd int, buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_GETDENTS64, uintptr(fd), uintptr(_p0), uintptr(len(buf)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpgid(pid int) (pgid int, err error) {
+	r0, _, e1 := RawSyscall(SYS_GETPGID, uintptr(pid), 0, 0)
+	pgid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpid() (pid int) {
+	r0, _ := rawSyscallNoError(SYS_GETPID, 0, 0, 0)
+	pid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getppid() (ppid int) {
+	r0, _ := rawSyscallNoError(SYS_GETPPID, 0, 0, 0)
+	ppid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getpriority(which int, who int) (prio int, err error) {
+	r0, _, e1 := Syscall(SYS_GETPRIORITY, uintptr(which), uintptr(who), 0)
+	prio = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getrusage(who int, rusage *Rusage) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETRUSAGE, uintptr(who), uintptr(unsafe.Pointer(rusage)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Gettid() (tid int) {
+	r0, _ := rawSyscallNoError(SYS_GETTID, 0, 0, 0)
+	tid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getxattr(path string, attr string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(dest) > 0 {
+		_p2 = unsafe.Pointer(&dest[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_GETXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(dest)), 0, 0)
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyAddWatch(fd int, pathname string, mask uint32) (watchdesc int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(pathname)
+	if err != nil {
+		return
+	}
+	r0, _, e1 := Syscall(SYS_INOTIFY_ADD_WATCH, uintptr(fd), uintptr(unsafe.Pointer(_p0)), uintptr(mask))
+	watchdesc = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyInit1(flags int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_INIT1, uintptr(flags), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyRmWatch(fd int, watchdesc uint32) (success int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_RM_WATCH, uintptr(fd), uintptr(watchdesc), 0)
+	success = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Kill(pid int, sig Signal) (err error) {
+	_, _, e1 := RawSyscall(SYS_KILL, uintptr(pid), uintptr(sig), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Klogctl(typ int, buf []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_SYSLOG, uintptr(typ), uintptr(_p0), uintptr(len(buf)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Listxattr(path string, dest []byte) (sz int, err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 unsafe.Pointer
+	if len(dest) > 0 {
+		_p1 = unsafe.Pointer(&dest[0])
+	} else {
+		_p1 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_LISTXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(_p1), uintptr(len(dest)))
+	sz = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mkdirat(dirfd int, path string, mode uint32) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_MKDIRAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mknodat(dirfd int, path string, mode uint32, dev int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_MKNODAT, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(mode), uintptr(dev), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Nanosleep(time *Timespec, leftover *Timespec) (err error) {
+	_, _, e1 := Syscall(SYS_NANOSLEEP, uintptr(unsafe.Pointer(time)), uintptr(unsafe.Pointer(leftover)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pause() (err error) {
+	_, _, e1 := Syscall(SYS_PAUSE, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func PivotRoot(newroot string, putold string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(newroot)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(putold)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_PIVOT_ROOT, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func prlimit(pid int, resource int, newlimit *Rlimit, old *Rlimit) (err error) {
+	_, _, e1 := RawSyscall6(SYS_PRLIMIT64, uintptr(pid), uintptr(resource), uintptr(unsafe.Pointer(newlimit)), uintptr(unsafe.Pointer(old)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func read(fd int, p []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_READ, uintptr(fd), uintptr(_p0), uintptr(len(p)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Removexattr(path string, attr string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_REMOVEXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Renameat(olddirfd int, oldpath string, newdirfd int, newpath string) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(oldpath)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(newpath)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_RENAMEAT, uintptr(olddirfd), uintptr(unsafe.Pointer(_p0)), uintptr(newdirfd), uintptr(unsafe.Pointer(_p1)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setdomainname(p []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_SETDOMAINNAME, uintptr(_p0), uintptr(len(p)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sethostname(p []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_SETHOSTNAME, uintptr(_p0), uintptr(len(p)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setpgid(pid int, pgid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETPGID, uintptr(pid), uintptr(pgid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setsid() (pid int, err error) {
+	r0, _, e1 := RawSyscall(SYS_SETSID, 0, 0, 0)
+	pid = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Settimeofday(tv *Timeval) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETTIMEOFDAY, uintptr(unsafe.Pointer(tv)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setpriority(which int, who int, prio int) (err error) {
+	_, _, e1 := Syscall(SYS_SETPRIORITY, uintptr(which), uintptr(who), uintptr(prio))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setxattr(path string, attr string, data []byte, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	var _p1 *byte
+	_p1, err = BytePtrFromString(attr)
+	if err != nil {
+		return
+	}
+	var _p2 unsafe.Pointer
+	if len(data) > 0 {
+		_p2 = unsafe.Pointer(&data[0])
+	} else {
+		_p2 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_SETXATTR, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(_p1)), uintptr(_p2), uintptr(len(data)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sync() {
+	Syscall(SYS_SYNC, 0, 0, 0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Sysinfo(info *Sysinfo_t) (err error) {
+	_, _, e1 := RawSyscall(SYS_SYSINFO, uintptr(unsafe.Pointer(info)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Tee(rfd int, wfd int, len int, flags int) (n int64, err error) {
+	r0, r1, e1 := Syscall6(SYS_TEE, uintptr(rfd), uintptr(wfd), uintptr(len), uintptr(flags), 0, 0)
+	n = int64(int64(r1)<<32 | int64(r0))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Tgkill(tgid int, tid int, sig Signal) (err error) {
+	_, _, e1 := RawSyscall(SYS_TGKILL, uintptr(tgid), uintptr(tid), uintptr(sig))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Times(tms *Tms) (ticks uintptr, err error) {
+	r0, _, e1 := RawSyscall(SYS_TIMES, uintptr(unsafe.Pointer(tms)), 0, 0)
+	ticks = uintptr(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Umask(mask int) (oldmask int) {
+	r0, _ := rawSyscallNoError(SYS_UMASK, uintptr(mask), 0, 0)
+	oldmask = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Uname(buf *Utsname) (err error) {
+	_, _, e1 := RawSyscall(SYS_UNAME, uintptr(unsafe.Pointer(buf)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Unmount(target string, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(target)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UMOUNT2, uintptr(unsafe.Pointer(_p0)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Unshare(flags int) (err error) {
+	_, _, e1 := Syscall(SYS_UNSHARE, uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Ustat(dev int, ubuf *Ustat_t) (err error) {
+	_, _, e1 := Syscall(SYS_USTAT, uintptr(dev), uintptr(unsafe.Pointer(ubuf)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Utime(path string, buf *Utimbuf) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall(SYS_UTIME, uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(buf)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func write(fd int, p []byte) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall(SYS_WRITE, uintptr(fd), uintptr(_p0), uintptr(len(p)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func exitThread(code int) (err error) {
+	_, _, e1 := Syscall(SYS_EXIT, uintptr(code), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func readlen(fd int, p *byte, np int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_READ, uintptr(fd), uintptr(unsafe.Pointer(p)), uintptr(np))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func writelen(fd int, p *byte, np int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_WRITE, uintptr(fd), uintptr(unsafe.Pointer(p)), uintptr(np))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func munmap(addr uintptr, length uintptr) (err error) {
+	_, _, e1 := Syscall(SYS_MUNMAP, uintptr(addr), uintptr(length), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Madvise(b []byte, advice int) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MADVISE, uintptr(_p0), uintptr(len(b)), uintptr(advice))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mprotect(b []byte, prot int) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MPROTECT, uintptr(_p0), uintptr(len(b)), uintptr(prot))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mlock(b []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MLOCK, uintptr(_p0), uintptr(len(b)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Munlock(b []byte) (err error) {
+	var _p0 unsafe.Pointer
+	if len(b) > 0 {
+		_p0 = unsafe.Pointer(&b[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall(SYS_MUNLOCK, uintptr(_p0), uintptr(len(b)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Mlockall(flags int) (err error) {
+	_, _, e1 := Syscall(SYS_MLOCKALL, uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Munlockall() (err error) {
+	_, _, e1 := Syscall(SYS_MUNLOCKALL, 0, 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func pipe2(p *[2]_C_int, flags int) (err error) {
+	_, _, e1 := RawSyscall(SYS_PIPE2, uintptr(unsafe.Pointer(p)), uintptr(flags), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func accept(s int, rsa *RawSockaddrAny, addrlen *_Socklen) (fd int, err error) {
+	r0, _, e1 := Syscall(SYS_ACCEPT, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func accept4(s int, rsa *RawSockaddrAny, addrlen *_Socklen, flags int) (fd int, err error) {
+	r0, _, e1 := Syscall6(SYS_ACCEPT4, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), uintptr(flags), 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func bind(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := Syscall(SYS_BIND, uintptr(s), uintptr(addr), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func connect(s int, addr unsafe.Pointer, addrlen _Socklen) (err error) {
+	_, _, e1 := Syscall(SYS_CONNECT, uintptr(s), uintptr(addr), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getgroups(n int, list *_Gid_t) (nn int, err error) {
+	r0, _, e1 := RawSyscall(SYS_GETGROUPS32, uintptr(n), uintptr(unsafe.Pointer(list)), 0)
+	nn = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setgroups(n int, list *_Gid_t) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETGROUPS32, uintptr(n), uintptr(unsafe.Pointer(list)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getsockopt(s int, level int, name int, val unsafe.Pointer, vallen *_Socklen) (err error) {
+	_, _, e1 := Syscall6(SYS_GETSOCKOPT, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(unsafe.Pointer(vallen)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setsockopt(s int, level int, name int, val unsafe.Pointer, vallen uintptr) (err error) {
+	_, _, e1 := Syscall6(SYS_SETSOCKOPT, uintptr(s), uintptr(level), uintptr(name), uintptr(val), uintptr(vallen), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func socket(domain int, typ int, proto int) (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_SOCKET, uintptr(domain), uintptr(typ), uintptr(proto))
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getpeername(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETPEERNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getsockname(fd int, rsa *RawSockaddrAny, addrlen *_Socklen) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETSOCKNAME, uintptr(fd), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func recvfrom(fd int, p []byte, flags int, from *RawSockaddrAny, fromlen *_Socklen) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_RECVFROM, uintptr(fd), uintptr(_p0), uintptr(len(p)), uintptr(flags), uintptr(unsafe.Pointer(from)), uintptr(unsafe.Pointer(fromlen)))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendto(s int, buf []byte, flags int, to unsafe.Pointer, addrlen _Socklen) (err error) {
+	var _p0 unsafe.Pointer
+	if len(buf) > 0 {
+		_p0 = unsafe.Pointer(&buf[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	_, _, e1 := Syscall6(SYS_SENDTO, uintptr(s), uintptr(_p0), uintptr(len(buf)), uintptr(flags), uintptr(to), uintptr(addrlen))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func socketpair(domain int, typ int, flags int, fd *[2]int32) (err error) {
+	_, _, e1 := RawSyscall6(SYS_SOCKETPAIR, uintptr(domain), uintptr(typ), uintptr(flags), uintptr(unsafe.Pointer(fd)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func recvmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_RECVMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendmsg(s int, msg *Msghdr, flags int) (n int, err error) {
+	r0, _, e1 := Syscall(SYS_SENDMSG, uintptr(s), uintptr(unsafe.Pointer(msg)), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Dup2(oldfd int, newfd int) (err error) {
+	_, _, e1 := Syscall(SYS_DUP2, uintptr(oldfd), uintptr(newfd), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fchown(fd int, uid int, gid int) (err error) {
+	_, _, e1 := Syscall(SYS_FCHOWN32, uintptr(fd), uintptr(uid), uintptr(gid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Fstat(fd int, stat *Stat_t) (err error) {
+	_, _, e1 := Syscall(SYS_FSTAT64, uintptr(fd), uintptr(unsafe.Pointer(stat)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func fstatat(dirfd int, path string, stat *Stat_t, flags int) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_FSTATAT64, uintptr(dirfd), uintptr(unsafe.Pointer(_p0)), uintptr(unsafe.Pointer(stat)), uintptr(flags), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getegid() (egid int) {
+	r0, _ := rawSyscallNoError(SYS_GETEGID32, 0, 0, 0)
+	egid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Geteuid() (euid int) {
+	r0, _ := rawSyscallNoError(SYS_GETEUID32, 0, 0, 0)
+	euid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getgid() (gid int) {
+	r0, _ := rawSyscallNoError(SYS_GETGID32, 0, 0, 0)
+	gid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Getuid() (uid int) {
+	r0, _ := rawSyscallNoError(SYS_GETUID32, 0, 0, 0)
+	uid = int(r0)
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func InotifyInit() (fd int, err error) {
+	r0, _, e1 := RawSyscall(SYS_INOTIFY_INIT, 0, 0, 0)
+	fd = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Listen(s int, n int) (err error) {
+	_, _, e1 := Syscall(SYS_LISTEN, uintptr(s), uintptr(n), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func sendfile(outfd int, infd int, offset *int64, count int) (written int, err error) {
+	r0, _, e1 := Syscall6(SYS_SENDFILE64, uintptr(outfd), uintptr(infd), uintptr(unsafe.Pointer(offset)), uintptr(count), 0, 0)
+	written = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Select(nfd int, r *FdSet, w *FdSet, e *FdSet, timeout *Timeval) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS__NEWSELECT, uintptr(nfd), uintptr(unsafe.Pointer(r)), uintptr(unsafe.Pointer(w)), uintptr(unsafe.Pointer(e)), uintptr(unsafe.Pointer(timeout)), 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setfsgid(gid int) (err error) {
+	_, _, e1 := Syscall(SYS_SETFSGID32, uintptr(gid), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setfsuid(uid int) (err error) {
+	_, _, e1 := Syscall(SYS_SETFSUID32, uintptr(uid), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setregid(rgid int, egid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETREGID32, uintptr(rgid), uintptr(egid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setresgid(rgid int, egid int, sgid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRESGID32, uintptr(rgid), uintptr(egid), uintptr(sgid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setresuid(ruid int, euid int, suid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRESUID32, uintptr(ruid), uintptr(euid), uintptr(suid))
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Setreuid(ruid int, euid int) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETREUID32, uintptr(ruid), uintptr(euid), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Shutdown(fd int, how int) (err error) {
+	_, _, e1 := Syscall(SYS_SHUTDOWN, uintptr(fd), uintptr(how), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Splice(rfd int, roff *int64, wfd int, woff *int64, len int, flags int) (n int, err error) {
+	r0, _, e1 := Syscall6(SYS_SPLICE, uintptr(rfd), uintptr(unsafe.Pointer(roff)), uintptr(wfd), uintptr(unsafe.Pointer(woff)), uintptr(len), uintptr(flags))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Gettimeofday(tv *Timeval) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETTIMEOFDAY, uintptr(unsafe.Pointer(tv)), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Time(t *Time_t) (tt Time_t, err error) {
+	r0, _, e1 := RawSyscall(SYS_TIME, uintptr(unsafe.Pointer(t)), 0, 0)
+	tt = Time_t(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pread(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_PREAD64, uintptr(fd), uintptr(_p0), uintptr(len(p)), 0, uintptr(offset), uintptr(offset>>32))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Pwrite(fd int, p []byte, offset int64) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(p) > 0 {
+		_p0 = unsafe.Pointer(&p[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_PWRITE64, uintptr(fd), uintptr(_p0), uintptr(len(p)), 0, uintptr(offset), uintptr(offset>>32))
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Truncate(path string, length int64) (err error) {
+	var _p0 *byte
+	_p0, err = BytePtrFromString(path)
+	if err != nil {
+		return
+	}
+	_, _, e1 := Syscall6(SYS_TRUNCATE64, uintptr(unsafe.Pointer(_p0)), 0, uintptr(length), uintptr(length>>32), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func Ftruncate(fd int, length int64) (err error) {
+	_, _, e1 := Syscall6(SYS_FTRUNCATE64, uintptr(fd), 0, uintptr(length), uintptr(length>>32), 0, 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func mmap2(addr uintptr, length uintptr, prot int, flags int, fd int, pageOffset uintptr) (xaddr uintptr, err error) {
+	r0, _, e1 := Syscall6(SYS_MMAP2, uintptr(addr), uintptr(length), uintptr(prot), uintptr(flags), uintptr(fd), uintptr(pageOffset))
+	xaddr = uintptr(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func EpollWait(epfd int, events []EpollEvent, msec int) (n int, err error) {
+	var _p0 unsafe.Pointer
+	if len(events) > 0 {
+		_p0 = unsafe.Pointer(&events[0])
+	} else {
+		_p0 = unsafe.Pointer(&_zero)
+	}
+	r0, _, e1 := Syscall6(SYS_EPOLL_WAIT, uintptr(epfd), uintptr(_p0), uintptr(len(events)), uintptr(msec), 0, 0)
+	n = int(r0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func getrlimit(resource int, rlim *rlimit32) (err error) {
+	_, _, e1 := RawSyscall(SYS_GETRLIMIT, uintptr(resource), uintptr(unsafe.Pointer(rlim)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
+
+// THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT
+
+func setrlimit(resource int, rlim *rlimit32) (err error) {
+	_, _, e1 := RawSyscall(SYS_SETRLIMIT, uintptr(resource), uintptr(unsafe.Pointer(rlim)), 0)
+	if e1 != 0 {
+		err = errnoErr(e1)
+	}
+	return
+}
diff --git a/src/syscall/zsysnum_linux_thumb.go b/src/syscall/zsysnum_linux_thumb.go
new file mode 100644
index 0000000000..7335db6847
--- /dev/null
+++ b/src/syscall/zsysnum_linux_thumb.go
@@ -0,0 +1,356 @@
+// mksysnum_linux.pl
+// Code generated by the command above; DO NOT EDIT.
+
+// +build thumb,linux
+
+package syscall
+
+const (
+	SYS_OABI_SYSCALL_BASE      = 0
+	SYS_SYSCALL_BASE           = 0
+	SYS_RESTART_SYSCALL        = 0
+	SYS_EXIT                   = 1
+	SYS_FORK                   = 2
+	SYS_READ                   = 3
+	SYS_WRITE                  = 4
+	SYS_OPEN                   = 5
+	SYS_CLOSE                  = 6
+	SYS_CREAT                  = 8
+	SYS_LINK                   = 9
+	SYS_UNLINK                 = 10
+	SYS_EXECVE                 = 11
+	SYS_CHDIR                  = 12
+	SYS_TIME                   = 13
+	SYS_MKNOD                  = 14
+	SYS_CHMOD                  = 15
+	SYS_LCHOWN                 = 16
+	SYS_LSEEK                  = 19
+	SYS_GETPID                 = 20
+	SYS_MOUNT                  = 21
+	SYS_UMOUNT                 = 22
+	SYS_SETUID                 = 23
+	SYS_GETUID                 = 24
+	SYS_STIME                  = 25
+	SYS_PTRACE                 = 26
+	SYS_ALARM                  = 27
+	SYS_PAUSE                  = 29
+	SYS_UTIME                  = 30
+	SYS_ACCESS                 = 33
+	SYS_NICE                   = 34
+	SYS_SYNC                   = 36
+	SYS_KILL                   = 37
+	SYS_RENAME                 = 38
+	SYS_MKDIR                  = 39
+	SYS_RMDIR                  = 40
+	SYS_DUP                    = 41
+	SYS_PIPE                   = 42
+	SYS_TIMES                  = 43
+	SYS_BRK                    = 45
+	SYS_SETGID                 = 46
+	SYS_GETGID                 = 47
+	SYS_GETEUID                = 49
+	SYS_GETEGID                = 50
+	SYS_ACCT                   = 51
+	SYS_UMOUNT2                = 52
+	SYS_IOCTL                  = 54
+	SYS_FCNTL                  = 55
+	SYS_SETPGID                = 57
+	SYS_UMASK                  = 60
+	SYS_CHROOT                 = 61
+	SYS_USTAT                  = 62
+	SYS_DUP2                   = 63
+	SYS_GETPPID                = 64
+	SYS_GETPGRP                = 65
+	SYS_SETSID                 = 66
+	SYS_SIGACTION              = 67
+	SYS_SETREUID               = 70
+	SYS_SETREGID               = 71
+	SYS_SIGSUSPEND             = 72
+	SYS_SIGPENDING             = 73
+	SYS_SETHOSTNAME            = 74
+	SYS_SETRLIMIT              = 75
+	SYS_GETRLIMIT              = 76
+	SYS_GETRUSAGE              = 77
+	SYS_GETTIMEOFDAY           = 78
+	SYS_SETTIMEOFDAY           = 79
+	SYS_GETGROUPS              = 80
+	SYS_SETGROUPS              = 81
+	SYS_SELECT                 = 82
+	SYS_SYMLINK                = 83
+	SYS_READLINK               = 85
+	SYS_USELIB                 = 86
+	SYS_SWAPON                 = 87
+	SYS_REBOOT                 = 88
+	SYS_READDIR                = 89
+	SYS_MMAP                   = 90
+	SYS_MUNMAP                 = 91
+	SYS_TRUNCATE               = 92
+	SYS_FTRUNCATE              = 93
+	SYS_FCHMOD                 = 94
+	SYS_FCHOWN                 = 95
+	SYS_GETPRIORITY            = 96
+	SYS_SETPRIORITY            = 97
+	SYS_STATFS                 = 99
+	SYS_FSTATFS                = 100
+	SYS_SOCKETCALL             = 102
+	SYS_SYSLOG                 = 103
+	SYS_SETITIMER              = 104
+	SYS_GETITIMER              = 105
+	SYS_STAT                   = 106
+	SYS_LSTAT                  = 107
+	SYS_FSTAT                  = 108
+	SYS_VHANGUP                = 111
+	SYS_SYSCALL                = 113
+	SYS_WAIT4                  = 114
+	SYS_SWAPOFF                = 115
+	SYS_SYSINFO                = 116
+	SYS_IPC                    = 117
+	SYS_FSYNC                  = 118
+	SYS_SIGRETURN              = 119
+	SYS_CLONE                  = 120
+	SYS_SETDOMAINNAME          = 121
+	SYS_UNAME                  = 122
+	SYS_ADJTIMEX               = 124
+	SYS_MPROTECT               = 125
+	SYS_SIGPROCMASK            = 126
+	SYS_INIT_MODULE            = 128
+	SYS_DELETE_MODULE          = 129
+	SYS_QUOTACTL               = 131
+	SYS_GETPGID                = 132
+	SYS_FCHDIR                 = 133
+	SYS_BDFLUSH                = 134
+	SYS_SYSFS                  = 135
+	SYS_PERSONALITY            = 136
+	SYS_SETFSUID               = 138
+	SYS_SETFSGID               = 139
+	SYS__LLSEEK                = 140
+	SYS_GETDENTS               = 141
+	SYS__NEWSELECT             = 142
+	SYS_FLOCK                  = 143
+	SYS_MSYNC                  = 144
+	SYS_READV                  = 145
+	SYS_WRITEV                 = 146
+	SYS_GETSID                 = 147
+	SYS_FDATASYNC              = 148
+	SYS__SYSCTL                = 149
+	SYS_MLOCK                  = 150
+	SYS_MUNLOCK                = 151
+	SYS_MLOCKALL               = 152
+	SYS_MUNLOCKALL             = 153
+	SYS_SCHED_SETPARAM         = 154
+	SYS_SCHED_GETPARAM         = 155
+	SYS_SCHED_SETSCHEDULER     = 156
+	SYS_SCHED_GETSCHEDULER     = 157
+	SYS_SCHED_YIELD            = 158
+	SYS_SCHED_GET_PRIORITY_MAX = 159
+	SYS_SCHED_GET_PRIORITY_MIN = 160
+	SYS_SCHED_RR_GET_INTERVAL  = 161
+	SYS_NANOSLEEP              = 162
+	SYS_MREMAP                 = 163
+	SYS_SETRESUID              = 164
+	SYS_GETRESUID              = 165
+	SYS_POLL                   = 168
+	SYS_NFSSERVCTL             = 169
+	SYS_SETRESGID              = 170
+	SYS_GETRESGID              = 171
+	SYS_PRCTL                  = 172
+	SYS_RT_SIGRETURN           = 173
+	SYS_RT_SIGACTION           = 174
+	SYS_RT_SIGPROCMASK         = 175
+	SYS_RT_SIGPENDING          = 176
+	SYS_RT_SIGTIMEDWAIT        = 177
+	SYS_RT_SIGQUEUEINFO        = 178
+	SYS_RT_SIGSUSPEND          = 179
+	SYS_PREAD64                = 180
+	SYS_PWRITE64               = 181
+	SYS_CHOWN                  = 182
+	SYS_GETCWD                 = 183
+	SYS_CAPGET                 = 184
+	SYS_CAPSET                 = 185
+	SYS_SIGALTSTACK            = 186
+	SYS_SENDFILE               = 187
+	SYS_VFORK                  = 190
+	SYS_UGETRLIMIT             = 191
+	SYS_MMAP2                  = 192
+	SYS_TRUNCATE64             = 193
+	SYS_FTRUNCATE64            = 194
+	SYS_STAT64                 = 195
+	SYS_LSTAT64                = 196
+	SYS_FSTAT64                = 197
+	SYS_LCHOWN32               = 198
+	SYS_GETUID32               = 199
+	SYS_GETGID32               = 200
+	SYS_GETEUID32              = 201
+	SYS_GETEGID32              = 202
+	SYS_SETREUID32             = 203
+	SYS_SETREGID32             = 204
+	SYS_GETGROUPS32            = 205
+	SYS_SETGROUPS32            = 206
+	SYS_FCHOWN32               = 207
+	SYS_SETRESUID32            = 208
+	SYS_GETRESUID32            = 209
+	SYS_SETRESGID32            = 210
+	SYS_GETRESGID32            = 211
+	SYS_CHOWN32                = 212
+	SYS_SETUID32               = 213
+	SYS_SETGID32               = 214
+	SYS_SETFSUID32             = 215
+	SYS_SETFSGID32             = 216
+	SYS_GETDENTS64             = 217
+	SYS_PIVOT_ROOT             = 218
+	SYS_MINCORE                = 219
+	SYS_MADVISE                = 220
+	SYS_FCNTL64                = 221
+	SYS_GETTID                 = 224
+	SYS_READAHEAD              = 225
+	SYS_SETXATTR               = 226
+	SYS_LSETXATTR              = 227
+	SYS_FSETXATTR              = 228
+	SYS_GETXATTR               = 229
+	SYS_LGETXATTR              = 230
+	SYS_FGETXATTR              = 231
+	SYS_LISTXATTR              = 232
+	SYS_LLISTXATTR             = 233
+	SYS_FLISTXATTR             = 234
+	SYS_REMOVEXATTR            = 235
+	SYS_LREMOVEXATTR           = 236
+	SYS_FREMOVEXATTR           = 237
+	SYS_TKILL                  = 238
+	SYS_SENDFILE64             = 239
+	SYS_FUTEX                  = 240
+	SYS_SCHED_SETAFFINITY      = 241
+	SYS_SCHED_GETAFFINITY      = 242
+	SYS_IO_SETUP               = 243
+	SYS_IO_DESTROY             = 244
+	SYS_IO_GETEVENTS           = 245
+	SYS_IO_SUBMIT              = 246
+	SYS_IO_CANCEL              = 247
+	SYS_EXIT_GROUP             = 248
+	SYS_LOOKUP_DCOOKIE         = 249
+	SYS_EPOLL_CREATE           = 250
+	SYS_EPOLL_CTL              = 251
+	SYS_EPOLL_WAIT             = 252
+	SYS_REMAP_FILE_PAGES       = 253
+	SYS_SET_TID_ADDRESS        = 256
+	SYS_TIMER_CREATE           = 257
+	SYS_TIMER_SETTIME          = 258
+	SYS_TIMER_GETTIME          = 259
+	SYS_TIMER_GETOVERRUN       = 260
+	SYS_TIMER_DELETE           = 261
+	SYS_CLOCK_SETTIME          = 262
+	SYS_CLOCK_GETTIME          = 263
+	SYS_CLOCK_GETRES           = 264
+	SYS_CLOCK_NANOSLEEP        = 265
+	SYS_STATFS64               = 266
+	SYS_FSTATFS64              = 267
+	SYS_TGKILL                 = 268
+	SYS_UTIMES                 = 269
+	SYS_ARM_FADVISE64_64       = 270
+	SYS_PCICONFIG_IOBASE       = 271
+	SYS_PCICONFIG_READ         = 272
+	SYS_PCICONFIG_WRITE        = 273
+	SYS_MQ_OPEN                = 274
+	SYS_MQ_UNLINK              = 275
+	SYS_MQ_TIMEDSEND           = 276
+	SYS_MQ_TIMEDRECEIVE        = 277
+	SYS_MQ_NOTIFY              = 278
+	SYS_MQ_GETSETATTR          = 279
+	SYS_WAITID                 = 280
+	SYS_SOCKET                 = 281
+	SYS_BIND                   = 282
+	SYS_CONNECT                = 283
+	SYS_LISTEN                 = 284
+	SYS_ACCEPT                 = 285
+	SYS_GETSOCKNAME            = 286
+	SYS_GETPEERNAME            = 287
+	SYS_SOCKETPAIR             = 288
+	SYS_SEND                   = 289
+	SYS_SENDTO                 = 290
+	SYS_RECV                   = 291
+	SYS_RECVFROM               = 292
+	SYS_SHUTDOWN               = 293
+	SYS_SETSOCKOPT             = 294
+	SYS_GETSOCKOPT             = 295
+	SYS_SENDMSG                = 296
+	SYS_RECVMSG                = 297
+	SYS_SEMOP                  = 298
+	SYS_SEMGET                 = 299
+	SYS_SEMCTL                 = 300
+	SYS_MSGSND                 = 301
+	SYS_MSGRCV                 = 302
+	SYS_MSGGET                 = 303
+	SYS_MSGCTL                 = 304
+	SYS_SHMAT                  = 305
+	SYS_SHMDT                  = 306
+	SYS_SHMGET                 = 307
+	SYS_SHMCTL                 = 308
+	SYS_ADD_KEY                = 309
+	SYS_REQUEST_KEY            = 310
+	SYS_KEYCTL                 = 311
+	SYS_SEMTIMEDOP             = 312
+	SYS_VSERVER                = 313
+	SYS_IOPRIO_SET             = 314
+	SYS_IOPRIO_GET             = 315
+	SYS_INOTIFY_INIT           = 316
+	SYS_INOTIFY_ADD_WATCH      = 317
+	SYS_INOTIFY_RM_WATCH       = 318
+	SYS_MBIND                  = 319
+	SYS_GET_MEMPOLICY          = 320
+	SYS_SET_MEMPOLICY          = 321
+	SYS_OPENAT                 = 322
+	SYS_MKDIRAT                = 323
+	SYS_MKNODAT                = 324
+	SYS_FCHOWNAT               = 325
+	SYS_FUTIMESAT              = 326
+	SYS_FSTATAT64              = 327
+	SYS_UNLINKAT               = 328
+	SYS_RENAMEAT               = 329
+	SYS_LINKAT                 = 330
+	SYS_SYMLINKAT              = 331
+	SYS_READLINKAT             = 332
+	SYS_FCHMODAT               = 333
+	SYS_FACCESSAT              = 334
+	SYS_PSELECT6               = 335
+	SYS_PPOLL                  = 336
+	SYS_UNSHARE                = 337
+	SYS_SET_ROBUST_LIST        = 338
+	SYS_GET_ROBUST_LIST        = 339
+	SYS_SPLICE                 = 340
+	SYS_ARM_SYNC_FILE_RANGE    = 341
+	SYS_TEE                    = 342
+	SYS_VMSPLICE               = 343
+	SYS_MOVE_PAGES             = 344
+	SYS_GETCPU                 = 345
+	SYS_EPOLL_PWAIT            = 346
+	SYS_KEXEC_LOAD             = 347
+	SYS_UTIMENSAT              = 348
+	SYS_SIGNALFD               = 349
+	SYS_TIMERFD_CREATE         = 350
+	SYS_EVENTFD                = 351
+	SYS_FALLOCATE              = 352
+	SYS_TIMERFD_SETTIME        = 353
+	SYS_TIMERFD_GETTIME        = 354
+	SYS_SIGNALFD4              = 355
+	SYS_EVENTFD2               = 356
+	SYS_EPOLL_CREATE1          = 357
+	SYS_DUP3                   = 358
+	SYS_PIPE2                  = 359
+	SYS_INOTIFY_INIT1          = 360
+	SYS_PREADV                 = 361
+	SYS_PWRITEV                = 362
+	SYS_RT_TGSIGQUEUEINFO      = 363
+	SYS_PERF_EVENT_OPEN        = 364
+	SYS_RECVMMSG               = 365
+	SYS_ACCEPT4                = 366
+	SYS_FANOTIFY_INIT          = 367
+	SYS_FANOTIFY_MARK          = 368
+	SYS_PRLIMIT64              = 369
+	SYS_NAME_TO_HANDLE_AT      = 370
+	SYS_OPEN_BY_HANDLE_AT      = 371
+	SYS_CLOCK_ADJTIME          = 372
+	SYS_SYNCFS                 = 373
+	SYS_SENDMMSG               = 374
+	SYS_SETNS                  = 375
+	SYS_PROCESS_VM_READV       = 376
+	SYS_PROCESS_VM_WRITEV      = 377
+)
diff --git a/src/syscall/ztypes_linux_thumb.go b/src/syscall/ztypes_linux_thumb.go
new file mode 100644
index 0000000000..c2ccf694d8
--- /dev/null
+++ b/src/syscall/ztypes_linux_thumb.go
@@ -0,0 +1,683 @@
+// Created by cgo -godefs - DO NOT EDIT
+// cgo -godefs types_linux.go
+
+// +build thumb,linux
+
+package syscall
+
+const (
+	sizeofPtr      = 0x4
+	sizeofShort    = 0x2
+	sizeofInt      = 0x4
+	sizeofLong     = 0x4
+	sizeofLongLong = 0x8
+	PathMax        = 0x1000
+)
+
+type (
+	_C_short     int16
+	_C_int       int32
+	_C_long      int32
+	_C_long_long int64
+)
+
+type Timespec struct {
+	Sec  int32
+	Nsec int32
+}
+
+type Timeval struct {
+	Sec  int32
+	Usec int32
+}
+
+type Timex struct {
+	Modes     uint32
+	Offset    int32
+	Freq      int32
+	Maxerror  int32
+	Esterror  int32
+	Status    int32
+	Constant  int32
+	Precision int32
+	Tolerance int32
+	Time      Timeval
+	Tick      int32
+	Ppsfreq   int32
+	Jitter    int32
+	Shift     int32
+	Stabil    int32
+	Jitcnt    int32
+	Calcnt    int32
+	Errcnt    int32
+	Stbcnt    int32
+	Tai       int32
+	Pad_cgo_0 [44]byte
+}
+
+type Time_t int32
+
+type Tms struct {
+	Utime  int32
+	Stime  int32
+	Cutime int32
+	Cstime int32
+}
+
+type Utimbuf struct {
+	Actime  int32
+	Modtime int32
+}
+
+type Rusage struct {
+	Utime    Timeval
+	Stime    Timeval
+	Maxrss   int32
+	Ixrss    int32
+	Idrss    int32
+	Isrss    int32
+	Minflt   int32
+	Majflt   int32
+	Nswap    int32
+	Inblock  int32
+	Oublock  int32
+	Msgsnd   int32
+	Msgrcv   int32
+	Nsignals int32
+	Nvcsw    int32
+	Nivcsw   int32
+}
+
+type Rlimit struct {
+	Cur uint64
+	Max uint64
+}
+
+type _Gid_t uint32
+
+type Stat_t struct {
+	Dev       uint64
+	X__pad1   uint16
+	Pad_cgo_0 [2]byte
+	X__st_ino uint32
+	Mode      uint32
+	Nlink     uint32
+	Uid       uint32
+	Gid       uint32
+	Rdev      uint64
+	X__pad2   uint16
+	Pad_cgo_1 [6]byte
+	Size      int64
+	Blksize   int32
+	Pad_cgo_2 [4]byte
+	Blocks    int64
+	Atim      Timespec
+	Mtim      Timespec
+	Ctim      Timespec
+	Ino       uint64
+}
+
+type Statfs_t struct {
+	Type      int32
+	Bsize     int32
+	Blocks    uint64
+	Bfree     uint64
+	Bavail    uint64
+	Files     uint64
+	Ffree     uint64
+	Fsid      Fsid
+	Namelen   int32
+	Frsize    int32
+	Flags     int32
+	Spare     [4]int32
+	Pad_cgo_0 [4]byte
+}
+
+type Dirent struct {
+	Ino       uint64
+	Off       int64
+	Reclen    uint16
+	Type      uint8
+	Name      [256]uint8
+	Pad_cgo_0 [5]byte
+}
+
+type Fsid struct {
+	X__val [2]int32
+}
+
+type Flock_t struct {
+	Type      int16
+	Whence    int16
+	Pad_cgo_0 [4]byte
+	Start     int64
+	Len       int64
+	Pid       int32
+	Pad_cgo_1 [4]byte
+}
+
+type RawSockaddrInet4 struct {
+	Family uint16
+	Port   uint16
+	Addr   [4]byte /* in_addr */
+	Zero   [8]uint8
+}
+
+type RawSockaddrInet6 struct {
+	Family   uint16
+	Port     uint16
+	Flowinfo uint32
+	Addr     [16]byte /* in6_addr */
+	Scope_id uint32
+}
+
+type RawSockaddrUnix struct {
+	Family uint16
+	Path   [108]int8
+}
+
+type RawSockaddrLinklayer struct {
+	Family   uint16
+	Protocol uint16
+	Ifindex  int32
+	Hatype   uint16
+	Pkttype  uint8
+	Halen    uint8
+	Addr     [8]uint8
+}
+
+type RawSockaddrNetlink struct {
+	Family uint16
+	Pad    uint16
+	Pid    uint32
+	Groups uint32
+}
+
+type RawSockaddr struct {
+	Family uint16
+	Data   [14]uint8
+}
+
+type RawSockaddrAny struct {
+	Addr RawSockaddr
+	Pad  [96]uint8
+}
+
+type _Socklen uint32
+
+type Linger struct {
+	Onoff  int32
+	Linger int32
+}
+
+type Iovec struct {
+	Base *byte
+	Len  uint32
+}
+
+type IPMreq struct {
+	Multiaddr [4]byte /* in_addr */
+	Interface [4]byte /* in_addr */
+}
+
+type IPMreqn struct {
+	Multiaddr [4]byte /* in_addr */
+	Address   [4]byte /* in_addr */
+	Ifindex   int32
+}
+
+type IPv6Mreq struct {
+	Multiaddr [16]byte /* in6_addr */
+	Interface uint32
+}
+
+type Msghdr struct {
+	Name       *byte
+	Namelen    uint32
+	Iov        *Iovec
+	Iovlen     uint32
+	Control    *byte
+	Controllen uint32
+	Flags      int32
+}
+
+type Cmsghdr struct {
+	Len   uint32
+	Level int32
+	Type  int32
+}
+
+type Inet4Pktinfo struct {
+	Ifindex  int32
+	Spec_dst [4]byte /* in_addr */
+	Addr     [4]byte /* in_addr */
+}
+
+type Inet6Pktinfo struct {
+	Addr    [16]byte /* in6_addr */
+	Ifindex uint32
+}
+
+type IPv6MTUInfo struct {
+	Addr RawSockaddrInet6
+	Mtu  uint32
+}
+
+type ICMPv6Filter struct {
+	Data [8]uint32
+}
+
+type Ucred struct {
+	Pid int32
+	Uid uint32
+	Gid uint32
+}
+
+type TCPInfo struct {
+	State          uint8
+	Ca_state       uint8
+	Retransmits    uint8
+	Probes         uint8
+	Backoff        uint8
+	Options        uint8
+	Pad_cgo_0      [2]byte
+	Rto            uint32
+	Ato            uint32
+	Snd_mss        uint32
+	Rcv_mss        uint32
+	Unacked        uint32
+	Sacked         uint32
+	Lost           uint32
+	Retrans        uint32
+	Fackets        uint32
+	Last_data_sent uint32
+	Last_ack_sent  uint32
+	Last_data_recv uint32
+	Last_ack_recv  uint32
+	Pmtu           uint32
+	Rcv_ssthresh   uint32
+	Rtt            uint32
+	Rttvar         uint32
+	Snd_ssthresh   uint32
+	Snd_cwnd       uint32
+	Advmss         uint32
+	Reordering     uint32
+	Rcv_rtt        uint32
+	Rcv_space      uint32
+	Total_retrans  uint32
+}
+
+const (
+	SizeofSockaddrInet4     = 0x10
+	SizeofSockaddrInet6     = 0x1c
+	SizeofSockaddrAny       = 0x70
+	SizeofSockaddrUnix      = 0x6e
+	SizeofSockaddrLinklayer = 0x14
+	SizeofSockaddrNetlink   = 0xc
+	SizeofLinger            = 0x8
+	SizeofIPMreq            = 0x8
+	SizeofIPMreqn           = 0xc
+	SizeofIPv6Mreq          = 0x14
+	SizeofMsghdr            = 0x1c
+	SizeofCmsghdr           = 0xc
+	SizeofInet4Pktinfo      = 0xc
+	SizeofInet6Pktinfo      = 0x14
+	SizeofIPv6MTUInfo       = 0x20
+	SizeofICMPv6Filter      = 0x20
+	SizeofUcred             = 0xc
+	SizeofTCPInfo           = 0x68
+)
+
+const (
+	IFA_UNSPEC          = 0x0
+	IFA_ADDRESS         = 0x1
+	IFA_LOCAL           = 0x2
+	IFA_LABEL           = 0x3
+	IFA_BROADCAST       = 0x4
+	IFA_ANYCAST         = 0x5
+	IFA_CACHEINFO       = 0x6
+	IFA_MULTICAST       = 0x7
+	IFLA_UNSPEC         = 0x0
+	IFLA_ADDRESS        = 0x1
+	IFLA_BROADCAST      = 0x2
+	IFLA_IFNAME         = 0x3
+	IFLA_MTU            = 0x4
+	IFLA_LINK           = 0x5
+	IFLA_QDISC          = 0x6
+	IFLA_STATS          = 0x7
+	IFLA_COST           = 0x8
+	IFLA_PRIORITY       = 0x9
+	IFLA_MASTER         = 0xa
+	IFLA_WIRELESS       = 0xb
+	IFLA_PROTINFO       = 0xc
+	IFLA_TXQLEN         = 0xd
+	IFLA_MAP            = 0xe
+	IFLA_WEIGHT         = 0xf
+	IFLA_OPERSTATE      = 0x10
+	IFLA_LINKMODE       = 0x11
+	IFLA_LINKINFO       = 0x12
+	IFLA_NET_NS_PID     = 0x13
+	IFLA_IFALIAS        = 0x14
+	IFLA_MAX            = 0x1d
+	RT_SCOPE_UNIVERSE   = 0x0
+	RT_SCOPE_SITE       = 0xc8
+	RT_SCOPE_LINK       = 0xfd
+	RT_SCOPE_HOST       = 0xfe
+	RT_SCOPE_NOWHERE    = 0xff
+	RT_TABLE_UNSPEC     = 0x0
+	RT_TABLE_COMPAT     = 0xfc
+	RT_TABLE_DEFAULT    = 0xfd
+	RT_TABLE_MAIN       = 0xfe
+	RT_TABLE_LOCAL      = 0xff
+	RT_TABLE_MAX        = 0xffffffff
+	RTA_UNSPEC          = 0x0
+	RTA_DST             = 0x1
+	RTA_SRC             = 0x2
+	RTA_IIF             = 0x3
+	RTA_OIF             = 0x4
+	RTA_GATEWAY         = 0x5
+	RTA_PRIORITY        = 0x6
+	RTA_PREFSRC         = 0x7
+	RTA_METRICS         = 0x8
+	RTA_MULTIPATH       = 0x9
+	RTA_FLOW            = 0xb
+	RTA_CACHEINFO       = 0xc
+	RTA_TABLE           = 0xf
+	RTN_UNSPEC          = 0x0
+	RTN_UNICAST         = 0x1
+	RTN_LOCAL           = 0x2
+	RTN_BROADCAST       = 0x3
+	RTN_ANYCAST         = 0x4
+	RTN_MULTICAST       = 0x5
+	RTN_BLACKHOLE       = 0x6
+	RTN_UNREACHABLE     = 0x7
+	RTN_PROHIBIT        = 0x8
+	RTN_THROW           = 0x9
+	RTN_NAT             = 0xa
+	RTN_XRESOLVE        = 0xb
+	RTNLGRP_NONE        = 0x0
+	RTNLGRP_LINK        = 0x1
+	RTNLGRP_NOTIFY      = 0x2
+	RTNLGRP_NEIGH       = 0x3
+	RTNLGRP_TC          = 0x4
+	RTNLGRP_IPV4_IFADDR = 0x5
+	RTNLGRP_IPV4_MROUTE = 0x6
+	RTNLGRP_IPV4_ROUTE  = 0x7
+	RTNLGRP_IPV4_RULE   = 0x8
+	RTNLGRP_IPV6_IFADDR = 0x9
+	RTNLGRP_IPV6_MROUTE = 0xa
+	RTNLGRP_IPV6_ROUTE  = 0xb
+	RTNLGRP_IPV6_IFINFO = 0xc
+	RTNLGRP_IPV6_PREFIX = 0x12
+	RTNLGRP_IPV6_RULE   = 0x13
+	RTNLGRP_ND_USEROPT  = 0x14
+	SizeofNlMsghdr      = 0x10
+	SizeofNlMsgerr      = 0x14
+	SizeofRtGenmsg      = 0x1
+	SizeofNlAttr        = 0x4
+	SizeofRtAttr        = 0x4
+	SizeofIfInfomsg     = 0x10
+	SizeofIfAddrmsg     = 0x8
+	SizeofRtMsg         = 0xc
+	SizeofRtNexthop     = 0x8
+)
+
+type NlMsghdr struct {
+	Len   uint32
+	Type  uint16
+	Flags uint16
+	Seq   uint32
+	Pid   uint32
+}
+
+type NlMsgerr struct {
+	Error int32
+	Msg   NlMsghdr
+}
+
+type RtGenmsg struct {
+	Family uint8
+}
+
+type NlAttr struct {
+	Len  uint16
+	Type uint16
+}
+
+type RtAttr struct {
+	Len  uint16
+	Type uint16
+}
+
+type IfInfomsg struct {
+	Family     uint8
+	X__ifi_pad uint8
+	Type       uint16
+	Index      int32
+	Flags      uint32
+	Change     uint32
+}
+
+type IfAddrmsg struct {
+	Family    uint8
+	Prefixlen uint8
+	Flags     uint8
+	Scope     uint8
+	Index     uint32
+}
+
+type RtMsg struct {
+	Family   uint8
+	Dst_len  uint8
+	Src_len  uint8
+	Tos      uint8
+	Table    uint8
+	Protocol uint8
+	Scope    uint8
+	Type     uint8
+	Flags    uint32
+}
+
+type RtNexthop struct {
+	Len     uint16
+	Flags   uint8
+	Hops    uint8
+	Ifindex int32
+}
+
+const (
+	SizeofSockFilter = 0x8
+	SizeofSockFprog  = 0x8
+)
+
+type SockFilter struct {
+	Code uint16
+	Jt   uint8
+	Jf   uint8
+	K    uint32
+}
+
+type SockFprog struct {
+	Len       uint16
+	Pad_cgo_0 [2]byte
+	Filter    *SockFilter
+}
+
+type InotifyEvent struct {
+	Wd     int32
+	Mask   uint32
+	Cookie uint32
+	Len    uint32
+	Name   [0]uint8
+}
+
+const SizeofInotifyEvent = 0x10
+
+type PtraceRegs struct {
+	Uregs [18]uint32
+}
+
+type FdSet struct {
+	Bits [32]int32
+}
+
+type Sysinfo_t struct {
+	Uptime    int32
+	Loads     [3]uint32
+	Totalram  uint32
+	Freeram   uint32
+	Sharedram uint32
+	Bufferram uint32
+	Totalswap uint32
+	Freeswap  uint32
+	Procs     uint16
+	Pad       uint16
+	Totalhigh uint32
+	Freehigh  uint32
+	Unit      uint32
+	X_f       [8]uint8
+}
+
+type Utsname struct {
+	Sysname    [65]uint8
+	Nodename   [65]uint8
+	Release    [65]uint8
+	Version    [65]uint8
+	Machine    [65]uint8
+	Domainname [65]uint8
+}
+
+type Ustat_t struct {
+	Tfree  int32
+	Tinode uint32
+	Fname  [6]uint8
+	Fpack  [6]uint8
+}
+
+type EpollEvent struct {
+	Events uint32
+	PadFd  int32
+	Fd     int32
+	Pad    int32
+}
+
+const (
+	_AT_FDCWD            = -0x64
+	_AT_REMOVEDIR        = 0x200
+	_AT_SYMLINK_NOFOLLOW = 0x100
+	_AT_EACCESS          = 0x200
+)
+
+type Termios struct {
+	Iflag     uint32
+	Oflag     uint32
+	Cflag     uint32
+	Lflag     uint32
+	Line      uint8
+	Cc        [32]uint8
+	Pad_cgo_0 [3]byte
+	Ispeed    uint32
+	Ospeed    uint32
+}
+
+const (
+	VINTR    = 0x0
+	VQUIT    = 0x1
+	VERASE   = 0x2
+	VKILL    = 0x3
+	VEOF     = 0x4
+	VTIME    = 0x5
+	VMIN     = 0x6
+	VSWTC    = 0x7
+	VSTART   = 0x8
+	VSTOP    = 0x9
+	VSUSP    = 0xa
+	VEOL     = 0xb
+	VREPRINT = 0xc
+	VDISCARD = 0xd
+	VWERASE  = 0xe
+	VLNEXT   = 0xf
+	VEOL2    = 0x10
+	IGNBRK   = 0x1
+	BRKINT   = 0x2
+	IGNPAR   = 0x4
+	PARMRK   = 0x8
+	INPCK    = 0x10
+	ISTRIP   = 0x20
+	INLCR    = 0x40
+	IGNCR    = 0x80
+	ICRNL    = 0x100
+	IUCLC    = 0x200
+	IXON     = 0x400
+	IXANY    = 0x800
+	IXOFF    = 0x1000
+	IMAXBEL  = 0x2000
+	IUTF8    = 0x4000
+	OPOST    = 0x1
+	OLCUC    = 0x2
+	ONLCR    = 0x4
+	OCRNL    = 0x8
+	ONOCR    = 0x10
+	ONLRET   = 0x20
+	OFILL    = 0x40
+	OFDEL    = 0x80
+	B0       = 0x0
+	B50      = 0x1
+	B75      = 0x2
+	B110     = 0x3
+	B134     = 0x4
+	B150     = 0x5
+	B200     = 0x6
+	B300     = 0x7
+	B600     = 0x8
+	B1200    = 0x9
+	B1800    = 0xa
+	B2400    = 0xb
+	B4800    = 0xc
+	B9600    = 0xd
+	B19200   = 0xe
+	B38400   = 0xf
+	CSIZE    = 0x30
+	CS5      = 0x0
+	CS6      = 0x10
+	CS7      = 0x20
+	CS8      = 0x30
+	CSTOPB   = 0x40
+	CREAD    = 0x80
+	PARENB   = 0x100
+	PARODD   = 0x200
+	HUPCL    = 0x400
+	CLOCAL   = 0x800
+	B57600   = 0x1001
+	B115200  = 0x1002
+	B230400  = 0x1003
+	B460800  = 0x1004
+	B500000  = 0x1005
+	B576000  = 0x1006
+	B921600  = 0x1007
+	B1000000 = 0x1008
+	B1152000 = 0x1009
+	B1500000 = 0x100a
+	B2000000 = 0x100b
+	B2500000 = 0x100c
+	B3000000 = 0x100d
+	B3500000 = 0x100e
+	B4000000 = 0x100f
+	ISIG     = 0x1
+	ICANON   = 0x2
+	XCASE    = 0x4
+	ECHO     = 0x8
+	ECHOE    = 0x10
+	ECHOK    = 0x20
+	ECHONL   = 0x40
+	NOFLSH   = 0x80
+	TOSTOP   = 0x100
+	ECHOCTL  = 0x200
+	ECHOPRT  = 0x400
+	ECHOKE   = 0x800
+	FLUSHO   = 0x1000
+	PENDIN   = 0x4000
+	IEXTEN   = 0x8000
+	TCGETS   = 0x5401
+	TCSETS   = 0x5402
+)
diff --git a/src/time/sys_noos.go b/src/time/sys_noos.go
new file mode 100644
index 0000000000..b38a3a9c97
--- /dev/null
+++ b/src/time/sys_noos.go
@@ -0,0 +1,22 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package time
+
+import "syscall"
+
+func open(name string) (uintptr, error) {
+	return 0, syscall.ENOENT
+}
+
+func closefd(fd uintptr) {
+}
+
+func read(fd uintptr, buf []byte) (int, error) {
+	return 0, syscall.EINVAL
+}
+
+func preadn(fd uintptr, buf []byte, off int) error {
+	return syscall.EINVAL
+}
diff --git a/src/time/zoneinfo_noos.go b/src/time/zoneinfo_noos.go
new file mode 100644
index 0000000000..a80d5933dd
--- /dev/null
+++ b/src/time/zoneinfo_noos.go
@@ -0,0 +1,11 @@
+// Copyright 2020 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package time
+
+var zoneSources = []string{}
+
+func initLocal() {
+	localLoc = *UTC
+}
diff --git a/src/vendor/golang.org/x/sys/cpu/cpu_arm.go b/src/vendor/golang.org/x/sys/cpu/cpu_armt.go
similarity index 91%
rename from src/vendor/golang.org/x/sys/cpu/cpu_arm.go
rename to src/vendor/golang.org/x/sys/cpu/cpu_armt.go
index 7f2348b7d4..affcc070f2 100644
--- a/src/vendor/golang.org/x/sys/cpu/cpu_arm.go
+++ b/src/vendor/golang.org/x/sys/cpu/cpu_armt.go
@@ -2,6 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
+// +build arm thumb
+
 package cpu
 
 const cacheLineSize = 32
diff --git a/test/fixedbugs/issue10607.go b/test/fixedbugs/issue10607.go
index 8831547da8..1ffeb0a773 100644
--- a/test/fixedbugs/issue10607.go
+++ b/test/fixedbugs/issue10607.go
@@ -1,4 +1,4 @@
-// +build linux,!ppc64
+// +build linux,!ppc64,!thumb
 // run
 
 // Copyright 2015 The Go Authors. All rights reserved.
diff --git a/test/fixedbugs/issue11656.go b/test/fixedbugs/issue11656.go
index 451ae6348f..a6b30d6368 100644
--- a/test/fixedbugs/issue11656.go
+++ b/test/fixedbugs/issue11656.go
@@ -45,19 +45,22 @@ func f(n int) {
 	if n > 0 {
 		f(n - 1)
 	}
-	var f struct {
-		x uintptr
-	}
+	ill := make([]byte, 64)
+	var f struct{ x uintptr }
+	f.x = uintptr(unsafe.Pointer(&ill[0]))
+	ptr := (*uintptr)(unsafe.Pointer(&f))
 
 	// We want to force an illegal instruction, to get a crash
 	// at a PC value != 0.
 	// Not all systems make the data section non-executable.
-	ill := make([]byte, 64)
 	switch runtime.GOARCH {
 	case "386", "amd64":
 		binary.LittleEndian.PutUint16(ill, 0x0b0f) // ud2
 	case "arm":
 		binary.LittleEndian.PutUint32(ill, 0xe7f000f0) // no name, but permanently undefined
+	case "thumb":
+		*ptr++
+		binary.LittleEndian.PutUint16(ill, 0xde00) // UDF #0
 	case "arm64":
 		binary.LittleEndian.PutUint32(ill, 0xd4207d00) // brk #1000
 	case "ppc64":
@@ -74,7 +77,6 @@ func f(n int) {
 		// Just leave it as 0 and hope for the best.
 	}
 
-	f.x = uintptr(unsafe.Pointer(&ill[0]))
-	fn := *(*func())(unsafe.Pointer(&f))
+	fn := *(*func())(unsafe.Pointer(&ptr))
 	fn()
 }
diff --git a/test/fixedbugs/issue22200b.go b/test/fixedbugs/issue22200b.go
index 8d4515eb05..ea1b3a6742 100644
--- a/test/fixedbugs/issue22200b.go
+++ b/test/fixedbugs/issue22200b.go
@@ -4,7 +4,7 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-// +build !386,!amd64p32,!arm,!mips,!mipsle
+// +build !386,!amd64p32,!arm,!thumb,!mips,!mipsle
 
 package p
 
diff --git a/test/fixedbugs/issue6036.go b/test/fixedbugs/issue6036.go
index 8ebef5a447..33e3b8c479 100644
--- a/test/fixedbugs/issue6036.go
+++ b/test/fixedbugs/issue6036.go
@@ -1,4 +1,4 @@
-// +build !386,!arm,!mips,!mipsle,!amd64p32
+// +build !386,!arm,!thumb,!mips,!mipsle,!amd64p32
 // compile
 
 // Copyright 2013 The Go Authors. All rights reserved.
diff --git a/test/nosplit.go b/test/nosplit.go
index 266e6077b1..e9ee02f4fa 100644
--- a/test/nosplit.go
+++ b/test/nosplit.go
@@ -275,7 +275,7 @@ TestCases:
 		case "ppc64", "ppc64le":
 			ptrSize = 8
 			fmt.Fprintf(&buf, "#define REGISTER (CTR)\n")
-		case "arm":
+		case "arm", "thumb":
 			fmt.Fprintf(&buf, "#define REGISTER (R0)\n")
 		case "arm64":
 			ptrSize = 8
diff --git a/test/run.go b/test/run.go
index 28ed865c50..af8705e170 100644
--- a/test/run.go
+++ b/test/run.go
@@ -1444,6 +1444,7 @@ var (
 		"386":     {"GO386", "387", "sse2"},
 		"amd64":   {},
 		"arm":     {"GOARM", "5", "6", "7"},
+		"thumb":   {},
 		"arm64":   {},
 		"mips":    {"GOMIPS", "hardfloat", "softfloat"},
 		"mips64":  {"GOMIPS64", "hardfloat", "softfloat"},
